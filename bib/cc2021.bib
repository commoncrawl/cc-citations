@InProceedings{cc:RobertsonLagusKajava:2021:covid-19-news-coverage-mood-map,
  title        = "A {COVID}-19 news coverage mood map of {E}urope",
  author       = "Robertson, Frankie and Lagus, Jarkko and Kajava, Kaisla",
  booktitle    = "Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation",
  month        = apr,
  year         = "2021",
  address      = "Online",
  publisher    = "Association for Computational Linguistics",
  URL          = "https://www.aclweb.org/anthology/2021.hackashop-1.15",
  pdf          = "https://www.aclweb.org/anthology/2021.hackashop-1.15.pdf",
  pages        = "110--115",
  abstract     = "We present a COVID-19 news dashboard which visualizes sentiment in pandemic news coverage in different
                 languages across Europe. The dashboard shows analyses for positive/neutral/negative sentiment and moral
                 sentiment for news articles across countries and languages. First we extract news articles from
                 news-crawl. Then we use a pre-trained multilingual BERT model for sentiment analysis of news article
                 headlines and a dictionary and word vectors -based method for moral sentiment analysis of news
                 articles. The resulting dashboard gives a unified overview of news events on COVID-19 news overall
                 sentiment, and the region and language of publication from the period starting from the beginning of
                 January 2020 to the end of January 2021.",
  cc-author-affiliation = "University of Jyväskylä, Finland; University of Helsinki, Finland",
  cc-class     = "nlp/corpus-construction, nlp/sentiment-analysis",
  cc-dataset-used = "CC-NEWS",
  cc-abstract  = "We used the news-please extractor (Hamborg et al.,2017) on news-crawl dumps to obtain a multilingual
                 corpus of European COVID-19 news. News-crawl is a web crawl provided by the CommonCrawl organisation
                 which is updated more frequently and contains only data from news websites
                 [²https://commoncrawl.org/2016/10/news-dataset-available/]. In order to keep the size of the corpus
                 man-ageable and the extraction time reasonable, a list ofinternet domain names of European state
                 broadcasters was first obtained from Wikidata, since filteringat the domain level allows for faster
                 processingof Common Crawl dumps. [...] The resulting corpus contains 468 thousand articles.",
}

@Misc{cc:DodgeSapMarasovicAgnewEtAl:2021:documenting-english-colossal-clean-crawled-corpus,
  title        = "Documenting the English Colossal Clean Crawled Corpus",
  author       = "Jesse Dodge and Maarten Sap and Ana Marasovic and William Agnew and Gabriel Ilharco and Dirk
                 Groeneveld and Matt Gardner",
  year         = "2021",
  eprint       = "2104.08758",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/2104.08758",
  pdf          = "https://arxiv.org/pdf/2104.08758.pdf",
  abstract     = "As language models are trained on ever more text, researchers are turning to some of the largest
                 corpora available. Unlike most other types of datasets in NLP, large unlabeled text corpora are often
                 presented with minimal documentation, and best practices for documenting them have not been
                 established. In this work we provide the first documentation for the Colossal Clean Crawled Corpus (C4;
                 Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common
                 Crawl. We begin with a high-level summary of the data, including distributions of where the text came
                 from and when it was written. We then give more detailed analysis on salient parts of this data,
                 including the most frequent sources of text (e.g., this http URL, which contains a significant
                 percentage of machine translated and/or OCR'd text), the effect that the filters had on the data (they
                 disproportionately remove text in AAE), and evidence that some other benchmark NLP dataset examples are
                 contained in the text. We release a web interface to an interactive, indexed copy of this dataset,
                 encouraging the community to continuously explore and report additional findings.",
  cc-author-affiliation = "Paul G. Allen School of Computer Science & Engineering, University of Washington, USA; Allen
                 Institute for Artificial Intelligence, USA",
  cc-class     = "nlp/corpus-construction, nlp/language-model",
  cc-derived-dataset-about = "Tensorflow-C4, Huggingface-Allenai-C4-English",
  cc-dataset-used = "CC-MAIN-2019-18 (WET)",
  cc-abstract  = "C4 is created by taking a snapshot of Common Crawl¹ and applying a number of filters to remove text
                 with the intention of retaining high-quality natural English. We host three different versions of the
                 data: C4.EN.NOCLEAN (C4 with only a language ID filter applied), C4.EN.NOBLOCKLIST (result of all
                 filters except one that discards documents from a list of banned words), and C4.EN (the result of all
                 filters). [...] To facilitate further discussion of the data we host an indexed version of C4 at
                 https://c4-search.apps.allenai.org/, allowing anyone to search it.",
}

@Misc{cc:CaswellKreutzerWangWahabEtAl:2021:audit-web-multilingual-datasets,
  title        = "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets",
  author       = "Isaac Caswell and Julia Kreutzer and Lisa Wang and Ahsan Wahab and Daan van Esch and Nasanbayar
                 Ulzii-Orshikh and Allahsera Tapo and Nishant Subramani and Artem Sokolov and Claytone Sikasote and
                 Monang Setyawan and Supheakmungkol Sarin and Sokhar Samb and Benoît Sagot and Clara Rivera and Annette
                 Rios and Isabel Papadimitriou and Salomey Osei and Pedro Javier Ortiz Suárez and Iroro Orife and
                 Kelechi Ogueji and Rubungo Andre Niyongabo and Toan Q. Nguyen and Mathias Müller and André Müller
                 and Shamsuddeen Hassan Muhammad and Nanda Muhammad and Ayanda Mnyakeni and Jamshidbek Mirzakhalov and
                 Tapiwanashe Matangira and Colin Leong and Nze Lawson and Sneha Kudugunta and Yacine Jernite and Mathias
                 Jenny and Orhan Firat and Bonaventure F. P. Dossou and Sakhile Dlamini and Nisansa de Silva and Sakine
                 Çabuk Ballı and Stella Biderman and Alessia Battisti and Ahmed Baruwa and Ankur Bapna and Pallavi
                 Baljekar and Israel Abebe Azime and Ayodele Awokoya and Duygu Ataman and Orevaoghene Ahia and
                 Oghenefego Ahia and Sweta Agrawal and Mofetoluwa Adeyemi",
  year         = "2021",
  eprint       = "2103.12028",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/2103.12028",
  pdf          = "https://arxiv.org/pdf/2103.12028.pdf",
  abstract     = "With the success of large-scale pre-training and multilingual modeling in Natural Language Processing
                 (NLP), recent years have seen a proliferation of large, web-mined text datasets covering hundreds of
                 languages. However, to date there has been no systematic analysis of the quality of these publicly
                 available datasets, or whether the datasets actually contain content in the languages they claim to
                 represent. In this work, we manually audit the quality of 205 language-specific corpora released with
                 five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4), and audit the correctness of
                 language codes in a sixth (JW300). We find that lower-resource corpora have systematic issues: at least
                 15 corpora are completely erroneous, and a significant fraction contains less than 50\% sentences of
                 acceptable quality. Similarly, we find 82 corpora that are mislabeled or use nonstandard/ambiguous
                 language codes. We demonstrate that these issues are easy to detect even for non-speakers of the
                 languages in question, and supplement the human judgements with automatic analyses. Inspired by our
                 analysis, we recommend techniques to evaluate and improve multilingual corpora and discuss the risks
                 that come with low-quality data releases.",
  cc-author-affiliation = "Google Research; Masakhane NLP; Turkic Interlingua; Haverford College; RobotsMali; Intel
                 Labs; University of Zambia; Google; AIMS-AMMI; Inria; University of Zurich; Stanford University; Kwame
                 Nkrumah University of Science and Technology; Sorbonne Université; Niger-Volta LTI; University of
                 WaterlooqUniversity of Electronic Science and Technology of China; University of Notre Dame; Bayero
                 University Kano; University of South Florida; Hugging Face; Jacobs University Bremen; University of
                 Moratuwa; EleutherAI; Obafemi Awolowo University; University of Ibadan; Instadeep; University of
                 Maryland; Defence Space Administration Abuja",
  cc-class     = "nlp/corpus-construction, nlp/web-as-corpus, nlp/parallel-corpus, nlp/low-resource-language",
  cc-derived-dataset-about = "CCAligned-2020, Tensorflow-C4-Multilingual, OSCAR",
  cc-snippet   = "We selected the corpora for their multilinguality and the inclusion of understudied languages in NLP.
                 With the exception of WikiMatrix and Paracrawl, all corpora are derived from CommonCrawl, and
                 distinguish themselves by the choice of filtering methods, LangID and automatic alignment technology.",
}

@Misc{cc:KalaharshaMehtre:2021:detecting-phishing-sites,
  title        = "Detecting Phishing Sites -- An Overview",
  author       = "P. Kalaharsha and B. M. Mehtre",
  year         = "2021",
  eprint       = "2103.12739",
  archiveprefix = "arXiv",
  primaryclass = "cs.CR",
  URL          = "https://arxiv.org/abs/2103.12739",
  pdf          = "https://arxiv.org/pdf/2103.12739.pdf",
  cc-author-affiliation = "Institute for Development and Research in Banking Technology (IDRBT), Hyderabad, Indiab;
                 School of Computer Science and Information Sciences (SCIS), University of Hyderabad, Hyderabad, India",
  cc-class     = "computer-security/internet-security, computer-security/malicious-domain-detection",
  cc-snippet   = "Alexa and Common crawl contains names of the legitimate sites which are likely to be used for phishing
                 [62][63]. [63:http://index.commoncrawl.org]",
  cc-description = "methods and datasets used in Phishing-detection papers",
}

@Misc{cc:XueConstantRobertsKaleEtAl:2021:mT5,
  title        = "m{T5}: {A} massively multilingual pre-trained text-to-text transformer",
  author       = "Linting Xue and Noah Constant and Adam Roberts and Mihir Kale and Rami Al-Rfou and Aditya Siddhant and
                 Aditya Barua and Colin Raffel",
  year         = "2021",
  eprint       = "2010.11934",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/2010.11934",
  pdf          = "https://arxiv.org/pdf/2010.11934.pdf",
  cc-author-affiliation = "Google Research",
  cc-class     = "nlp/corpus-creation, nlp/web-as-corpus, nlp/language-model",
  cc-derived-dataset-about = "Tensorflow-C4-Multilingual",
  cc-snippet   = "[...] we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based
                 dataset covering 101 languages.",
}

@Misc{cc:ConneauKhandelwalGoyalChaudharyEtAl:2020:unsupervised-cross-lingual-representation-learning,
  title        = "Unsupervised Cross-lingual Representation Learning at Scale",
  author       = "Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and
                 Francisco Guzmán and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov",
  year         = "2020",
  eprint       = "1911.02116",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/1911.02116",
  pdf          = "https://arxiv.org/pdf/1911.02116.pdf",
  cc-author-affiliation = "Facebook AI",
  cc-class     = "nlp/corpus-creation, nlp/web-as-corpus, nlp/language-model",
  cc-snippet   = "We train a Transformer-based masked language model on one hundred languages, using more than two
                 terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, [...] We train on cleaned CommonCrawls
                 (Wenzek et al., 2019 [=CCNet]), which increase the amount of data for low-resource languages by two
                 orders of magnitude on average.",
  cc-derived-dataset-used = "CCNet",
}

@Article{cc:TahirMehmood:2021:corpulyzer,
  title        = "Corpulyzer: {A} Novel Framework for Building Low Resource Language Corpora",
  author       = "Tahir, Bilal and Mehmood, Muhammad Amir",
  journal      = "IEEE Access",
  volume       = "9",
  pages        = "8546--8563",
  year         = "2021",
  publisher    = "IEEE",
  doi          = "10.1109/ACCESS.2021.3049793",
  URL          = "https://ieeexplore.ieee.org/document/9316706",
  pdf          = "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316706",
  cc-author-affiliation = "University of Engineering and Technology, Lahore, Pakistan",
  cc-class     = "nlp/corpus-creation, nlp/web-as-corpus, nlp/low-resource-language",
  cc-snippet   = "Leveraging dataset from Common Crawl Corpus (CCC), first, we prepare a list of seed URLs by filtering
                 the Urdu language webpages. Next, we use Corpulyzer to crawl the World-Wide-Web (WWW) over a period of
                 four years (2016-2020). We build Urdu web corpus “UrduWeb20” that consists of 8.0 million Urdu
                 webpages crawled from 6,590 websites. [...] building a corpus of a low-resource language from CCC is a
                 challenging task due to: i) sampling techniques, ii) filtering of webpages of target languages, and
                 iii) full parsing of CCC. [...] we build upon our previous approach [40] where we developed a dataset
                 consisting of 1.28 million Urdu webpages from CCC 2016 dataset. [...] In general, CCC release meta-data
                 as well as the crawled content where former is lightweight and easier to analyze and latter requires
                 huge bandwidth to download and store the data. As an alternate strategy, we build three datasets using
                 CC released data: i) CC-meta, ii) CC-Urdu-meta, and ii) CC-Urdu-crawl. First, we build CC-meta dataset
                 to explore the impact of URL selection and crawling strategies of Common Crawl in general. This dataset
                 consists of meta-information of 29.1 billion URLs in 11 common crawl releases from September2018 –
                 June2019. This meta-information of each release is available in the form of compressed files (>200GB
                 size) with information of webpage URL, MIME-type, and charset etc [94]. Next, we build CC-Urdu-meta
                 dataset by filtering out Urdu webpages. We note that from August 2018 onward releases [95], CC also
                 provides ISO6 language code of top three languages present in webpages after parsing HTML of the
                 webpage from CLD2.",
}

@Misc{cc:LuccioniViviano:2021:undesirable-content-in-CC-corpus,
  title        = "What's in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus",
  author       = "Alexandra Sasha Luccioni and Joseph D. Viviano",
  year         = "2021",
  eprint       = "2105.02732",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = https://arxiv.org/abs/2105.02732,
  pdf          = https://arxiv.org/pdf/2105.02732.pdf,
  cc-snippet   = "Given its size, both downloading and analyzing the Common Crawl are time-consuming and costly
                 endeavors. The most recent version of the Common Crawl
                 [https://commoncrawl.org/2020/12/nov-dec-2020-crawl-archive-now-available/], dating from
                 November/December 2020, has 2.6 billion web pages in raw text format, saved in ‘shards’ each
                 containing of tens of thousands of pages. Given our hardware constraints, we chose to focus on a subset
                 of the corpus, randomly sampling 1% of the files it contains, roughly amounting toroughly 81 GB of
                 textual content or 5,835,339 webpages in total, which we analyzed in terms of hate speech, adult
                 content, and efficacy of perplexity-based filtering. All code used in these analysis are publicly
                 available¹ [¹https://github.com/josephdviviano/whatsinthebox]. [...] We found that the three
                 approaches compared suggest similar proportions of websites containing hate speech: 5.24% of websites
                 from our sample were flagged by DELIMIT, 4.02% by HateSonar,and 6.38% by the n-gram approach². [²We
                 are conscious of the high false positive rate of n-gram approaches and therefore only consider sites to
                 be flagged if they contain 3 or more n-grams from the list.] Qualitative analysis of a sample of sites
                 flagged by each approach showed that while n-grams picked up on racial slurs, HateSonar picked up on
                 debates about racial supremacy and conspiracy theories. Many of the sites that DELIMIT flagged were
                 adult content with mentions of violent acts towards specific ethnic groups, illustrating the fine line
                 between sexual violence and hate speech. [...] While it can be argued that the Common Crawl corpus is
                 an accurate portrayal of the discourse of modern society – which includes sexual content, hate
                 speech, racial biases, and gender biases – we believe that it is up for debate whether this discourse
                 is the one that we, as a community, want to use to train the models that translate our texts, influence
                 our search results and answer our questions. Notably, the Common Crawl overrepresents those populations
                 that are avid users of the internet: younger, English-speaking individuals from developed countries,
                 [...]",
  cc-author-affiliation = "Université de Montréal, Canada; Mila Québec AI Institute, Canada",
  cc-class     = "ai/ethics-of-machine-learning, nlp/corpus-construction, nlp/text-corpora",
}

