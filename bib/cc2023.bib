
@Article{cc:SafiSingh:2023:phishing-website-detection,
  title        = "A Systematic Literature Review on Phishing Website Detection Techniques",
  journal      = "Journal of King Saud University - Computer and Information Sciences",
  year         = "2023",
  ISSN         = "1319-1578",
  doi          = "https://doi.org/10.1016/j.jksuci.2023.01.004",
  URL          = "https://www.sciencedirect.com/science/article/pii/S1319157823000034",
  author       = "Asadullah Safi and Satwinder Singh",
  keywords     = "Phishing, Phishing Detection, Deep Learning, Cyber Security, Machine Learning",
  abstract     = "Phishing is a fraud attempt in which an attacker acts as a trusted person or entity to obtain
                 sensitive information from an internet user. In this Systematic Literature Survey (SLR), different
                 phishing detection approaches, namely Lists Based, Visual Similarity, Heuristic, Machine Learning, and
                 Deep Learning based techniques, are studied and compared. For this purpose, several algorithms, data
                 sets, and techniques for phishing website detection are revealed with the proposed research questions.
                 A systematic Literature survey was conducted on 80 scientific papers published in the last five years
                 in research journals, conferences, leading workshops, the thesis of researchers, book chapters, and
                 from high-rank websites. The work carried out in this study is an update in the previous systematic
                 literature surveys with more focus on the latest trends in phishing detection techniques. This study
                 enhances readers' understanding of different types of phishing website detection techniques, the data
                 sets used, and the comparative performance of algorithms used. Machine Learning techniques have been
                 applied the most, i.e., 57 as per studies, according to the SLR. In addition, the survey revealed that
                 while gathering the data sets, researchers primarily accessed two sources: 53 studies accessed the
                 PhishTank website (53 for the phishing data set) and 29 studies used Alexa's website for downloading
                 legitimate data sets. Also, as per the literature survey, most studies used Machine Learning
                 techniques; 31 used Random Forest Classifier. Finally, as per different studies, Convolution Neural
                 Network (CNN) achieved the highest Accuracy, 99.98%, for detecting phishing websites.",
  cc-snippet   = "[phishing website detection research relying] Common Crawl (Rao et al., 2019); (Rashid et al., 2020) ;
                 (Geyik et al., 2021) ; (Korkmaz and Sahingoz, 2020) ; (Chiew et al., 2019) ; (Feng and Yue, 2020) ;
                 (Wei et al., 2020)",
  cc-author-affiliation = "Nangarhar University, Afghanistan; Central University of Punjab, Bathinda, Punjab, India",
  cc-class     = "computer-security/internet-security, web-security",
}

@Misc{cc:GoldsteinSastryMusserDiRestaEtAl:2023:generative-language-models-threads-and-mitigations,
  doi          = "10.48550/ARXIV.2301.04246",
  URL          = "https://arxiv.org/abs/2301.04246",
  author       = "Goldstein, Josh A. and Sastry, Girish and Musser, Micah and DiResta, Renee and Gentzel, Matthew and
                 Sedova, Katerina",
  keywords     = "Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information
                 sciences",
  title        = "Generative Language Models and Automated Influence Operations: Emerging Threats and Potential
                 Mitigations",
  publisher    = "arXiv",
  year         = "2023",
  cc-author-affiliation = "Georgetown University’s Center for Security and Emerging Technology, USA; OpenAI; Stanford
                 Internet Observatory, USA",
  cc-class     = "nlp/generative-language-models, ai/ethics-of-machine-learning, cc-cited-not-used",
  cc-snippet   = "While some of this data is typically taken from relatively structured sources such as Wikipedia, a
                 large majority of data usually comes from tools like Common Crawl that scrape the web for publicly
                 available text.¹⁴⁷ [147. CommonCrawl freely publishes its archives of web data. See “So you’re
                 ready to get started.,” Common Crawl, accessed June 27, 2022,
                 https://commoncrawl.org/the-data/get-started/. But anyone can build their own software for web scraping
                 or use other tools to extract data from websites.]",
}

@PhdThesis{cc:Wang:2023:Large-Web-Archive-Collection,
  title        = "Large Web Archive Collection Infrastructure and Services",
  author       = "Wang, Xinyue",
  year         = "2023",
  school       = "Virginia Tech",
  pdf          = "https://vtechworks.lib.vt.edu/bitstream/handle/10919/113345/Wang_X_D_2023.pdf",
  URL          = "http://hdl.handle.net/10919/113345",
  cc-author-affiliation = "Virginia Tech, USA",
  cc-class     = "web-archiving, data formats, big data, data processing, WARC, Parquet, CDX",
  abstract     = "The web has evolved to be the primary carrier of human knowledge during the information age. The
                 ephemeral nature of much web content makes web knowledge preservation vital in preserving human
                 knowledge and memories. Web archives are created to preserve the current web and make it available for
                 future reuse. In addition to its preservation purpose, web archive data is also used as a source for
                 research and for lost information discovery. However, the reuse of web archive data is inherently
                 challenging because of the scale of data size and requirements of big data tools to serve and analyze
                 web archive data efficiently. In this research, we propose to build a web archive big data processing
                 infrastructure that can support efficient and scalable web archive reuse like quantitative data
                 analysis and browsing services. We adopt industry frameworks and tools to establish a platform that can
                 provide high-performance computation for web archive initiatives and users. We propose to convert the
                 standard web archive data file format to a columnar data format for efficient future reuse. Our
                 experiments show that our proposed design can significantly improve quantitative data analysis tasks
                 for common web archive data usage. Our design can also serve an efficient web browsing service without
                 adopting a sophisticated web hosting architecture. In addition to the standard web archive data, we
                 also integrate Twitter data into our design as a unique web archive resource. Twitter is a prominent
                 source of data for researchers in a variety of fields and an integral element of the web's history. We
                 aggregate the Twitter data from different sources and integrate it into the suggested design for reuse.
                 We are able to greatly increase the processing performance of workloads around social media data by
                 overcoming the data loading bottleneck with a web-archive-like Parquet data format.",
  cc-snippet   = "We use Common Crawl’s web archiving data crawled from May 20 to 23, 2018. The data set consists of
                 1219 Gzip compressed WARC files totaling 0.98 TB, and contains 53,324,440 records. The WARC files are
                 organized by crawling time, each containing records crawled from a mutually exclusive time span. We
                 then reformat the WARC files to yield the following five datasets for comparison: 1) the original WARC
                 files; 2) case 1 plus CDX index files built against all the original WARC files; 3) Parquet files
                 containing the same information as case 1, with most columns in String type; 4) the same as case 3 but
                 the Timestamp column in INT64 Timestamp type; 5) Avro, [...]",
}

@Article{cc:Terzis:2023:Building-Programmable-Commons,
  title        = "Building Programmable Commons",
  author       = "Terzis, Petros",
  year         = "2023",
  publisher    = "SocArXiv",
  URL          = "https://osf.io/preprints/socarxiv/yuef5/",
  doi          = "10.31235/osf.io/yuef5",
  cc-author-affiliation = "University College London, United Kingdom",
  cc-class     = "digital-commons, public-commons, cc-cited-not-used",
  cc-snippet   = "Programmable commons and the public value of programmability are thus introduced as parts of a broader
                 political project that aspires to democratise access to, and management of these resources. By drawing
                 on the history of a family of commons -namely intellectual commons, infrastructure commons, and global
                 commons-, this paper explores the material form and impact of infocomputational technologies and
                 presents a blend of bottom-up and top-down initiatives for their commons-based organisation and
                 governance.",
}
