
@Article{cc:SafiSingh:2023:phishing-website-detection,
  title        = "A Systematic Literature Review on Phishing Website Detection Techniques",
  journal      = "Journal of King Saud University - Computer and Information Sciences",
  year         = "2023",
  ISSN         = "1319-1578",
  doi          = "https://doi.org/10.1016/j.jksuci.2023.01.004",
  URL          = "https://www.sciencedirect.com/science/article/pii/S1319157823000034",
  author       = "Asadullah Safi and Satwinder Singh",
  keywords     = "Phishing, Phishing Detection, Deep Learning, Cyber Security, Machine Learning",
  abstract     = "Phishing is a fraud attempt in which an attacker acts as a trusted person or entity to obtain
                 sensitive information from an internet user. In this Systematic Literature Survey (SLR), different
                 phishing detection approaches, namely Lists Based, Visual Similarity, Heuristic, Machine Learning, and
                 Deep Learning based techniques, are studied and compared. For this purpose, several algorithms, data
                 sets, and techniques for phishing website detection are revealed with the proposed research questions.
                 A systematic Literature survey was conducted on 80 scientific papers published in the last five years
                 in research journals, conferences, leading workshops, the thesis of researchers, book chapters, and
                 from high-rank websites. The work carried out in this study is an update in the previous systematic
                 literature surveys with more focus on the latest trends in phishing detection techniques. This study
                 enhances readers' understanding of different types of phishing website detection techniques, the data
                 sets used, and the comparative performance of algorithms used. Machine Learning techniques have been
                 applied the most, i.e., 57 as per studies, according to the SLR. In addition, the survey revealed that
                 while gathering the data sets, researchers primarily accessed two sources: 53 studies accessed the
                 PhishTank website (53 for the phishing data set) and 29 studies used Alexa's website for downloading
                 legitimate data sets. Also, as per the literature survey, most studies used Machine Learning
                 techniques; 31 used Random Forest Classifier. Finally, as per different studies, Convolution Neural
                 Network (CNN) achieved the highest Accuracy, 99.98%, for detecting phishing websites.",
  cc-snippet   = "[phishing website detection research relying] Common Crawl (Rao et al., 2019); (Rashid et al., 2020) ;
                 (Geyik et al., 2021) ; (Korkmaz and Sahingoz, 2020) ; (Chiew et al., 2019) ; (Feng and Yue, 2020) ;
                 (Wei et al., 2020)",
  cc-author-affiliation = "Nangarhar University, Afghanistan; Central University of Punjab, Bathinda, Punjab, India",
  cc-class     = "computer-security/internet-security, web-security",
}

@Misc{cc:GoldsteinSastryMusserDiRestaEtAl:2023:generative-language-models-threads-and-mitigations,
  doi          = "10.48550/ARXIV.2301.04246",
  URL          = "https://arxiv.org/abs/2301.04246",
  author       = "Goldstein, Josh A. and Sastry, Girish and Musser, Micah and DiResta, Renee and Gentzel, Matthew and
                 Sedova, Katerina",
  keywords     = "Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information
                 sciences",
  title        = "Generative Language Models and Automated Influence Operations: Emerging Threats and Potential
                 Mitigations",
  publisher    = "arXiv",
  year         = "2023",
  cc-author-affiliation = "Georgetown University’s Center for Security and Emerging Technology, USA; OpenAI; Stanford
                 Internet Observatory, USA",
  cc-class     = "nlp/generative-language-models, ai/ethics-of-machine-learning, cc-cited-not-used",
  cc-snippet   = "While some of this data is typically taken from relatively structured sources such as Wikipedia, a
                 large majority of data usually comes from tools like Common Crawl that scrape the web for publicly
                 available text.¹⁴⁷ [147. CommonCrawl freely publishes its archives of web data. See “So you’re
                 ready to get started.,” Common Crawl, accessed June 27, 2022,
                 https://commoncrawl.org/the-data/get-started/. But anyone can build their own software for web scraping
                 or use other tools to extract data from websites.]",
}

@PhdThesis{cc:Wang:2023:large-web-archive-collection,
  title        = "Large Web Archive Collection Infrastructure and Services",
  author       = "Wang, Xinyue",
  year         = "2023",
  school       = "Virginia Tech",
  pdf          = "https://vtechworks.lib.vt.edu/bitstream/handle/10919/113345/Wang_X_D_2023.pdf",
  URL          = "http://hdl.handle.net/10919/113345",
  cc-author-affiliation = "Virginia Tech, USA",
  cc-class     = "web-archiving, data formats, big data, data processing, WARC, Parquet, CDX",
  abstract     = "The web has evolved to be the primary carrier of human knowledge during the information age. The
                 ephemeral nature of much web content makes web knowledge preservation vital in preserving human
                 knowledge and memories. Web archives are created to preserve the current web and make it available for
                 future reuse. In addition to its preservation purpose, web archive data is also used as a source for
                 research and for lost information discovery. However, the reuse of web archive data is inherently
                 challenging because of the scale of data size and requirements of big data tools to serve and analyze
                 web archive data efficiently. In this research, we propose to build a web archive big data processing
                 infrastructure that can support efficient and scalable web archive reuse like quantitative data
                 analysis and browsing services. We adopt industry frameworks and tools to establish a platform that can
                 provide high-performance computation for web archive initiatives and users. We propose to convert the
                 standard web archive data file format to a columnar data format for efficient future reuse. Our
                 experiments show that our proposed design can significantly improve quantitative data analysis tasks
                 for common web archive data usage. Our design can also serve an efficient web browsing service without
                 adopting a sophisticated web hosting architecture. In addition to the standard web archive data, we
                 also integrate Twitter data into our design as a unique web archive resource. Twitter is a prominent
                 source of data for researchers in a variety of fields and an integral element of the web's history. We
                 aggregate the Twitter data from different sources and integrate it into the suggested design for reuse.
                 We are able to greatly increase the processing performance of workloads around social media data by
                 overcoming the data loading bottleneck with a web-archive-like Parquet data format.",
  cc-snippet   = "We use Common Crawl’s web archiving data crawled from May 20 to 23, 2018. The data set consists of
                 1219 Gzip compressed WARC files totaling 0.98 TB, and contains 53,324,440 records. The WARC files are
                 organized by crawling time, each containing records crawled from a mutually exclusive time span. We
                 then reformat the WARC files to yield the following five datasets for comparison: 1) the original WARC
                 files; 2) case 1 plus CDX index files built against all the original WARC files; 3) Parquet files
                 containing the same information as case 1, with most columns in String type; 4) the same as case 3 but
                 the Timestamp column in INT64 Timestamp type; 5) Avro, [...]",
}

@Article{cc:Terzis:2023:programmable-commons,
  title        = "Building Programmable Commons",
  author       = "Terzis, Petros",
  year         = "2023",
  publisher    = "SocArXiv",
  URL          = "https://osf.io/preprints/socarxiv/yuef5/",
  doi          = "10.31235/osf.io/yuef5",
  cc-author-affiliation = "University College London, United Kingdom",
  cc-class     = "digital-commons, public-commons, cc-cited-not-used",
  cc-snippet   = "Programmable commons and the public value of programmability are thus introduced as parts of a broader
                 political project that aspires to democratise access to, and management of these resources. By drawing
                 on the history of a family of commons -namely intellectual commons, infrastructure commons, and global
                 commons-, this paper explores the material form and impact of infocomputational technologies and
                 presents a blend of bottom-up and top-down initiatives for their commons-based organisation and
                 governance.",
}

@Misc{cc:HanleyKumarDurumeric:2023:conspiracy-theories,
  doi          = "10.48550/ARXIV.2301.10880",
  URL          = "https://arxiv.org/abs/2301.10880",
  author       = "Hanley, Hans W. A. and Kumar, Deepak and Durumeric, Zakir",
  title        = "A Golden Age: Conspiracy Theories' Relationship with Misinformation Outlets, News Media, and the Wider
                 Internet",
  publisher    = "arXiv",
  year         = "2023",
  abstract     = "Do we live in a {"}Golden Age of Conspiracy Theories?{"} In the last few decades, conspiracy theories
                 have proliferated on the Internet with some having dangerous real-world consequences. A large
                 contingent of those who participated in the January 6th attack on the US Capitol believed fervently in
                 the QAnon conspiracy theory. In this work, we study the relationships amongst five prominent conspiracy
                 theories (QAnon, COVID, UFO/Aliens, 9-11, and Flat-Earth) and each of their respective relationships to
                 the news media, both mainstream and fringe. Identifying and publishing a set of 755 different
                 conspiracy theory websites dedicated to our five conspiracy theories, we find that each set often
                 hyperlinks to the same external domains, with COVID and QAnon conspiracy theory websites largest amount
                 of shared connections. Examining the role of news media, we further find that not only do outlets known
                 for spreading misinformation hyperlink to our set of conspiracy theory websites more often than
                 mainstream websites but this hyperlinking has increased dramatically between 2018 and 2021, with the
                 advent of QAnon and the start of COVID-19 pandemic. Using partial Granger-causality, we uncover several
                 positive correlative relationships between the hyperlinks from misinformation websites and the
                 popularity of conspiracy theory websites, suggesting the prominent role that misinformation news
                 outlets play in popularizing many conspiracy theories.",
  cc-snippet   = "Using our own web scrapes and pages historically scraped by Common Crawl,¹
                 [¹https://commoncrawl.org/] we then document the state and the changing behaviors of the conspiracy
                 theory ecosystem and their relationship to a separate set of 530 known misinformation outlets, 565
                 authentic news websites, and 528 non-news websites. [...] Utilizing the Common Crawl harmonic and
                 PageRank centrality measures that measure a website’s centrality across all of the crawled Internet,
                 we then find many of the websites in our dataset have relatively high network centrality, suggesting
                 that many of them are not peripheral on the Internet but actually near the Internet’s core/are
                 mainstream. Indeed examining, the hyperlink connections between news media and these conspiracy
                 theories, we find that many of them rely heavily on mainstream as well as misinformation outlets
                 (compared to non-news websites) for their information, with many popular misinformation outlets also
                 hyperlinking back to many of these conspiracy theory websites. [...] 4.1 Common Crawl Page Retrieval
                 and Website Crawling To gather the set of hyperlinks between our websites, we utilize Common Crawl data
                 [92]—widely considered the most complete publicly available source of web crawl data—and our own
                 website crawls. For each website in our dataset, we collect all the domain’s HTML pages that were
                 indexed by Common Crawl before August 2021. In addition to Common Crawl data, we further utilize our
                 own website scrapes. We utilize our own crawls, in addition to Common Crawl, due to noisiness, missing
                 pages, and missing domains within the Common Crawl dataset [85]. For example, 309 particularly small
                 conspiracy theory domains were not contained within the Common Crawl dataset (i.e. these websites often
                 only contained a few dozen pages). Thus for each website in our dataset, we further gather all the HTML
                 pages 10 hops from each website’s homepage (i.e., we collect all URLs linked from the homepage (1st
                 hop), then all URLs linked from the pages that were linked by the homepage (2nd hop), and so forth).
                 For each HTML page from our scrapes and Common Crawl, we parse the HTML, detect the date that page was
                 published, and collect hyperlinks to other pages (i.e., HTML <a> tags). Altogether we gather the
                 available Common Crawl pages and scrape the HTML for our 755 conspiracy theory, 530 misinformation, 565
                 authentic news, and 528 non-news websites. [...] Utilizing Common Crawl network data [ 61] over the
                 indexed Internet (87.7 million websites), we thus determine the network centrality of our set of
                 conspiracy-focused websites to understand if each conspiracy theory website category is “core”
                 (regularly utilized on the Internet) or “peripheral”. We utilize centralities across Common
                 Crawl’s dataset rather than our partial one in order to get a sense of each conspiracy theory’s
                 centrality on the entire Internet. While only 446 of our conspiracy theory websites are within the
                 Common Crawl dataset, this analysis allows us to fully understand the relative roles that each
                 conspiracy theory website group in our dataset plays on the wider Internet.",
  cc-author-affiliation = "Stanford University, USA",
  cc-class     = "nlp/fake-news-detection, misinformation, disinformation, conspiracy theories, hyperlink-graph",
}

@Misc{cc:PeetersDerBizer:2023:WDC-products,
  doi          = "10.48550/ARXIV.2301.09521",
  URL          = "https://arxiv.org/abs/2301.09521",
  author       = "Peeters, Ralph and Der, Reng Chiz and Bizer, Christian",
  title        = "{WDC} Products: {A} Multi-Dimensional Entity Matching Benchmark",
  publisher    = "arXiv",
  year         = "2023",
  cc-snippet   = "The first step of the pipeline is the extraction of large amounts of product offers from the Common
                 Crawl⁴ [⁴https://commoncrawl.org/] using schema.org annotations. Some product offers contain
                 product identifiers like MPNs and GTINs which allow us to group offers into [...] The Web Data Commons6
                 project regularly extracts schema.org annotations from the Common Crawl, the largest web corpus
                 available to the public, in order to monitor the adoption of semantic annotations on the Web and to
                 provide the extracted data for public download. The WDC Products benchmark uses product offers from the
                 WDC Product Data Corpus V2020 (PDC2020)7. The corpus was created by extracting schema.org product data
                 from the September 2020 version of the Common Crawl. The extracted data goes through a pipeline of
                 cleansing steps such as removing offers from listing pages as well as advertisements that are contained
                 in a page in addition to the main offer [31]. The resulting PDC2020 corpus consists of ∼98 million
                 product offers originating from 603,000 websites.",
  cc-dataset-used = "CC-MAIN-2020-40",
  cc-author-affiliation = "University of Mannheim, Germany",
  cc-class     = "semantic-web, semantic-web/microformats, e-commerce, linked data, schema.org annotations",
}

@Misc{cc:Amatriain:2023:transformer-models-catalog,
  doi          = "10.48550/ARXIV.2302.07730",
  URL          = "https://arxiv.org/abs/2302.07730",
  pdf          = "https://arxiv.org/pdf/2302.07730.pdf",
  author       = "Xavier Amatriain",
  title        = "Transformer models: an introduction and catalog",
  publisher    = "arXiv",
  year         = "2023",
  cc-author-affiliation = "amatriain.net",
  cc-class     = "nlp/language-model, nlp/transformer-language-model, nlp/multi-modal-language-model",
}

@Misc{cc:CarliniJagielskiChoquette-ChooPalekaEtAl:2023:poisoning-web-scale-training-datasets,
  doi          = "10.48550/ARXIV.2302.10149",
  URL          = "https://arxiv.org/abs/2302.10149",
  pdf          = "https://arxiv.org/pdf/2302.10149.pdf",
  author       = "Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo, Christopher A. and Paleka, Daniel and
                 Pearce, Will and Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and Tramèr, Florian",
  keywords     = "Cryptography and Security (cs.CR), Machine Learning (cs.LG), FOS: Computer and information sciences,
                 FOS: Computer and information sciences",
  title        = "Poisoning Web-Scale Training Datasets is Practical",
  publisher    = "arXiv",
  year         = "2023",
  cc-author-affiliation = "Google; ETH Zurich, Switzerland; NVIDIA; Robust Intelligence",
  cc-class     = "nlp/corpus-creation, computer-security, nlp/language-model, nlp/transformer-language-model,
                 nlp/multi-modal-language-model",
  abstract     = "Deep learning models are often trained on distributed, webscale datasets crawled from the internet. In
                 this paper, we introduce two new dataset poisoning attacks that intentionally introduce malicious
                 examples to a model's performance. Our attacks are immediately practical and could, today, poison 10
                 popular datasets. Our first attack, split-view poisoning, exploits the mutable nature of internet
                 content to ensure a dataset annotator's initial view of the dataset differs from the view downloaded by
                 subsequent clients. By exploiting specific invalid trust assumptions, we show how we could have
                 poisoned 0.01% of the LAION-400M or COYO-700M datasets for just $60 USD. Our second attack,
                 frontrunning poisoning, targets web-scale datasets that periodically snapshot crowd-sourced content --
                 such as Wikipedia -- where an attacker only needs a time-limited window to inject malicious examples.
                 In light of both attacks, we notify the maintainers of each affected dataset and recommended several
                 low-overhead defenses.",
  cc-snippet   = "B.3 Common Crawl Common Crawl is a petabyte-scale corpus of web crawl data that is repeatedly captured
                 on a roughly monthly basis. Each archive is a complete re-crawl of the internet that records the full
                 activity, including all requests of the crawler and the host responses—with both HTTP headers and
                 content. As such, each archive contains a static snapshot of all crawled pages at the time of visit.
                 This may include new page content not seen during a previous crawl, and may exclude content that has
                 become stale since the previous crawl. For example, data crawled during September 24 through October 8,
                 2022 contains 3.15 billion web pages with 380 TiB of uncompressed content from 34 million registered
                 domains—1.3 billion URLs were not visited in any of the prior crawls.14 The Common Crawl dataset is
                 vulnerable to an attack which is similar to both our frontrunning and split-view poisoning attacks. The
                 adversary can purchase an expired domain which was previously contained in the Common Crawl, and it
                 will be re-crawled with the adversary’s choice of content, which will then appear in subsequent
                 Common Crawl snap- shots. Notice that, differently from the snapshot-poisoning attack on Wikipedia,
                 there is no content moderation here and so the adversary simply needs to continue to control the domain
                 to poison all future Common Crawl snapshots. Buying recently-expired domains that existed in previous
                 Common Crawl snapshots allows a stronger form of attack where the attack can inject entirely new links
                 into the crawl. This can be accomplished by adding links or subdomains to poisoned domains, and
                 allowing the crawler to discover the new poisoned domains. Thus, an adversary may inject arbitrarily
                 many pages into the Common Crawl dataset, not only from the originally expired subset. We do not
                 implement this attack following our ethics statements outlined earlier. Since Common Crawl WARC files
                 have been hosted by Amazon on a AWS Athena (serverless service)15, domain reconnaissance work to
                 analyze URLs is inexpensive. Scan- ning through 10 years of Common Crawl data to analyze domains from
                 popular TLDs and high number of Common Crawl entries cost us USD$ 0.84. While additional analysis might
                 somewhat increase this cost, it remains an inexpensive way to search for vulnerable domains. Buying
                 recently expired domains, or domains that have a dangling DNS record with an active IP address is
                 preferred, as domains that failed to return a 200-OK status in consecutive crawls seem to be moved to a
                 lower priority. For example, among expired domains we purchased, just one domain accounts for more than
                 90% of all status codes among the purchased domains, while other domains we purchased as early as
                 12/20/2020 have seen relatively less scraping traffic across a 3 year period.16 Because Common Crawl is
                 enormous and uncurated (to accurately reflect the content of the internet) poisoning all of Common
                 Crawl is impractical due to size. Additionally, it is not always apparent how consumers of this data
                 are process- ing it for downstream machine learning tasks. However, there exist many derivative
                 datasets which are constructed by curating a relevant subset of the Common Crawl. This includes the
                 LAION-5B image dataset [57], the text dataset known as the Pile [23], the multilingual text dataset
                 CC-100 [78], and the CCMatrix dataset [61], a translation dataset of pairs of translated sentences.
                 Such curation actually amplifies the power of an attack: an attack which adds 1MB of text to the Common
                 Crawl would be poisoning a 2.5 · 10−9 fraction of the Common Crawl, but if this text bypasses the
                 curation done for the CC-100 dataset, it could instead poison a 1.2 · 10−5 fraction of the English
                 corpus, or even a full 9.1% of the Oromo corpus.",
}

@Misc{cc:HuangDongWangHaoEtAl:2023:language-is-not-all-you-need,
  doi          = "10.48550/ARXIV.2302.14045",
  URL          = "https://arxiv.org/abs/2302.14045",
  author       = "Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and
                 Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen
                 and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu",
  title        = "Language Is Not All You Need: Aligning Perception with Language Models",
  publisher    = "arXiv",
  year         = "2023",
  cc-author-affiliation = "Microsoft",
  cc-class     = "nlp/language-model, nlp/transformer-language-model, nlp/multi-modal-language-model",
  cc-dataset-used = "CC-MAIN-2020-50, CC-MAIN-2021-04",
  cc-derived-dataset-used = "The-Pile-English, CC-Stories, RealNews, LAION-400M, LAION-2B, COYO-700M",
  cc-snippet   = "Text Corpora We train our model with The Pile [GBB+20] and Common Crawl (CC). The Pile is a massive
                 English text dataset built for training large-scale language models, which is produced from a variety
                 of data sources. We exclude data splits from GitHub, arXiv, Stack Exchange, and PubMed Central. We also
                 include the Common Crawl snapshots (2020-50 and 2021-04) datasets, CC-Stories, and RealNews datasets
                 [SPP+19 , SPN+22]. The entire datasets have been purged of duplicate and near-duplicate documents, as
                 well as filtered to exclude downstream task data. Refer to Appendix B.1.1 for detailed descriptions of
                 training text corpora.¶ Image-Caption Pairs The image-caption pairs are constructed from several
                 datasets, including English LAION-2B [ SBV+22 ], LAION-400M [ SVB+21], COYO-700M [BPK+22 ], and
                 Conceptual Captions [ SDGS18, CSDS21]. English LAION-2B, LAION-400M, and COYO-700M are collected from
                 web pages of the Common Crawl web data by extracting image sources and the corresponding alt-text.
                 Conceptual Captions are also from internet web pages. More details can be found in Appendix B.1.2. ¶
                 Interleaved Image-Text Data We collect interleaved multimodal data from the Common Crawl snapshot,
                 which is a publicly available archive of web pages. We use a filtering process to select about 71M web
                 pages from the original 2B web pages in the snapshot. We then extract the text and images from the HTML
                 of each selected web page. For each document, we limit the number of images to five to reduce noise and
                 redundancy. We also randomly discard half of the documents that only have one image to increase the
                 diversity. We provide more details about the data collection process in Appendix B.1.3. By using this
                 corpus, we enable KOSMOS-1 to handle interleaved text and image and improve its few-shot ability.",
}

@Misc{cc:TouvronLavrilIzacardMartinetEtAl:2023:LLaMA,
  doi          = "10.48550/ARXIV.2302.13971",
  URL          = "https://arxiv.org/abs/2302.13971",
  pdf          = "https://arxiv.org/pdf/2302.13971.pdf",
  author       = "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne
                 and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and
                 Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume",
  keywords     = "Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and
                 information sciences",
  title        = "{LL}a{MA}: Open and Efficient Foundation Language Models",
  publisher    = "arXiv",
  year         = "2023",
  abstract     = "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We
                 train our models on trillions of tokens, and show that it is possible to train state-of-the-art models
                 using publicly available datasets exclusively, without resorting to proprietary and inaccessible
                 datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is
                 competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the
                 research community.",
  cc-author-affiliation = "Meta AI",
  cc-class     = "nlp/language-model, nlp/transformer-language-model, nlp/multi-modal-language-model",
  cc-dataset-used = "five CommonCrawl dumps, ranging from 2017 to 2020",
  cc-derived-dataset-about = "Tensorflow-C4",
  cc-snippet   = "English CommonCrawl [67%]. We preprocess five CommonCrawl dumps, ranging from 2017 to 2020, with the
                 CCNet pipeline (Wenzek et al., 2020). This process deduplicates the data at the line level, performs
                 language identification with a fastText linear classifier to remove non-English pages and filters low
                 quality content with an n- gram language model. In addition, we trained a linear model to classify
                 pages used as references in Wikipedia v.s. randomly sampled pages, and discarded pages not classified
                 as references.",
}

@Misc{cc:Ammar:2023:dynamic-graph-processing,
  year         = "2023",
  title        = "Systems and Algorithms for Dynamic Graph Processing",
  author       = "Khaled Ammar",
  pdfurl       = "https://uwspace.uwaterloo.ca/bitstream/handle/10012/19195/Ammar_Khaled.pdf",
  cc-derived-dataset-used = "WDC-hyperlinkgraph, WDC-hyperlinkgraph (2014)",
  cc-snippet   = "Common Crawl experiments. Sixteen machines load 64 billion edges, index them, and track motifs in 20
                 batches of 10K random edge changes.",
  cc-author-affiliation = "University of Waterloo, Ontario, Canada",
  cc-class     = "graph-processing, web-science/hyperlinkgraph",
}

@Misc{cc:HuangSiddarth:2023:generative-AI-and-digital-commons,
  title        = "Generative {AI} and the Digital Commons",
  author       = "Saffron Huang and Divya Siddarth",
  year         = "2023",
  eprint       = "2303.11074",
  archiveprefix = "arXiv",
  primaryclass = "cs.CY",
  pdf          = "https://arxiv.org/pdf/2303.11074.pdf",
  cc-author-affiliation = "Collective Intelligence Project (cip.org)",
  cc-class     = "digital-commons, public-commons, nlp/corpus-construction, nlp/language-models,
                 nlp/generative-language-models, cc-cited-not-used",
  c-snippet    = "GFMs are trained on the digital commons. Generative foundation models leverage large databases of
                 scraped information (text, code, images) from the internet to train highly capable models. This depends
                 on the availability of public, scrapable data and leverages the “collective intelligence” of
                 humanity, including the painstakingly edited Wikipedia, millennia’s worth of books, billions of
                 Reddit comments, hundreds of terabytes’ worth of images, and more³ [³LAION-5B, which Stable
                 Diffusion is trained on, has 5 billion text-image pairs (Schuhmann et al., 2022).The Pile has 100+GB of
                 books (Gao et al., 2020)]. They also rely on non- profits like Common Crawl (which build and maintain
                 open repositories of web crawl data), Creative Commons (for open licenses for the data used), open
                 source libraries, and other digital infrastructure. They also take advantage of aggregated user
                 preferences; e.g. the WebText dataset underlying the GPT family of models uses Reddit “karma
                 scores” to select content for inclusion. All of this is common digital information and infrastructure
                 that many people contribute to.",
}

@Misc{cc:ChanBradleyRajkumar:2023:reclaiming-digital-commons,
  title        = "Reclaiming the Digital Commons: {A} Public Data Trust for Training Data",
  author       = "Alan Chan and Herbie Bradley and Nitarshan Rajkumar",
  year         = "2023",
  eprint       = "2303.09001",
  archiveprefix = "arXiv",
  primaryclass = "cs.CY",
  pdf          = "https://arxiv.org/pdf/2303.09001.pdf",
  cc-snippet   = "The data trust could also start from existing efforts, such as the Common Crawl.",
  abstract     = "Democratization of AI means not only that people can freely use AI, but also that people can
                 collectively decide how AI is to be used. In particular, collective decision-making power is required
                 to redress the negative externalities from the development of increasingly advanced AI systems,
                 including degradation of the digital commons and unemployment from automation. The rapid pace of AI
                 development and deployment currently leaves little room for this power. Monopolized in the hands of
                 private corporations, the development of the most capable foundation models has proceeded largely
                 without public input. There is currently no implemented mechanism for ensuring that the economic value
                 generated by such models is redistributed to account for their negative externalities. The citizens
                 that have generated the data necessary to train models do not have input on how their data are to be
                 used. In this work, we propose that a public data trust assert control over training data for
                 foundation models. In particular, this trust should scrape the internet as a digital commons, to
                 license to commercial model developers for a percentage cut of revenues from deployment. First, we
                 argue in detail for the existence of such a trust. We also discuss feasibility and potential risks.
                 Second, we detail a number of ways for a data trust to incentivize model developers to use training
                 data only from the trust. We propose a mix of verification mechanisms, potential regulatory action, and
                 positive incentives. We conclude by highlighting other potential benefits of our proposed data trust
                 and connecting our work to ongoing efforts in data and compute governance.",
  URL          = "https://arxiv.org/abs/2303.09001",
  cc-author-affiliation = "University of Cambridge, United Kingdom; Mila, Université de Montréal, Canada; EleutherAI",
  cc-class     = "digital-commons, public-commons, nlp/corpus-construction, nlp/language-models,
                 nlp/generative-language-models, cc-cited-not-used",
}
