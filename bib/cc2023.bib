
@Article{cc:SafiSingh:2023:phishing-website-detection,
  title        = "A Systematic Literature Review on Phishing Website Detection Techniques",
  journal      = "Journal of King Saud University - Computer and Information Sciences",
  year         = "2023",
  ISSN         = "1319-1578",
  doi          = "https://doi.org/10.1016/j.jksuci.2023.01.004",
  URL          = "https://www.sciencedirect.com/science/article/pii/S1319157823000034",
  author       = "Asadullah Safi and Satwinder Singh",
  keywords     = "Phishing, Phishing Detection, Deep Learning, Cyber Security, Machine Learning",
  abstract     = "Phishing is a fraud attempt in which an attacker acts as a trusted person or entity to obtain
                 sensitive information from an internet user. In this Systematic Literature Survey (SLR), different
                 phishing detection approaches, namely Lists Based, Visual Similarity, Heuristic, Machine Learning, and
                 Deep Learning based techniques, are studied and compared. For this purpose, several algorithms, data
                 sets, and techniques for phishing website detection are revealed with the proposed research questions.
                 A systematic Literature survey was conducted on 80 scientific papers published in the last five years
                 in research journals, conferences, leading workshops, the thesis of researchers, book chapters, and
                 from high-rank websites. The work carried out in this study is an update in the previous systematic
                 literature surveys with more focus on the latest trends in phishing detection techniques. This study
                 enhances readers' understanding of different types of phishing website detection techniques, the data
                 sets used, and the comparative performance of algorithms used. Machine Learning techniques have been
                 applied the most, i.e., 57 as per studies, according to the SLR. In addition, the survey revealed that
                 while gathering the data sets, researchers primarily accessed two sources: 53 studies accessed the
                 PhishTank website (53 for the phishing data set) and 29 studies used Alexa's website for downloading
                 legitimate data sets. Also, as per the literature survey, most studies used Machine Learning
                 techniques; 31 used Random Forest Classifier. Finally, as per different studies, Convolution Neural
                 Network (CNN) achieved the highest Accuracy, 99.98%, for detecting phishing websites.",
  cc-snippet   = "[phishing website detection research relying] Common Crawl (Rao et al., 2019); (Rashid et al., 2020) ;
                 (Geyik et al., 2021) ; (Korkmaz and Sahingoz, 2020) ; (Chiew et al., 2019) ; (Feng and Yue, 2020) ;
                 (Wei et al., 2020)",
  cc-author-affiliation = "Nangarhar University, Afghanistan; Central University of Punjab, Bathinda, Punjab, India",
  cc-class     = "computer-security/internet-security, web-security",
}

@Misc{cc:GoldsteinSastryMusserDiRestaEtAl:2023:generative-language-models-threads-and-mitigations,
  doi          = "10.48550/ARXIV.2301.04246",
  URL          = "https://arxiv.org/abs/2301.04246",
  author       = "Goldstein, Josh A. and Sastry, Girish and Musser, Micah and DiResta, Renee and Gentzel, Matthew and
                 Sedova, Katerina",
  keywords     = "Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information
                 sciences",
  title        = "Generative Language Models and Automated Influence Operations: Emerging Threats and Potential
                 Mitigations",
  publisher    = "arXiv",
  year         = "2023",
  cc-author-affiliation = "Georgetown University’s Center for Security and Emerging Technology, USA; OpenAI; Stanford
                 Internet Observatory, USA",
  cc-class     = "nlp/generative-language-models, ai/ethics-of-machine-learning, cc-cited-not-used",
  cc-snippet   = "While some of this data is typically taken from relatively structured sources such as Wikipedia, a
                 large majority of data usually comes from tools like Common Crawl that scrape the web for publicly
                 available text.¹⁴⁷ [147. CommonCrawl freely publishes its archives of web data. See “So you’re
                 ready to get started.,” Common Crawl, accessed June 27, 2022,
                 https://commoncrawl.org/the-data/get-started/. But anyone can build their own software for web scraping
                 or use other tools to extract data from websites.]",
}

@PhdThesis{cc:Wang:2023:large-web-archive-collection,
  title        = "Large Web Archive Collection Infrastructure and Services",
  author       = "Wang, Xinyue",
  year         = "2023",
  school       = "Virginia Tech",
  pdf          = "https://vtechworks.lib.vt.edu/bitstream/handle/10919/113345/Wang_X_D_2023.pdf",
  URL          = "http://hdl.handle.net/10919/113345",
  cc-author-affiliation = "Virginia Tech, USA",
  cc-class     = "web-archiving, data formats, big data, data processing, WARC, Parquet, CDX",
  abstract     = "The web has evolved to be the primary carrier of human knowledge during the information age. The
                 ephemeral nature of much web content makes web knowledge preservation vital in preserving human
                 knowledge and memories. Web archives are created to preserve the current web and make it available for
                 future reuse. In addition to its preservation purpose, web archive data is also used as a source for
                 research and for lost information discovery. However, the reuse of web archive data is inherently
                 challenging because of the scale of data size and requirements of big data tools to serve and analyze
                 web archive data efficiently. In this research, we propose to build a web archive big data processing
                 infrastructure that can support efficient and scalable web archive reuse like quantitative data
                 analysis and browsing services. We adopt industry frameworks and tools to establish a platform that can
                 provide high-performance computation for web archive initiatives and users. We propose to convert the
                 standard web archive data file format to a columnar data format for efficient future reuse. Our
                 experiments show that our proposed design can significantly improve quantitative data analysis tasks
                 for common web archive data usage. Our design can also serve an efficient web browsing service without
                 adopting a sophisticated web hosting architecture. In addition to the standard web archive data, we
                 also integrate Twitter data into our design as a unique web archive resource. Twitter is a prominent
                 source of data for researchers in a variety of fields and an integral element of the web's history. We
                 aggregate the Twitter data from different sources and integrate it into the suggested design for reuse.
                 We are able to greatly increase the processing performance of workloads around social media data by
                 overcoming the data loading bottleneck with a web-archive-like Parquet data format.",
  cc-snippet   = "We use Common Crawl’s web archiving data crawled from May 20 to 23, 2018. The data set consists of
                 1219 Gzip compressed WARC files totaling 0.98 TB, and contains 53,324,440 records. The WARC files are
                 organized by crawling time, each containing records crawled from a mutually exclusive time span. We
                 then reformat the WARC files to yield the following five datasets for comparison: 1) the original WARC
                 files; 2) case 1 plus CDX index files built against all the original WARC files; 3) Parquet files
                 containing the same information as case 1, with most columns in String type; 4) the same as case 3 but
                 the Timestamp column in INT64 Timestamp type; 5) Avro, [...]",
}

@Article{cc:Terzis:2023:programmable-commons,
  title        = "Building Programmable Commons",
  author       = "Terzis, Petros",
  year         = "2023",
  publisher    = "SocArXiv",
  URL          = "https://osf.io/preprints/socarxiv/yuef5/",
  doi          = "10.31235/osf.io/yuef5",
  cc-author-affiliation = "University College London, United Kingdom",
  cc-class     = "digital-commons, public-commons, cc-cited-not-used",
  cc-snippet   = "Programmable commons and the public value of programmability are thus introduced as parts of a broader
                 political project that aspires to democratise access to, and management of these resources. By drawing
                 on the history of a family of commons -namely intellectual commons, infrastructure commons, and global
                 commons-, this paper explores the material form and impact of infocomputational technologies and
                 presents a blend of bottom-up and top-down initiatives for their commons-based organisation and
                 governance.",
}

@Misc{cc:HanleyKumarDurumeric:2023:conspiracy-theories,
  doi          = "10.48550/ARXIV.2301.10880",
  URL          = "https://arxiv.org/abs/2301.10880",
  author       = "Hanley, Hans W. A. and Kumar, Deepak and Durumeric, Zakir",
  title        = "A Golden Age: Conspiracy Theories' Relationship with Misinformation Outlets, News Media, and the Wider
                 Internet",
  publisher    = "arXiv",
  year         = "2023",
  abstract     = "Do we live in a {"}Golden Age of Conspiracy Theories?{"} In the last few decades, conspiracy theories
                 have proliferated on the Internet with some having dangerous real-world consequences. A large
                 contingent of those who participated in the January 6th attack on the US Capitol believed fervently in
                 the QAnon conspiracy theory. In this work, we study the relationships amongst five prominent conspiracy
                 theories (QAnon, COVID, UFO/Aliens, 9-11, and Flat-Earth) and each of their respective relationships to
                 the news media, both mainstream and fringe. Identifying and publishing a set of 755 different
                 conspiracy theory websites dedicated to our five conspiracy theories, we find that each set often
                 hyperlinks to the same external domains, with COVID and QAnon conspiracy theory websites largest amount
                 of shared connections. Examining the role of news media, we further find that not only do outlets known
                 for spreading misinformation hyperlink to our set of conspiracy theory websites more often than
                 mainstream websites but this hyperlinking has increased dramatically between 2018 and 2021, with the
                 advent of QAnon and the start of COVID-19 pandemic. Using partial Granger-causality, we uncover several
                 positive correlative relationships between the hyperlinks from misinformation websites and the
                 popularity of conspiracy theory websites, suggesting the prominent role that misinformation news
                 outlets play in popularizing many conspiracy theories.",
  cc-snippet   = "Using our own web scrapes and pages historically scraped by Common Crawl,¹
                 [¹https://commoncrawl.org/] we then document the state and the changing behaviors of the conspiracy
                 theory ecosystem and their relationship to a separate set of 530 known misinformation outlets, 565
                 authentic news websites, and 528 non-news websites. [...] Utilizing the Common Crawl harmonic and
                 PageRank centrality measures that measure a website’s centrality across all of the crawled Internet,
                 we then find many of the websites in our dataset have relatively high network centrality, suggesting
                 that many of them are not peripheral on the Internet but actually near the Internet’s core/are
                 mainstream. Indeed examining, the hyperlink connections between news media and these conspiracy
                 theories, we find that many of them rely heavily on mainstream as well as misinformation outlets
                 (compared to non-news websites) for their information, with many popular misinformation outlets also
                 hyperlinking back to many of these conspiracy theory websites. [...] 4.1 Common Crawl Page Retrieval
                 and Website Crawling To gather the set of hyperlinks between our websites, we utilize Common Crawl data
                 [92]—widely considered the most complete publicly available source of web crawl data—and our own
                 website crawls. For each website in our dataset, we collect all the domain’s HTML pages that were
                 indexed by Common Crawl before August 2021. In addition to Common Crawl data, we further utilize our
                 own website scrapes. We utilize our own crawls, in addition to Common Crawl, due to noisiness, missing
                 pages, and missing domains within the Common Crawl dataset [85]. For example, 309 particularly small
                 conspiracy theory domains were not contained within the Common Crawl dataset (i.e. these websites often
                 only contained a few dozen pages). Thus for each website in our dataset, we further gather all the HTML
                 pages 10 hops from each website’s homepage (i.e., we collect all URLs linked from the homepage (1st
                 hop), then all URLs linked from the pages that were linked by the homepage (2nd hop), and so forth).
                 For each HTML page from our scrapes and Common Crawl, we parse the HTML, detect the date that page was
                 published, and collect hyperlinks to other pages (i.e., HTML <a> tags). Altogether we gather the
                 available Common Crawl pages and scrape the HTML for our 755 conspiracy theory, 530 misinformation, 565
                 authentic news, and 528 non-news websites. [...] Utilizing Common Crawl network data [ 61] over the
                 indexed Internet (87.7 million websites), we thus determine the network centrality of our set of
                 conspiracy-focused websites to understand if each conspiracy theory website category is “core”
                 (regularly utilized on the Internet) or “peripheral”. We utilize centralities across Common
                 Crawl’s dataset rather than our partial one in order to get a sense of each conspiracy theory’s
                 centrality on the entire Internet. While only 446 of our conspiracy theory websites are within the
                 Common Crawl dataset, this analysis allows us to fully understand the relative roles that each
                 conspiracy theory website group in our dataset plays on the wider Internet.",
  cc-author-affiliation = "Stanford University, USA",
  cc-class     = "nlp/fake-news-detection, misinformation, disinformation, conspiracy theories, hyperlink-graph",
}

@Misc{cc:PeetersDerBizer:2023:WDC-products,
  doi          = "10.48550/ARXIV.2301.09521",
  URL          = "https://arxiv.org/abs/2301.09521",
  author       = "Peeters, Ralph and Der, Reng Chiz and Bizer, Christian",
  title        = "{WDC} Products: {A} Multi-Dimensional Entity Matching Benchmark",
  publisher    = "arXiv",
  year         = "2023",
  cc-snippet   = "The first step of the pipeline is the extraction of large amounts of product offers from the Common
                 Crawl⁴ [⁴https://commoncrawl.org/] using schema.org annotations. Some product offers contain
                 product identifiers like MPNs and GTINs which allow us to group offers into [...] The Web Data Commons6
                 project regularly extracts schema.org annotations from the Common Crawl, the largest web corpus
                 available to the public, in order to monitor the adoption of semantic annotations on the Web and to
                 provide the extracted data for public download. The WDC Products benchmark uses product offers from the
                 WDC Product Data Corpus V2020 (PDC2020)7. The corpus was created by extracting schema.org product data
                 from the September 2020 version of the Common Crawl. The extracted data goes through a pipeline of
                 cleansing steps such as removing offers from listing pages as well as advertisements that are contained
                 in a page in addition to the main offer [31]. The resulting PDC2020 corpus consists of ∼98 million
                 product offers originating from 603,000 websites.",
  cc-dataset-used = "CC-MAIN-2020-40",
  cc-author-affiliation = "University of Mannheim, Germany",
  cc-class     = "semantic-web, semantic-web/microformats, e-commerce, linked data, schema.org annotations",
}
