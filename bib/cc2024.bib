@InProceedings{cc:GongMccarthyRizoiuBoldi:2024:Australian-domain-space,
  author       = "Gong, Xian and Mccarthy, Paul X. and Rizoiu, Marian-Andrei and Boldi, Paolo",
  title        = "Harmony in the Australian Domain Space",
  year         = "2024",
  ISBN         = "9798400703348",
  publisher    = "Association for Computing Machinery",
  address      = "New York, NY, USA",
  URL          = "https://doi.org/10.1145/3614419.3643998",
  doi          = "10.1145/3614419.3643998",
  abstract     = "In this paper we use for the first time a systematic approach in the study of harmonic centrality at a
                 Web domain level, and gather a number of significant new findings about the Australian web. In
                 particular, we explore the relationship between economic diversity at the firm level and the structure
                 of the Web within the Australian domain space, using harmonic centrality as the main structural
                 feature. The distribution of harmonic centrality values is analyzed over time, and we find that the
                 distributions exhibit a consistent pattern across the different years. The observed distribution is
                 well captured by a partition of the domain space into six clusters; the temporal movement of domain
                 names across these six positions yields insights into the Australian Domain Space and exhibits
                 correlations with other non-structural characteristics. From a more global perspective, we find a
                 significant correlation between the median harmonic centrality of all domains in each OECD country and
                 one measure of global trust, the WJP Rule of Law Index. Further investigation demonstrates that 35
                 countries in OECD share similar harmonic centrality distributions. The observed homogeneity in
                 distribution presents a compelling avenue for exploration, potentially unveiling critical corporate,
                 regional, or national insights.",
  booktitle    = "Proceedings of the 16th ACM Web Science Conference",
  pages        = "92--102",
  numpages     = "11",
  location     = "Stuttgart, Germany",
  series       = "WEBSCI '24",
  cc-author-affiliation = "University of Technology, Australia; University of New South Wales, Australia; Università
                 degli Studi di Milano, Italy",
  cc-class     = "",
  cc-snippet   = "There are many public collections of web crawls, but one that is known for being very reliable and
                 quite wide in scope is the Common Crawl1. Common Crawl’s measurements are preferred for web and
                 network analysis due to their extensive coverage, regular updates, and large-scale, publicly accessible
                 datasets, which reduces the need for resource-intensive data collection and is applicable across
                 various research in a reproducible way. [...]",
}

@Article{cc:CarragherWilliamsCarley:2024:Misinformation-resilient-search-rankings,
  author       = "Carragher, Peter and Williams, Evan M. and Carley, Kathleen M.",
  title        = "Misinformation Resilient Search Rankings with Webgraph-based Interventions",
  year         = "2024",
  publisher    = "Association for Computing Machinery",
  address      = "New York, NY, USA",
  ISSN         = "2157-6904",
  URL          = "https://doi.org/10.1145/3670410",
  doi          = "10.1145/3670410",
  abstract     = "The proliferation of unreliable news domains on the internet has had wide-reaching negative impacts on
                 society. We introduce and evaluate interventions aimed at reducing traffic to unreliable news domains
                 from search engines while maintaining traffic to reliable domains. We build these interventions on the
                 principles of fairness (penalize sites for what is in their control), generality (label/fact-check
                 agnostic), targeted (increase the cost of adversarial behavior), and scalability (works at webscale).
                 We refine our methods on small-scale webdata as a testbed and then generalize the interventions to a
                 large-scale webgraph containing 93.9M domains and 1.6B edges. We demonstrate that our methods penalize
                 unreliable domains far more than reliable domains in both settings and we explore multiple avenues to
                 mitigate unintended effects on both the small-scale and large-scale webgraph experiments. These results
                 indicate the potential of our approach to reduce the spread of misinformation and foster a more
                 reliable online information ecosystem. This research contributes to the development of targeted
                 strategies to enhance the trustworthiness and quality of search engine results, ultimately benefiting
                 users and the broader digital community.",
  note         = "Just Accepted",
  journal      = "ACM Trans. Intell. Syst. Technol.",
  month        = jun,
  keywords     = "search engine optimization, misinformation, website reliability, pagerank",
  cc-author-affiliation = "Carnegie Mellon University, USA",
  cc-class     = "web-science/hyperlinkgraph, misinformation, disinformation, domain-ranking",
}

@InProceedings{cc:FontanaVignaZacchiroli:2024:WebGraph-in-Rust,
  author       = "Tommaso Fontana and Sebastiano Vigna and Stefano Zacchiroli",
  editor       = "Tat{-}Seng Chua and Chong{-}Wah Ngo and Roy Ka{-}Wei Lee and Ravi Kumar and Hady W. Lauw",
  title        = "WebGraph: The Next Generation (Is in Rust)",
  booktitle    = "Companion Proceedings of the {ACM} on Web Conference 2024, {WWW} 2024, Singapore, Singapore, May
                 13-17, 2024",
  pages        = "686--689",
  publisher    = "{ACM}",
  year         = "2024",
  URL          = "https://doi.org/10.1145/3589335.3651581",
  doi          = "10.1145/3589335.3651581",
  pdf          = "https://hal.science/hal-04494627/document",
  timestamp    = "Fri, 17 May 2024 21:42:50 +0200",
  cc-author-affiliation = "Inria, DGDI, Paris, France; Università degli Studi di Milano, Dipartimento di Informatica,
                 Milan, Italy; LTCI, Télécom Paris, Institut Polytechnique de Paris, Palaiseau, France",
  cc-class     = "web-science/hyperlinkgraph, graph-processing, programming-languages/Java, programming-languages/Rust,
                 cc-cited-not-used",
  cc-snippet   = "Moreover, open data projects such as Common Crawl and Software Heritage (SWH) [5] have used WebGraph
                 to compress and distribute their data.",
}

@InProceedings{cc:Thompson:2024:Longitudinal-web-analytics,
  title        = "Improved methodology for longitudinal Web analytics using Common Crawl",
  abstract     = "Common Crawl is a multi-petabyte longitudinal dataset containing over 100 billion web pages which is
                 widely used as a source of language data for sequence model training and in web science research. Each
                 of its constituent archives is on the order of 75TB in size. Using it for research, particularly
                 longitudinal studies, which necessarily involve multiple archives, is therefore very expensive in terms
                 of compute time and storage space and/or web bandwidth. Two new methods for mitigating this problem are
                 presented here, based on exploiting and extending the much smaller (<200 gigabytes (GB) compressed)
                 index which is available for each archive. By adding Last-Modified timestamps to the index we enable
                 longitudinal exploration using only a single archive. By comparing the distribution of index features
                 for each of the 100 segments into which archive is divided with their distribution over the whole
                 archive, we have identified the least and most representative segments for a number of recent archives.
                 Using this allows the segment(s) that are most representative of an archive to be used as proxies for
                 the whole. We illustrate this approach in an analysis of changes in URI length over time, leading to an
                 unanticipated insight into the how the creation of Web pages has changed over time.",
  author       = "Thompson, {Henry S}",
  year         = "2024",
  month        = jan,
  day          = "31",
  language     = "English",
  booktitle    = "WebSci '24: Proceedings of the 16th ACM Web Science Conference 2024",
  publisher    = "ACM",
  note         = "16th ACM Web Science Conference 2024, Websci 2024 ; Conference date: 21-05-2024 Through 24-05-2024",
  url2         = "https://arxiv.org/abs/2404.09770",
  URL          = "https://www.research.ed.ac.uk/en/publications/improved-methodology-for-longitudinal-web-analytics-using-common-",
  pdf          = "https://arxiv.org/pdf/2404.09770.pdf",
  cc-author-affiliation = "The University of Edinburgh, Edinburgh, United Kingdom",
  cc-class     = "web-archiving, web-dataset",
}

@InProceedings{cc:ElOuadi:2024:Comparison-CC-News-GDELT,
  title        = "Comparison of Common Crawl News \& {GDELT}",
  abstract     = "The corpus of worldwide news is important for natural language processing, knowledge graphs, large
                 language models, and other technical efforts. Additionally, this corpus is important for understanding
                 the people, places, organizations, and events that interact in real-time every day. This paper compares
                 two news datasets used for these tasks today, namely the Global Database of Events, Language, and Tone
                 (GDELT) and Common Crawl News. Our research highlights the strengths and limitations of each dataset,
                 analyzing their content and coverage. Notably, while GDELT relies on broadcasts, prints, and web news
                 from across the globe, Common Crawl focuses on news sites from around the world gathered through web
                 crawling. Our analysis revealed considerable differences in where the two datasets gather their news
                 sources.",
  author       = "El Ouadi, Ameir and Beskow, David",
  year         = "2024",
  month        = apr,
  language     = "English",
  booktitle    = "2024 IEEE International Systems Conference (SysCon)",
  publisher    = "IEEE",
  pages        = "1--3",
  URL          = "https://doi.org/10.1109/SysCon61195.2024.10553540",
}

@Misc{cc:LiuLuoShanVoelkerEtAl:2024:Somesite-I-used-to-crawl,
  title        = "Somesite {I} Used To Crawl: Awareness, Agency and Efficacy in Protecting Content Creators From {AI}
                 Crawlers",
  author       = "Enze Liu and Elisa Luo and Shawn Shan and Geoffrey M. Voelker and Ben Y. Zhao and Stefan Savage",
  year         = "2024",
  eprint       = "2411.15091",
  archiveprefix = "arXiv",
  primaryclass = "cs.HC",
  URL          = "https://arxiv.org/abs/2411.15091",
  abstract     = "The success of generative AI relies heavily on training on data scraped through extensive crawling of
                 the Internet, a practice that has raised significant copyright, privacy, and ethical concerns. While
                 few measures are designed to resist a resource-rich adversary determined to scrape a site, crawlers can
                 be impacted by a range of existing tools such as robots.txt, NoAI meta tags, and active crawler
                 blocking by reverse proxies. In this work, we seek to understand the ability and efficacy of today's
                 networking tools to protect content creators against AI-related crawling. For targeted populations like
                 human artists, do they have the technical knowledge and agency to utilize crawler-blocking tools such
                 as robots.txt, and can such tools be effective? Using large scale measurements and a targeted user
                 study of 182 professional artists, we find strong demand for tools like robots.txt, but significantly
                 constrained by significant hurdles in technical awareness, agency in deploying them, and limited
                 efficacy against unresponsive crawlers. We further test and evaluate network level crawler blockers by
                 reverse-proxies, and find that despite very limited deployment today, their reliable and comprehensive
                 blocking of AI-crawlers make them the strongest protection for artists moving forward.",
  cc-author-affiliation = "UC San Diego, USA; University of Chicago, USA",
  cc-class     = "robots.txt, web-crawling, legal/copyright, ai/ethics-of-machine-learning",
  cc-snippet   = "Historic Robots.txt Data from Common Crawl. We used the robots.txt files crawled by the Common Crawl
                 from 2022 to 2024. We find that the coverage of the Common Crawl robots.txt data is approximately 83\%.
                 Ultimately, for sites that did not return a 200 or 404 HTTP status code to Common Crawl, we cannot
                 determine whether the site indeed had a robots.txt or not.1 A summary of our robot.txt historical
                 dataset can be found in Table 4 in the appendix, and more details on how we deduplicate and clean the
                 Common Crawl data can be found in Appendix 10.2.",
}

@Article{cc:KnockelDałekAljizawiAhmedEtAl:2024:Banned-books,
  title        = "Banned {Books}: {Analysis} of {Censorship} on {Amazon}.com",
  copyright    = "Attribution-ShareAlike 4.0 International",
  shorttitle   = "Banned {Books}",
  URL          = "https://tspace.library.utoronto.ca/handle/1807/141434",
  pdf          = "https://tspace.library.utoronto.ca/bitstream/1807/141434/1/Banned_Books.pdf",
  abstract     = "We analyze the system Amazon deploys on the US “amazon.com” storefront to restrict shipments of
                 certain products to specific regions. We found 17,050 products that Amazon restricted from being
                 shipped to at least one world region. While many of the shipping restrictions are related to
                 regulations involving WiFi, car seats, and other heavily regulated product categories, the most common
                 product category restricted by Amazon in our study was books.",
  language     = "en\_ca",
  urldate      = "2024-11-28",
  author       = "Knockel, Jeffrey and Dałek, Jakub and Aljizawi, Noura and Ahmed, Mohamed and Meletti, Levi and Lau,
                 Justin",
  month        = nov,
  year         = "2024",
  cc-author-affiliation = "Citizen Lab, Munk School of Global Affairs & Public Policy, University of Toronto, Canada",
  cc-class     = "political science, internet-censorship",
  cc-snippet   = "To test which products we could ship to these five countries, we required a method for sampling a
                 sufficiently diverse set of Amazon products to test. To address this requirement, we made use of the
                 Common Crawl data set provided by the Com- mon Crawl Foundation. This data set is a diverse, open
                 Internet-wide sample of Web pages scraped beginning in 2008. In April 2023, we downloaded all of the
                 archives up to and including the February/March 2023 archive. To avoid excessive storage requirements
                 of storing the entire data set, we downloaded the archive in streaming fashion, filtering out any
                 Amazon product URL into a file without storing any other data from the data set. We processed the
                 Common Crawl data from 2013 through March 2023, as March 2023 was the most recent data set available at
                 the time that we began our testing. Although we were only interested in prod- ucts available on the
                 amazon.com storefront, since products are often available on multiple storefronts, we collected
                 products from the 23 Amazon dedicated storefronts that were in use at this time.¶ Using this method,
                 we collected a list of 114,542,719 Amazon URLs.",
}

@Article{cc:BaackMozillaInsights:2024:Training-data-Common-Crawls-impact,
  title        = "Training Data for the Price of a Sandwich",
  subtitle     = "Common Crawl's Impact on Generative AI",
  author       = "Stefan Baack and Mozilla Insights",
  year         = "2024",
  URL          = "https://foundation.mozilla.org/en/research/library/generative-ai-training-data/common-crawl/",
  cc-author-affiliation = "Mozilla Foundation",
  cc-class     = "nlp/corpus-construction, nlp/language-model, ai/generative-ai",
}

@InProceedings{cc:AasmanBenDavidBrugger:2024:Transnational-web-archive-studies,
  title        = "The Routledge Companion to Transnational Web Archive Studies",
  author       = "Susan Aasman and Anat Ben-David and Niels Brügger",
  year         = "2024",
  URL          = "https://api.semanticscholar.org/CorpusID:274145465",
  cc-author-affiliation = "University of Groningen, The Netherlands; Open University of Israel, Israel; Aarhus
                 University, Denmark",
  cc-class     = "web-archiving, web-science",
}

@Misc{cc:LongpreMahariLeeLundEtAl:2024:Consent-in-crisis,
  title        = "Consent in Crisis: The Rapid Decline of the {AI} Data Commons",
  author       = "Shayne Longpre and Robert Mahari and Ariel Lee and Campbell Lund and Hamidah Oderinwale and William
                 Brannon and Nayan Saxena and Naana Obeng-Marnu and Tobin South and Cole Hunter and Kevin Klyman and
                 Christopher Klamm and Hailey Schoelkopf and Nikhil Singh and Manuel Cherep and Ahmad Anis and An Dinh
                 and Caroline Chitongo and Da Yin and Damien Sileo and Deividas Mataciunas and Diganta Misra and Emad
                 Alghamdi and Enrico Shippole and Jianguo Zhang and Joanna Materzynska and Kun Qian and Kush Tiwary and
                 Lester Miranda and Manan Dey and Minnie Liang and Mohammed Hamdy and Niklas Muennighoff and Seonghyeon
                 Ye and Seungone Kim and Shrestha Mohanty and Vipul Gupta and Vivek Sharma and Vu Minh Chien and Xuhui
                 Zhou and Yizhi Li and Caiming Xiong and Luis Villa and Stella Biderman and Hanlin Li and Daphne
                 Ippolito and Sara Hooker and Jad Kabbara and Sandy Pentland",
  year         = "2024",
  eprint       = "2407.14933",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/2407.14933",
  pdf          = "https://www.dataprovenance.org/Consent_in_Crisis.pdf",
  cc-snippet   = "In our study, we focus on three popular, open-source, and permissively licensed data sources which are
                 derived from Common Crawl, the largest publicly available crawl of the web, which has collected and
                 stored hundreds of billions of web pages since 2008.",
  cc-author-affiliation = "Data Provenance",
  cc-class     = "legal/copyright, ai/ethics-of-machine-learning, nlp/language-model, ai/foundation-model, web-crawling,
                 robots.txt, terms-of-service",
}

@Article{cc:Fontana:2024:Web-scraping-jurisprudence,
  author       = "Fontana, Avv. Gino",
  title        = "Web scraping: Jurisprudence and legal doctrines",
  journal      = "The Journal of World Intellectual Property",
  year         = "2024",
  pages        = "1--16",
  keywords     = "artificial generative intelligence, artificial intelligence, copyright, data scraping, intellectual
                 property, web scraping",
  doi          = "https://doi.org/10.1111/jwip.12331",
  URL          = "https://onlinelibrary.wiley.com/doi/abs/10.1111/jwip.12331",
  eprint       = "https://onlinelibrary.wiley.com/doi/pdf/10.1111/jwip.12331",
  abstract     = "Abstract Web scraping is a technique that allows the extraction of online information and data to
                 train Generative Artificial Intelligence (GenAI) systems. Although the use of deep learning algorithms
                 to produce user-requested outputs (texts, images, music and code) based on models learned from vast
                 data sets dates back a few decades, its use has become fundamental with the recent development of GenAI
                 and has been accompanied by the emergence of the first legal disputes. Doctrine and jurisprudence are
                 called upon to consider the legal consequences arising from the combination of web scraping and GenAI,
                 often encountering inadequate and fragmented legislation. Laws and regulations vary significantly
                 across different countries and regions, reflecting diverse priorities and legal approaches. However,
                 while doctrine, regardless of the latitudes, agrees in condemning the illicit acts and abuses due not
                 so much to the extraction method but to the use of the extracted data (where protected by intellectual
                 property rights), jurisprudence (particularly in Europe and North America) has already had the
                 opportunity to express divergent opinions in some leading cases.",
  cc-snippet   = "In the case of ChatGPT, for example, during the pretraining phase large quantities of textual data are
                 used, extracted from archives like Common Crawl, Wikipedia or Google Books but also from platforms like
                 Reddit.⁸ Without web scraping no AI or GenAI system could generate any results, the objective of web
                 scraping, in fact, is not the archiving or transmission of content but data mining,9 or deep learning
                 based on the reading of the collected data.",
  cc-author-affiliation = "Digital University Pegaso, Napoli, Italy",
  cc-class     = "legal/copyright, legal/fair-use, nlp/language-model, ai/foundation-model, web-crawling, robots.txt",
}

@Misc{cc:RiveroGonzalo:2024:When-online-content-disappears,
  title        = "When {Online} {Content} {Disappears}",
  URL          = "https://www.pewresearch.org/data-labs/2024/05/17/when-online-content-disappears/",
  abstract     = "A quarter of all webpages that existed at one point between 2013 and 2023 are no longer accessible.",
  language     = "en-US",
  urldate      = "2025-01-30",
  journal      = "Pew Research Center",
  author       = "Rivero, Samuel Bestvater, Emma Remy {and} Gonzalo, Athena Chapekis",
  month        = may,
  year         = "2024",
  cc-snippet   = "To conduct this part of our analysis, we collected a random sample of just under 1 million webpages
                 from the archives of Common Crawl, an internet archive service that periodically collects snapshots of
                 the internet as it exists at different points in time. We sampled pages collected by Common Crawl each
                 year from 2013 through 2023 (approximately 90,000 pages per year) and checked to see if those pages
                 still exist today. We found that 25% of all the pages we collected from 2013 through 2023 were no
                 longer accessible as of October 2023. This figure is the sum of two different types of broken pages:
                 16% of pages are individually inaccessible but come from an otherwise functional root-level domain; the
                 other 9% are inaccessible because their entire root domain is no longer functional.¶ Data collection
                 for World Wide Web websites, government websites and news websites¶ To examine attrition on the
                 broader internet, we collected three samples of web crawl data from Common Crawl, a nonprofit
                 organization that collects and maintains a large web archive. This data has been collected monthly
                 starting in 2008. We used this archive to create a historical sample of URLs from the broader internet
                 dating back to 2013, as well as contemporaneous snapshots of pages from government entities and news
                 websites.",
  cc-author-affiliation = "Pew Research Center, Washington DC, USA",
  cc-class     = "web-science, web-archiving",
}

@Misc{cc:Amarikwa:2024:Internet-Openness-at-Risk,
  address      = "Rochester, NY",
  type         = "{SSRN} {Scholarly} {Paper}",
  title        = "Internet {Openness} at {Risk}: {Generative} {AI}'s {Impact} on {Data} {Scraping}",
  shorttitle   = "Internet {Openness} at {Risk}",
  URL          = "https://papers.ssrn.com/abstract=4723713",
  doi          = "10.2139/ssrn.4723713",
  abstract     = "Modern scraping practices—the automated extraction of data from online websites—by companies
                 employing generative AI models threatens the foundational and essential openness of the internet. There
                 are calls for regulating the use of scraping in generative AI models, but lawmakers, concerned about
                 its impact on U.S. global AI leadership, have failed to act. This article presents two legal frameworks
                 aimed at regulating generative AI scraping. The adverse possession framework addresses property rights
                 and allows for the use of copyrighted works where the author abandons or fails to claim their works.
                 The public records framework addresses privacy rights and treats personal information made publicly
                 available by the subject as a public record with context-based privacy exemptions. These frameworks
                 seek to strike a balance between private interests in development and the public’s interest in
                 safeguarding its property and privacy rights.",
  language     = "en",
  urldate      = "2025-03-20",
  author       = "Amarikwa, Melany",
  month        = feb,
  year         = "2024",
  keywords     = "Internet Openness at Risk: Generative AI's Impact on Data Scraping, Melany Amarikwa, SSRN",
  cc-author-affiliation = "University of Pennsylvania Law School, USA",
  cc-class     = "legal/copyright, ai/ethics-of-machine-learning, nlp/language-model, ai/foundation-model, web-crawling,
                 robots.txt, terms-of-service, cc-cited-not-used",
  cc-snippet   = "CommonCrawl offers users “a copy of the Internet,”40 serving as one of the largest and most widely
                 utilized repositories of scraped data.41 [⁴¹ See JAY M. PATEL, GETTING STRUCTURED DATA FROM THE
                 INTERNET 278 (2020) (“When we take the common crawl data cumulatively, across monthly crawls since
                 2008, it represents one of the largest publicly accessible web crawl data corpuses on a petabyte scale,
                 and this is one major reason why it’s been used so widely in academia and the industry.”).]",
}
