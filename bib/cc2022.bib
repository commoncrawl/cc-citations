@Misc{cc:SnæbjarnarsonSímonarsonRagnarssonIngólfsdóttirEtAl:2022:warm-start-and-clean-crawled-corpus,
  title        = "A Warm Start and a Clean Crawled Corpus -- {A} Recipe for Good Language Models",
  author       = "Vésteinn Snæbjarnarson and Haukur Barri Símonarson and Pétur Orri Ragnarsson and Svanhvít Lilja
                 Ingólfsdóttir and Haukur Páll Jónsson and Vilhjálmur Þorsteinsson and Hafsteinn Einarsson",
  year         = "2022",
  pdf          = "https://arxiv.org/pdf/2201.05601.pdf",
  URL          = "https://arxiv.org/abs/2201.05601",
  abstract     = "We train several language models for Icelandic, including IceBERT, that achieve state-of-the-art
                 performance in a variety of downstream tasks, including part-of-speech tagging, named entity
                 recognition, grammatical error detection and constituency parsing. To train the models we introduce a
                 new corpus of Icelandic text, the Icelandic Common Crawl Corpus (IC3), a collection of high quality
                 texts found online by targeting the Icelandic top-level-domain (TLD). Several other public data sources
                 are also collected for a total of 16GB of Icelandic text. To enhance the evaluation of model
                 performance and to raise the bar in baselines for Icelandic, we translate and adapt the WinoGrande
                 dataset for co-reference resolution. Through these efforts we demonstrate that a properly cleaned
                 crawled corpus is sufficient to achieve state-of-the-art results in NLP applications for low to medium
                 resource languages, by comparison with models trained on a curated corpus. We further show that
                 initializing models using existing multilingual models can lead to state-of-the-art results for some
                 downstream tasks.",
  cc-author-affiliation = "Miðeind ehf., Iceland; University of Iceland, Iceland",
  cc-class     = "nlp/corpus-construction, nlp/language-model",
  cc-dataset-used = "CDX, WARC, ARC 2008 – March 2020",
  cc-snippet   = "3.1. The Icelandic Common Crawl Corpus¶ The Common Crawl Foundation is a non-profit organization that
                 scrapes large semi-random subsets of the internet regularly and hosts timestamped and compressed dumps
                 of the web online¹⁰ [¹⁰https://commoncrawl.org/the-data/get-started/]. Each dump contains
                 billions of web pages occupying hundreds of terabytes. Parsing these files directly requires storage
                 and computing power not directly available to most and can come at a significant financial cost. The
                 foundation also hosts indices of URIs and their locations within the large zipped dump files. While
                 these indices are also large, their processing is feasible with a few terabytes of storage.¶ 3.1.1.
                 Extracting Icelandic Common Crawl data¶ The Common Crawl indices, which contain URI and byte offsets
                 within the compressed dumps, are used to reduce the search space when looking for Icelandic texts. The
                 Common Crawl Index Server has a public API¹¹ [¹¹https://index.commoncrawl.org/] where URIs can be
                 queried based on attributes such as date, MIME-type and substring. Using the API eliminates the need to
                 fetch the massive index files. To extract Icelandic, the .is pattern is targeted to match the Icelandic
                 top level domain (TLD), resulting in 63.5 million retrieved pages with URIs and byte locations within
                 the compressed Common Crawl dumps. The computational efficiency of our method can be attributed to
                 these steps. Given the predominant use of the .is TLD for Icelandic web content, we assume that other
                 TLDs have a much lower proportion of Icelandic content. That said, a nontrivial amount of text in
                 Icelandic is still likely to be found outside the .is domain and could be extracted by, e.g., parsing
                 the whole Common Crawl, albeit at a much higher computational cost.¶ By targeting only the
                 byte-offsets corresponding to the Icelandic TLD we extract candidate websites that have a high
                 proportion of Icelandic content. In total, the compressed content is 687GiB on disk. All dumps since
                 the start of the Common Crawl in 2008 until March 2020 were included.¶ Plain text was extracted from
                 the collected WARC (Web Archive format) files using jusText (Pomikálek, 2011)12 to remove boilerplate
                 content and HTML tags.",
}

@Misc{cc:ArtetxeAldabeAgerriPerez-de-ViñaspreEtAl:2022:corpus-quality-low-resource-languages,
  doi          = "10.48550/ARXIV.2203.08111",
  URL          = "https://arxiv.org/abs/2203.08111",
  author       = "Artetxe, Mikel and Aldabe, Itziar and Agerri, Rodrigo and Perez-de-Viñaspre, Olatz and Soroa, Aitor",
  keywords     = "Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS:
                 Computer and information sciences, FOS: Computer and information sciences",
  title        = "Does Corpus Quality Really Matter for Low-Resource Languages?",
  publisher    = "arXiv",
  year         = "2022",
  abstract     = "The vast majority of non-English corpora are derived from automatically filtered versions of
                 CommonCrawl. While prior work has identified major issues on the quality of these datasets (Kreutzer et
                 al., 2021), it is not clear how this impacts downstream performance. Taking Basque as a case study, we
                 explore tailored crawling (manually identifying and scraping websites with high-quality content) as an
                 alternative to filtering CommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque
                 portion of popular multilingual corpora like CC100 and mC4, yet it has a much higher quality according
                 to native annotators. For instance, 66% of documents are rated as high-quality for EusCrawl, in
                 contrast with <33% for both mC4 and CC100. Nevertheless, we obtain similar results on downstream tasks
                 regardless of the corpus used for pre-training. Our work suggests that NLU performance in low-resource
                 languages is primarily constrained by the quantity rather than the quality of the data, prompting for
                 methods to exploit more diverse data sources.",
  cc-snippet   = "In this paper, we explore tailored crawling (i.e., manually identifying and scraping websites with
                 high-quality content) as an alternative to filtering CommonCrawl. Taking Basque as a case study, we
                 collect 12.5M documents from 33 websites with Creative Commons content. The resulting corpus, called
                 EusCrawl, is similar in size to the Basque portion of CC100 and mC4, but it has substantially less
                 issues and a higher perceived quality according to our blind audit with native annotators. However, we
                 find that this improvement does not carry over to downstream tasks, as masked language models
                 pre-trained on either corpora obtain similar results on 5 NLU benchmarks. Our results suggests that
                 data quantity and domain play a more important role, prompting for methods to exploit more diverse
                 sources of data in low-resource languages.",
  cc-author-affiliation = "Meta AI; HiTZ Center - Ixa, University of the Basque Country UPV/EHU",
  cc-class     = "nlp/corpus-construction, nlp/corpus-representativeness, nlp/corpus-quality, nlp/language-models,
                 nlp/low-resource-languages",
}

@Misc{cc:BidermanBichenoGao:2022:datasheet-for-Pile,
  title        = "Datasheet for the Pile",
  author       = "Stella Biderman and Kieran Bicheno and Leo Gao",
  year         = "2022",
  pdf          = "https://arxiv.org/pdf/2201.07311.pdf",
  URL          = "https://arxiv.org/abs/2201.07311",
  abstract     = "This datasheet describes the Pile, a 825 GiB dataset of human-authored text compiled by EleutherAI for
                 use in large-scale language modeling. The Pile is comprised of 22 different text sources, ranging from
                 original scrapes done for this project, to text data made available by the data owners, to third-party
                 scrapes available online.",
  cc-author-affiliation = "EleutherAI",
  cc-class     = "nlp/corpus-construction, nlp/corpus-datasheet, nlp/corpus-representativeness",
  cc-derived-dataset-about = "The-Pile-English",
  cc-snippet   = "Pile-CC: The Pile-CC dataset is a sample from the Common Crawl WARCs that has been converted to text
                 using jusText [Endrédy and Novák, 2013].¶ [...] Pile-CC: The Pile-CC dataset was created to be
                 included in the Pile. The underlying data comes from the Common Crawl, which was created to give people
                 access to the wealth of information contained in the internet. Its creators were concerned that only
                 data mining companies would be able to collect this data, and has the explicit aim of democratizing
                 technology.¶ [...] Pile-CC: The data is sourced from Common Crawl, a non-profit 501(c)(3) organization
                 founded by Gil Elbaz. The data from Common Crawl was processed by EleutherAI into Pile-CC.¶ [...]
                 Pile-CC: Instances are webpages.¶ [...] Pile-CC: 54, 953, 117 documents, totaling 227.12 GiB.¶ [...]
                 Pile-CC: A tiny fraction of the entire Common Crawl was included, chosen arbitrarily and heavily
                 filtered as detailed in Gao et al. [2020].¶ [...] Pile-CC: Data in the Pile-CC dataset were scraped
                 from websites by the Common Craw and then downloaded directly from the Common Craw by EleutherAI.¶
                 [...] Pile-CC: The earliest date of contents in Pile-CC is unknown.¶",
}

@TechReport{cc:Andersson-Schwarz:2022:quantification-online-linguistic-data,
  title        = "The hitchhiker's guide Method handbook for quantification of online linguistic data in a
                 country-specific context. Official research report, Linguistic Explorations of Societies (Work Package
                 1)",
  author       = "Andersson Schwarz, Jonas",
  year         = "2022",
  pdf          = "https://gupea.ub.gu.se/bitstream/handle/2077/70890/2022_1_Andersson%20Schwarz.pdf",
  cc-snippet   = "Central actors (in no particular order)¶ CommonCrawl. California-based non-profit organization that
                 makes monthly crawls of the openly available Web and provides datasets and metadata to the public
                 freely. The CommonCrawl corpus contains petabytes of data including raw web page data, metadata data
                 and text data collected since 2011. Since 2012, CommonCrawl’s archive is hosted by Amazon Web
                 Services as part of its Public Data Sets program. Every crawl contains around 300 terabytes of data and
                 roughly 3 billion pages. In 2020, a filtered version of this CommonCrawl archive was used to train
                 OpenAI’s GPT-3 language model.¶ [...] Similarly, CommonCrawl (2021) provides an aggregate listing
                 the percentages of their database covered by each language – measured as the primary language of each
                 html document, as identified by the Compact Language Detector 2 (CLD2) algorithm. This was included as
                 a good benchmark to compare with.¶ [...] In comparison, when plotting the cur- rently stated language
                 distribution of CommonCrawl (2021) in relation to the same population numbers of L1 and L2 speakers,
                 the CommonCrawl distribution displays a similarly low kurtosis and skewness.",
  cc-author-affiliation = "Göteborgs Universitet, Sweden",
  cc-class     = "nlp/corpus-construction, nlp/corpus-representativeness",
}

@Misc{cc:MorishitaChousaSuzukiNagata:2022:JParaCrawl,
  doi          = "10.48550/ARXIV.2202.12607",
  URL          = "https://arxiv.org/abs/2202.12607",
  pdf          = "https://arxiv.org/pdf/2202.12607.pdf",
  author       = "Morishita, Makoto and Chousa, Katsuki and Suzuki, Jun and Nagata, Masaaki",
  title        = "{JP}araCrawl v3.0: {A} Large-scale English-Japanese Parallel Corpus",
  year         = "2022",
  abstract     = "Most current machine translation models are mainly trained with parallel corpora, and their
                 translation accuracy largely depends on the quality and quantity of the corpora. Although there are
                 billions of parallel sentences for a few language pairs, effectively dealing with most language pairs
                 is difficult due to a lack of publicly available parallel corpora. This paper creates a large parallel
                 corpus for English-Japanese, a language pair for which only limited resources are available, compared
                 to such resource-rich languages as English-German. It introduces a new web-based English-Japanese
                 parallel corpus named JParaCrawl v3.0. Our new corpus contains more than 21 million unique parallel
                 sentence pairs, which is more than twice as many as the previous JParaCrawl v2.0 corpus. Through
                 experiments, we empirically show how our new corpus boosts the accuracy of machine translation models
                 on various domains. The JParaCrawl v3.0 corpus will eventually be publicly available online for
                 research purposes.",
  cc-snippet   = "Our method extracts parallel sentences from the web. Thus, the first step is finding a website that
                 has parallel sentences. This method is based on the hypothesis that websites containing the same
                 English and Japanese sentences might have parallel texts. To list such parallel websites, we analyzed
                 all the Common Crawl text archive data released from March 2019 to August 2021³. [³During this
                 period, the Common Crawl project released 25 archives, and their text size was about 212 TB.] We
                 identified the language in the archive by CLD2⁴ [⁴ https://github.com/CLD2Owners/cld2] and listed
                 100,000 large websites that roughly have the same size of English and Japanese texts. For this step, we
                 used extractor⁵ [⁵ 5https://github.com/paracrawl/extractor] that was provided by the ParaCrawl
                 project.",
  cc-author-affiliation = "NTT Communication Science Laboratories, NTT Corporation, Japan",
  cc-class     = "nlp/machine-translation, nlp/parallel-corpus, nlp/corpus-construction",
}

@InProceedings{cc:LAKIMAlmazroueiAlhaolDebbahEtAl:2022:carbon-footprint,
  title        = "A Holistic Assessment of the Carbon Footprint of Noor, a Very Large Arabic Language Model",
  author       = "Imad LAKIM and Ebtesam Almazrouei and Ibrahim Abu Alhaol and Merouane Debbah and Julien Launay",
  booktitle    = "Challenges {\&} Perspectives in Creating Large Language Models",
  year         = "2022",
  URL          = "https://openreview.net/forum?id=B-lS3zH8Zq",
  abstract     = "As ever larger language models grow more ubiquitous, it is crucial to consider their environmental
                 impact. Characterised by extreme size and resource use, recent generations of models have been
                 criticised for their voracious appetite for compute, and thus significant carbon footprint. Although
                 reporting of carbon impact has grown more common in machine learning papers, this reporting is usually
                 limited to compute resources used strictly for training. In this work, we propose a holistic assessment
                 of the footprint of an extreme-scale language model, Noor. Noor is an ongoing project aiming to develop
                 the largest multi-task Arabic language models--with up to 13B parameters--leveraging zero-shot
                 generalisation to enable a wide range of downstream tasks via natural language instructions. We assess
                 the total carbon bill of the entire project: starting with data collection and storage costs, including
                 research and development budgets, pretraining costs, future serving estimates, and other exogenous
                 costs necessary for this international cooperation. Notably, we find that inference costs and exogenous
                 factors can have a significant impact on total budget. Finally, we discuss pathways to reduce the
                 carbon footprint of extreme-scale models.",
  cc-author-affiliation = "TII, Abu Dhabi, Arabic Emirates; LightOn, Paris, France",
  cc-class     = "nlp/language-model, nlp/transformer-language-model, carbon-footprint",
  cc-snippet   = "We use Common Crawl (CC) for acquiring large amounts of web data. Each CC dump is on average around
                 10TB, and we discard it immediately after processing it. On average, it takes 24 hours to fully process
                 a dump: we used 21 dumps from CC, meaning we stored 210TB of data for 24hours, equivalent to 57 kWh of
                 energy consumption. After processing the dumps, we got on average 1.2TB of data per dump, thus 25TB in
                 total. Considering that this data will be stored for 6 months, we end up with 1.3 MWh of energy
                 consumption for the bulk data. Note that we keep the processed data in all languages (not just Modern
                 Standard Arabic).",
}

@Article{cc:Gutiérrez-FandiñoPérez-FernándezArmengol-EstapéGriolEtAl:2022:esCorpius,
  author       = "Gutiérrez-Fandiño, Asier and Pérez-Fernández, David and Armengol-Estapé, Jordi and Griol, David
                 and Callejas, Zoraida",
  title        = "{esCorpius: A Massive Spanish Crawling Corpus}",
  journal      = "arXiv e-prints",
  keywords     = "Computer Science - Computation and Language, Computer Science - Artificial Intelligence",
  year         = "2022",
  eid          = "arXiv:2206.15147",
  URL          = "https://ui.adsabs.harvard.edu/abs/2022arXiv220615147G",
  pdf          = "https://arxiv.org/pdf/2206.15147.pdf",
  adsnote      = "Provided by the SAO/NASA Astrophysics Data System",
  cc-snippet   = "[…] In this paper, we introduce esCorpius, a Spanish crawling corpus obtained from near 1 Pb of
                 Common Crawl data. It is the most extensive corpus in Spanish with this level of quality in the
                 extraction, purification and deduplication of web textual content […] A total of 39,502 compressed
                 WARC (Web Archive) from Common Crawl files were processed (see section 3.3 for more details). The
                 compressed information occupied about 180 TB and the size of the processed decompressed information is
                 estimated to be more than 0.8 PB. Prior to content deduplication, the downloaded corpus was composed of
                 106.768.594.753 words, 3.129.248.875 lines and 163.518.405 web pages. The deduplicated and cleaned
                 corpus size is 346.262.072.705 bytes (322.5 GB), with 104.073.706 total number of lines, 50.040.055.322
                 tokens, 1.125.798.968 paragraphs and 2.421.598.201 sentences.",
  cc-author-affiliation = "LHF Labs; Universidad Autónoma de Madrid, Spain; University of Edinburgh, United Kingdom;
                 Universidad de Granada, Spain",
  cc-class     = "nlp/corpus-construction, nlp/text-corpora",
}

@InProceedings{cc:OverwijkXiongCallan:2022:ClueWeb22,
  author       = "Overwijk, Arnold and Xiong, Chenyan and Callan, Jamie",
  title        = "ClueWeb22: 10 Billion Web Documents with Rich Information",
  year         = "2022",
  ISBN         = "978-1-4503-8732-3",
  publisher    = "Association for Computing Machinery",
  address      = "New York, NY, USA",
  URL          = "https://doi.org/10.1145/3477495.3536321",
  pdf          = "https://dl.acm.org/doi/pdf/10.1145/3477495.3536321",
  doi          = "10.1145/3477495.3536321",
  abstract     = "ClueWeb22, the newest iteration of the ClueWeb line of datasets, is the result of more than a year of
                 collaboration between industry and academia. Its design is influenced by the research needs of the
                 academic community and the real-world needs of large-scale industry systems. Compared with earlier
                 ClueWeb datasets, the ClueWeb22 corpus is larger, more varied, and has higher-quality documents. Its
                 core is raw HTML, but it includes clean text versions of documents to lower the barrier to entry.
                 Several aspects of ClueWeb22 are available to the research community for the first time at this scale,
                 for example, visual representations of rendered web pages, parsed structured information from the HTML
                 document, and the alignment of document distributions (domains, languages, and topics) to commercial
                 web search.This talk shares the design and construction of ClueWeb22, and discusses its new features.
                 We believe this newer, larger, and richer ClueWeb corpus will enable and support a broad range of
                 research in IR, NLP, and deep learning.",
  booktitle    = "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information
                 Retrieval",
  pages        = "3360–3362",
  numpages     = "3",
  keywords     = "clueweb, web corpus, dataset",
  location     = "Madrid, Spain",
  series       = "SIGIR '22",
  cc-snippet   = "One approach is to sift CommonCrawl data, eg, the C4 dataset used to pretrain T5 [10], which provides
                 sufficient quantity, but the quality quickly becomes a concern. For example, the cleaned CommonCrawl
                 reflects a quite weird distribution of the web [5]. Language models pretrained on C4 often perform
                 worse than models pretrained on higher quality corpora at the same scale. With ClueWeb22, we aim to
                 provide the web corpus for research in the near future. The design of ClueWeb22 emphasizes on these
                 goals: 1) to reflect the distribution of the web in real scenarios; 2) to provide web pages at large
                 quantity and also with high quality; 3) to enable new research directions by including information
                 important in industry but previously not publicly available.",
  cc-author-affiliation = "Microsoft; Carnegie Mellon University",
  cc-class     = "cc-cited-not-used, nlp/corpus-construction, nlp/text-corpora, information-retrieval",
}

@Misc{cc:ZhangRollerGoyalArtetxeEtAl:2022:OPT-open-pretrained-transformer,
  doi          = "10.48550/ARXIV.2205.01068",
  URL          = "https://arxiv.org/abs/2205.01068",
  pdf          = "https://arxiv.org/pdf/2205.01068.pdf",
  snippet      = "All corpora were previously collected or filtered to contain predominantly English text, but a small
                 amount of non-English data is still present within the corpus via CommonCrawl. We removed duplicated
                 documents across all datasets by filtering …",
  cc-author-affiliation = "Meta AI",
  author       = "Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui
                 and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott,
                 Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali
                 and Wang, Tianlu and Zettlemoyer, Luke",
  title        = "{OPT}: Open Pre-trained Transformer Language Models",
  publisher    = "arXiv",
  year         = "2022",
  cc-derived-dataset-used = "CC-Stories, Pile-CC, CC-NEWS-RoBERTa-v2",
  cc-class     = "nlp/language-model, nlp/transformer-language-model, nlp/corpus-construction",
}

@InProceedings{cc:LugeonPiccardiWest:2022:Homepage2Vec,
  title        = "Homepage2Vec: Language-Agnostic Website Embedding and Classification",
  author       = "Lugeon, Sylvain and Piccardi, Tiziano and West, Robert",
  booktitle    = "Proceedings of the International AAAI Conference on Web and Social Media",
  volume       = "16",
  pages        = "1285--1291",
  year         = "2022",
  URL          = "https://ojs.aaai.org/index.php/ICWSM/article/download/19380/19152",
  abstract     = "Top-level domain. Some top-level domains (TLD) such as .edu or .biz can offer a good hint about the
                 website's content. For example, a typical use case for .edu is university websites, whereas .biz is
                 commonly associated with business activities. Following this intuition, we collected from Common
                 Crawl,5 a large-scale sample of the Web, the 19 most frequent TLDs: .com, .org, .net, .info, .xyz,
                 .club, .biz, .top, .edu, .online, .pro, .site, .vip, .icu, .buzz, .app, .asia, .gov, .space, excluding
                 the country code TLD (ccTLD) because they indicate geographic origin, not website content. We represent
                 this feature with a one-hot encoding vector of 19 dimensions.",
  cc-author-affiliation = "EPFL, Switzerland",
  cc-class     = "nlp/text-classification, web-site-classification",
}

@InProceedings{cc:ZirngiblDeuschSattlerAulbachEtAl:2022:domain-parking,
  title        = "Domain Parking: Largely Present, Rarely Considered!",
  author       = "Zirngibl, Johannes and Deusch, Steffen and Sattler, Patrick and Aulbach, Juliane and Carle, Georg and
                 Jonker, Mattijs",
  booktitle    = "Proc. Network Traffic Measurement and Analysis Conference (TMA) 2022",
  year         = "2022",
  abstract     = "Domain parking typically involves leveraging advertisements to generate revenue on otherwise inactive
                 domain names. Their content is rarely of real value to users and tends to be highly similar across
                 parked domains. They have commonalities beyond content alone: parked domains can share hosting and DNS
                 infrastructure. Parking rarely receives special treatment in existing studies (e.g., content analyses
                 or infrastructure concentration studies). While the presence and possible bias introduced by parked
                 pages is sometimes acknowledged in studies, the studies still treat parked domains as any other, either
                 because differentiation is infeasible, or because doing so is considered out-of-scope. We argue that
                 the impact of parked domains on analyses regarding the current state and future development of the
                 Internet should not be overlooked. In this paper, we motivate this argument through quantification, and
                 take steps towards helping other researchers identify parked domains. We systematically collect a list
                 of 82 parking services and develop DNS-based indicators to help identify parked domains. We next
                 quantify the presence of parked domains, using large-scale DNS data containing hundreds of millions of
                 registered domain names, representative for a significant part of the global DNS namespace. Overall, we
                 pinpoint 60 M parked domains, which is a significant percentage of all names under consideration (23 %)
                 and identify up to 4 % of domains from top lists to be parked. These findings demonstrate that the
                 effect of parked pages is potentially pronounced. We also break down into the various parking services
                 and DNS zones. This helps us demonstrate and further discuss the effect that domain parking can have on
                 research and Internet consolidation.",
  cc-snippet   = "Common Crawl While visual identification allowed us to validate the inferences to a reasonable extent,
                 we wanted to upscale validation. Therefore, we consider Common Crawl (CC) data [21] [C. Crawl. (2022)
                 The Common Crawl Corpus. [Online]. Available: https://commoncrawl.org/] and calculate the similarity of
                 pages. Common Crawl is an open repository of web crawl data, collected at monthly intervals, accounting
                 for hundreds of millions of unique domain names, and many more URLs. We consider CC data for Jan 2022
                 and the ∼60 M parked domains that we identify on Jan 28th, 2022. We extract the HTML content of
                 parked pages from CC data, only considering URLs that contain exactly the registered domain.
                 Furthermore, we require the crawl target to have been the landing page (i.e., the path of the URL is /)
                 and also to have resulted in a useful response (i.e., HTTP status code of 200). Given these filters,
                 ∼1.29 M HTML rich responses can be obtained. We extract visible text and tokenize it into words,
                 remove stop words, apply lemmatization, and create a vector for the most-frequently used words for each
                 page.",
  URL          = "https://mediatum.ub.tum.de/1661842",
  pdf          = "https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/zirngibl2022prevalenceofparking.pdf",
  cc-author-affiliation = "Technical University of Munich, Germany; University of Twente, The Netherlands",
  cc-class     = "web-science, internet/DNS, internet/domain-parking",
}

@InProceedings{cc:LuccioniCorrySridharanAnannyEtAl:2022:deprecating-datasets,
  author       = "Luccioni, Alexandra Sasha and Corry, Frances and Sridharan, Hamsini and Ananny, Mike and Schultz,
                 Jason and Crawford, Kate",
  title        = "A Framework for Deprecating Datasets: Standardizing Documentation, Identification, and Communication",
  year         = "2022",
  ISBN         = "978-1-4503-9352-2",
  publisher    = "Association for Computing Machinery",
  address      = "New York, NY, USA",
  URL          = "https://doi.org/10.1145/3531146.3533086",
  pdf          = "https://facctconference.org/static/pdfs_2022/facct22-17.pdf",
  doi          = "10.1145/3531146.3533086",
  abstract     = "Datasets are central to training machine learning (ML) models. The ML community has recently made
                 significant improvements to data stewardship and documentation practices across the model development
                 life cycle. However, the act of deprecating, or deleting, datasets has been largely overlooked, and
                 there are currently no standardized approaches for structuring this stage of the dataset life cycle. In
                 this paper, we study the practice of dataset deprecation in ML, identify several cases of datasets that
                 continued to circulate despite having been deprecated, and describe the different technical, legal,
                 ethical, and organizational issues raised by such continuations. We then propose a Dataset Deprecation
                 Framework that includes considerations of risk, mitigation of impact, appeal mechanisms, timeline,
                 post-deprecation protocols, and publication checks that can be adapted and implemented by the ML
                 community. Finally, we propose creating a centralized, sustainable repository system for archiving
                 datasets, tracking dataset modifications or deprecations, and facilitating practices of care and
                 stewardship that can be integrated into research and publication processes.",
  booktitle    = "2022 ACM Conference on Fairness, Accountability, and Transparency",
  pages        = "199–212",
  numpages     = "14",
  keywords     = "datasets, data stewardship data management dataset deprecation",
  location     = "Seoul, Republic of Korea",
  series       = "FAccT '22",
  cc-snippet   = "When it comes to filtering large text datasets scraped from the Web, given their sheer size (C4
                 represents 2.3 TB of data, whereas the Common Crawl has 139TB), filtering them is complex and
                 time-consuming, although approaches have been proposed for reducing duplicates and train-test overlap
                 [53]. [...] In practice, documenting and deprecating these datasets is akin to a game of whack-a-mole,
                 since new versions of the Common Crawl come out every few months. Analyzing what they contain and their
                 degrees of contamination through common evaluation tasks would take significant effort.",
  cc-author-affiliation = "Hugging Face; University of Southern California, USA; New York University, USA; Microsoft
                 Research, USA",
  cc-class     = "ai/ethics-of-machine-learning, nlp/text-corpora, nlp/corpus-construction, cc-cited-not-used",
}

@Article{cc:KreutzerCaswellWangWahabEtAl:2022:audit-web-multilingual-datasets,
  author       = "Kreutzer, Julia and Caswell, Isaac and Wang, Lisa and Wahab, Ahsan and van Esch, Daan and
                 Ulzii-Orshikh, Nasanbayar and Tapo, Allahsera and Subramani, Nishant and Sokolov, Artem and Sikasote,
                 Claytone and Setyawan, Monang and Sarin, Supheakmungkol and Samb, Sokhar and Sagot, Benoît and Rivera,
                 Clara and Rios, Annette and Papadimitriou, Isabel and Osei, Salomey and Suarez, Pedro Ortiz and Orife,
                 Iroro and Ogueji, Kelechi and Rubungo, Andre Niyongabo and Nguyen, Toan Q. and Müller, Mathias and
                 Müller, André and Muhammad, Shamsuddeen Hassan and Muhammad, Nanda and Mnyakeni, Ayanda and
                 Mirzakhalov, Jamshidbek and Matangira, Tapiwanashe and Leong, Colin and Lawson, Nze and Kudugunta,
                 Sneha and Jernite, Yacine and Jenny, Mathias and Firat, Orhan and Dossou, Bonaventure F. P. and
                 Dlamini, Sakhile and de Silva, Nisansa and Çabuk Ballı, Sakine and Biderman, Stella and Battisti,
                 Alessia and Baruwa, Ahmed and Bapna, Ankur and Baljekar, Pallavi and Azime, Israel Abebe and Awokoya,
                 Ayodele and Ataman, Duygu and Ahia, Orevaoghene and Ahia, Oghenefego and Agrawal, Sweta and Adeyemi,
                 Mofetoluwa",
  title        = "{Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets}",
  journal      = "Transactions of the Association for Computational Linguistics",
  volume       = "10",
  pages        = "50--72",
  year         = "2022",
  month        = "01",
  abstract     = "{With the success of large-scale pre-training and multilingual modeling in Natural Language Processing
                 (NLP), recent years have seen a proliferation of large, Web-mined text datasets covering hundreds of
                 languages. We manually audit the quality of 205 language-specific corpora released with five major
                 public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic
                 issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50\\%
                 sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language
                 codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and
                 supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and
                 improve multilingual corpora and discuss potential risks that come with low-quality data releases.}",
  ISSN         = "2307-387X",
  doi          = "10.1162/tacl_a_00447",
  URL          = "https://doi.org/10.1162/tacl\_a\_00447",
  eprint       = "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00447/1986585/tacl\_a\_00447.pdf",
  cc-class     = "nlp/corpus-construction, nlp/web-as-corpus, nlp/parallel-corpus, nlp/low-resource-language",
  cc-derived-dataset-about = "CCAligned-2020, Tensorflow-C4-Multilingual, OSCAR",
  cc-snippet   = "We selected the corpora for their multilinguality and the inclusion of understudied languages in NLP.
                 With the exception of WikiMatrix and Paracrawl, all corpora are derived from CommonCrawl, and
                 distinguish themselves by the choice of filtering methods, LangID and automatic alignment technology.",
  cc-author-affiliation = "Google Research; Masakhane NLP; Turkic Interlingua; Haverford College; RobotsMali; Intel
                 Labs; University of Zambia; Google; AIMS-AMMI; Inria; University of Zurich; Stanford University; Kwame
                 Nkrumah University of Science and Technology; Sorbonne Université; Niger-Volta LTI; University of
                 Waterloo; University of Electronic Science and Technology of China; University of Notre Dame; Bayero
                 University Kano; University of South Florida; Hugging Face; Jacobs University Bremen; University of
                 Moratuwa; EleutherAI; Obafemi Awolowo University; University of Ibadan; Instadeep; University of
                 Maryland; Defence Space Administration Abuja",
}
