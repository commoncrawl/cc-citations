@InProceedings{cc:MackenzieBenhamPetriTrippasEtAl:2020:cc-news-en,
  author       = "Mackenzie, Joel and Benham, Rodger and Petri, Matthias and Trippas, Johanne R. and Culpepper, J. Shane
                 and Moffat, Alistair",
  title        = "{CC}-News-En: {A} large English news corpus",
  year         = "2020",
  ISBN         = "978-1-4503-6859-9",
  publisher    = "Association for Computing Machinery",
  address      = "New York, NY, USA",
  URL          = "https://doi.org/10.1145/3340531.3412762",
  doi          = "10.1145/3340531.3412762",
  abstract     = "We describe a static, open-access news corpus using data from the Common Crawl Foundation, who provide
                 free, publicly available web archives, including a continuous crawl of international news articles
                 published in multiple languages. Our derived corpus, CC-News-En, contains 44 million English documents
                 collected between September 2016 and March 2018. The collection is comparable in size with the number
                 of documents typically found in a single shard of a large-scale, distributed search engine, and is four
                 times larger than the news collections previously used in offline information retrieval experiments. To
                 complement the corpus, 173 topics were curated using titles from Reddit threads, forming a temporally
                 representative sampling of relevant news topics over the 583 day collection window. Information needs
                 were then generated using automatic summarization tools to produce textual and audio representations,
                 and used to elicit query variations from crowdworkers, with a total of 10,437 queries collected against
                 the 173 topics. Of these, 10,089 include key-stroke level instrumentation that captures the timings of
                 character insertions and deletions made by the workers while typing their queries. These new resources
                 support a wide variety of experiments, including large-scale efficiency exercises and query
                 auto-completion synthesis, with scope for future addition of relevance judgments to support offline
                 effectiveness experiments and hence batch evaluation campaigns.",
  booktitle    = "Proceedings of the 29th ACM International Conference on Information & Knowledge Management",
  pages        = "3077--3084",
  numpages     = "8",
  keywords     = "corpus, user query variations, collection, news search, crowdsourcing",
  location     = "Virtual Event, Ireland",
  series       = "CIKM '20",
  cc-snippet   = "Our derived corpus, CC-News-En, contains 44 million English documents collected between September 2016
                 and March 2018. [...] One such example is the CommonCrawl Foundation,[¹ ] who generate large-scale
                 crawls of the web at regular intervals. A key philosophy behind the Common Crawlis to democratize data,
                 allowing open access with no fees. In late 2016, the Common Crawl Foundation announced a news-specific
                 crawl (CC-News), [² ] with documents being added on a daily basis, and covering sources from a wide
                 range of countries and languages. Here we derive a static, English segment of the CC-Newscrawl that we
                 refer to as CC-News-En. Due to the storage and computation costs involved in filtering out non-English
                 documents, we make the complete corpus available as a free resource, along with asuite of tools which
                 can be used to replicate corpus extraction from the original source CC-News data. We also provide a set
                 of 10,437 user query variations over 173 query topics, including keystroke-level data collected from a
                 novel crowdworking experiment. Our goal is to encourage reproducible and replicable experimentation,
                 with greatly reduced barriers to entry. [...] A total of 2,291 CC-News WARC files were processed to
                 build CC-News-En, covering the period 26 August 2016 to 31 March 2018, inclusive. The first and last
                 WARC files inthis collection are as follows: •CC-NEWS-20160826124520-00000.warc.gz
                 •CC-NEWS-20180331191315-00143.warc.gz The resulting subset of compressed WARC files occupies 2.14 TiB
                 of disk space, and contains a total of 102.5 million documents in over 100 languages. [...] Missing
                 Documents and Temporal Gaps. During the creation of the collection, the
                 CC-NEWS-20170812163812-00038.warc.gz file was not processed correctly by our pipeline, and was
                 subsequently dropped from the CC-News-En corpus. In addition, there are six days within the 583 day
                 period where no WARC files were added to the original CC-News crawl: 22/09/2016 – 25/09/2016
                 inclusive, 18/12/2017, and 22/12/2017. These gaps typically correspond to hardware and software
                 upgrades on the crawl servers.[¹⁸ Private correspondence with Common Crawl Engineers.] It is also
                 important to note that both CC-News and CC-News-En are not intended to be complete crawls of their
                 sources, but rather, to provide a reproducible sample of these sites.",
  cc-author-affiliation = "The University of Melbourne, Melbourne, Australia; RMIT University, Melbourne, Australia;
                 Amazon Alexa, Manhattan Beach, CA, USA",
  cc-class     = "nlp/text-corpora, nlp/corpus-construction, ir/information-extraction",
  cc-dataset-used = "CC-NEWS",
}

@InProceedings{cc:ElKishkyChaudharyGuzmanKoehn:2020:ccaligned,
  title        = "{CCA}ligned: {A} Massive collection of cross-lingual web-document pairs",
  author       = "El-Kishky, Ahmed and Chaudhary, Vishrav and Guzmán, Francisco and Koehn, Philipp",
  booktitle    = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
  year         = "2020",
  address      = "Online",
  publisher    = "Association for Computational Linguistics",
  URL          = "https://www.aclweb.org/anthology/2020.emnlp-main.480",
  pages        = "5960--5969",
  abstract     = "Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that
                 are of comparable content or translations of each other. In this paper, we exploit the signals embedded
                 in URLs to label web documents at scale with an average precision of 94.5{\%} across different language
                 pairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify web document pairs that
                 are translations of each other. We release a new web dataset consisting of over 392 million URL pairs
                 from Common Crawl covering documents in 8144 language pairs of which 137 pairs include English. In
                 addition to curating this massive dataset, we introduce baseline methods that leverage cross-lingual
                 representations to identify aligned documents based on their textual content. Finally, we demonstrate
                 the value of this parallel documents dataset through a downstream task of mining parallel sentences and
                 measuring the quality of machine translations from models trained on this mined data. Our objective in
                 releasing this dataset is to foster new research in cross-lingual NLP across a variety of low, medium,
                 and high-resource languages.",
  cc-snippet   = "[...] we exploit the signals embedded in URLs to label web documents at scale with an average
                 precision of 94.5{\%} across different language pairs. We mine sixty-eight snapshots of the Common
                 Crawl corpus and identify web document pairs that are translations of each other. We release a new web
                 dataset consisting of over 392 million URL pairs from Common Crawl covering documents in 8144 language
                 pairs of which 137 pairs include English. [...] Starting from 68 Com-mon Crawl snapshots with a raw
                 document count of 169.4 billion documents, upon deduplication, the resultant corpus is approximately
                 29.6 billion web documents from 107.8 million distinct web domains – a 83{\%} reduction from the raw
                 corpus.",
  cc-class     = "nlp/machine-translation, nlp/text-corpora, nlp/parallel-corpus, nlp/cross-lingual-document-alignment",
  cc-derived-dataset-about = "CCAligned-2020",
  cc-author-affiliation = "Facebook AI; Johns Hopkins University",
}

@Misc{cc:BrownMannRyderSubbiahEtAl:2020:language-models,
  title        = "Language models are few-shot learners",
  author       = "Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla
                 Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini
                 Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh
                 and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric
                 Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and
                 Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei",
  year         = "2020",
  eprint       = "2005.14165",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/2005.14165",
  cc-author-affiliation = "Johns Hopkins University; OpenAI",
  cc-class     = "nlp/language-model, ai/deep-learning, nlp/autoregressive-transformer-language-model,
                 nlp/question-answering, nlp/machine-translation, nlp/text-generation",
  cc-snippet   = "Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset [...]
                 constituting nearly a trillion words. [...] However, we have found that unfiltered or lightly filtered
                 versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3
                 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of
                 CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy
                 deduplication at the document level, within and across datasets, to prevent redundancy and preserve the
                 integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added
                 known high-quality reference corpora to the training mix to augment CommonCrawl and increase its
                 diversity. Details of the first two points (processing of Common Crawl) are described in Appendix A.",
}

@Misc{cc:JazbecPásztorFaltingsAntulov-FantulinEtAl:2020:news-and-information-transfer,
  title        = "On the impact of publicly available news and information transfer to financial markets",
  author       = "Metod Jazbec and Barna Pásztor and Felix Faltings and Nino Antulov-Fantulin and Petter N. Kolm",
  year         = "2020",
  URL          = "https://arxiv.org/abs/2010.12002",
  cc-author-affiliation = "ETH Zurich, Switzerland; New York University, New York, USA",
  abstract     = "We quantify the propagation and absorption of large-scale publicly available news articles from the
                 World Wide Web to financial markets. To extract publicly available information, we use the news
                 archives from the Common Crawl, a nonprofit organization that crawls a large part of the web. We
                 develop a processing pipeline to identify news articles associated with the constituent companies in
                 the S&P 500 index, an equity market index that measures the stock performance of U.S. companies. Using
                 machine learning techniques, we extract sentiment scores from the Common Crawl News data and employ
                 tools from information theory to quantify the information transfer from public news articles to the
                 U.S. stock market. Furthermore, we analyze and quantify the economic significance of the news-based
                 information with a simple sentiment-based portfolio trading strategy. Our findings provides support for
                 that information in publicly available news on the World Wide Web has a statistically and economically
                 significant impact on events in financial markets.",
  cc-class     = "statistical-finance; ai/machine-learning; nlp/sentiment-analysis",
  cc-snippet   = "In this article, we use news articles from the Common Crawl News, a subset of the Common Crawl’s
                 petabytes of publicly available World Wide Web archives, to measure the impact of the arrival of new
                 information about the constituent stocks in the S&P 500 index at the time of publishing. To the best of
                 our knowledge, our study is the first one to use the Common Crawl in this way. We develop a cloud-based
                 processing pipeline that identifies news articles in the Common Crawl News data that are related to the
                 companies in the S&P 500. As the Common Crawl public data archives are getting bigger, they are opening
                 doors for many real-world “data-hungry” applications such as transformers models GPT49 and BERT50,
                 a recent class of deep learning language models. We believe that public sources of news data is
                 important not only for natural language processing (NLP) and finance communities but also for more
                 general studies in complex systems and computational social sciences that are aiming to characterize
                 (mis)information propagation and dynamics in techno-socio-economic systems. The abundance of
                 high-frequency data around the financial systems enables complex systems researchers to have
                 microscopic observables that allow verification of different models, theories, and hypotheses.",
  cc-dataset-used = "CC-NEWS",
}

@Misc{cc:SquarcinaTempestaVeroneseCalzavaraEtAl:2020:related-domain-attacks,
  title        = "Can {I} take your subdomain? Exploring related-domain attacks in the modern web",
  author       = "Marco Squarcina and Mauro Tempesta and Lorenzo Veronese and Stefano Calzavara and Matteo Maffei",
  year         = "2020",
  URL          = "https://arxiv.org/abs/2012.01946",
  pdf          = "https://arxiv.org/pdf/2012.01946.pdf",
  cc-author-affiliation = "TU Wien, Austria; Università Ca’ Foscari Venezia, Italy",
  cc-class     = "computer-security/internet-security, related-domain attacks",
  cc-snippet   = "Our web security analysis aims at quantifying the number of domains hosting web applications that can
                 be exploited by taking over the vulnerable domains discovered by RDScan. In particular, for every apex
                 domain with at least one vulnerable subdomain, we selected from the CommonCrawl dataset [¹⁹ Common
                 Crawl. Host- and domain-level webgraphs feb/mar/may 2020.
                 https://commoncrawl.org/2020/06/host-and-domain-level-web-graphs-febmarmay-2020/, 2020.] the list of
                 200 most popular related-domains according to the Pagerank score [11]. From the homepage of these
                 domains,we extracted the same-origin links that appear in the HTML code.",
  cc-dataset-used = "hyperlinkgraph/cc-main-2020-feb-mar-may/hostgraph",
}

@Article{cc:RaffelShazeerRobertsLeeEtAl:2020:Exploring-limits-of-transfer,
  title        = "Exploring the limits of transfer learning with a unified text-to-text transformer",
  author       = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena,
                 Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.",
  journal      = "Journal of Machine Learning Research",
  volume       = "21",
  number       = "140",
  pages        = "1--67",
  year         = "2020",
  URL          = "http://jmlr.org/papers/v21/20-074.html",
  pdf          = "https://www.jmlr.org/papers/volume21/20-074/20-074.pdf",
  abstract     = "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a
                 downstream task, has emerged as a powerful technique in natural language processing (NLP). The
                 effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and
                 practice. In this paper, we explore the landscape of transfer learning techniques for NLP by
                 introducing a unified framework that converts all text-based language problems into a text-to-text
                 format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets,
                 transfer approaches, and other factors on dozens of language understanding tasks. By combining the
                 insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve
                 state-of-the-art results on many benchmarks covering summarization, question answering, text
                 classification, and more. To facilitate future work on transfer learning for NLP, we release our data
                 set, pre-trained models, and code.",
  cc-author-affiliation = "Google, Mountain View, CA, USA",
  cc-derived-dataset-about = "tensorflow-c4",
  cc-dataset-used = "CC-MAIN-2019-18 (WET)",
  cc-snippet   = "We also introduce our approach for treating every problem as a text-to-text task and describe our
                 “Colossal Clean Crawled Corpus” (C4), the Common Crawl-based data set we created as a source of
                 unlabeled text data. [...] Common Crawl is a publicly-available web archive that provides “web
                 extracted text”by removing markup and other non-text content from the scraped HTML files. This
                 process produces around 20TB of scraped text data each month. Unfortunately, the majority of the
                 resulting text is not natural language. Instead, it largely comprises gibberish or boiler-plate text
                 like menus, error messages, or duplicate text. Furthermore, a good deal of the scraped text contains
                 content that is unlikely to be helpful for any of the tasks we consider (offensive language,
                 placeholder text, source code, etc.). To address these issues, we used the following heuristics for
                 cleaning up Common Crawl’s web extracted text: [...] To assemble our base data set, we downloaded the
                 web extracted text from April 2019and applied the aforementioned filtering. This produces a collection
                 of text that is not only orders of magnitude larger than most data sets used for pre-training (about
                 750 GB) but also comprises reasonably clean and natural English text. We dub this data set the
                 “Colossal Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets.⁸
                 [⁸https://www.tensorflow.org/datasets/catalog/c4]",
  cc-class     = "nlp/corpus-construction, nlp/language-model",
}

@Book{cc:Patel:2020:structured-data-from-internet,
  title        = "Getting structured data from the internet",
  author       = "Jay M. Patel",
  year         = "2020",
  doi          = "https://doi.org/10.1007/978-1-4842-6576-5",
  publisher    = "Apress",
  URL          = "https://www.apress.com/gp/book/9781484265758",
  cc-author-affiliation = "Specrom Analytics, Ahmedabad, India",
  cc-snippet   = "[Chapter 6: Introduction to Common Crawl Datasets + Chapter 7: Web Crawl Processing on Big Data
                 Scale]",
  cc-class     = "web-mining",
}
