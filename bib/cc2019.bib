@Book{cc:BrueggerMiligan:2018:SAGE-handbook-web-history,
  title        = "The {SAGE} Handbook of Web History",
  author       = "Brügger, Nils and Milligan, Ian",
  year         = "2019",
  URL          = "https://us.sagepub.com/en-us/nam/the-sage-handbook-of-web-history/book252251",
  publisher    = "SAGE Publications Limited",
  cc-author-affiliation = "Aarhus University, Denmark; University of Waterloo, Canada",
  cc-class     = "web-science, web history",
}

@Article{cc:WenzekLachauxConneauChaudharyEtAl:2019:CCNet,
  author       = "Guillaume Wenzek and Marie-Anne Lachaux and Alexis Conneau and Vishrav Chaudhary and Francisco Guzmán
                 and Armand Joulin and Edouard Grave",
  title        = "{CCN}et: Extracting high quality monolingual datasets from web crawl data",
  journal      = "CoRR",
  year         = "2019",
  URL          = "https://arxiv.org/abs/1911.00359",
  eprint       = "1911.00359",
  timestamp    = "Mon, 11 Nov 2019 18:38:09 +0100",
  cc-author-affiliation = "Facebook AI",
  abstract     = "Pre-training text representations have led to significant improvements in many areas of natural
                 language processing. The quality of these models benefits greatly from the size of the pretraining
                 corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to
                 extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our
                 pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018),
                 that deduplicates documents and identifies their language. We augment this pipeline with a filtering
                 step to select documents that are close to high quality corpora like Wikipedia.",
  cc-snippet   = "[about https://github.com/facebookresearch/cc_net] In this paper, we present a data collection
                 pipeline that allows to gather massive monolingual corpora of high quality in a variety of languages,
                 including many low-resource ones. The principles of our pipeline are general and we show the results of
                 its application to data collected by the Common Crawl project.¹ Common Crawl is a massive non-curated
                 dataset of webpages in many languages, mixed together in temporal snapshots of the web.",
  cc-class     = "nlp/corpus-construction, nlp/web-as-corpus, nlp/low-resource-language",
}

@InProceedings{cc:RadfordWuChildLuanEtAl:2019:language-models,
  title        = "Language models are unsupervised multitask learners",
  author       = "A. Radford and Jeffrey Wu and R. Child and David Luan and Dario Amodei and Ilya Sutskever",
  year         = "2019",
  URL          = "https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe",
  cc-snippet   = "A promising source of diverse and nearly unlimited text is web scrapes such as Common Crawl. While
                 these archives are many orders of magnitude larger than current language modeling datasets, they have
                 significant data quality issues. Trinh & Le (2018) used Common Crawl in their work on commonsense
                 reasoning but noted a large amount of documents “whose content are mostly unintelligible”. We
                 ob-served similar data issues in our initial experiments with Common Crawl. Trinh & Le (2018)’s best
                 results were achieved using a small subsample of Common Crawl which included only documents most
                 similar to their target dataset,the Winograd Schema Challenge. While this is a pragmatic approach to
                 improve performance on a specific task, we want to avoid making assumptions about the tasks to be
                 performed ahead of time.Instead, we created a new web scrape which emphasizes document quality. To do
                 this we only scraped web pages which have been curated/filtered by humans. Manually filtering a full
                 web scrape would be exceptionally expensive so as a starting point, we scraped all outbound links from
                 Reddit, a social media platform, which received at least 3 karma. This can be thought of as a heuristic
                 indicator for whether other users found the link interesting, educational, or just funny. The resulting
                 dataset, WebText, contains the text subsetof these 45 million links.",
  cc-class     = "cc-cited-not-used",
  cc-author-affiliation = "OpenAI, San Francisco, California, United States",
}

@InProceedings{cc:Ortiz-SuarezSagotRomary:2019:processing-huge-corpora,
  title        = "{Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures}",
  author       = "Ortiz Suárez, Pedro Javier and Sagot, Benoît and Romary, Laurent",
  URL          = "https://hal.inria.fr/hal-02148693",
  booktitle    = "7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)",
  address      = "Cardiff, United Kingdom",
  editor       = "Piotr Bański and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc
                 Kupietz and Harald Lüngen and Caroline Iliadi",
  publisher    = "{Leibniz-Institut für Deutsche Sprache}",
  year         = "2019",
  doi          = "10.14618/IDS-PUB-9021",
  pdf          = "https://hal.inria.fr/hal-02148693/file/Asynchronous_Pipeline_for_Processing_Huge_Corpora_on_Medium_to_Low_Resource_Infrastructures.pdf",
  cc-author-affiliation = "Inria, Paris, France; Sorbonne University, Paris, France",
  cc-class     = "nlp/corpus-construction",
  cc-derived-dataset-about = "OSCAR",
  cc-dataset-used = "CC-MAIN-2018-47 (WET)",
  cc-snippet   = "We use the November 2018 snapshot which surpasses 20TB of uncompressed data and contains more than 50
                 thousand plain text files where each file consists of the plain text from multiple websites along its
                 metadata header. From now on, when we mention the “Common Crawl” corpus, we refer to this
                 particular November 2018 snapshot.",
}

@MastersThesis{cc:Mottl:2019:branchenklassifikation,
  author       = "Dominik Mottl",
  title        = "Multi-Label Branchenklassifikation von Web-Texten",
  year         = "2019",
  pdf          = "https://fbmn.h-da.de/uploads/Themen/WS18_thesis_mottl.pdf",
  cc-snippet   = "NER of company names and linking to DBpedia performed on English texts in 712 WET files of November
                 2018 crawl (CC-MAIN-2018-47) using cc-pyspark.",
  cc-author-affiliation = "Hochschule Darmstadt, Germany",
  cc-class     = "nlp/NER, entity-linking",
}

@Misc{cc:Nagel:2019:accessing-warc-files-via-sql,
  author       = "Nagel, Sebastian",
  title        = "Accessing {WARC} files via {SQL}",
  year         = "2019",
  howpublished = "Poster at IIPC Web Archiving Conference, 6–7 June 2019, Zagreb, Croatia",
  pdf          = "https://netpreserve.org/ga2019/wp-content/uploads/2019/07/IIPCWAC2019-SEBASTIAN_NAGEL-Accessing_WARC_files_via_SQL-poster.pdf",
  URL          = "https://digital.library.unt.edu/ark:/67531/metadc1608961/",
  cc-author-affiliation = "Common Crawl, USA",
  cc-dataset-used = "cc-index-table",
  cc-class     = "web-archiving, SQL, Parquet",
}

@Misc{cc:LiuOttGoyalDuEtAl:2019:RoBERTa-Robustly-Optimized-BERT,
  title        = "Ro{BERT}a: {A} Robustly Optimized {BERT} Pretraining Approach",
  author       = "Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy
                 and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov",
  year         = "2019",
  eprint       = "1907.11692",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/1907.11692",
  pdf          = "https://arxiv.org/pdf/1907.11692.pdf",
  cc-author-affiliation = "Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle,
                 WA, USA; Facebook AI",
  cc-class     = "nlp/corpus-creation, nlp/language-model",
  cc-dataset-used = "CC-NEWS",
  cc-snippet   = "We find that BERT was significantly undertrained and propose an improved recipe for training BERT
                 models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT
                 methods. Our modifications are simple, they include: (1) training the model longer, with bigger
                 batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer
                 sequences; and (4) dynamically changing the masking pattern applied to the training data. We also
                 collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better
                 control for training set size effects. [...] CC-NEWS, which we collected from the English portion of
                 the CommonCrawl News dataset (Nagel, 2016). The data contains 63 million English news articles crawled
                 between September 2016 and February 2019. (76GB after filtering).⁴ [⁴ We use news-please (Hamborg
                 et al.,2017) to collect and extract CC-NEWS. CC-NEWS is similar to the REALNEWS dataset described in
                 Zellers et al. (2019).]",
}
