@InProceedings{cc:PenningtonSocherManning:2014:GloVe-word-vectors,
  title        = "{GloVe}: Global vectors for word representation",
  author       = "Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.",
  booktitle    = "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
  pages        = "1532--1543",
  year         = "2014",
  URL          = "https://aclanthology.org/D14-1162.pdf",
  cc-author-affiliation = "Stanford University, California, USA",
  cc-class     = "nlp/word-embeddings",
  cc-snippet   = "We trained our model on five corpora of varying sizes: [...] and on 42 billion tokens of web data,
                 from Common Crawl⁵ [⁵ To demonstrate the scalability of the model, we also trained it on a much
                 larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase
                 the vocabulary, so the results are not directly comparable.].",
}
