@Article{cc:ChibaNakanoKoide:2025:DomainHarvester,
  author       = "Chiba, Daiki and Nakano, Hiroki and Koide, Takashi",
  journal      = "IEEE Access",
  title        = "{DomainHarvester}: Uncovering Trustworthy Domains Beyond Popularity Rankings",
  year         = "2025",
  volume       = "13",
  pages        = "28167--28188",
  keywords     = "Virtual assistants; Accuracy; Feature extraction; Domain Name System; Accesslists; Uniform resource
                 locators; Reliability; Postal services; Message authentication; Measurement; Web security; domain name;
                 top list; allow list",
  DOI          = "10.1109/ACCESS.2025.3539882",
  URL          = "https://ieeexplore.ieee.org/abstract/document/10877793",
  abstract     = "Allow lists are crucial in cybersecurity for distinguishing safe websites from potential threats.
                 Traditional approaches relying on website popularity often fail to capture trustworthy but less-visited
                 domains, leading to increased false positives and overlooked niche websites. This paper presents
                 DomainHarvester, an innovative bottom-up system that leverages the web’s hyperlink structure and a
                 Transformer-based machine learning approach to systematically identify and include these
                 underrepresented yet legitimate domains. Results demonstrate how DomainHarvester dynamically curates an
                 expanded allow list (DHList), substantially reducing the risk of false positives while retaining high
                 precision in excluding malicious sites. Comprehensive evaluations and a real-world case study with a
                 managed security services provider illustrate the efficacy and practicality of this approach. By
                 integrating DomainHarvester, organizations and researchers can benefit from a more inclusive and
                 globally representative cybersecurity allow list, addressing limitations in existing top-list-based
                 solutions.",
  cc-author-affiliation = "NTT Security Holdings Corporation & NTT Corporation, Tokyo, Japan",
  cc-class     = "web-science, domain-ranking, internet-security, cc-citet-not-used",
  cc-snippet   = "Top lists serve a variety of purposes across many services. [...] They serve as data sources for web
                 crawling in projects like Common Crawl [39] [³⁹Common Crawl, ‘‘Common Crawl - Open Repository of
                 Web Crawl Data,’’ 2024. [Online]. Available: https://commoncrawl.org/], which provides data for AI
                 training, including models like ChatGPT.",
}

@Misc{cc:HosseinbeigiTaherinezhadFailiBaghbaniEtAl:2025:Matina,
  title        = "Matina: {A} Large-Scale 73{B} Token Persian Text Corpus",
  author       = "Sara Bourbour Hosseinbeigi and Fatemeh Taherinezhad and Heshaam Faili and Hamed Baghbani and Fatemeh
                 Nadi and Mostafa Amiri",
  year         = "2025",
  eprint       = "2502.09188",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/2502.09188",
  pdf          = "https://arxiv.org/pdf/2502.09188",
  cc-author-affiliation = "Tarbiat Modares University, Iran; University of Tehran, Iran",
  cc-class     = "nlp/corpus-construction, nlp/language-specific-corpus, nlp/language-model, nlp/large-language-models",
  abstract     = "Text corpora are essential for training models used in tasks like summarization, translation, and
                 large language models (LLMs). While various efforts have been made to collect monolingual and
                 multilingual datasets in many languages, Persian has often been underrepresented due to limited
                 resources for data collection and preprocessing. Existing Persian datasets are typically small and lack
                 content diversity, consisting mainly of weblogs and news articles. This shortage of high-quality,
                 varied data has slowed the development of NLP models and open-source LLMs for Persian. Since model
                 performance depends heavily on the quality of training data, we address this gap by introducing the
                 Matina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed and deduplicated to ensure
                 high data quality. We further assess its effectiveness by training and evaluating transformer-based
                 models on key NLP tasks. Both the dataset and preprocessing codes are publicly available, enabling
                 researchers to build on and improve this resource for future Persian NLP advancements.",
  cc-snippet   = "Matina’s web-based data is divided into two parts: data crawled by our team and data taken from two
                 public databases using the Common Crawl (Crawl, 2008) dataset. This dual-source strategy uses both
                 proprietary and publically available data to increase the corpus’s breadth and diversity. In any
                 language, certain domains are recognized for their reliability and high-quality information. We
                 identified such domains in Persian and crawled them to extract relevant textual content. This step
                 helped minimize the inclusion of irrelevant elements such as advertisements, tags, or comments. Text
                 extracted from headings and paragraphs was merged to form unified documents, with additional
                 informative fields (e.g., summaries or subheadings) incorporated as metadata, if available. Because
                 these domains were manually selected, language detection and URL filtering were unnecessary. We also
                 ensured that the selected URLs did not contain harmful, sensitive, or adult content.",
}

@Misc{cc:KoenigRauchWoerter:2025:Monitoring-of-economic-shocks,
  title        = "Real-time Monitoring of Economic Shocks using Company Websites",
  author       = "Michael Koenig and Jakob Rauch and Martin Woerter",
  year         = "2025",
  eprint       = "2502.17161",
  archiveprefix = "arXiv",
  primaryclass = "econ.GN",
  keywords     = "large language models, natural language processing, crisis, economic shocks, economic monitoring,
                 Covid-19",
  URL          = "https://arxiv.org/abs/2502.17161",
  abstract     = "Understanding the effects of economic shocks on firms is critical for analyzing economic growth and
                 resilience. We introduce a Web-Based Affectedness Indicator (WAI), a general-purpose tool for real-time
                 monitoring of economic disruptions across diverse contexts. By leveraging Large Language Model (LLM)
                 assisted classification and information extraction on texts from over five million company websites,
                 WAI quantifies the degree and nature of firms' responses to external shocks. Using the COVID-19
                 pandemic as a specific application, we show that WAI is highly correlated with pandemic containment
                 measures and reliably predicts firm performance. Unlike traditional data sources, WAI provides timely
                 firm-level information across industries and geographies worldwide that would otherwise be unavailable
                 due to institutional and data availability constraints. This methodology offers significant potential
                 for monitoring and mitigating the impact of technological, political, financial, health or
                 environmental crises, and represents a transformative tool for adaptive policy-making and economic
                 resilience.",
  cc-author-affiliation = "ETH Zurich, Switzerland; Vrije Universiteit Amsterdam, The Netherlands; Centre for Economic
                 Policy Research (CEPR), London, United Kingdom",
  cc-class     = "economics, economic-monitoring, web-archiving, nlp/large-language-models",
  cc-snippet   = "We extract content from company websites in CommonCrawl, classify Covid-19 impact using a large
                 language model, and track changes over time to analyze firm-level impacts across sectors and different
                 geographies.",
}

@Misc{cc:FiazTahirShamsHussain:2025:UrduLLaMA-1.0-dataset,
  title        = "Urdu{LL}a{MA} 1.0: Dataset Curation, Preprocessing, and Evaluation in Low-Resource Settings",
  author       = "Layba Fiaz and Munief Hassan Tahir and Sana Shams and Sarmad Hussain",
  year         = "2025",
  eprint       = "2502.16961",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/2502.16961",
  cc-author-affiliation = "University of Engineering and Technology, Lahore, Pakistan",
  cc-class     = "nlp/language-model, nlp/text-corpora, nlp/low-resource-language",
  cc-derived-dataset-used = "CC-100, OSCAR",
  cc-snippet   = "[...] we supplemented our in-house dataset with data from several publicly available sources,
                 including CC-100 (Wenzek et al., 2020), the Urdu corpus from OSCAR (Ortiz Suárez et al., 2020), the
                 Urdu Web Corpus (Shafiq et al., 2020), Urdu data from XLSum (Hasan et al., 2021), and (Goldhahn et al.,
                 2012).",
}

@Misc{cc:LongpreSinghCherepTiwaryEtAl:2025:Data-provenance-gap,
  title        = "Bridging the Data Provenance Gap Across Text, Speech and Video",
  author       = "Shayne Longpre and Nikhil Singh and Manuel Cherep and Kushagra Tiwary and Joanna Materzynska and
                 William Brannon and Robert Mahari and Naana Obeng-Marnu and Manan Dey and Mohammed Hamdy and Nayan
                 Saxena and Ahmad Mustafa Anis and Emad A. Alghamdi and Vu Minh Chien and Da Yin and Kun Qian and Yizhi
                 Li and Minnie Liang and An Dinh and Shrestha Mohanty and Deividas Mataciunas and Tobin South and
                 Jianguo Zhang and Ariel N. Lee and Campbell S. Lund and Christopher Klamm and Damien Sileo and Diganta
                 Misra and Enrico Shippole and Kevin Klyman and Lester JV Miranda and Niklas Muennighoff and Seonghyeon
                 Ye and Seungone Kim and Vipul Gupta and Vivek Sharma and Xuhui Zhou and Caiming Xiong and Luis Villa
                 and Stella Biderman and Alex Pentland and Sara Hooker and Jad Kabbara",
  year         = "2025",
  eprint       = "2412.17847",
  archiveprefix = "arXiv",
  primaryclass = "cs.AI",
  URL          = "https://arxiv.org/abs/2412.17847",
  cc-author-affiliation = "The Data Provenance Initiative",
  cc-class     = "dataset-creation, dataset-curation, data-provenance, cc-not-used, cc-not-cited",
}

@Misc{cc:YuLiuXiong:2025:Craw4LLM-Efficient-Web-Crawling,
  title        = "Craw4{LLM}: Efficient Web Crawling for {LLM} Pretraining",
  author       = "Shi Yu and Zhiyuan Liu and Chenyan Xiong",
  year         = "2025",
  eprint       = "2502.13347",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/2502.13347",
  cc-author-affiliation = "Tsinghua University, China; Carnegie Mellon University, USA",
  cc-class     = "web-crawling, nlp/web-as-corpus, nlp/text-corpora, nlp/large-language-models, cc-cited-not-used",
  cc-snippet   = "Pretraining datasets are typically built from large-scale web crawls such as Common Crawl
                 (CommonCrawl, 2007), which may contain TBs of data spanning billions of web-pages (Penedo et al., 2024;
                 Weber et al., 2024). [...] Existing work often discards over 90\% of the raw data collected from the
                 web (Li et al., 2024; Penedo et al., 2024; Tang et al., 2024) highlighting the inefficiency of current
                 web crawlers in collecting LLM pretraining data. Common web crawlers like Common Crawl prioritize pages
                 based on graph connectivity metrics like PageRank (Page et al., 1999; Cho et al., 1998) or harmonic
                 centrality (Boldi and Vigna, 2014; Baack, 2024), which favor documents with a high number of inlinks
                 (indegree) (Fortunato et al., 2008) rather than those most relevant for pretraining. This misalignment
                 not only leads to waste in computational resources during excessive data processing for LLM developers,
                 but also incentivizes over-crawling, which burdens website operators with redundant traffic and
                 increases ethical and legal risks related to fair use of data and copyright (Longpre et al., 2024; New
                 York Times, 2023). [...] CRAW4LLM achieves the same performance while crawling only 21\% of the
                 documents required by the indegree-based crawler, or 48\% when considering all visited documents. These
                 results highlight the efficiency of CRAW4LLM, demonstrating its potential to reduce website burdens and
                 mitigate over-crawling.",
}

@Misc{cc:BaackBidermanOdrozekSkowronEtAl:2025:Open-datasets-for-LLM-training,
  title        = "Towards Best Practices for Open Datasets for {LLM} Training",
  author       = "Stefan Baack and Stella Biderman and Kasia Odrozek and Aviya Skowron and Ayah Bdeir and Jillian
                 Bommarito and Jennifer Ding and Maximilian Gahntz and Paul Keller and Pierre-Carl Langlais and Greg
                 Lindahl and Sebastian Majstorovic and Nik Marda and Guilherme Penedo and Maarten Van Segbroeck and
                 Jennifer Wang and Leandro von Werra and Mitchell Baker and Julie Belião and Kasia Chmielinski and
                 Marzieh Fadaee and Lisa Gutermuth and Hynek Kydlíček and Greg Leppert and EM Lewis-Jong and Solana
                 Larsen and Shayne Longpre and Angela Oduor Lungati and Cullen Miller and Victor Miller and Max Ryabinin
                 and Kathleen Siminyu and Andrew Strait and Mark Surman and Anna Tumadóttir and Maurice Weber and
                 Rebecca Weiss and Lee White and Thomas Wolf",
  year         = "2025",
  eprint       = "2501.08365",
  archiveprefix = "arXiv",
  primaryclass = "cs.CY",
  URL          = "https://arxiv.org/abs/2501.08365",
  cc-author-affiliation = "",
  cc-class     = "dataset-creation, dataset-curation, data-provenance, license/creative-commons, license/public-domain",
  abstract     = "Many AI companies are training their large language models (LLMs) on data without the permission of
                 the copyright owners. The permissibility of doing so varies by jurisdiction: in countries like the EU
                 and Japan, this is allowed under certain restrictions, while in the United States, the legal landscape
                 is more ambiguous. Regardless of the legal status, concerns from creative producers have led to several
                 high-profile copyright lawsuits, and the threat of litigation is commonly cited as a reason for the
                 recent trend towards minimizing the information shared about training datasets by both corporate and
                 public interest actors. This trend in limiting data information causes harm by hindering transparency,
                 accountability, and innovation in the broader ecosystem by denying researchers, auditors, and impacted
                 individuals access to the information needed to understand AI models. While this could be mitigated by
                 training language models on open access and public domain data, at the time of writing, there are no
                 such models (trained at a meaningful scale) due to the substantial technical and sociological
                 challenges in assembling the necessary corpus. These challenges include incomplete and unreliable
                 metadata, the cost and complexity of digitizing physical records, and the diverse set of legal and
                 technical skills required to ensure relevance and responsibility in a quickly changing landscape.
                 Building towards a future where AI systems can be trained on openly licensed data that is responsibly
                 curated and governed requires collaboration across legal, technical, and policy domains, along with
                 investments in metadata standards, digitization, and fostering a culture of openness.",
  cc-derived-dataset-about = "Common-Pile",
}

@Misc{cc:ChangHe:2025:Liabilities-of-Robotstxt,
  title        = "The Liabilities of Robots.txt",
  author       = "Chien-yi Chang and Xin He",
  year         = "2025",
  eprint       = "2503.06035",
  archiveprefix = "arXiv",
  primaryclass = "cs.CY",
  URL          = "https://arxiv.org/abs/2503.06035",
  abstract     = "The robots.txt file, introduced as part of the Robots Exclusion Protocol in 1994, provides webmasters
                 with a mechanism to communicate access permissions to automated bots. While broadly adopted as a
                 community standard, the legal liabilities associated with violating robots.txt remain ambiguous. The
                 rapid rise of large language models, which depend on extensive datasets for training, has amplified
                 these challenges, prompting webmasters to increasingly use robots.txt to restrict the activities of
                 bots engaged in large-scale data collection. This paper clarifies the liabilities associated with
                 robots.txt within the contexts of contract, copyright, and tort law. Drawing on key cases, legal
                 principles, and scholarly discourse, it proposes a legal framework for web scraping disputes. It also
                 addresses the growing fragmentation of the internet, as restrictive practices by webmasters threaten
                 the principles of openness and collaboration. Through balancing innovation with accountability, this
                 paper offers insights to ensure that robots.txt remains an equitable protocol for the internet and thus
                 contributes to digital governance in the age of AI.",
  cc-author-affiliation = "Faculty of Law, University of Hong Kong, Hong Kong SAR",
  cc-class     = "robots.txt, web-crawling, legal/copyright, ai/ethics-of-machine-learning",
}

@Article{cc:KimSohnJoChoiEtAl:2025:Do-not-trust-licenses-you-see,
  title        = "Do Not Trust Licenses You See--Dataset Compliance Requires Massive-Scale {AI}-Powered Lifecycle
                 Tracing",
  author       = "Kim, Jaekyeom and Sohn, Sungryull and Jo, Gerrard Jeongwon and Choi, Jihoon and Bae, Kyunghoon and
                 Lee, Hwayoung and Park, Yongmin and Lee, Honglak",
  journal      = "arXiv preprint arXiv:2503.02784",
  year         = "2025",
  URL          = "https://arxiv.org/abs/2503.02784",
  pdf          = "https://asset-nexus.lgresearch.ai/pdf/Do_Not_Trust_Licenses_You_See.pdf",
  abstract     = "This paper argues that a dataset's legal risk cannot be accurately assessed by its license terms
                 alone; instead, tracking dataset redistribution and its full lifecycle is essential. However, this
                 process is too complex for legal experts to handle manually at scale. Tracking dataset provenance,
                 verifying redistribution rights, and assessing evolving legal risks across multiple stages require a
                 level of precision and efficiency that exceeds human capabilities. Addressing this challenge
                 effectively demands AI agents that can systematically trace dataset redistribution, analyze compliance,
                 and identify legal risks. We develop an automated data compliance system called NEXUS and show that AI
                 can perform these tasks with higher accuracy, efficiency, and cost-effectiveness than human experts.
                 Our massive legal analysis of 17,429 unique entities and 8,072 license terms using this approach
                 reveals the discrepancies in legal rights between the original datasets before redistribution and their
                 redistributed subsets, underscoring the necessity of the data lifecycle-aware compliance. For instance,
                 we find that out of 2,852 datasets with commercially viable individual license terms, only 605 (21\%)
                 are legally permissible for commercialization. This work sets a new standard for AI data governance,
                 advocating for a framework that systematically examines the entire lifecycle of dataset redistribution
                 to ensure transparent, legal, and responsible dataset management.",
  cc-author-affiliation = "LG AI Research",
  cc-class     = "legal/copyright, ai/ethics-of-machine-learning, robots.txt, web-crawling",
}

@Article{cc:Havlikova:2025:rightsholders-opt-out-from-gen-ai-training,
  title        = "Technical Challenges of Rightsholders’ Opt-out From {Gen} {AI} Training after {Robert} {Kneschke} v.
                 {LAION}",
  volume       = "16",
  ISSN         = "2190-3387",
  URL          = "https://www.jipitec.eu/jipitec/article/view/422",
  language     = "en",
  number       = "1",
  urldate      = "2025-03-30",
  journal      = "JIPITEC – Journal of Intellectual Property, Information Technology and E-Commerce Law",
  author       = "Havlikova, Stepanka",
  month        = mar,
  year         = "2025",
  keywords     = "Artificial Intelligence, Copyright, Machine-readable, Text and Data Mining, Web Scraping",
  abstract     = "This paper explores the evolving legal landscape surrounding generative AI model training on publicly
                 available - often copyrighted - data, spot-lighting the challenges in the wake of recent decision of
                 German Court in Robert Kneschke v. LAION. On top of already explored implementation of copyright res-
                 ervations by machine-to-machine and human-to-machine communication, this paper explores potential gaps
                 and technical challenges stemming from the text and data mining exception including technical is- sues
                 surrounding Robots.txt as well as data memorisation and regurgitation of verbatim snippets in AI
                 outputs.",
  cc-author-affiliation = "Institute of Law and Technology at Masaryk University, Czech Republic",
  cc-class     = "legal/copyright, robots.txt, text-and-data-mining, nlp/generative-language-models,
                 nlp/large-language-models",
  cc-snippet   = "Common Crawl, non-profit foundation producing and maintaining an open repository of web crawl
                 data,⁹⁴ published its recommended structure of Robots.txt to prevent Common Crawl from crawling a
                 website and recommended implementing “CCBot” to the user-agent line.⁹⁵ According to a study
                 published in 2020, OpenAI’s GPT-3 was trained using data mostly collected from Common Crawl.⁹⁶ On
                 the other hand, Common Crawl is used for a variety of other purposes unrelated to generative artificial
                 intelligence.⁹⁷ [...] Although the Common Crawl Foundation proclaims to comply with Robots.txt and
                 no follow policies of the scraped websites (for these purposes the Common Crawl Foundation even issued
                 its own Robots.txt guidance recommending implementing “CCBot” to the user-agent line¹²³), at the
                 same time Common Crawl’s publicly available Terms of use explicitly limit Common Crawl’s liability
                 for third party IP infringements and explicitly state that Crawled Content may be subject to separate
                 terms of use or terms of service from the owners of such Crawled Content.¹²⁴ These aspects add
                 additional layer of complexity in potential disputes over lawfulness of text and data mining.",
}

@Article{cc:Awad:2025:Generative-AIs-copyright-enigma,
  title        = "Generative {AI}'s Copyright Enigma: {A} Comparative Study of Fair Use and Fair Dealing",
  volume       = "14",
  URL          = "https://www.repository.law.indiana.edu/ipt/vol14/iss2/2",
  shorttitle   = "Generative {AI}'s Copyright Enigma",
  number       = "2",
  journaltitle = "{IP} Theory: Vol. 14: Iss. 2, Article 2",
  author       = "Awad, Taysir",
  date         = "2025-01-01",
  year         = "2025",
  abstract     = "At the dawn of this decade, generative Artificial Intelligence (AI) models were at the apogee of
                 modern science and technology. Their emergence introduced the world to a new paradigm of creativity and
                 innovation, where machines can synthesize art, literature, and design with unprecedented
                 sophistication, blurring the boundaries between human ingenuity and algorithmic computation. These
                 models have the capacity to regenerate Oscar Wilde with the depiction of Ansel Adams, rewrite Harry
                 Potter with William Shakespear’s proverbial tongue, and redesign St. Peter’s Basilica with Gothic
                 arches, Seljuk carved stones, and an Antoni Gaudi roof architecture, relocated in the heart of New York
                 City with the facile of a prompt. Despite this novelty, the copyright industry was perturbed by what
                 they considered a twofold threat to their livelihood. On the one hand, authors feared their works were
                 being exploited, likely reproduced, without adequate remuneration. On the other hand, they feared these
                 models were ominously generating new works, possibly derivative works, that directly compete with the
                 very works that were used in the model’s formation. The matter is currently being adjudicated in
                 courts across the globe.¶ This article examines whether fair use can protect AI companies from
                 copyright liability for both the training process, which allegedly relies on the process of feeding AI
                 datasets (foundation models) predominantly with copyrighted material scraped off the internet for
                 purposes of machine learning; and the outputs that directly compete with the sampled material exploited
                 during the training process. Although several scholars have begun to articulate their own thesis
                 vis-à-vis its legality domestically, this article compares and anticipates how the judiciary in the
                 United States will address these issues as opposed to courts in fair dealing jurisdictions across the
                 globe. The article will expose how the dichotomy between the fair use doctrine and the fair dealing
                 clause imperatively affects the progression of these types of technologies, concluding that fair use is
                 more conductive to development than its foreign counterparts.",
  cc-author-affiliation = "Georgetown University Law Center, USA",
  cc-class     = "legal/copyright, legal/fair-use, nlp/language-model, ai/foundation-model,
                 nlp/multi-modal-language-model, nlp/multimodal-corpora, ai/image-dataset",
}

@Misc{cc:KandpalRaffel:2025:Position-LLM-training-data,
  title        = "Position: The Most Expensive Part of an {LLM} should be its Training Data",
  author       = "Nikhil Kandpal and Colin Raffel",
  year         = "2025",
  eprint       = "2504.12427",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/2504.12427",
  abstract     = "Training a state-of-the-art Large Language Model (LLM) is an increasingly expensive endeavor due to
                 growing computational, hardware, energy, and engineering demands. Yet, an often-overlooked (and seldom
                 paid) expense is the human labor behind these models' training data. Every LLM is built on an
                 unfathomable amount of human effort: trillions of carefully written words sourced from books, academic
                 papers, codebases, social media, and more. This position paper aims to assign a monetary value to this
                 labor and argues that the most expensive part of producing an LLM should be the compensation provided
                 to training data producers for their work. To support this position, we study 64 LLMs released between
                 2016 and 2024, estimating what it would cost to pay people to produce their training datasets from
                 scratch. Even under highly conservative estimates of wage rates, the costs of these models' training
                 datasets are 10-1000 times larger than the costs to train the models themselves, representing a
                 significant financial liability for LLM providers. In the face of the massive gap between the value of
                 training data and the lack of compensation for its creation, we highlight and discuss research
                 directions that could enable fairer practices in the future.",
  cc-snippet   = "Unlike other resources needed to produce LLMs, like hardware or energy, most training data has
                 historically been collected for virtually no cost by mining text from the public Internet (Common
                 Crawl). This web-scraped data is foundational to LLMs, particularly in their pre-training phase, where
                 models require vast amounts of text to learn general linguistic and world knowledge. Notably, we are
                 not aware of any compensation being given to the creators of this web text, despite their pivotal role
                 in the success of modern LLMs.",
  cc-author-affiliation = "University of Toronto; Vector Institute",
  cc-class     = "legal/copyright, legal/fair-use, nlp/language-model, ai/foundation-model",
}

@InProceedings{cc:Hanley:2025:International-propaganda-and-influence-networks,
  title        = "Tracking and Identifying International Propaganda and Influence Networks Online",
  author       = "Hanley, Hans WA",
  booktitle    = "Proceedings of the {AAAI} Conference on Artificial Intelligence",
  volume       = "39",
  number       = "28",
  pages        = "29263--29264",
  year         = "2025",
  URL          = "https://ojs.aaai.org/index.php/AAAI/article/view/35209/37364",
  abstract     = "Misinformation and propaganda undermine trust in institutions, spread falsehoods, and sometimes incite
                 violence. However, recent advancements in transformer-based AI models can help combat the proliferation
                 of disinformation globally and in real time. In this work, I propose and develop a system using these
                 models to scalably identify, track, and analyze the spread of narratives from over 40,000 international
                 news websites. First, by employing novel multilingual Matryoshka embeddings and hierarchical level-wise
                 clustering, my proposed system identifies news stories, topics, and themes across these thousands of
                 news websites. Second, by utilizing multilingual stance detection, my system assesses the biases and
                 factual inconsistencies in news articles, enabling the identification of websites that spread
                 propaganda or misinformation. Finally, through network inference methods, my system uncovers
                 connections among websites disseminating slanted or false content. My approach illustrates how AI can
                 be utilized to mitigate the global spread of harmful misinformation and propaganda.",
  cc-snippet   = "To track the spread of international news narratives, I created and continue to maintain an evolving
                 catalog of popular news-related websites to scrape. To do so, every three months, I use the CommonCrawl
                 website index and the Cloud Domain Intelligence API to compile and refine a list of websites dedicated
                 to “news.” Upon generating this list of news websites, I crawl them daily, collecting each site’s
                 homepage, RSS feed, and the corresponding linked news articles, extracting each page’s article
                 contents.",
  cc-author-affiliation = "Stanford University, USA",
  cc-class     = "misinformation, propaganda, nlp/fake-news-detection, nlp/language-model",
}

@Article{cc:PanneerselvamSethuramanEmersonKanakam:2025:Survey-on-web-based-phishing-techniques,
  author       = "Panneerselvam, Dhanavanthini and Sethuraman, Sibi Chakkaravarthy and Emerson, Ajith Jubilson and
                 Kanakam, Tarun Kumar",
  title        = "A Concise Survey on Modern Web-Based Phishing Techniques and Advanced Mitigation Strategies",
  journal      = "Transactions on Emerging Telecommunications Technologies",
  volume       = "36",
  number       = "4",
  pages        = "e70119",
  keywords     = "deep learning, machine learning, phishing, url phishing, web phishing",
  DOI          = "https://doi.org/10.1002/ett.70119",
  URL          = "https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.70119",
  eprint       = "https://onlinelibrary.wiley.com/doi/pdf/10.1002/ett.70119",
  abstract     = "Phishing is a tactical technique practiced by cyber-criminals, wherein the target systems are
                 approached, made vulnerable, and exploited. A Phisher who does the act of phishing is always creative,
                 calculative, and persistent. This potentially leads to the increase in the success rate of phishing and
                 the individuals who are technically expertise even falls in phishing campaigns. This article discusses
                 about the various web-based phishing techniques used by the modern day cyber criminals. Various
                 mitigation techniques related to the state of the art machine learning and deep learning techniques are
                 also studied. The article also extensively discusses about the features utilized for the detection.
                 Additionally, a qualitative and quantitative comparison of different studies for mitigating the web
                 phishing attacks is also examined.",
  year         = "2025",
  cc-class     = "computer-security/internet-security, web-security, cc-cited-not-used",
  cc-author-affiliation = "Siddhartha Academy of Higher Education, Karnataka, India; School of Computer Science
                 Engineering, VIT-AP, Andhra Pradesh, India",
}

@Misc{cc:TianYuSunWang:2025:Survey-of-malicious-URL-detection-techniques,
  title        = "From Past to Present: {A} Survey of Malicious {URL} Detection Techniques, Datasets and Code
                 Repositories",
  author       = "Ye Tian and Yanqiu Yu and Jianguo Sun and Yanbin Wang",
  year         = "2025",
  eprint       = "2504.16449",
  archiveprefix = "arXiv",
  primaryclass = "cs.CR",
  URL          = "https://arxiv.org/abs/2504.16449",
  abstract     = "Malicious URLs persistently threaten the cybersecurity ecosystem, by either deceiving users into
                 divulging private data or distributing harmful payloads to infiltrate host systems. The detection of
                 malicious URLs is a protracted arms race between defenders and attackers. Gaining timely insights into
                 the current state of this ongoing battle holds significant importance. However, existing reviews
                 exhibit 4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures understanding of
                 how detection approaches exploit specific modal information channels; 2) They fail to incorporate
                 pivotal LLM/Transformer-based defenses; 3) No open-source implementations are collected to facilitate
                 benchmarking; 4) Insufficient dataset coverage. This paper presents a comprehensive review of malicious
                 URL detection technologies, systematically analyzing methods from traditional blacklisting to advanced
                 deep learning approaches (e.g. Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel
                 modality-based taxonomy that categorizes existing works according to their primary data modalities
                 (URL, HTML, Visual, etc.). This hierarchical classification enables both rigorous technical analysis
                 and clear understanding of multimodal information utilization. Furthermore, to establish a profile of
                 accessible datasets and address the lack of standardized benchmarking (where current studies often lack
                 proper baseline comparisons), we curate and analyze: 1) publicly available datasets (2016-2024), and 2)
                 open-source implementations from published works(2013-2025). Then, we outline essential design
                 principles and architectural frameworks for product-level implementations. The review concludes by
                 examining emerging challenges and proposing actionable directions for future research. We maintain a
                 GitHub repository for ongoing curating datasets and open-source implementations:
                 https://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master",
  keywords     = "malicious URL detection, multimodality, data, code, machine learning",
  cc-class     = "computer-security/internet-security, web-security, cc-cited-not-used",
  cc-author-affiliation = "Hangzhou Institute of Technology, Xidian University, Hangzhou, Zhejiang, China",
}

@Misc{cc:KargaranYvonSchütze:2025:GlotCC,
  title        = "{GlotCC}: An Open Broad-Coverage {CommonCrawl} Corpus and Pipeline for Minority Languages",
  author       = "Amir Hossein Kargaran and François Yvon and Hinrich Schütze",
  year         = "2025",
  eprint       = "2410.23825",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/2410.23825",
  abstract     = "The need for large text corpora has increased with the advent of pretrained language models and, in
                 particular, the discovery of scaling laws for these models. Most available corpora have sufficient data
                 only for languages with large dominant communities. However, there is no corpus available that (i)
                 covers a wide range of minority languages; (ii) is generated by an open-source reproducible pipeline;
                 and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, a clean,
                 document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages.
                 We make GlotCC and the system used to generate it - including the pipeline, language identification
                 model, and filters - available to the research community.",
  cc-author-affiliation = "LMU Munich, Germany; Sorbonne Université & CNRS, ISIR, Paris, France",
  cc-class     = "nlp/corpus-construction, nlp/language-identification, nlp/low-resource-language",
  cc-derived-dataset-about = "GlotCC",
}

@Article{cc:SaadiBelhadefGuessasHafirassou:2025:Enhancing-fake-news-detection,
  title        = "Enhancing Fake News Detection with Transformer Models and Summarization",
  volume       = "15",
  ISSN         = "1792-8036",
  URL          = "https://etasr.com/index.php/ETASR/article/view/10678",
  DOI          = "10.48084/etasr.10678",
  abstract     = "This study evaluates the performance of transformer-based models such as BERT, RoBERTa, and XLNet for
                 fake news detection. Using supervised and unsupervised deep learning techniques, we optimized
                 classification accuracy while reducing computational costs through text summarization. The results show
                 that RoBERTa, fine-tuned with summarized content, achieves 98.39\% accuracy, outperforming the other
                 models. Additionally, we assessed AI-generated misinformation using GPT-2, confirming that transformer
                 models effectively distinguish real from synthetic news. We utilized the GPT-2 model instead of more
                 recent models like GPT-4, as our objective was to generate fake news locally and compare it with
                 pretrained models from the same time period.",
  language     = "en",
  number       = "3",
  urldate      = "2025-05-16",
  journal      = "Engineering, Technology \& Applied Science Research",
  author       = "Saadi, Abdelhalim and Belhadef, Hacene and Guessas, Akram and Hafirassou, Oussama",
  month        = jun,
  year         = "2025",
  keywords     = "DL, GPT-2, NLP, RoBERTa, fake news detection, text classification, transformers",
  pages        = "23253--23259",
  cc-author-affiliation = "Setif 1 University – Ferhat Abbas, Algeria; University of Abdelhamid Mehri – Constantine
                 2, Algeria",
  cc-class     = "nlp/fake-news-detection, nlp/text-classification",
  cc-snippet   = "A. Dataset Selection Two primary datasets were used for training and evaluation:¶ * Gonzalo/Fake News
                 Dataset: This dataset consists of real and fake news articles, providing a balanced corpus for model
                 training. It includes both headlines and full article texts, allowing models to learn contextual
                 differences between genuine and fabricated news. They contain 40587 articles [13].¶ * CC_News Dataset:
                 Is a collection of real news articles used to generate synthetic fake news samples using GPT-2. This
                 dataset helps in assessing how well models can differentiate between AI-generated fake news and
                 human-written articles. It contains 708241 English language news articles published between Jan 2017
                 and December 2019 [14].",
  cc-derived-dataset-used = "vblagoje-cc-news",
}

@MastersThesis{cc:Govender:2025:Graph-embeddings-for-website-classification,
  author       = "Govender, Praven",
  year         = "2025",
  URL          = "http://www.theseus.fi/handle/10024/886636",
  DOI          = "https://urn.fi/URN:NBN:fi:amk-2025051411598",
  abstract     = "This thesis explores the practical utility of domain-level graph embeddings derived from hyperlink
                 structures at web scale. Using a 21TB subset of Common Crawl (n.d.) data, we demonstrate how these
                 embeddings can effectively capture relationships between websites without requiring content processing.
                 The study achieves competitive results in political bias and factual reporting classification on news
                 publishers compared to state-of-the-art approaches, while offering greater flexibility due to the use
                 of embeddings. Beyond classification, we show how the same embeddings enable various analytical tasks
                 including similarity analysis and clustering, demonstrating their flexibility as a practical tool for
                 web-scale analysis. Key contributions include a methodology for creating and utilizing domain-level
                 graph embeddings at scale, strong empirical evidence of their effectiveness in website classification,
                 and insights into their broader applications for understanding website relationships through link
                 structures alone.",
  title        = "Links Are All You Need: Graph Embeddings for Website Analysis and Classification",
  keywords     = "machine learning, data mining, hyperlinks, websites, classification, graphs (network theory), Big Data
                 Analytics",
  cc-author-affiliation = "Arcada University of Applied Sciences, Finland",
  cc-class     = "web-science/hyperlinkgraph, web-site-classification",
}

@Misc{cc:BurnsParcalabescuWäldchenBarlowEtAl:2025:Aleph-Alpha-GermanWeb,
  title        = "{Aleph-Alpha-GermanWeb}: Improving German-language {LLM} pre-training with model-based data curation
                 and synthetic data generation",
  author       = "Thomas F Burns and Letitia Parcalabescu and Stephan Wäldchen and Michael Barlow and Gregor Ziegltrum
                 and Volker Stampa and Bastian Harren and Björn Deiseroth",
  year         = "2025",
  eprint       = "2505.00022",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/2505.00022",
  abstract     = "Scaling data quantity is essential for large language models (LLMs), yet recent findings show that
                 data quality can significantly boost performance and training efficiency. We introduce a
                 German-language dataset curation pipeline that combines heuristic and model-based filtering techniques
                 with synthetic data generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a large-scale
                 German pre-training dataset which draws from: (1) Common Crawl web data, (2) FineWeb2, and (3)
                 synthetically-generated data conditioned on actual, organic web data. We evaluate our dataset by
                 pre-training both a 1B Llama-style model and an 8B tokenizer-free hierarchical autoregressive
                 transformer (HAT). A comparison on German-language benchmarks, including MMMLU, shows significant
                 performance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage holds at the 8B scale
                 even when FineWeb2 is enriched by human-curated high-quality data sources such as Wikipedia. Our
                 findings support the growing body of evidence that model-based data curation and synthetic data
                 generation can significantly enhance LLM pre-training datasets.",
  cc-snippet   = "In this paper we present a data curation pipeline specifically for German-language LLM pre-training.
                 We use it to derive our own dataset, Aleph-Alpha-GermanWeb, from three sources: (1) Common Crawl [13]
                 web data not included in FineWeb2; (2) FineWeb2; and (3) synthetic data conditioned on actual, organic
                 web data. Our contributions are: 1. To curate the Common Crawl data, we applied a pipeline similar to
                 (but which we show can perform better than) FineWeb2. We then augmented the dataset with
                 synthetically-generated data and applied model-based quality classification methods; [...] We
                 downloaded six Common Crawl [13] web data dumps, spanning from September 2024 to February 2025. Each
                 dump captured raw web page data, e.g., HTML, metadata such as URLs, and text extracts.",
  cc-author-affiliation = "Aleph Alpha Research, Germany",
  cc-class     = "nlp/language-models, nlp/large-language-models, nlp/text-corpora",
}

@InProceedings{cc:AnsarSperottoHolz:2025:Web-crawl-refusals,
  author       = "Ansar, Mostafa and Sperotto, Anna and Holz, Ralph",
  editor       = "Testart, Cecilia and van Rijswijk-Deij, Roland and Stiller, Burkhard",
  title        = "Web Crawl Refusals: Insights From Common Crawl",
  booktitle    = "Passive and Active Measurement",
  year         = "2025",
  publisher    = "Springer Nature Switzerland",
  address      = "Cham",
  pages        = "197--214",
  abstract     = "Web crawlers are an indispensable tool for collecting research data. However, they may be blocked by
                 servers for various reasons. This can reduce their coverage. In this early-stage work, we investigate
                 server-side blocks encountered by Common Crawl (CC). We analyze page contents to cover a broader range
                 of refusals than previous work. We construct fine-grained regular expressions to identify refusal pages
                 with precision, finding that at least 1.68{\%} of sites in a CC snapshot exhibit a form of explicit
                 refusal. Significant contributors include large hosters. Our analysis categorizes the forms of refusal
                 messages, from straight blocks to challenges and rate-limiting responses. We are able to extract the
                 reasons for nearly half of the refusals we identify. We find an inconsistent and even incorrect use of
                 HTTP status codes to indicate refusals. Examining the temporal dynamics of refusals, we find that most
                 blocks resolve within one hour, but also that 80{\%} of refusing domains block every request by CC. Our
                 results show that website blocks deserve more attention as they have a relevant impact on crawling
                 projects. We also conclude that standardization to signal refusals would be beneficial for both site
                 operators and web crawlers.",
  ISBN         = "978-30318-5-9-6-0-1",
  DOI          = "https://doi.org/10.1007/978-3-031-85960-1_9",
  URL          = "https://link.springer.com/chapter/10.1007/978-3-031-85960-1_9",
  cc-author-affiliation = "University of Twente, Enschede, The Netherlands; University of Münster, Germany",
  cc-class     = "web-crawling, web-crawling/server-side-blocking",
  cc-snippet   = "CC is a non-proﬁt initiative. The crawler is designed [18] to be “polite” and comply with the
                 Robots Exclusion Protocol, using e.g. ﬁve seconds wait between requests to the same host and
                 employing an exponential back-oﬀ strategy on error. It identiﬁes itself with a User-Agent header
                 that is speciﬁc to CC. It does not process content that is dynamically generated on client-side, e.g.
                 using JavaScript. According to CC’s documentation, the fetching process happens over 14 days and is
                 handled by 20 AWS EC2 spot nodes in N. Virginia. The documentation does not provide the public IP
                 addresses. [...] Dataset. We use the CC-MAIN-2023-50 snapshot, which was compiled between 2023-11-28
                 and 2023-12-12 [7]. We use two parts of this snapshot: the general index ﬁle Columnar URL index and
                 the archive non-200 responses. The non-200 responses archive contains all fetching attempts that did
                 not receive a status code 200 (“200 OK”) as a reply (see the end of this section for limitations of
                 this approach). It consists of 3.43 TB of compressed data. We extract timestamps, server IPs, URIs,
                 Fully Qualiﬁed Domain Name (FQDNs), registered domains, HTTP status codes and headers, and page
                 textual contents. We use pyasn [5] to map IPs to AS numbers. We use the index to obtain records with
                 status code 200 for FQDNs of interest.¶ Data Pruning. We obtain 561 × 106 records from the non-200
                 responses, summarized in Table 1. We prune all responses with status codes for redirections (3xx ;
                 these are rescheduled by CC for later crawls) as well as those where the content could not be found
                 (404 ). This leaves us with 21.7 × 106 records.",
}

@Article{cc:MousaHassanRashidAl-Saady:2025:Safeguarding-patient-Data,
  title        = "Safeguarding Patient Data: Machine Learning for Phishing {URL} Detection in Healthcare Systems",
  volume       = "131",
  URL          = "https://akademiabaru.com/submit/index.php/ard/article/view/6248",
  DOI          = "10.37934/ard.131.1.4760",
  abstractnote = "Since the healthcare industry depends more and more on digital infrastructure, it is a perfect target
                 for cyberattacks especially phishing. Designed to pass for real healthcare websites, phishing URLs
                 seriously compromise patient data security. Effective strikes may cause disruptions in patient care,
                 financial losses, and Protected Health Information (PHI) breaches. This work investigates the use of
                 machine learning (ML) approaches for robust and accurate phishing URL detection in healthcare systems
                 in order to handle this important problem. We examine a Multilayer Perceptron (MLP) neural
                 network-based detection model and evaluate its performance against known techniques, Decision Tree (DT)
                 and Naive Bayes (NB). Comprehensive URL datasets—more especially, the ISCX-URL-2016 dataset for
                 training and testing—as well as the CIC-InvesBanking-2017 dataset combined with live phishing feeds
                 for validation help to train and rigorously validate the models. With a high accuracy of 87.45\% on
                 test data and a precision of 84\% on unseen validation data, our experimental results show that the
                 proposed MLP model much exceeds DT and NB. This emphasizes how ML—more especially, MLP—may improve
                 cybersecurity defences in healthcare, hence securing private patient data and the integrity of
                 healthcare processes.",
  number       = "1",
  journal      = "Journal of Advanced Research Design",
  author       = "Mousa, Alaa Abdulshaheed and Hassan, Saif Al-Deen H. and Rashid, Mohammed Kareem and Al-Saady,
                 Moumal",
  year         = "2025",
  month        = may,
  pages        = "47--60",
  keywords     = "Phishing URL detection; machine learning; healthcare cybersecurity; multilayer perceptron; decision
                 tree; Naive Bayes; patient data security",
  cc-snippet   = "For validation, we constructed a more extensive and realistic dataset by combining the
                 CIC-InvesBanking-2017 dataset with live phishing URLs from active threat intelligence feeds. Benign
                 URLs for validation were sourced from a 2023 snapshot of the Common Crawl dataset, representing a broad
                 spectrum of contemporary web content.",
  cc-author-affiliation = "University of Misan, Maysan, Iraq; Deakin University, Australia",
  cc-class     = "computer-security/internet-security, web-security, healthcare cybersecurity",
}

@Misc{cc:EhrmanntrautWunderlePfisterJannidisEtAl:2025:ModernGBERT,
  title        = "{ModernGBERT}: German-only {1B} Encoder Model Trained from Scratch",
  author       = "Anton Ehrmanntraut and Julia Wunderle and Jan Pfister and Fotis Jannidis and Andreas Hotho",
  year         = "2025",
  eprint       = "2505.13136",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
  URL          = "https://arxiv.org/abs/2505.13136",
  abstract     = "Despite the prominence of decoder-only language models, encoders remain crucial for
                 resource-constrained applications. We introduce ModernGBERT (134M, 1B), a fully transparent family of
                 German encoder models trained from scratch, incorporating architectural innovations from ModernBERT. To
                 evaluate the practical trade-offs of training encoders from scratch, we also present LLäMmlein2Vec
                 (120M, 1B, 7B), a family of encoders derived from German decoder-only models via LLM2Vec. We benchmark
                 all models on natural language understanding, text embedding, and long-context reasoning tasks,
                 enabling a controlled comparison between dedicated encoders and converted decoders. Our results show
                 that ModernGBERT 1B outperforms prior state-of-the-art German encoders as well as encoders adapted via
                 LLM2Vec, with regard to performance and parameter-efficiency. All models, training data, checkpoints
                 and code are publicly available, advancing the German NLP ecosystem with transparent, high-performance
                 encoder models.",
  cc-author-affiliation = "Julius-Maximilians-Universität Würzburg, Germany",
  cc-class     = "nlp/language-model, German",
  cc-snippet   = "We pre-trained ModernGBERT on the same data as LLäMmlein decoder models (Pfister et al., 2024), using
                 the open-source RedPajamaV2 dataset (Weber et al., 2024).⁶ This dataset comprises German CommonCrawl
                 snapshots from 2014–2023.",
}

@Misc{cc:VeselovskyPiccardiAndersonWestEtAl:2025:Web2Wiki,
  title        = "{Web2Wiki}: Characterizing Wikipedia Linking Across the Web",
  author       = "Veniamin Veselovsky and Tiziano Piccardi and Ashton Anderson and Robert West and Akhil Arora",
  year         = "2025",
  eprint       = "2505.15837",
  archiveprefix = "arXiv",
  primaryclass = "cs.SI",
  URL          = "https://arxiv.org/abs/2505.15837",
  abstract     = "Wikipedia is one of the most visited websites globally, yet its role beyond its own platform remains
                 largely unexplored. In this paper, we present the first large-scale analysis of how Wikipedia is
                 referenced across the Web. Using a dataset from Common Crawl, we identify over 90 million Wikipedia
                 links spanning 1.68\% of Web domains and examine their distribution, context, and function. Our
                 analysis of English Wikipedia reveals three key findings: (1) Wikipedia is most frequently cited by
                 news and science websites for informational purposes, while commercial websites reference it less
                 often. (2) The majority of Wikipedia links appear within the main content rather than in boilerplate or
                 user-generated sections, highlighting their role in structured knowledge presentation. (3) Most links
                 (95\%) serve as explanatory references rather than as evidence or attribution, reinforcing Wikipedia's
                 function as a background knowledge provider. While this study focuses on English Wikipedia, our
                 publicly released Web2Wiki dataset includes links from multiple language editions, supporting future
                 research on Wikipedia's global influence on the Web.",
  cc-author-affiliation = "Princeton University, USA; Stanford University, USA; University of Toronto, Canada; EPFL,
                 Switzerland; Aarhus University, Denmark",
  cc-class     = "web-science, Wikipedia backlinks, Wikipedia reverse links, web-science/hyperlinkgraph,
                 nlp/text-classification",
  cc-snippet   = "The dataset used in this paper was extracted from the February 2021 Common Crawl dump²
                 [²https://commoncrawl.org/2021/03/february-march-2021-crawl-archive-now-available/] — the largest
                 public scrape of HTML pages on the Web created prior to February 2021—through a regex search of <a>
                 tags, the HTML element for hyperlinking. Specifically, the WEB2WIKI dataset, released with this
                 article, contains a large sample of the publicly accessible HTML pages that link to Wikimedia projects,
                 as well as the bare webpage–article link pairs. Given the web-scraping policies of Common Crawl (cf.
                 Appendix. A), the dataset contains links to Wikipedia articles available on the surface Web—the
                 portion of the Web accessible to the general public and searchable with standard Web search
                 engines³—, excluding websites such as Facebook, Reddit, Twitter, Quora, or YouTube that require
                 registering an account to see their content.",
}

@Article{cc:KrieschLosacker:2025:GeolocatedGermanNews,
  author       = "Kriesch, Lukas and Losacker, Sebastian",
  journal      = "Scientific Data",
  title        = "A geolocated dataset of German news articles",
  year         = "2025",
  volume       = "12",
  DOI          = "10.1038/s41597-025-05422-w",
  URL          = "https://doi.org/10.1038/s41597-025-05422-w",
  abstract     = "The emergence of large language models and the exponential growth of digitized text data have
                 revolutionized research methodologies across a broad range of social sciences. News data is crucial for
                 the social sciences as it provides real-time insights into public discourse and societal trends. In
                 this paper, we provide insights into how news articles can be geolocated and how the texts can then be
                 further analyzed. We collect data from the CommonCrawl News dataset and clean the text data. We then
                 use a named-entity recognition model for geocoding. Finally, we transform the news articles into text
                 embeddings using SBERT, enabling semantic searches within the news data corpus. In the paper, we apply
                 this process to all German news articles and make the German location data, as well as the embeddings,
                 available for download. We compile a dataset containing text embeddings for about 50 million German
                 news articles, of which about 70\% include geographic locations. The process can be replicated for news
                 data from other countries.",
  cc-author-affiliation = "Justus Liebig University Giessen, Giessen, Germany",
  cc-class     = "dataset-annotation, dataset-curation, geolocated-dataset, news-dataset",
  cc-dataset-used = "CC-NEWS",
  cc-snippet   = "For data acquisition, we utilize the Common Crawl News dataset, a resource curated by the non-profit
                 organization Common Crawl, which has been systematically crawling the web since 2007. This organization
                 releases new collections of web content at intervals of 1 to 2 months. Since August 2016, Common Crawl
                 has maintained a dedicated news dataset, using RSS/Atom feeds and news sitemaps to discover and
                 aggregate links to articles across a wide spectrum of news platforms.",
}

@Misc{cc:HeGovindanMadhyastha:2025:Website-Written-by-LLMs,
  title        = "Preprint: Did {I} Just Browse A Website Written by {LLMs?}",
  author       = "Sichang {"}Steven{"} He and Ramesh Govindan and Harsha V. Madhyastha",
  year         = "2025",
  eprint       = "2507.13933",
  archiveprefix = "arXiv",
  primaryclass = "cs.NI",
  URL          = "https://arxiv.org/abs/2507.13933",
  abstract     = "Increasingly, web content is automatically generated by large language models (LLMs) with little human
                 input. We call this {"}LLM-dominant{"} content. Since LLMs plagiarize and hallucinate, LLM-dominant
                 content can be unreliable and unethical. Yet, websites rarely disclose such content, and human readers
                 struggle to distinguish it. Thus, we must develop reliable detectors for LLM-dominant content. However,
                 state-of-the-art LLM detectors are insufficient, because they perform well mainly on clean, prose-like
                 text, while web content has complex markup and diverse genres. We propose a highly reliable, scalable
                 pipeline that classifies entire websites. Instead of naively classifying text extracted from each page,
                 we classify each site based on an LLM text detector's outputs of multiple prose-like pages. We train
                 and evaluate our detector by collecting 2 distinct ground truth datasets totaling 120 sites, and obtain
                 100\% accuracies testing across them. In the wild, we detect a sizable portion of sites as LLM-dominant
                 among 10k sites in search engine results and 10k in Common Crawl archives. We find LLM-dominant sites
                 are growing in prevalence and rank highly in search results, raising questions about their impact on
                 end users and the overall Web ecosystem.",
  cc-snippet   = "Common Crawl. To understand the historical trend, we analyzed 10,479 random sites from Common Crawl
                 archives from 2020 to 2025 (284,523 pages). Overall, only 451 sites (4.30\%) are detected as
                 LLM(-dominant), much lower than the 9.84\% in search results, suggesting web search surfaces much more
                 LLM sites than randomly sampling the Web. However, of the 4938 sites crawled entirely after ChatGPT
                 launched, a sizable 358 (7.25\%) are LLM; of the 764 sites started being crawled between 2024 and 2025,
                 an even larger 77 (10.08\%) are LLM. This indicates the portion of LLM-dominant sites is growing.
                 Finally, of the 1315 sites crawled entirely be- fore ChatGPT launched, 16 (1.22\%) are misclassified as
                 LLM, mainly due to pages evading our filters and can be eliminated; this shows our detector has low but
                 non-zero FPR.",
  cc-author-affiliation = "University of Southern California, USA",
  cc-class     = "web-science, web-site-classification, machine-generated content, nlp/large-language-models,
                 nlp/generative-language-models",
}
