{"year":"2016","title":"A Case Study of Complex Graph Analysis in Distributed Memory: Implementation and Optimization","authors":["GM Slota, S Rajamanickam, K Madduri"],"snippet":"... Focusing on one of the largest publicly-available hyperlink graphs (the 2012 Web Data Commons graph1, which was in- turn extracted from the open Common Crawl web corpus2), we develop parallel implementations for the Blue Waters supercomputer, one of the world's most ...","url":["http://www.personal.psu.edu/users/g/m/gms5016/pub/Dist-IPDPS16.pdf"]}
{"year":"2016","title":"A Convolutional Encoder Model for Neural Machine Translation","authors":["J Gehring, M Auli, D Grangier, YN Dauphin - arXiv preprint arXiv:1611.02344, 2016"],"snippet":"... WMT'15 English-German. We use all available parallel training data, namely Europarl v7, Common Crawl and News Commentary v10 and apply the standard Moses tokenization to obtain 3.9M sentence pairs (Koehn et al., 2007). We report results on newstest2015. ...","url":["https://arxiv.org/pdf/1611.02344"]}
{"year":"2016","title":"A Deep Fusion Model for Domain Adaptation in Phrase-based MT","authors":["N Durrani, S Joty, A Abdelali, H Sajjad"],"snippet":"... test-13 993 18K 17K test-13 1169 26K 28K Table 1: Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations ...","url":["https://www.aclweb.org/anthology/C/C16/C16-1299.pdf"]}
{"year":"2016","title":"A Large DataBase of Hypernymy Relations Extracted from the Web","authors":["J Seitner, C Bizer, K Eckert, S Faralli, R Meusel… - … of the 10th edition of the …, 2016"],"snippet":"... 3http://webdatacommons.org/framework/ 4http://commoncrawl.org ... The corpus is provided by the Common Crawl Foundation on AWS S3 as free download.6 The extraction of ... isadb/) and can be used to repeat the tuple extraction for different or newer Common Crawl releases. ...","url":["http://webdatacommons.org/isadb/lrec2016.pdf"]}
{"year":"2016","title":"A Maturity Model for Public Administration as Open Translation Data Providers","authors":["N Bel, ML Forcada, A Gómez-Pérez - arXiv preprint arXiv:1607.01990, 2016"],"snippet":"... There are techniques to mitigate the need of large quantities of parallel text, but most often at the expense of resulting translation quality. As a reference of the magnitude we can take as a standard corpus the Common Crawl corpus (Smith et al. ...","url":["http://arxiv.org/pdf/1607.01990"]}
{"year":"2016","title":"A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference","authors":["B Paria, KM Annervaz, A Dukkipati, A Chatterjee… - arXiv preprint arXiv: …, 2016"],"snippet":"... We used batch normalization [Ioffe and Szegedy, 2015] while training. The various model parameters used are mentioned in Table I. We experimented with both GloVe vectors trained1 on Common Crawl dataset as well as Word2Vec vector trained2 on Google news dataset. ...","url":["https://arxiv.org/pdf/1611.04741"]}
{"year":"2016","title":"A practical guide to big data research in psychology.","authors":["EE Chen, SP Wojcik - Psychological Methods, 2016"],"snippet":"... as well as general collections, such as Amazon Web Services' Public Data Sets repository (AWS, nd, http://aws.amazon.com/public-data-sets/) which includes the 1000 Genomes Project, with full genomic sequences for 1,700 individuals, and the Common Crawl Corpus, with ...","url":["http://psycnet.apa.org/journals/met/21/4/458/"]}
{"year":"2016","title":"A semantic based Web page classification strategy using multi-layered domain ontology","authors":["AI Saleh, MF Al Rahmawy, AE Abulwafa - World Wide Web, 2016"],"snippet":"Page 1. A semantic based Web page classification strategy using multi-layered domain ontology Ahmed I. Saleh1 & Mohammed F. Al Rahmawy2 & Arwa E. Abulwafa1 Received: 3 February 2016 /Revised: 13 August 2016 /Accepted ...","url":["http://link.springer.com/article/10.1007/s11280-016-0415-z"]}
{"year":"2016","title":"A Story of Discrimination and Unfairness","authors":["A Caliskan-Islam, J Bryson, A Narayanan"],"snippet":"... power has led to high quality language models such as word2vec [7] and GloVe [8]. These language models, which consist of up to half a million unique words, are trained on billions of documents from sources such as Wikipedia, CommonCrawl, GoogleNews, and Twitter. ...","url":["https://www.securityweek2016.tu-darmstadt.de/fileadmin/user_upload/Group_securityweek2016/pets2016/9_a_story.pdf"]}
{"year":"2016","title":"A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs","authors":["S Longpre, S Pradhan, C Xiong, R Socher - arXiv preprint arXiv:1611.05104, 2016"],"snippet":"... All models in this paper used publicly available 300 dimensional word vectors, pre-trained using Glove on 840 million tokens of Common Crawl Data (Pennington et al., 2014), and both the word vectors and the subsequent weight matrices were trained using Adam with a ...","url":["https://arxiv.org/pdf/1611.05104"]}
{"year":"2016","title":"A Web Application to Search a Large Repository of Taxonomic Relations from the Web","authors":["S Faralli, C Bizer, K Eckert, R Meusel, SP Ponzetto"],"snippet":"... 1 https://commoncrawl.org 2 http://webdatacommons.org/framework/ 3 https://www.mongodb. com ... of the two noun phrases involved in the isa relations into pre-modifiers, head and post-modifiers [6], as well as the frequency of occurrence of the relation in the Common Crawl...","url":["http://ceur-ws.org/Vol-1690/paper58.pdf"]}
{"year":"2016","title":"Abu-MaTran at WMT 2016 Translation Task: Deep Learning, Morphological Segmentation and Tuning on Character Sequences","authors":["VM Sánchez-Cartagena, A Toral - Proceedings of the First Conference on Machine …, 2016"],"snippet":"... 362 Page 2. Corpus Sentences (k) Words (M) Europarl v8 2 121 39.5 Common Crawl 113 995 2 416.7 News Crawl 2014–15 6 741 83.1 Table 1: Finnish monolingual data, after preprocessing, used to train the LMs of our SMT submission. ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2322.pdf"]}
{"year":"2016","title":"Action Classification via Concepts and Attributes","authors":["A Rosenfeld, S Ullman - arXiv preprint arXiv:1605.07824, 2016"],"snippet":"... To assign GloVe [19] vectors to object names or attributes, we use the pre-trained model on the Common-Crawl (42B) corpus, which contains a vocabulary of 1.9M words. We break up phrases into their words and assign to them their mean GloVe vector. ...","url":["http://arxiv.org/pdf/1605.07824"]}
{"year":"2016","title":"Active Content-Based Crowdsourcing Task Selection","authors":["P Bansal, C Eickhoff, T Hofmann"],"snippet":"... Pennington et. al. [28] showed distributed text representations to capture more semantic information when the models are trained on Wikipedia text, as opposed to other large corpora such as the Common Crawl. This is attributed ...","url":["https://www.researchgate.net/profile/Piyush_Bansal4/publication/305442609_Active_Content-Based_Crowdsourcing_Task_Selection/links/578f416d08ae81b44671ad85.pdf"]}
{"year":"2016","title":"Adverse Drug Reaction Classification With Deep Neural Networks","authors":["T Huynh, Y He, A Willis, S Rüger"],"snippet":"... 4http://commoncrawl.org/ 5Source code is available at https://github.com/trunghlt/ AdverseDrugReaction 879 Page 4. max pooling feedforward layer convolutional layer (a) Convolutional Neural Network (CNN) (b) Recurrent Convolutional Neural Network (RCNN) ...","url":["http://www.aclweb.org/anthology/C/C16/C16-1084.pdf"]}
{"year":"2016","title":"All Your Data Are Belong to us. European Perspectives on Privacy Issues in 'Free'Online Machine Translation Services","authors":["P Kamocki, J O'Regan, M Stauch - Privacy and Identity Management. Time for a …, 2016"],"snippet":"... http://​www.​cnet.​com/​news/​google-translate-now-serves-200-million-people-daily/​. Accessed 23 Oct 2014. Smith, JR, Saint-Amand, H., Plamada, M., Koehn, P., Callison-Burch, C., Lopez, A.: Dirt cheap web-scale parallel text from the Common Crawl...","url":["http://link.springer.com/chapter/10.1007/978-3-319-41763-9_18"]}
{"year":"2016","title":"An Analysis of Real-World XML Queries","authors":["P Hlísta, I Holubová - OTM Confederated International Conferences\" On the …, 2016"],"snippet":"... crawler. Or, there is another option – Common Crawl [1], an open repository of web crawled data that is universally accessible and analyzable, containing petabytes of data collected over the last 7 years. ... 3.1 Common Crawl. We ...","url":["http://link.springer.com/chapter/10.1007/978-3-319-48472-3_36"]}
{"year":"2016","title":"An Attentive Neural Architecture for Fine-grained Entity Type Classification","authors":["S Shimaoka, P Stenetorp, K Inui, S Riedel - arXiv preprint arXiv:1604.05525, 2016"],"snippet":"... appearing in the training set. Specifically, we used the freely available 300 dimensional cased word embeddings trained on 840 billion to- kens from the Common Crawl supplied by Pennington et al. (2014). As embeddings ...","url":["http://arxiv.org/pdf/1604.05525"]}
{"year":"2016","title":"Analysing Structured Scholarly Data Embedded in Web Pages","authors":["P Sahoo, U Gadiraju, R Yu, S Saha, S Dietze"],"snippet":"... the following section. 2.2 Methodology and Dataset For our investigation, we use the Web Data Commons (WDC) dataset, being the largest available corpus of markup, extracted from the Common Crawl. Of the crawled web ...","url":["http://cs.unibo.it/save-sd/2016/papers/pdf/sahoo-savesd2016.pdf"]}
{"year":"2016","title":"ArabicWeb16: A New Crawl for Today's Arabic Web","authors":["R Suwaileh, M Kutlu, N Fathima, T Elsayed, M Lease"],"snippet":"... English content dominates the crawl [12]. While Common Crawl could be mined to identify and ex- tract a useful Arabic subset akin to ArClueWeb09, this would address only recency, not coverage. To address the above concerns ...","url":["http://www.ischool.utexas.edu/~ml/papers/sigir16-arabicweb.pdf"]}
{"year":"2016","title":"Ask Your Neurons: A Deep Learning Approach to Visual Question Answering","authors":["M Malinowski, M Rohrbach, M Fritz - arXiv preprint arXiv:1605.02697, 2016"],"snippet":"Page 1. Noname manuscript Ask Your Neurons: A Deep Learning Approach to Visual Question Answering Mateusz Malinowski · Marcus Rohrbach · Mario Fritz Abstract We address a question answering task on realworld images that is set up as a Visual Turing Test. ...","url":["http://arxiv.org/pdf/1605.02697"]}
{"year":"2016","title":"Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations","authors":["P Blair, Y Merhav, J Barry - arXiv preprint arXiv:1611.01547, 2016"],"snippet":"... (2013a), the 840-billion token Common Crawl corpus-trained GloVe model released by Pennington et al. (2014), and the English, Spanish, German, Japanese, and Chinese MultiCCA vectors5 from Ammar et al. ... Outliers OOV GloVe Common Crawl 75.53 38.57 5 6.33 5.70 ...","url":["https://arxiv.org/pdf/1611.01547"]}
{"year":"2016","title":"Automated Haiku Generation based on Word Vector Models","authors":["AF Aji"],"snippet":"... and Page 28. 16 Chapter 3. Design Common Crawl data. Those data also come with various vector dimension size from 50-D to 300-D. Those pre-trained word vectors are used directly for this project as they take considerably ...","url":["http://project-archive.inf.ed.ac.uk/msc/20150275/msc_proj.pdf"]}
{"year":"2016","title":"Automatic Construction of Morphologically Motivated Translation Models for Highly Inflected, Low-Resource Languages","authors":["J Hewitt, M Post, D Yarowsky - AMTA 2016, Vol., 2016"],"snippet":"... sentences of Europarl (Koehn, 2005), SETIMES3 (Tyers and Alperen, 2010), extracted from OPUS (Tiedemann, 2009), or Common Crawl (Bojar et al ... Turkish, we train models on 29000 sentences of biblical data with 1000 and 20000 sentences of CommonCrawl and SETIMES ...","url":["https://www.researchgate.net/profile/John_Ortega3/publication/309765044_Fuzzy-match_repair_using_black-box_machine_translation_systems_what_can_be_expected/links/5822496f08ae7ea5be6af317.pdf#page=183"]}
{"year":"2016","title":"B1A3D2 LUC@ WMT 2016: a Bilingual1 Document2 Alignment3 Platform Based on Lucene","authors":["L Jakubina, P Langlais"],"snippet":"... 2013. Dirt cheap web-scale parallel text from the common crawl. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1374–1383. Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and Moshe Dubiner. 2010. ...","url":["http://www-etud.iro.umontreal.ca/~jakubinl/publication/badluc_jaklan_wmt16_stbad.pdf"]}
{"year":"2016","title":"Big Data Facilitation and Management","authors":["J Fagerli"],"snippet":"Page 1. Faculty of Science and Technology Department of Computer Science Big Data Facilitation and Management A requirements analysis and initial evaluation of a big biological data processing service — Jarl Fagerli INF ...","url":["http://bdps.cs.uit.no/papers/capstone-jarl.pdf"]}
{"year":"2016","title":"Bootstrap, Review, Decode: Using Out-of-Domain Textual Data to Improve Image Captioning","authors":["W Chen, A Lucchi, T Hofmann - arXiv preprint arXiv:1611.05321, 2016"],"snippet":"... We report the performance of our model and competing methods in terms of six standard metrics used for image captioning as described in [4]. During the bootstrap learning phase, we use both the 20082010 News-CommonCrawl and Europarl corpus 2 as out- of-domain ...","url":["https://arxiv.org/pdf/1611.05321"]}
{"year":"2016","title":"bot. zen@ EVALITA 2016-A minimally-deep learning PoS-tagger (trained for Italian Tweets)","authors":["EW Stemle"],"snippet":"... The data was only distributed to the task participants. 4.1.4 C4Corpus (w2v) c4corpus8 is a full documents Italian Web corpus that has been extracted from CommonCrawl, the largest publicly available general Web crawl to date. ...","url":["http://ceur-ws.org/Vol-1749/paper_020.pdf"]}
{"year":"2016","title":"Building mutually beneficial relationships between question retrieval and answer ranking to improve performance of community question answering","authors":["M Lan, G Wu, C Xiao, Y Wu, J Wu - Neural Networks (IJCNN), 2016 International Joint …, 2016"],"snippet":"... The first is the 300-dimensional version of word2vec [23] vectors, which is trained on part of Google News dataset (about 100 billion words). The second is 300-dimensional Glove vectors [24] which is trained on 840 billion tokens of Common Crawl data. ...","url":["http://ieeexplore.ieee.org/abstract/document/7727286/"]}
{"year":"2016","title":"C4Corpus: Multilingual Web-size corpus with free license","authors":["I Habernal, O Zayed, I Gurevych"],"snippet":"... documents. Our project is entitled C4Corpus, an abbreviation of Creative Commons from Common Crawl Corpus and is hosted under the DKPro umbrella4 at https:// github.com/dkpro/dkpro-c4corpus under ASL 2.0 license. ...","url":["https://www.ukp.tu-darmstadt.de/fileadmin/user_upload/Group_UKP/publikationen/2016/lrec2016-c4corpus-camera-ready.pdf"]}
{"year":"2016","title":"Capturing Pragmatic Knowledge in Article Usage Prediction using LSTMs","authors":["J Kabbara, Y Feng, JCK Cheung"],"snippet":"... GloVe: The embedding is initialized by the global vectors Pennington et al. (2014) that are trained on the Common Crawl corpus (840 billion tokens). Both word2vec and GloVe word embeddings consist of 300 dimensions. ...","url":["https://www.aclweb.org/anthology/C/C16/C16-1247.pdf"]}
{"year":"2016","title":"Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship Attribution","authors":["S Ruder, P Ghaffari, JG Breslin - arXiv preprint arXiv:1609.06686, 2016"],"snippet":"... σ: Standard deviation of document number. d: Median document size (tokens). All word embedding channels are initialized with 300-dimensional GloVe vectors (Pennington et al., 2014) trained on 840B tokens of the Common Crawl corpus11. ...","url":["http://arxiv.org/pdf/1609.06686"]}
{"year":"2016","title":"Citation Classification for Behavioral Analysis of a Scientific Field","authors":["D Jurgens, S Kumar, R Hoover, D McFarland… - arXiv preprint arXiv: …, 2016"],"snippet":"... The classifier is implemented using SciKit (Pedregosa et al., 2011) and syntactic processing was done using CoreNLP (Manning et al., 2014). Selectional preferences used pretrained 300-dimensional vectors from the 840B token Common Crawl (Pennington et al., 2014). ...","url":["http://arxiv.org/pdf/1609.00435"]}
{"year":"2016","title":"CNRC at SemEval-2016 Task 1: Experiments in crosslingual semantic textual similarity","authors":["C Lo, C Goutte, M Simard - Proceedings of SemEval, 2016"],"snippet":"... The system was 3We use the glm function in R. 669 Page 3. trained using standard resources – Europarl, Common Crawl (CC) and News & Commentary (NC) – totaling approximately 110M words in each language. Phrase ...","url":["http://anthology.aclweb.org/S/S16/S16-1102.pdf"]}
{"year":"2016","title":"Commonsense Knowledge Base Completion","authors":["X Li, A Taheri, L Tu, K Gimpel"],"snippet":"... We use the GloVe (Pennington et al., 2014) embeddings trained on 840 billion tokens of Common Crawl web text and the PARAGRAM-SimLex embeddings of Wieting et al. (2015), which were tuned to have strong performance on the SimLex-999 task (Hill et al., 2015). ...","url":["http://ttic.uchicago.edu/~kgimpel/papers/li+etal.acl16.pdf"]}
{"year":"2016","title":"Comparing Topic Coverage in Breadth-First and Depth-First Crawls Using Anchor Texts","authors":["AP de Vries - Research and Advanced Technology for Digital …, 2016","T Samar, MC Traub, J van Ossenbruggen, AP de Vries - International Conference on …, 2016"],"snippet":"... nl domain, with the goal to crawl websites as completes as possible. The second crawl was collected by the Common Crawl foundation using a breadth-first strategy on the entire Web, this strategy focuses on discovering as many links as possible. ...","url":["http://books.google.de/books?hl=en&lr=lang_en&id=VmTUDAAAQBAJ&oi=fnd&pg=PA133&dq=%22common+crawl%22&ots=STVgD4vke3&sig=Gr5Q94wWtvFSfT_EYf1cQGP-Mrg","http://link.springer.com/chapter/10.1007/978-3-319-43997-6_11"]}
{"year":"2016","title":"COMPARISON OF DISTRIBUTIONAL SEMANTIC MODELS FOR RECOGNIZING TEXTUAL ENTAILMENT.","authors":["Y WIBISONO, DWIH WIDYANTORO… - Journal of Theoretical & …, 2016"],"snippet":"... To our knowledge, this paper is the first study of various DSM on RTE. We found that DSM improves entailment accuracy, with the best DSM is GloVe trained with 42 billion tokens taken from Common Crawl corpus. ... Glove_42B Common Crawl 42 billion tokens ...","url":["http://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=19928645&AN=120026939&h=neaFgJXHcv5SjyzIFWIJp046Uq5Cr3qfiPCmXc4DYTEi9kN6SN9YQqm1CUdjmDg%2BwZzzXWI6ftJLniJiB6Go1g%3D%3D&crl=c"]}
{"year":"2016","title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge","authors":["R Speer, J Chin, C Havasi - arXiv preprint arXiv:1612.03975, 2016"],"snippet":"... 2013), and the GloVe 1.2 embeddings trained on 840 billion words of the Common Crawl (Pennington, Socher, and Manning 2014). These matrices are downloadable, and we will be using them both as a point of comparison and as inputs to an ensemble. ...","url":["https://arxiv.org/pdf/1612.03975"]}
{"year":"2016","title":"Content Selection through Paraphrase Detection: Capturing different Semantic Realisations of the Same Idea","authors":["E Lloret, C Gardent - WebNLG 2016, 2016"],"snippet":"... either sentences or pred-arg structures, GLoVe pre-trained WE vectors (Pennington et al., 2014) were used, specifically the ones derived from Wikipedia 2014+ Gi- gaword 5 corpora, containing around 6 billion to- kens; and the ones derived from a Common Crawl, with 840 ...","url":["https://webnlg2016.sciencesconf.org/data/pages/book.pdf#page=33"]}
{"year":"2016","title":"Corporate Smart Content Evaluation","authors":["R Schäfermeier, AA Todor, A La Fleur, A Hasan… - 2016"],"snippet":"Page 1. Fraunhofer FOKUS FRAUNHOFER INSTITUTE FOR OPEN COMMUNICATION SYSTEMS FOKUS STUDY – CORPORATE SMART CONTENT EVALUATION Page 2. Page 3. STUDY – CORPORATE SMART CONTENT EVALUATION ...","url":["http://www.diss.fu-berlin.de/docs/servlets/MCRFileNodeServlet/FUDOCS_derivate_000000006523/CSCStudie2016.pdf"]}
{"year":"2016","title":"Crawl and crowd to bring machine translation to under-resourced languages","authors":["A Toral, M Esplá-Gomis, F Klubička, N Ljubešić… - Language Resources and …"],"snippet":"... Wikipedia. The CommonCrawl project 5 should be mentioned here as it allows researchers to traverse a frequently updated crawl of the whole web in search of specific data, and therefore bypass the data collection process. ...","url":["http://link.springer.com/article/10.1007/s10579-016-9363-6"]}
{"year":"2016","title":"Cross Site Product Page Classification with Supervised Machine Learning","authors":["J HUSS"],"snippet":"... An other data set used often is Common Crawl [1], which is a possible source that contain product specification pages. The data of Common Crawl is not complete with HTML-source code and it was collected in 2013, which creates many dead links. ...","url":["http://www.nada.kth.se/~ann/exjobb/jakob_huss.pdf"]}
{"year":"2016","title":"CSA++: Fast Pattern Search for Large Alphabets","authors":["S Gog, A Moffat, M Petri - arXiv preprint arXiv:1605.05404, 2016"],"snippet":"... The latter were extracted from a sentence-parsed prefix of the German and Spanish sections of the CommonCrawl5. The four 200 ... translation process described by Shareghi et al., corresponding to 40,000 sentences randomly selected from the German part of Common Crawl...","url":["http://arxiv.org/pdf/1605.05404"]}
{"year":"2016","title":"CUNI-LMU Submissions in WMT2016: Chimera Constrained and Beaten","authors":["A Tamchyna, R Sudarikov, O Bojar, A Fraser - Proceedings of the First Conference on …, 2016"],"snippet":"... We use all the data available to constrained submissions: Europarl v8 (Koehn, 2005) and SE- TIMES2 (Tiedemann, 2009) parallel corpora and News 2015 and Common Crawl monolingual corpora.1 We split the official development set into two halves; we use the first part for ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2325.pdf"]}
{"year":"2016","title":"D6. 3: Improved Corpus-based Approaches","authors":["CP Escartin, LS Torres, CO UoW, AZ UMA, S Pal - 2016"],"snippet":"... Based on this system and the data retrieved from Common Crawl, several websites were identified as possible candidates for crawling. ... 8http://commoncrawl.org/ 9For a description of this tool, see Section 3.1.2 in this Deliverable. 6 Page 9. ...","url":["http://expert-itn.eu/sites/default/files/outputs/expert_d6.3_20160921_improved_corpus-based_approaches.pdf"]}
{"year":"2016","title":"Data Selection for IT Texts using Paragraph Vector","authors":["MS Duma, W Menzel - Proceedings of the First Conference on Machine …, 2016"],"snippet":"... models/doc2vec.html 3http://commoncrawl.org/ 4https://github.com/melix/jlangdetect 5-gram LMs using the SRILM toolkit (Stolcke, 2002) with Kneser-Ney discounting (Kneser and Ney, 1995) on the target side of the Commoncrawl and IT corpora. ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2331.pdf"]}
{"year":"2016","title":"David W. Embley, Mukkai S. Krishnamoorthy, George Nagy &","authors":["S Seth"],"snippet":"... tabulated data on the web even before Big Data became a byword [1]. Assuming “that an average table contains on average 50 facts it is possible to extract more than 600 billion facts taking into account only the 12 billion sample tables found in the Common Crawl” [2]. Tables ...","url":["https://www.ecse.rpi.edu/~nagy/PDF_chrono/2016_Converting%20Web%20Tables,IJDAR,%2010.1007_s10032-016-0259-1.pdf"]}
{"year":"2016","title":"Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translatin","authors":["J Zhou, Y Cao, X Wang, P Li, W Xu - arXiv preprint arXiv:1606.04199, 2016"],"snippet":"... 4.1 Data sets For both tasks, we use the full WMT'14 parallel corpus as our training data. The detailed data sets are listed below: • English-to-French: Europarl v7, Common Crawl, UN, News Commentary, Gigaword • English-to-German: Europarl v7, Common</","url":["http://arxiv.org/pdf/1606.04199"]}
{"year":"2016","title":"Deeper Machine Translation and Evaluation for German","authors":["E Avramidis, V Macketanz, A Burchardt, J Helcl… - 2nd Deep Machine …, 2016"],"snippet":"... corpus entries words Chromium browser 6.3 K 55.1 K Drupal 4.7 K 57.4 K Libreoffice help 46.8 K 1.1 M Libreoffice UI 35.6 K 143.7 K Ubuntu Saucy 182.9 K 1.6 M Europarl (mono) 2.2 M 54.0 M News (mono) 89M 1.7 B Commoncrawl (parallel) 2.4 M 53.6 M Europarl (parallel ...","url":["http://www.aclweb.org/anthology/W/W16/W16-64.pdf#page=35"]}
{"year":"2016","title":"DeepNNNER: Applying BLSTM-CNNs and Extended Lexicons to Named Entity Recognition in Tweets","authors":["F Dugas, E Nichols - WNUT 2016, 2016"],"snippet":"... 850M words) 50 130K 51.43% 75.31% 84.63% 82.18% GloVe 6B Gigaword5+ Wikipedia (6B words) 50 400k 54.22% 82.71% 86.02% 84.17% GloVe 27B Twitter microposts (27B words) 50 1.2 M 57.47% 90.47% 83.67% 97.66% GloVe 42B Common Crawl (42B words) 300 1.9 ...","url":["http://www.aclweb.org/anthology/W/W16/W16-39.pdf#page=190"]}
{"year":"2016","title":"Detecting Opinion Polarities using Kernel Methods","authors":["R Kaljahi, J Foster - PEOPLES 2016, 2016"],"snippet":"... 0.7, 0.8, 0.9}. The pre-trained word vectors used are the publicly available ones trained using GloVe (Pennington et al., 2014) trained on 42B-token corpus of Common Crawl (1.9 M vocabulary) with 300 dimensions. 4 4.1 Word ...","url":["http://www.aclweb.org/anthology/W/W16/W16-43.pdf#page=74"]}
{"year":"2016","title":"DEVELOPMENT AND APPLICATION OF A STAGE-GATE PROCESS TO REDUCE THE UNERLYING RISKS OF IT SERVICE PROJECTS","authors":["E JEONG, SR JEONG, MS RAO, VV KUMAR… - Journal of Theoretical and …, 2016"],"snippet":"... RTE. To our knowledge, this paper is the first study of various DSM on RTE. We found that DSM improves entailment accuracy, with the best DSM is GloVe trained with 42 billion tokens taken from Common Crawl corpus. We ...","url":["http://jatit.org/volumes/ninetythree2.php"]}
{"year":"2016","title":"DFKI's system for WMT16 IT-domain task, including analysis of systematic errors","authors":["E Avramidis, A Burchardt, V Macketanz, A Srivastava - Proceedings of the First …, 2016"],"snippet":"... Europarl (mono) 2.2M 54.0M News (mono) 89M 1.7B Commoncrawl (parallel) 2.4M 53.6M Europarl (parallel) 1.9M 50.1M MultiUN (parallel) 167.6K 5.8M News Crawl (parallel) 201.3K 5.1M Table 1: Size of corpora used for SMT. ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2329.pdf"]}
{"year":"2016","title":"Dialogue Act Classification in Domain-Independent Conversations Using a Deep Recurrent Neural Network","authors":["H Khanpour, N Guntakandla, R Nielsen"],"snippet":"... Word2vec embeddings were learned from Google News (Mikolov et al., 2013), and separately, from Wikipedia1. The Glove embeddings were pretrained on the 840 billion token Common Crawl corpus. Method Resource Dimension Accuracy (%) Word2vec Wikipedia 75 70.73 ...","url":["http://www.aclweb.org/anthology/C/C16/C16-1189.pdf"]}
{"year":"2016","title":"Discontinuous Verb Phrases in Parsing and Machine Translation of English and German","authors":["S Loaiciga Sanchez, K Gulordava - 2016","S Loáiciga, K Gulordava"],"snippet":"... all 2https://translate.google.com/ 2841 Page 5. monolingual data (Heafield et al., 2013). Approximately 4.5 million sentences were used for training, combining Europarlv7, CommonCrawl and News data. Optimization weights ...","url":["http://archive-ouverte.unige.ch/unige:84454/ATTACHMENT01","http://www.lrec-conf.org/proceedings/lrec2016/pdf/628_Paper.pdf"]}
{"year":"2016","title":"Discovering Disease-associated Drugs Using Web Crawl Data","authors":["H Kim, S Park - 2016"],"snippet":"... System Online) database is used for the biomedical literature. 81.7GB sized text data of Common Crawl which Amazon Web Services hosts is used for the web crawl data. Gene symbol, Disease name and Drug name which ...","url":["http://delab.yonsei.ac.kr/files/paper/Kim_ACMSAC2016_CR.PDF"]}
{"year":"2016","title":"Discriminating between similar languages and arabic dialect identification: A report on the third dsl shared task","authors":["S Malmasi, M Zampieri, N Ljubešic, P Nakov, A Ali… - Proceedings of the 3rd …, 2016"],"snippet":"... PITEOG used their own custom web-based corpus, with no further details provided. • SUKI created an additional dataset using web pages in the Common Crawl corpus. 5 Results for Subtask 2: Arabic Dialect Identification ...","url":["https://pdfs.semanticscholar.org/7478/2d315d5cd472bef874ffbca589cc2285a99f.pdf"]}
{"year":"2016","title":"Distributed Graph Storage And Querying System","authors":["J Balaji - 2016"],"snippet":"... generation and consumption of such interrelated data. The number of Facebook users grew from 500 million in 2010 to 1.5 billion in 2014 [1]. The Common Crawl web corpora [2] covered for 2012 contains 3.5 billion web pages and 128 billion hyperlinks between these ...","url":["http://scholarworks.gsu.edu/cgi/viewcontent.cgi?article=1112&context=cs_diss"]}
{"year":"2016","title":"Distributed Platforms and Cloud Services: Enabling Machine Learning for Big Data","authors":["D Pop, G Iuhasz, D Petcu - Data Science and Big Data Computing, 2016"],"snippet":"... Data sets are growing faster, being common now to reach numbers of 100 TB or more. The Sloan Digital Sky Survey occupies 5 TB of storage, the Common Crawl Web corpus is 81 TB in size, and the 1000 Genomes Project requires 200 TB of space, just to name a few. ...","url":["http://link.springer.com/chapter/10.1007/978-3-319-31861-5_7"]}
{"year":"2016","title":"DRAFT: Interoperation Among Web Archiving Technologies","authors":["DSH Rosenthal, N Taylor, J Bailey - 2016"],"snippet":"... WET (WARC Encapsulated Text, or Parsed Text), specified by Common Crawl and Internet Archive respectively, to represent only the text extracted from Web resources. 3 Page 4. ... [12] Common CrawlCommon Crawl. https:// commoncrawl.org/.","url":["http://www.lockss.org/tmp/Interoperation2016.pdf"]}
{"year":"2016","title":"Dual Learning for Machine Translation","authors":["Y Xia, D He, T Qin, L Wang, N Yu, TY Liu, WY Ma - arXiv preprint arXiv:1611.00179, 2016"],"snippet":"... In detail, we used the same bilingual corpora from WMT'14 as used in [1, 6], which contains 12M sentence pairs extracting from five datasets: Europarl v7, Common Crawl corpus, UN corpus, News Commentary, and 109French-English corpus. ...","url":["https://arxiv.org/pdf/1611.00179"]}
{"year":"2016","title":"Dynamic Coattention Networks For Question Answering","authors":["C Xiong, V Zhong, R Socher - arXiv preprint arXiv:1611.01604, 2016"],"snippet":"... We use as GloVe word vectors pretrained on the 840B Common Crawl corpus (Pennington et al., 2014). We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out-of-vocabulary words to zero. ...","url":["https://arxiv.org/pdf/1611.01604"]}
{"year":"2016","title":"Edinburgh's Statistical Machine Translation Systems for WMT16","authors":["P Williams, R Sennrich, M Nadejde, M Huck, B Haddow… - Proceedings of the First …, 2016"],"snippet":"... Our final system used two different countbased 5-gram language models (one trained on all data, including the WMT16 Romanian CommonCrawl corpus, without pruning, and one trained on news2015 monolingual only), a neural language model trained on news2015 ...","url":["http://www.statmt.org/wmt16/pdf/W16-2327.pdf"]}
{"year":"2016","title":"Efficient construction of metadata-enhanced web corpora","authors":["A Barbaresi - ACL 2016, 2016"],"snippet":"... 3.3 Extraction I designed a text extraction targeting specifically WordPress pages, which is transferable to a whole range of self-hosted websites using WordPress, al- lowing to reach various blogger profiles thanks 8http://commoncrawl. org 9https://github. ...","url":["http://iiegn.eu/assets/outputs/WAC-X:2016.pdf#page=17"]}
{"year":"2016","title":"Efficient Data Selection for Bilingual Terminology Extraction from Comparable Corpora","authors":["A Hazem, E Morin"],"snippet":"... We used the FrenchEnglish aligned version at OPUS provided by JRC (Tiedemann, 2012). Common crawl corpus (CC) is a petabytes of data collected over 7 years of web crawling set of raw web page data and text extracts5. ...","url":["http://www.aclweb.org/anthology/C/C16/C16-1321.pdf"]}
{"year":"2016","title":"Electronic Commerce Meets the Semantic Web","authors":["J Jovanovic, E Bagheri - IT Professional, 2016"],"snippet":"... The latest Common Crawl corpus (Winter 2014; http://commoncrawl.org) consists of 2.01 billion HTML pages collected from more than 15.68 million pay-level domains (PLDs). An analysis of this corpus shows that 30 percent ...","url":["http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7535074"]}
{"year":"2016","title":"Embedded Sensors System Applied to Wearable Motion Analysis in Sports","authors":["A Valade, A Costes, A Bouillod, M Mangin, P Acco… - 2016"],"snippet":"... the general aspect is good: the mean error is 2.5 degrees and the standard deviation is of 6 degrees. In a second time, we tested the system behaviour on a common crawl swimming movement to ensure the functionality on complex actions. 5.2.2 Tests on Sportsmen ...","url":["https://www.researchgate.net/profile/Anthony_Bouillod/publication/301721501_Embedded_Sensors_System_Applied_to_Wearable_Motion_Analysis_in_Sports/links/572dc45708aee022975a5858.pdf"]}
{"year":"2016","title":"Enabling Network Security Through Active DNS Datasets","authors":["A Kountouras, P Kintis, C Lever, Y Chen, Y Nadji… - Research in Attacks, …, 2016","Y Nadji, D Dagon, M Antonakakis, R Joffe - … in Attacks, Intrusions, and Defenses: 19th …, 2016"],"url":["http://www.cc.gatech.edu/~ynadji3/docs/pubs/activedns.pdf"]}
{"year":"2016","title":"Engineering a Distributed Full-Text Index","authors":["J Fischer, F Kurpicz, P Sanders - arXiv preprint arXiv:1610.03332, 2016"],"snippet":"... For our experiments we use the common crawl corpus as input.1 It provides world wide web crawl data and contains raw content, text only and metadata of the crawled websites from about 1.23 billion web pages. In total the corpus has a size of 541 TB (as of 27.07.2016). ...","url":["https://arxiv.org/pdf/1610.03332"]}
{"year":"2016","title":"Engineering top-k document retrieval systems based on succinct data structures","authors":["S Gog, P Sanders"],"snippet":"... relevant documents to a given query”. For large sets of documents, eg web crawls like gov2, clueweb, or CommonCrawl, time-efficient solutions rely on precomputed index data structures. For collections of natural language ...","url":["https://formal.iti.kit.edu/teaching/projektgruppe/themen/WiSe1617/sanders16.pdf"]}
{"year":"2016","title":"Enriching Product Ads with Metadata from HTML Annotations","authors":["P Ristoski, P Mika - The Semantic Web. Latest Advances and New …, 2016"],"snippet":"... Offers - WDC Microdata Dataset: The latest extraction of WebDataCommons includes over 5 billion entities marked up by one of the three main HTML markup languages (ie, Microdata, Microformats and RDFa) and has been retrieved from the CommonCrawl 2014 corpus 5 ...","url":["http://link.springer.com/chapter/10.1007/978-3-319-34129-3_10"]}
{"year":"2016","title":"Examining the Relationship between Preordering and Word Order Freedom in Machine Translation","authors":["J Daiber, M Stanojevic, W Aziz, K Sima'an"],"snippet":"Page 1. Examining the Relationship between Preordering and Word Order Freedom in Machine Translation Joachim Daiber Miloš Stanojevic Wilker Aziz Khalil Sima'an Institute for Logic, Language and Computation (ILLC) University of Amsterdam {initial.last}@uva.nl Abstract ...","url":["http://jodaiber.github.io/doc/wmt2016.pdf"]}
{"year":"2016","title":"Explorations in Identifying and Summarizing Subjective Content in Text","authors":["P Kumar, V Venugopal"],"snippet":"... puted word embeddings using pre-trained word vectors from GloVe [14] (300-dimensional vectors trained on the 840B token CommonCrawl dataset) to encode our input tokens for both the opinion identification and the opinion summarization task. ...","url":["http://cs224d.stanford.edu/reports/poorna.pdf"]}
{"year":"2016","title":"Exploring Corpora","authors":["C Barrière - Natural Language Understanding in a Semantic Web …, 2016"],"snippet":"Page 1. Chapter 5 Exploring Corpora In previous chapters, we have worked with very small corpora. Some were even constructed manually to specifically illustrate a particular point, and they have generally contained ten to twenty sentences each. ...","url":["http://link.springer.com/chapter/10.1007/978-3-319-41337-2_5"]}
{"year":"2016","title":"Exploring the Application Potential of Relational Web Tables","authors":["C Bizer"],"snippet":"... crawls. This situation has changed in 2012 with the University of Mannheim [7] and in 2014 with the Dresden University of Technology [3] starting to extract web table corpora from the CommonCrawl, a large public web corpus. ...","url":["http://ceur-ws.org/Vol-1670/paper-07.pdf"]}
{"year":"2016","title":"Fast Connected Components Computation in Large Graphs by Vertex Pruning","authors":["A Lulli, E Carlini, P Dazzi, C Lucchese, L Ricci"],"snippet":"... Consider, for instance, the Web graph (Common Crawl provides 3.5 billion pages with 128 billion hyperlinks[1]), the Linked Open Data datasets (the LOD2 project indexes for 5.7 billion triples/edges [2]) or the Facebook and Twitter social networks (respectively 1.35 billion and ...","url":["http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7515231"]}
{"year":"2016","title":"Fast Gated Neural Domain Adaptation: Language Model as a Case Study","authors":["J Zhang, X Wu, A Way, Q Liu"],"snippet":"... The glove 6b model is trained on Wikipedia data and the English Gigaword Fifth Edition corpus;7 the glove 42b model is trained on the Common Crawl data; and the glove 840b model is trained on the the Common Crawl and additional web data. ...","url":["http://www.computing.dcu.ie/~away/PUBS/2016/Fast_Gated_Neural_Domain_Adaptation.pdf"]}
{"year":"2016","title":"Fast, Small and Exact: Infinite-order Language Modelling with Compressed Suffix Trees","authors":["E Shareghi, M Petri, G Haffari, T Cohn - arXiv preprint arXiv:1608.04465, 2016"],"snippet":"Page 1. Fast, Small and Exact: Infinite-order Language Modelling with Compressed Suffix Trees Ehsan Shareghi,♭ Matthias Petri,♮ Gholamreza Haffari♭ and Trevor Cohn♮ ♭ Faculty of Information Technology, Monash University ...","url":["http://arxiv.org/pdf/1608.04465"]}
{"year":"2016","title":"FBK HLT-MT Participation in the 1st Translation Memory Cleaning Shared Task","authors":["D Ataman, MJ Sabet, M Turchi, M Negri"],"snippet":"... The English-German corpus was formed using KDE4, GNOME, OpenOffice, PHP, Ubuntu, Tatoeba (Tiedemann, 2012), Europarl v.7 (Koehn, 2005), CommonCrawl (WMT 2013) (Bojar et al., 2013), News Commentary v.11 (WMT 2015) (Bojar et al., 2015), MultiUN (Eisele and ...","url":["http://rgcl.wlv.ac.uk/wp-content/uploads/2016/05/fbkhltmt-workingnote.pdf"]}
{"year":"2016","title":"Findings of the 2016 Conference on Machine Translation (WMT16)","authors":["O Bojar, R Chatterjee, C Federmann, Y Graham…"],"snippet":"... Some training corpora were identical from last year (Europarl3, United Nations, French-English 109 corpus, Common Crawl, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and ... Monolingual data sets from CommonCrawl (Buck et al ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2301.pdf","https://cris.fbk.eu/bitstream/11582/307240/1/W16-2301.pdf"]}
{"year":"2016","title":"Findings of the WMT 2016 Bilingual Document Alignment Shared Task","authors":["C Buck, P Koehn - Proceedings of the First Conference on Machine …, 2016"],"snippet":"... Espl`a-Gomis, 2009). 8NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/ shared-task/ 9http://commoncrawl.org/ 10https://sourceforge.net/p/bitextor/wiki/Home/ 555 Page 3. 3 Training and Test Data We made available ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2347.pdf"]}
{"year":"2016","title":"Finki at SemEval-2016 Task 4: Deep Learning Architecture for Twitter Sentiment Analysis","authors":["D Stojanovski, G Strezoski, G Madjarov, I Dimitrovski - Proceedings of SemEval, 2016"],"snippet":"... Our system finki, employs both convolutional and gated recurrent neural networks to obtain a more diverse tweet representation. The network is trained on top of GloVe word embeddings pre-trained on the Common Crawl dataset. ... 154 Page 2. Crawl dataset. ...","url":["http://m-mitchell.com/NAACL-2016/SemEval/pdf/SemEval23.pdf"]}
{"year":"2016","title":"Gathering Alternative Surface Forms for DBpedia Entities","authors":["V Bryl, C Bizer, H Paulheim"],"snippet":"... Surface forms have been extracted in a number of works from Wikipedia labels, redirects, disambiguations and anchor texts of internal Wikipedia links, which we complement with anchor texts of external Wikipedia links from the Common Crawl web corpus. ...","url":["http://ceur-ws.org/Vol-1581/paper2.pdf"]}
{"year":"2016","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation","authors":["Y Wu, M Schuster, Z Chen, QV Le, M Norouzi… - arXiv preprint arXiv: …, 2016"],"snippet":"Page 1. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi yonghui,schuster,zhifengc,qvl,mnorouzi@google.com ...","url":["http://arxiv.org/pdf/1609.08144"]}
{"year":"2016","title":"Grammatical Error Correction: Machine Translation and Classifiers","authors":["A Rozovskaya, D Roth - Urbana"],"snippet":"... Many teams also used native English datasets. The most common ones are the Web1T corpus (Brants and Franz, 2006), the CommonCrawl dataset, which is similar to Web1T, and the English Wikipedia. Several teams used off-the-shelf spellcheckers. ...","url":["http://cogcomp.cs.illinois.edu/papers/RozovskayaRo16.pdf"]}
{"year":"2016","title":"Guided Alignment Training for Topic-Aware Neural Machine Translation","authors":["W Chen, E Matusov, S Khadivi, JT Peter - arXiv preprint arXiv:1607.01628, 2016"],"snippet":"... We first trained a baseline NMT model on English-French WMT data (common-crawl, Europarl v7, and news commentary corpora) for two epochs to get the best result on a development set, and then continued training the same model on the in-domain training set for a few ...","url":["http://arxiv.org/pdf/1607.01628"]}
{"year":"2016","title":"Guided Neural Machine Translation","authors":["F Stahlberg"],"snippet":"Page 1. Guided Neural Machine Translation Felix Stahlberg Department of Engineering University of Cambridge This first year report is submitted as part of the degree of Doctor of Philosophy Queens' College August 2016 Page 2. Page 3. Declaration ...","url":["http://xilef-software.e8u.de/sites/default/files/store/firstyear/firstyear-sgnmt.pdf"]}
{"year":"2016","title":"HeLI, a Word-Based Backoff Method for Language Identification","authors":["T Jauhiainen, K Lindén, H Jauhiainen - Proceedings of the VarDial Workshop, 2016"],"snippet":"... We collected from the Common Crawl 3 corpus all the web pages from the respective domains as in Table 2. When language models were created directly from the pages, the accuracy on the DSL development corpus was 49.86%, which was much ... 3http://commoncrawl.org/ ...","url":["http://web.science.mq.edu.au/~smalmasi/vardial3/pdf/VarDial320.pdf"]}
{"year":"2016","title":"Hybrid Morphological Segmentation for Phrase-Based Machine Translation","authors":["SA Grönroos, S Virpioja, M Kurimo - Proceedings of the First Conference on Machine …, 2016"],"snippet":"... Due to TheanoLM limitations, only the Europarl and News data (but not Common Crawl) were used for training. ... As monolingual data, we used the Finnish side of Europarl-v8, news.2014.fi.shuffled.v2, news.2015.fi.shuffled and Common Crawl...","url":["http://www.aclweb.org/anthology/W/W16/W16-2312.pdf"]}
{"year":"2016","title":"IIT Bombay's English-Indonesian submission at WAT: Integrating Neural Language Models with SMT","authors":["SSAKP Bhattacharyya - WAT 2016, 2016"],"snippet":"... Since Commoncrawl provides raw data by web scraping, the Indonesian data obtained was cleaned for noisy sentences and then tokenized and truecased for training the language model. ... statmt. org/europarl/ 6 http://commoncrawl. org/ 70 Page 85. ...","url":["http://www.aclweb.org/anthology/W/W16/W16-46.pdf#page=82"]}
{"year":"2016","title":"Impact of Data Placement on Resilience in Large-Scale Object Storage Systems","authors":["P Carns, K Harms, J Jenkins, M Mubarak, R Ross…"],"snippet":"... Figure 2 compares the distribution of file sizes across three different example data populations.1 The first two (the contents of the 1000 Genomes [19] catalog of gene sequencing data and the Common Crawl Corpus [20] catalog of web crawler data) are available as Amazon ...","url":["http://storageconference.us/2016/Papers/ImpactOfDataPlacement.pdf"]}
{"year":"2016","title":"Improving Translation Selection with Supersenses","authors":["H Tang, D Xiong, OL de Lacalle, E Agirre"],"snippet":"... We set the Gaussian prior to 1 to avoid overfitting. 4There are 12 subcorpora: commoncrawl, europarl, kde4, news2007, news2008, news2009, news2010, news2011, news2012, newscommentary, openoffice, un 5http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html ...","url":["http://www.aclweb.org/anthology/C/C16/C16-1293.pdf"]}
{"year":"2016","title":"INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual Aspect-based Sentiment Analysis","authors":["S Ruder12, P Ghaffari, JG Breslin - Proceedings of SemEval, 2016"],"snippet":"... respective task. English word embeddings are initialized with 300-dimensional GloVe vectors (Pennington et al., 2014) trained on 840B tokens of the Common Crawl corpus for the unconstrained submission. Word embeddings ...","url":["http://www.aclweb.org/anthology/S/S16/S16-1053.pdf"]}
{"year":"2016","title":"Is an Image Worth More than a Thousand Words? On the Fine-Grain Semantic Differences between Visual and Linguistic Representations","authors":["G Collell, MF Moens"],"snippet":"... 3.2 Word Embeddings We employ 300-dimensional GloVe vectors (Pennington et al., 2014) pre-trained in the largest available corpus (840B tokens and a 2.2M words vocabulary from Common Crawl corpus) from the author's website1. ...","url":["http://www.aclweb.org/anthology/C/C16/C16-1264.pdf"]}
{"year":"2016","title":"JU-USAAR: A Domain Adaptive MT System","authors":["K Pahari, A Kuila, S Pal, SK Naskar, S Bandyopadhyay… - Proceedings of the First …, 2016"],"snippet":"... In this task, the information technology (IT) do- main English–German parallel corpus released in the WMT-2016 IT-domain shared task serves as the in-domain data and the Europarl, News and Common Crawl English–German parallel corpus released in the Translation Task ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2333.pdf"]}
{"year":"2016","title":"Language Models with GloVe Word Embeddings","authors":["V Makarenkov, B Shapira, L Rokach - arXiv preprint arXiv:1610.03759, 2016"],"snippet":"... Despite the huge size of the Common Crawl corpus, some words may not exist with the embeddings, so we set these words to random vectors, and use the same embeddings consistently if we encounter the same unseen word again in the text. ...","url":["https://arxiv.org/pdf/1610.03759"]}
{"year":"2016","title":"Language Semantic Embeddings in Deep Visual Representation","authors":["Y Chen - 2016"],"snippet":"... As for the final system (shown in Figure 1.1), word representation is learned based on pre-trained GloVe model with Common Crawl data [5]. 2. Examine how to learn visual representation from weakly supervised dataset (Pixabay is the image data source) with ConvNets. ...","url":["http://www.nada.kth.se/~ann/exjobb/yanbei_chen.pdf"]}
{"year":"2016","title":"Large-scale evaluation of splicing localization algorithms for web images","authors":["M Zampoglou, S Papadopoulos, Y Kompatsiaris - Multimedia Tools and Applications, 2016"],"snippet":"... In reality, especially for the Web-based forensics case, despite the recent proliferation of PNG files, JPEG remains the norm: it is indicative that among the contents of the Common Crawl corpus,2 87 % of identifiable image suffixes correspond to JPEG (.jpg, .jpeg). ...","url":["http://link.springer.com/article/10.1007/s11042-016-3795-2"]}
{"year":"2016","title":"Latent Space Inference of Internet-Scale Networks","authors":["Q Ho, J Yin, EP Xing - Journal of Machine Learning Research, 2016"],"snippet":"Page 1. Journal of Machine Learning Research 17 (2016) 1-41 Submitted 4/15; Published 4/16 Latent Space Inference of Internet-Scale Networks Qirong Ho∗ hoqirong@gmail.com Institute for Infocomm Research A*STAR Singapore 138632 Junming Yin∗ ...","url":["http://www.jmlr.org/papers/volume17/15-142/15-142.pdf"]}
{"year":"2016","title":"Learning to recognise named entities in tweets by exploiting weakly labelled data","authors":["KJ Espinosa, R Batista-Navarro, S Ananiadou - WNUT 2016, 2016"],"snippet":"... Pre-trained Word Embeddings Description Text Type Twitter 2B tweets, 27B tokens, 1.2 M vo- cabulary words, uncased, 100 di- mensions Tweets Common Crawl 840B tokens, 2.2 M vocabulary words, cased, 300 dimensions Web Pages Wikipedia 2014+ Gigaword 5 6B tokens ...","url":["http://www.aclweb.org/anthology/W/W16/W16-39.pdf#page=165"]}
{"year":"2016","title":"Learning to refine text based recommendations","authors":["Y Gu, T Lei, R Barzilay, T Jaakkola"],"snippet":"... Word Vectors: For the ingredient/product prediction task, we used the GloVe pre-trained vectors (Common Crawl, 42 billion tokens, 300dimensional) (Pennington et al., 2014). The word vectors for the AskUbuntu vectors are pre-trained using the AskUbuntu and Wikipedia ...","url":["https://people.csail.mit.edu/taolei/papers/emnlp16_recommendation.pdf"]}
{"year":"2016","title":"Learning to translate from graded and negative relevance information","authors":["L Jehl, S Riezler"],"snippet":"Page 1. Learning to translate from graded and negative relevance information Laura Jehl Computational Linguistics Heidelberg University 69120 Heidelberg, Germany jehl@cl.uni-heidelberg.de Stefan Riezler Computational ...","url":["https://pdfs.semanticscholar.org/79ee/9b20f0776affab912a3528d604e152cc1217.pdf"]}
{"year":"2016","title":"Lexical Coherence Graph Modeling Using Word Embeddings","authors":["M Mesgar, M Strube - Proceedings of NAACL-HLT, 2016"],"snippet":"... 1971). We use a pretrained model of GloVe for word embeddings. This model is trained on Common Crawl with 840B tokens, 2.2M vocabulary. We represent each word by a vector with length 300 (Pennington et al., 2014). For ...","url":["http://www.aclweb.org/anthology/N/N16/N16-1167.pdf"]}
{"year":"2016","title":"LIMSI@ WMT'16: Machine translation of news","authors":["A Allauzen, L Aufrant, F Burlot, E Knyazeva… - Proc. of the ACL 2016 First …, 2016"],"snippet":"... Having noticed many sentence alignment errors and out-of-domain parts in the Russian common-crawl parallel corpus, we have used a bilingual sentence aligner3 and proceeded to a domain adaptation filtering using the same procedure as for monolingual data (see ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2304.pdf"]}
{"year":"2016","title":"Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing","authors":["M Junczys-Dowmunt, R Grundkiewicz - arXiv preprint arXiv:1605.04800, 2016"],"snippet":"... 4. The German monolingual common crawl corpus — a very large resource of raw German text from the Common Crawl project — admissible for the WMT-16 news translation and IT translation tasks. 3.2 Preand post-processing ...","url":["http://arxiv.org/pdf/1605.04800"]}
{"year":"2016","title":"Lurking Malice in the Cloud: Understanding and Detecting Cloud Repository as a Malicious Service","authors":["X Liao, S Alrwais, K Yuan, L Xing, XF Wang, S Hao… - Proceedings of the 2016 …, 2016"],"snippet":"... Running the scanner over all the data collected by the Common Crawl [?], which indexed five billion web pages, for those associated with all major cloud storage providers (including Amazon S3, Cloudfront, Google Drive, etc.), we found around 1 million sites utilizing 6,885 ...","url":["http://dl.acm.org/citation.cfm?id=2978349"]}
{"year":"2016","title":"Machine Translation Quality and Post-Editor Productivity","authors":["M Sanchez-Torron, P Koehn - AMTA 2016, Vol., 2016"],"snippet":"... corresponding Spanish human reference translations. We trained nine MT systems with training data from the European Parliament proceedings, News Commentary, Common Crawl, and United Nations. The systems are phrase ...","url":["https://www.researchgate.net/profile/John_Ortega3/publication/309765044_Fuzzy-match_repair_using_black-box_machine_translation_systems_what_can_be_expected/links/5822496f08ae7ea5be6af317.pdf#page=22"]}
{"year":"2016","title":"Machine Translation Through Learning From a Communication Game","authors":["D He, Y Xia, T Qin, L Wang, N Yu, T Liu, WY Ma - Advances In Neural Information …, 2016"],"snippet":"... In detail, we used the same bilingual corpora from WMT'14 as used in [1, 5], which contains 12M sentence pairs extracting from five datasets: Europarl v7, Common Crawl corpus, UN corpus, News Commentary, and 109French-English corpus. ...","url":["http://papers.nips.cc/paper/6468-machine-translation-through-learning-from-a-communication-game.pdf"]}
{"year":"2016","title":"Measuring semantic similarity of words using concept networks","authors":["G Recski, E Iklódi, K Pajkossy, A Kornai"],"snippet":"... We extend this set of models with GloVe vectors4 (Pennington et al., 2014), trained on 840 billion tokens of Common Crawl data5, and the two word embeddings mentioned in Section 1 that have recently been evaluated on the SimLex dataset: the 500-dimension SP model6 ...","url":["http://www.kornai.com/Papers/wordsim.pdf"]}
{"year":"2016","title":"Models and Inference for Prefix-Constrained Machine Translation","authors":["J Wuebker, S Green, J DeNero, S Hasan, MT Luong"],"snippet":"... The English-French bilingual training data consists of 4.9M sentence pairs from the Common Crawl and Europarl corpora from WMT 2015 (Bo- jar et al., 2015). The LM was estimated from the target side of the bitext. For English-German we run large-scale experiments. ...","url":["http://nlp.stanford.edu/pubs/wuebker2016acl_prefix.pdf"]}
{"year":"2016","title":"Multi-cultural Wikipedia mining of geopolitics interactions leveraging reduced Google matrix analysis","authors":["KM Frahm, SE Zant, K Jaffrès-Runser… - arXiv preprint arXiv: …, 2016"],"snippet":"... At present directed networks of real systems can be very large (about 4.2 million articles for the English Wikipedia edition in 2013 [13] or 3.5 billion web pages for a publicly ac- cessible web crawl that was gathered by the Common Crawl Foundation in 2012 [18]). ...","url":["https://arxiv.org/pdf/1612.07920"]}
{"year":"2016","title":"Multi-Perspective Context Matching for Machine Comprehension","authors":["Z Wang, H Mi, W Hamza, R Florian - arXiv preprint arXiv:1612.04211, 2016"],"snippet":"... jpurkar et al., 2016). To initialize the word embeddings in the word representation layer, we use the 300-dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus (Pennington et al., 2014). For the out ...","url":["https://arxiv.org/pdf/1612.04211"]}
{"year":"2016","title":"N-gram language models for massively parallel devices","authors":["N Bogoychev, A Lopez"],"snippet":"... benchmark task computes perplexity on data ex- tracted from the Common Crawl dataset used for the 2013 Workshop on Machine Translation, which ... statmt.org/moses/RELEASE-3.0/models/ fr- en/lm/europarl.lm.1 7http://www.statmt.org/wmt13/training-parallelcommoncrawl.tgz ...","url":["http://homepages.inf.ed.ac.uk/s1031254/publications/n-gram-language.pdf"]}
{"year":"2016","title":"Neural Architectures for Fine-grained Entity Type Classification","authors":["S Shimaoka, P Stenetorp, K Inui, S Riedel - arXiv preprint arXiv:1606.01341, 2016"],"snippet":"... Rocktäschel et al., 2015). For this purpose, we used the freely available 300-dimensional cased word embeddings trained on 840 billion tokens from the Common Crawl supplied by Pennington et al. (2014). For words not present ...","url":["http://arxiv.org/pdf/1606.01341"]}
{"year":"2016","title":"Neural Interactive Translation Prediction","authors":["R Knowles, P Koehn - AMTA 2016, Vol., 2016"],"snippet":"... The data consists of a 115 million word parallel corpus (Europarl, News Commentary, CommonCrawl), 3http://www. statmt. ... and about 75 billion words of additional English monolingual data (LDC Gigaword, monolingual news, monolingual CommonCrawl). ...","url":["https://www.researchgate.net/profile/John_Ortega3/publication/309765044_Fuzzy-match_repair_using_black-box_machine_translation_systems_what_can_be_expected/links/5822496f08ae7ea5be6af317.pdf#page=113"]}
{"year":"2016","title":"Neural Machine Translation with Pivot Languages","authors":["Y Cheng, Y Liu, Q Yang, M Sun, W Xu - arXiv preprint arXiv:1611.04928, 2016"],"snippet":"... We use the statistical significance test with paired bootstrap resampling [Koehn, 2004]. Table 1 shows the Spanish-English and English-French corpora from WMT which include Common Crawl, News Commentary, Europarl v7 and UN. ...","url":["https://arxiv.org/pdf/1611.04928"]}
{"year":"2016","title":"Neural Machine Translation with Recurrent Attention Modeling","authors":["Z Yang, Z Hu, Y Deng, C Dyer, A Smola - arXiv preprint arXiv:1607.05108, 2016"],"snippet":"... 3 Experiments & Results 3.1 Data sets We experiment with two data sets: WMT EnglishGerman and NIST Chinese-English. • English-German The German-English data set contains Europarl, Common Crawl and News Commentary corpus. ...","url":["http://arxiv.org/pdf/1607.05108"]}
{"year":"2016","title":"Neural Network-based Word Alignment through Score Aggregation","authors":["J Legrand, M Auli, R Collobert - arXiv preprint arXiv:1606.09560, 2016"],"snippet":"... For LSE, we set r = 1 in (4). We initialize the word embeddings with a simple PCA computed over the matrix of word co- occurrence counts (Lebret and Collobert, 2014). The co-occurrence counts were computed over the common crawl corpus provided by WMT16. ...","url":["http://arxiv.org/pdf/1606.09560"]}
{"year":"2016","title":"Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision","authors":["C Liang, J Berant, Q Le, KD Forbus, N Lao - arXiv preprint arXiv:1611.00020, 2016"],"snippet":"... All the weight matrices are initialized with a uniform distribution in [− √ 3 d , √ 3 d] where d is the input dimension. For pretrained word embeddings, we used the 300 dimension GloVe word embeddings trained on 840B common crawl corpus [? ]. ...","url":["https://arxiv.org/pdf/1611.00020"]}
{"year":"2016","title":"NewsQA: A Machine Comprehension Dataset","authors":["A Trischler, T Wang, X Yuan, J Harris, A Sordoni… - arXiv preprint arXiv: …, 2016"],"snippet":"... Both mLSTM and BARB are implemented with the Keras framework (Chollet, 2015) using the Theano (Bergstra et al., 2010) backend. Word embeddings are initialized using GloVe vectors (Pennington et al., 2014) pre-trained on the 840-billion Common Crawl corpus. ...","url":["https://arxiv.org/pdf/1611.09830"]}
{"year":"2016","title":"Normalized Log-Linear Interpolation of Backoff Language Models is Efficient","authors":["K Heafield, C Geigle, S Massung, L Schwartz - Urbana"],"snippet":"Page 1. Normalized Log-Linear Interpolation of Backoff Language Models is Efficient Kenneth Heafield University of Edinburgh 10 Crichton Street Edinburgh EH8 9AB United Kingdom kheafiel@inf.ed.ac.uk Chase Geigle Sean ...","url":["https://kheafield.com/professional/edinburgh/interpolate_paper.pdf"]}
{"year":"2016","title":"NRC Russian-English Machine Translation System for WMT 2016","authors":["C Lo, C Cherry, G Foster, D Stewart, R Islam… - Proceedings of the First …, 2016"],"snippet":"... They include the CommonCrawl corpus, the NewsCommentary v11 corpus, the Yandex corpus and the Wikipedia headlines corpus. ... Due to resource limits, we have not used the newly re- leased 3 billion sentence CommonCrawl monolingual English corpus. ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2317.pdf"]}
{"year":"2016","title":"of Deliverable: Multimedia Linking and Mining","authors":["K Andreadou, S Papadopoulos, M Zampoglou… - 2016"],"snippet":"Page 1. D3.2 – Multimedia Linking and Mining Version: v1.4 – Final, Date: 02/03/2016 PROJECT TITLE: REVEAL CONTRACT NO. FP7-610928 PROJECT COORDINATOR: INTRASOFT INTERNATIONAL SA WWW.REVEALPROJECT.EU PAGE 1 OF 139 REVEAL FP7-610928 ...","url":["http://revealproject.eu/wp-content/uploads/D3.2Multimedia-Linking-and-Mining.pdf"]}
{"year":"2016","title":"On Approximately Searching for Similar Word Embeddings","authors":["K Sugawara, H Kobayashi, M Iwasaki"],"snippet":"... GV 300-dimensional embeddings (Pennington et al., 2014a) learned by the global vectors for word representation (GloVe) model (Pennington et al., 2014b) using Common Crawl corpora, which contain about 2 million words and 42 billion tokens. ...","url":["http://www.aclweb.org/anthology/P/P16/P16-1214.pdf"]}
{"year":"2016","title":"On Bias-free Crawling and Representative Web Corpora","authors":["R Schäfer, H Allee - ACL 2016, 2016"],"snippet":"... Language Resources and Evaluation. Online first: DOI 10.1007/s10579-016-9359-2. Roland Schäfer. 2016b. CommonCOW: Massively huge web corpora from CommonCrawl data and a method to distribute them freely under restrictive EU copyright laws. ...","url":["http://iiegn.eu/assets/outputs/WAC-X:2016.pdf#page=81"]}
{"year":"2016","title":"On the Ubiquity of Web Tracking: Insights from a Billion-Page Web Crawl","authors":["S Schelter, J Kunegis - arXiv preprint arXiv:1607.07403, 2016"],"snippet":"... We extract third-party embeddings from more than 3.5 billion web pages of the CommonCrawl 2012 corpus, and aggregate those to a dataset containing more than 140 million third-party embeddings in over 41 million domains. ...","url":["http://arxiv.org/pdf/1607.07403"]}
{"year":"2016","title":"Online tracking: A 1-million-site measurement and analysis","authors":["S Englehardt, A Narayanan - 2016"],"snippet":"... AdFisher builds on similar technologies as OpenWPM (Selenium, xvfb), but is not intended for tracking measurements. Common Crawl4 uses an Apache Nutch based crawler. The Common Crawl dataset is the largest publicly available web crawl5, with billions of page visits. ...","url":["http://senglehardt.com/papers/ccs16_online_tracking.pdf"]}
{"year":"2016","title":"Optimizing Interactive Development of Data-Intensive Applications","authors":["M Interlandi, SD Tetali, MA Gulzar, J Noor, T Condie… - Proceedings of the Seventh …, 2016"],"snippet":"... 1. 311 service requests dataset. https://data.cityofnewyork.us/Social-Services/311-ServiceRequests-from-2010-to-Present/erm2-nwe9. 2. Common crawl dataset. http://commoncrawl.org. 3. Hadoop. http://hadoop.apache.org. 4. Spark. http://spark.apache.org. 5. WikiReverse. ...","url":["http://dl.acm.org/citation.cfm?id=2987565"]}
{"year":"2016","title":"Paragraph Vector for Data Selection in Statistical Machine Translation","authors":["MS Duma, W Menzel"],"snippet":"... As general domain data we chose the Commoncrawl corpus1 as it is a relatively large corpus and contains crawled data from a variety of domains as well as texts having different discourse types 1http://commoncrawl.org/ (including spoken discourse). ...","url":["https://www.linguistics.rub.de/konvens16/pub/11_konvensproc.pdf"]}
{"year":"2016","title":"Parallel Graph Processing on Modern Multi-Core Servers: New Findings and Remaining Challenges","authors":["A Eisenman, L Cherkasova, G Magalhaes, Q Cai…"],"snippet":"Page 1. Parallel Graph Processing on Modern Multi-Core Servers: New Findings and Remaining Challenges Assaf Eisenman1,2, Ludmila Cherkasova2, Guilherme Magalhaes3, Qiong Cai2, Sachin Katti1 1Stanford University ...","url":["http://www.labs.hpe.com/people/lucy_cherkasova/papers/main-mascots16.pdf"]}
{"year":"2016","title":"ParFDA for Instance Selection for Statistical Machine Translation","authors":["E Biçici - Proceedings of the First Conference on Machine …, 2016"],"snippet":"... Compared with last year, this year we do not use Common Crawl parallel corpus except for en-ru. We use Common Crawl monolingual corpus fi, ro, and tr datasets and we extended the LM corpora with previous years' corpora. ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2306.pdf"]}
{"year":"2016","title":"Partitioning Trillion-edge Graphs in Minutes","authors":["GM Slota, S Rajamanickam, K Devine, K Madduri - arXiv preprint arXiv:1610.07220, 2016"],"snippet":"Page 1. Partitioning Trillion-edge Graphs in Minutes George M. Slota Computer Science Department Rensselaer Polytechnic Institute Troy, NY slotag@rpi.edu Sivasankaran Rajamanickam & Karen Devine Scalable Algorithms ...","url":["https://arxiv.org/pdf/1610.07220"]}
{"year":"2016","title":"Performance Optimization Techniques and Tools for Distributed Graph Processing","authors":["V Kalavri - 2016"],"snippet":"Page 1. Performance Optimization Techniques and Tools for Distributed Graph Processing VASILIKI KALAVRI School of Information and Communication Technology KTH Royal Institute of Technology Stockholm, Sweden 2016 ...","url":["http://www.diva-portal.org/smash/get/diva2:968786/FULLTEXT02"]}
{"year":"2016","title":"Phishing Classification using Lexical and Statistical Frequencies of URLs","authors":["S Villegas, AC Bahnsen, J Vargas"],"snippet":"... We used a sample of 1.2 million phishing URLs extracted from Phishtank and 1.2 million ham URLs from the CommonCrawl corpus to train the model. Classification based on URLs facilitates a defense against all phishing attacks due to the feature they all share, a URL. ...","url":["http://albahnsen.com/files/Phishing%20Classification%20using%20Lexical%20and%20Statistical%20Frequencies%20of%20URLs.pdf"]}
{"year":"2016","title":"Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction","authors":["M Junczys-Dowmunt, R Grundkiewicz - arXiv preprint arXiv:1605.06353, 2016"],"snippet":"... (2016). We marked the participants of the CoNLL-2014 shared task as unrestricted as some participants made use of Common Crawl data or Google n-grams. 3 Dense feature optimization ... Wikipedia 213.08 M 3.37 G CommonCrawl (u) 59.13 G 975.63 G ...","url":["http://arxiv.org/pdf/1605.06353"]}
{"year":"2016","title":"Phrase-Based SMT for Finnish with More Data, Better Models and Alternative Alignment and Translation Tools","authors":["J Tiedemann, F Cap, J Kanerva, F Ginter, S Stymne… - Proceedings of the First …, 2016"],"snippet":"... on document level. Five-gram language models are trained using KenLM (Heafield et al., 2013). The English language model based on the provided Common Crawl data is limited to trigrams. Pre-Processing Tools: For processing ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2326.pdf"]}
{"year":"2016","title":"PJAIT Systems for the WMT 2016","authors":["K Wołk, K Marasek - Proceedings of the First Conference on Machine …, 2016"],"snippet":"... “BASE” in the tables represents the baseline SMT system. “EXT” indicates results for the baseline system, using the baseline settings but extended with additional permissible data (limited to parallel Europarl v7, Common Crawl, ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2328.pdf"]}
{"year":"2016","title":"Porting an Open Information Extraction System from English to German","authors":["T Falke, G Stanovsky, I Gurevych, I Dagan"],"snippet":"... For this purpose, we created a new dataset consisting of 300 German sentences, randomly sampled from three sources of different genres: news articles from TIGER (Brants et al., 2004), German web pages from CommonCrawl (Habernal et al., 2016) and featured Wikipedia ...","url":["https://www.ukp.tu-darmstadt.de/fileadmin/user_upload/Group_UKP/publikationen/2016/EMNLP_2016_PropsDE_cr.pdf"]}
{"year":"2016","title":"Practical Variable Length Gap Pattern Matching","authors":["J Bader, S Gog, M Petri - Experimental Algorithms, 2016"],"snippet":"... implemented on top of SDSL [7] data structures. We use three datasets from different application domains: The CC data set is a \\(371 \\,\\mathrm{GiB}\\) prefix of a recent \\(145 \\,\\mathrm{TiB}\\) web crawl from commoncrawl.​org. ...","url":["http://link.springer.com/chapter/10.1007/978-3-319-38851-9_1"]}
{"year":"2016","title":"Pre-Translation for Neural Machine Translation","authors":["J Niehues, E Cho, TL Ha, A Waibel - arXiv preprint arXiv:1610.05243, 2016"],"snippet":"... The systems were trained on all parallel data available for the WMT 20161. The news commentary corpus, the European parliament proceedings and the common crawl corpus sum up to 3.7M sentences and around 90M words. ...","url":["https://arxiv.org/pdf/1610.05243"]}
{"year":"2016","title":"Predicting Motivations of Actions by Leveraging Text","authors":["C Vondrick, D Oktay, H Pirsiavash, A Torralba - … of the IEEE Conference on Computer …, 2016"],"snippet":"... In ECCV. 2012. [5] C. Buck, K. Heafield, and B. van Ooyen. N-gram counts and language models from the common crawl. LREC, 2014. [6] X. Chen, A. Shrivastava, and A. Gupta. Neil: Extracting visual knowledge from web data. In ICCV, 2013. 3004 Page 9. ...","url":["http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Vondrick_Predicting_Motivations_of_CVPR_2016_paper.html"]}
{"year":"2016","title":"Privacy issues in online machine translation services–European perspective","authors":["P Kamocki, J O'Regan - 2016"],"snippet":"... 1/11/2014. Retrieved from http://itre.cis.upenn.edu/~myl/languagelog/archives/005 492.html Smith, JR et al. (2013). Dirt cheap web-scale parallel text from the Common Crawl. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. ...","url":["https://ids-pub.bsz-bw.de/files/5043/Kamocki-ORegan_Privacy_issues_in_online_machine_translation_2016.pdf"]}
{"year":"2016","title":"Query Answering to IQ Test Questions Using Word Embedding","authors":["M Frąckowiak, J Dutkiewicz, C Jędrzejek, M Retinger… - Multimedia and Network …, 2017"],"snippet":"... The pre-trained model based on Google News [16]. Embedding vector size 300, the negative sampling count as 3. 8. Glove Small. Pre-trained model based on Glove approach [20] using common crawl data, accessible on [15]. Embedding vector size 300. 9. Glove Large. ...","url":["http://link.springer.com/chapter/10.1007/978-3-319-43982-2_25"]}
{"year":"2016","title":"Query Expansion with Locally-Trained Word Embeddings","authors":["F Diaz, B Mitra, N Craswell - arXiv preprint arXiv:1605.07891, 2016"],"snippet":"... the entire corpus. Instead of training a global embedding on the large web collection, we use a GloVe embedding trained on Common Crawl data.4 We train local embeddings using one of three retrieval sources. First, we consider ...","url":["http://arxiv.org/pdf/1605.07891"]}
{"year":"2016","title":"Real-Time Presentation Tracking Using Semantic Keyword Spotting","authors":["R Asadi, HJ Fell, T Bickmore, H Trinh"],"snippet":"... gathered from a large corpus. We use a pre-trained vector representation with 1.9 million uncased words and vectors with 300 elements. It was trained using 42 billion tokens of web data from Common Crawl. We will use both the ...","url":["http://relationalagents.com/publications/Interspeech2016.pdf"]}
{"year":"2016","title":"Recurrent versus Recursive Approaches Towards Compositionality in Semantic Vector Spaces","authors":["A Nayebi, H Blundell"],"snippet":"... 0.01, and 1They were in fact trained on 840 billion tokens of Common Crawl data, as in http://nlp.stanford.edu/ projects/glove/. Adagrad (with the default learning rate of 0.01) as our optimizer, with a minibatch size of 300. Addi ...","url":["http://web.stanford.edu/~anayebi/projects/CS_224U_Final_Project_Writeup.pdf"]}
{"year":"2016","title":"Relatedness","authors":["C Barrière - Natural Language Understanding in a Semantic Web …, 2016"],"snippet":"... GloVe has some datasets trained on Wikipedia 2014 + Gigaword 5 (large news corpus) for a total of 6 billion tokens, covering a 400K vocabulary. It has other datasets based on an even larger corpus, the Common Crawl. To ...","url":["http://link.springer.com/chapter/10.1007/978-3-319-41337-2_10"]}
{"year":"2016","title":"Reordering space design in statistical machine translation","authors":["N Pécheux, A Allauzen, J Niehues, F Yvon - Language Resources and Evaluation"],"snippet":"... in (Allauzen et al. 2013), and, for English-Czech, the Europarl and CommonCrawl parallel WMT'12 corpora. For each task, a 4-gram language model is estimated using the target side of the training data. We use Ncode with ...","url":["http://link.springer.com/article/10.1007/s10579-016-9353-8"]}
{"year":"2016","title":"Richer Interpolative Smoothing Based on Modified Kneser-Ney Language Modeling","authors":["E Shareghi, T Cohn, G Haffari"],"snippet":"... Interdependency of m, data size, and discounts To explore the correlation between these factors we selected the German and investigated this correlation on two different training data sizes: Europarl (61M words), and CommonCrawl 2014 (984M words). ...","url":["http://people.eng.unimelb.edu.au/tcohn/papers/shareghi16emnlp.pdf"]}
{"year":"2016","title":"Scaling Up Word Clustering","authors":["J Dehdari, L Tan, J van Genabith"],"snippet":"... The parallel data comes from the WMT-2015 Common Crawl Corpus, News Commentary, Yandex 1M Corpus, and the Wiki Headlines Corpus.7 The monolingual data consists of 2007– 2014 News Commentary and News Crawl articles. ...","url":["http://anthology.aclweb.org/N/N16/N16-3009.pdf"]}
{"year":"2016","title":"Selecting Domain-Specific Concepts for Question Generation With Lightly-Supervised Methods","authors":["Y Jin, PTV Le"],"snippet":"... 3 Datasets We make use of two datasets obtained from the In- ternet. One is 200k company profiles from CrunchBase. Another is 57k common crawl business news articles. We refer to these two corpora as “Company Profile Corpus” and “News Corpus”. ...","url":["https://www.researchgate.net/profile/Yiping_Jin2/publication/304751113_Selecting_Domain-Specific_Concepts_for_Question_Generation_With_Lightly-Supervised_Methods/links/57cfc28208ae057987ac127c.pdf"]}
{"year":"2016","title":"Semantic Snippets via Query-Biased Ranking of Linked Data Entities","authors":["M Alsarem - 2016"],"snippet":"Page 1. Semantic Snippets via Query-Biased Ranking of Linked Data Entities Mazen Alsarem To cite this version: Mazen Alsarem. Semantic Snippets via Query-Biased Ranking of Linked Data Entities. In- formation Retrieval [cs.IR]. ...","url":["https://hal.archives-ouvertes.fr/tel-01327769/document"]}
{"year":"2016","title":"Semantic word embedding neural network language models for automatic speech recognition","authors":["K Audhkhasi, A Sethy, B Ramabhadran - 2016 IEEE International Conference on …, 2016"],"snippet":"... The Gigaword corpus was a suitable choice because of its focus on news domain data in- stead of generic data sets such as Wikipedia or Common Crawl. We used a symmetric window size of 10 words for constructing the word co-occurrence matrix. ...","url":["http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7472828"]}
{"year":"2016","title":"Semantics derived automatically from language corpora necessarily contain human biases","authors":["A Caliskan-Islam, JJ Bryson, A Narayanan - arXiv preprint arXiv:1608.07187, 2016"],"snippet":"... Page 9. GloVe authors provide trained embeddings, which is a “Common Crawl” corpus obtained from a large-scale crawl of the web, containing 840 billion tokens (roughly, words). Tokens in this corpus are case-sensitive and ...","url":["http://arxiv.org/pdf/1608.07187"]}
{"year":"2016","title":"Session: P44-Corpus Creation and Querying (1)","authors":["MK Bingel, P Banski, A Witt, C Data, F Lefevre, M Diab…"],"snippet":"... 960 Roland Schäfer CommonCOW: Massively Huge Web Corpora from CommonCrawl Data and a Method to Distribute them Freely under Restrictive EU Copyright Laws 990 Ioannis Manousos Katakis, Georgios Petasis and Vangelis Karkaletsis ...","url":["https://pdfs.semanticscholar.org/f62d/6d9b67532ccd66915481b7cb4047ba03a1f2.pdf"]}
{"year":"2016","title":"Shared Task on Quality Assessment for Text Simplification","authors":["S Štajner, M Popovic, H Saggion, L Specia, M Fishel - Training"],"snippet":"... The parameters for the ensemble were obtained using particle swarm optimisation under multiple cross-validation scenarios. 2. Treelstm – The metric uses GloVe word vectors8 trained on the Common Crawl corpus and dependency parse trees. ...","url":["https://www.researchgate.net/profile/Maja_Popovic7/publication/301229567_Shared_Task_on_Quality_Assessment_for_Text_Simplification/links/570e179e08ae3199889d4eb5.pdf"]}
{"year":"2016","title":"Sheffield Systems for the English-Romanian Translation Task","authors":["F Blain, X Song, L Specia"],"snippet":"... For the two last, we use subsets of both the News Commentary (93%) and the Common Crawl (13%), selected using XenC- v2.12 (Rousseau, 2013) in mode 23 with the parallel corpora (Europarl7, SETimes2) as in-domain data. ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2307.pdf"]}
{"year":"2016","title":"SoK: Applying Machine Learning in Security-A Survey","authors":["H Jiang, J Nagra, P Ahammad - arXiv preprint arXiv:1611.03186, 2016"],"snippet":"Page 1. SoK: Applying Machine Learning in Security - A Survey Heju Jiang* , Jasvir Nagra, Parvez Ahammad ∗ Instart Logic, Inc. {hjiang, jnagra, pahammad }@instartlogic.com ABSTRACT The idea of applying machine learning ...","url":["https://arxiv.org/pdf/1611.03186"]}
{"year":"2016","title":"Source Sentence Simplification for Statistical Machine Translation","authors":["E Hasler, A de Gispert, F Stahlberg, A Waite, B Byrne - Computer Speech & Language, 2016"],"snippet":"... translation lattices. We trained an English-German system on the WMT 2015 training data (Bojar et al., 2015) comprising 4.2M parallel sentences from the Europarl, News Commentary v10 and Commoncrawl corpora. We word ...","url":["http://www.sciencedirect.com/science/article/pii/S0885230816301711"]}
{"year":"2016","title":"Syntactically Guided Neural Machine Translation","authors":["F Stahlberg, E Hasler, A Waite, B Byrne - arXiv preprint arXiv:1605.04569, 2016"],"snippet":"... The En-De training set includes Europarl v7, Common Crawl, and News Commentary v10. Sentence pairs with sentences longer than 80 words or length ratios exceeding 2.4:1 were deleted, as were Common Crawl sentences from other languages (Shuyo, 2010). ...","url":["http://arxiv.org/pdf/1605.04569"]}
{"year":"2016","title":"SYSTEMS AND METHODS FOR SPEECH TRANSCRIPTION","authors":["A Hannun, C Case, J Casper, B Catanzaro, G Diamos… - US Patent 20,160,171,974, 2016"],"snippet":"... the decoding. The language model was trained on 220 million phrases of the Common Crawl (available at commoncrawl.org), selected such that at least 95% of the characters of each phrase were in the alphabet. Only the most ...","url":["http://www.freepatentsonline.com/y2016/0171974.html"]}
{"year":"2016","title":"TAIPAN: Automatic Property Mapping for Tabular Data","authors":["I Ermilov, ACN Ngomo"],"snippet":"... RAM. Gold Standard We aimed to use T2D entity-level Gold Standard (T2D), a reference dataset which consists of 1 748 tables and reflects the actual distribution of the data in the Common Crawl,5 to evaluate our algorithms. ...","url":["http://svn.aksw.org/papers/2016/EKAW_Taipan/public.pdf"]}
{"year":"2016","title":"Target-Side Context for Discriminative Models in Statistical Machine Translation","authors":["A Tamchyna, A Fraser, O Bojar, M Junczys-Dowmunt - arXiv preprint arXiv: …, 2016"],"snippet":"... Our English-German system is trained on the data available for the WMT14 translation task: Europarl (Koehn, 2005) and the Common Crawl corpus,3 roughly 4.3 million sentence pairs altogether. We tune the system on the WMT13 test set and we test on the WMT14 set. ...","url":["http://arxiv.org/pdf/1607.01149"]}
{"year":"2016","title":"TAXI at SemEval-2016 Task 13: a Taxonomy Induction Method based on Lexico-Syntactic Patterns, Substrings and Focused Crawling","authors":["A Panchenko, S Faralli, E Ruppert, S Remus, H Naets…"],"snippet":"... 59G 59.2 – – – CommonCrawl 168000.0 ‡ – – – FocusedCrawl Food 22.8 7.9 3.4 3.6 ... WebISA. In addition to PattaMaika and PatternSim, we used a publicly available database of English hypernym relations extracted from the CommonCrawl corpus (Seitner et al., 2016). ...","url":["http://web.informatik.uni-mannheim.de/ponzetto/pubs/panchenko16.pdf"]}
{"year":"2016","title":"Temporal Attention-Gated Model for Robust Sequence Classification","authors":["W Pei, T Baltrušaitis, DMJ Tax, LP Morency - arXiv preprint arXiv:1612.00385, 2016"],"snippet":"... 4.2.2 Experimental Setup We utilize 300-d Glove word vectors pretrained over the Common Crawl [27] as the features for each word of the sentences. Our model is well suitable to perform sentiment analysis using sentence-level labels. ...","url":["https://arxiv.org/pdf/1612.00385"]}
{"year":"2016","title":"The 2016 KIT IWSLT Speech-to-Text Systems for English and German","authors":["TS Nguyen, M Müller, M Sperber, T Zenkel, K Kilgour…"],"snippet":"... Page 4. Text corpus # Words TED 3.6m Fisher 10.4m Switchboard 1.4m TEDLIUM dataselection 155m News + News-commentary + -crawl 4,478m Commoncrawl 185m GIGA 2323m Table 3: English language modeling data. Text corpus # Words ...","url":["http://workshop2016.iwslt.org/downloads/IWSLT_2016_paper_24.pdf"]}
{"year":"2016","title":"The AFRL-MITLL WMT16 News-Translation Task Systems","authors":["J Gwinnup, T Anderson, G Erdmann, K Young, M Kazi… - Proceedings of the First …, 2016"],"snippet":"... systems used only the constrained data supplied when training. 2.1 Data Usage In training our systems we drew on all the available data, filtering the new English Common Crawl monolingual data as described in §2.4 and §3.1. ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2313.pdf"]}
{"year":"2016","title":"The CogALex-V Shared Task on the Corpus-Based Identification of Semantic Relations","authors":["E Santus, A Gladkova, S Evert, A Lenci - COLING 2016, 2016"],"snippet":"... Team Method (s) Corpus size Corpus GHHH Word analogies, linear regression and multi-task CNN 100B 6B 840B Google News (pre-trained word2vec embeddings, 300 dim.); Wikipedia+ Gigaword 5 (pre-trained GloVe embeddings, 300 dim.), Common Crawl (pre-trained ...","url":["https://sites.google.com/site/cogalex2016/home/accepted-papers/CogALex-V_Proceedings.pdf#page=83"]}
{"year":"2016","title":"The Edinburgh/LMU Hierarchical Machine Translation System for WMT 2016","authors":["M Huck, A Fraser, B Haddow - Proc. of the ACL 2016 First Conf. on Machine …, 2016"],"snippet":"... baseline. Utilizing a larger amount of target-side monolingual resources by appending the CommonCrawl corpus to the background LM's training data is very beneficial and increases the BLEU scores by around one point. ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2315.pdf"]}
{"year":"2016","title":"The Edit Distance Transducer in Action: The University of Cambridge English-German System at WMT16","authors":["F Stahlberg, E Hasler, B Byrne - arXiv preprint arXiv:1606.04963, 2016"],"snippet":"... The parallel training data includes Europarl v7, Common Crawl, and News Commentary v10. Sentence pairs with sentences longer than 80 words or length ratios exceeding 2.4:1 were deleted, as were Common Crawl sentences from other languages (Shuyo, 2010). ...","url":["http://arxiv.org/pdf/1606.04963"]}
{"year":"2016","title":"The ILSP/ARC submission to the WMT 2016 Bilingual Document Alignment Shared Task","authors":["V Papavassiliou, P Prokopidis, S Piperidis - Proceedings of the First Conference on …, 2016"],"snippet":"... 1http://commoncrawl.org/ 2http://nlp.ilsp.gr/redmine/ilsp-fc/ 3Including modules for metadata extraction, language identification, boilerplate removal, document clean-up, text classification and sentence alignment 733 ... Dirt cheap web-scale parallel text from the common crawl...","url":["http://www.aclweb.org/anthology/W/W16/W16-2375.pdf"]}
{"year":"2016","title":"The JHU Machine Translation Systems for WMT 2016","authors":["S Ding, K Duh, H Khayrallah, P Koehn, M Post - … of the First Conference on Machine …, 2016"],"snippet":"... 2.3 Huge Language Model This year, large corpora of monolingual data were extracted from Common Crawl (Buck et al., 2014). We used this data to train 5-gram KneserNey smoothed language models, pruning out 3– 5 gram singletons. ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2310.pdf"]}
{"year":"2016","title":"The Karlsruhe Institute of Technology Systems for the News Translation Task in WMT 2016","authors":["TL Ha, E Cho, J Niehues, M Mediani, M Sperber… - Proceedings of the First …, 2016"],"snippet":"... To im- prove the quality of the Common Crawl corpus be- ing used in training, we filtered out noisy sentence pairs using an SVM classifier as described in (Me- diani et al., 2011). All of our translation systems are basically phrase-based. ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2314.pdf"]}
{"year":"2016","title":"The NTNU-YZU System in the AESW Shared Task: Automated Evalua-tion of Scientific Writing Using a Convolutional Neural Network","authors":["LH Lee, BL Lin, LC Yu, YH Tseng"],"snippet":"... For the GloVe representation, we adopted 4 different datasets for training the vectors including one from Wikipedia 2014 and Gigaword 5 (400K vo- cabulary), two common crawl datasets (uncased 1.9M vocabulary, and cased 2.2M vocabulary) and one Twitter dataset (1.2M ...","url":["http://anthology.aclweb.org/W/W16/W16-0513.pdf"]}
{"year":"2016","title":"The RWTH Aachen Machine Translation System for IWSLT 2016","authors":["JT Peter, A Guta, N Rossenbach, M Graça, H Ney"],"snippet":"... ich war fünf Mal dort oben . . Figure 1: An example of multiple phrasal segmentations taken from the common crawl corpus. The JTR sequence is indicated by blue arcs. The distinct phrasal segmentations are shown in red and shaded green colour. log-linear framework. ...","url":["http://workshop2016.iwslt.org/downloads/IWSLT_2016_paper_23.pdf"]}
{"year":"2016","title":"Topics of Controversy: An Empirical Analysis of Web Censorship Lists","authors":["Z Weinberg, M Sharif, J Szurdi, N Christin - Proceedings on Privacy Enhancing …, 2017"],"snippet":"... Common Crawl Finally, this is the closest available ap- proximation to an unbiased sample of the entire Web. The Common Crawl Foundation continuously operates a large-scale Web crawl and publishes the results [27]. Each crawl contains at least a billion pages. ...","url":["https://www.andrew.cmu.edu/user/nicolasc/publications/Weinberg-PETS17.pdf"]}
{"year":"2016","title":"Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder","authors":["TL Ha, J Niehues, A Waibel - arXiv preprint arXiv:1611.04798, 2016"],"snippet":"... translation and the web-crawled parallel data (CommonCrawl). ... network. We mix the TED parallel corpus and the substantial monolingual corpus (EPPS+NC+ CommonCrawl) and train a mix-source NMT system from those data. ...","url":["https://arxiv.org/pdf/1611.04798"]}
{"year":"2016","title":"Towards a Complete View of the Certificate Ecosystem","authors":["B VanderSloot, J Amann, M Bernhard, Z Durumeric… - 2016"],"snippet":"... In 36th IEEE Symposium on Security and Privacy, May 2015. [5] Certificate Transparency: Extended validation in Chrome. https://www.certificate-transparency.org/ev-ct-plan. [6] Common Crawl. https://commoncrawl.org/. [7] The DROWN attack. https://drownattack.com/. ...","url":["https://jhalderm.com/pub/papers/https-perspectives-imc16.pdf"]}
{"year":"2016","title":"Towards More Accurate Statistical Profiling of Deployed schema. org Microdata","authors":["R Meusel, D Ritze, H Paulheim - Journal of Data and Information Quality (JDIQ), 2016"],"snippet":"... Springer International Publishing. 44. Alex Stolz and Martin Hepp. 2015. Towards crawling the web for structured data: Pitfalls of common crawl for e-commerce. In Proceedings of the 6th International Workshop on Consuming Linked Data (COLD ISWC'15). ...","url":["http://dl.acm.org/citation.cfm?id=2992788"]}
{"year":"2016","title":"Translation of Unknown Words in Low Resource Languages","authors":["B Gujral, H Khayrallah, P Koehn"],"snippet":"... worthy trade-off. Word Embedding: For this technique, we collect Hindi monolingual data from Wikipedia dump (Al-Rfou et al., 2013) and Commoncrawl (Buck et al., 2014),2 with a total of about 29 million tokens. For Uzbek, the ...","url":["https://pdfs.semanticscholar.org/f130/2e20b4dabb48b8442f857426c28b205287f1.pdf"]}
{"year":"2016","title":"TripleSent: a triple store of events associated with their prototypical sentiment","authors":["V Hoste, E Lefever, S van der Waart van Gulik… - eKNOW 2016: The Eighth …, 2016"],"snippet":"... These events will be obtained by extracting patterns for highly explicit sentiment expressions (eg, “I hate” or “I love”) or from large web data crawls (eg, commoncrawl.org), which will subsequently be syntactically and semantically parsed to extract events and sentiment triples. ...","url":["https://biblio.ugent.be/publication/8071695/file/8071708"]}
{"year":"2016","title":"Undercounting File Downloads from Institutional Repositories","authors":["P OBrien, K Arlitsch, L Sterman, J Mixter, J Wheeler… - Journal of Library …, 2016"],"snippet":"Page 1. © Patrick OBrien, Kenning Arlitsch, Leila Sterman, Jeff Mixter, Jonathan Wheeler, and Susan Borda Address correspondence to Patrick OBrien, Semantic Web Research Director, Montana State University, PO Box 173320, Bozeman, MT 59717-3320, USA. ...","url":["http://scholarworks.montana.edu/xmlui/bitstream/handle/1/9943/IR-Undercounting-preprint_2016-07.pdf?sequence=3&isAllowed=y"]}
{"year":"2016","title":"User Modeling in Language Learning with Macaronic Texts","authors":["A Renduchintala, R Knowles, P Koehn, J Eisner - Proceedings of ACL, 2016"],"snippet":"... We translated each German sentence using the Moses Statistical Machine Translation (SMT) toolkit (Koehn et al., 2007). The SMT system was trained on the German-English Commoncrawl parallel text used in WMT 2015 (Bojar et al., 2015). ...","url":["https://www.cs.jhu.edu/~jason/papers/renduchintala+al.acl16-macmodel.pdf"]}
{"year":"2016","title":"Using Feedforward and Recurrent Neural Networks to Predict a Blogger's Age","authors":["T Moon, E Liu"],"snippet":"... The embedding matrix L ∈ RV ×d is initialized for d = 300 with GloVe word vectors trained on the Common Crawl data set [9]. If a token does not correspond to any pre-trained word vector, a random word vector is generated with Xavier initialization [2]. The unembedded vector ...","url":["http://cs224d.stanford.edu/reports/tym1.pdf"]}
{"year":"2016","title":"Vive la petite différence! Exploiting small differences for gender attribution of short texts","authors":["F Gralinski, R Jaworski, Ł Borchmann, P Wierzchon"],"snippet":"... The procedure of preparing the HSSS corpus was to take Common Crawl-based Web corpus1 of Polish [4] and grep for lines ... Classification with Deep Learning (2015) 4. Buck, C., Heafield, K., van Ooyen, B.: N-gram counts and language models from the common crawl...","url":["http://www.staff.amu.edu.pl/~rjawor/tsd-article.pdf"]}
{"year":"2016","title":"Vive la Petite Différence!","authors":["F Graliński, R Jaworski, Ł Borchmann, P Wierzchoń - International Conference on …, 2016"],"snippet":"... The research was conducted on the publicly available corpus called “He Said She Said”, consisting of a large number of short texts from the Polish version of Common Crawl... Keywords. Gender attribution Text classification Corpus Common Crawl Research reproducibility. ...","url":["http://link.springer.com/chapter/10.1007/978-3-319-45510-5_7"]}
{"year":"2016","title":"VoldemortKG: Mapping schema. org and Web Entities to Linked Open Data","authors":["A Tonon, V Felder, DE Difallah, P Cudré-Mauroux"],"snippet":"... that apply to the Common Crawl corpus.12 4 The VoldemortKG Knowledge Graph To demonstrate the potential of the dataset we release, we built a proof of concept knowledge graph called VoldemortKG. VoldemortKG integrates schema.org 12 http://commoncrawl.org/terms ...","url":["http://daplab.ch/wp-content/uploads/2016/08/voldemort.pdf"]}
{"year":"2016","title":"What does the Web remember of its deleted past? An archival reconstruction of the former Yugoslav top-level domain","authors":["A Ben-David - New Media & Society, 2016"],"snippet":"... The completeness of the reconstruction effort could have been aided by consulting other large repositories of temporal Web data, such as Common Crawl, or by simply contacting the Internet Archive and requesting for all domains in the .yu domain. ...","url":["http://nms.sagepub.com/content/early/2016/04/27/1461444816643790.abstract"]}
{"year":"2016","title":"What Makes Word-level Neural Machine Translation Hard: A Case Study on English-German Translation","authors":["F Hirschmann, J Nam, J Fürnkranz"],"snippet":"... 5.1 Dataset & Preprocessing Our models were trained on the data provided by the 2014 Workshop on Machine Translation (WMT). Specifically, we used the Europarl v7, Common Crawl, and News Commentary corpora. Our ...","url":["http://www.aclweb.org/anthology/C/C16/C16-1301.pdf"]}
{"year":"2016","title":"WHAT: A Big Data Approach for Accounting of Modern Web Services","authors":["M Trevisan, I Drago, M Mellia, HH Song, M Baldi - 2016"],"snippet":"... [5] D. Plonka and P. Barford, “Flexible Traffic and Host Profiling via DNS Rendezvous,” in Proc. of the SATIN, 2011, pp. 1–8. [6] “Common Crawl,” http://commoncrawl.org/. [7] A. Finamore et al., “Experiences of Internet Traffic Monitoring with Tstat,” IEEE Netw., vol. 25, no. 3, pp. ...","url":["http://www.tlc-networks.polito.it/mellia/papers/BMLIT_web_meter.pdf"]}
{"year":"2016","title":"Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM","authors":["I Habernal, I Gurevych"],"snippet":"... Memory (BLSTM) neural network for end-to-end processing.9 The input layer relies on pre-trained word embeddings, in particular GloVe (Pennington et al., 2014) trained on 840B tokens from Common Crawl;10 the embedding weights are further updated during training. ...","url":["https://www.informatik.tu-darmstadt.de/fileadmin/user_upload/Group_UKP/publikationen/2016/acl2016-convincing-arguments-camera-ready.pdf"]}
{"year":"2016","title":"Wikipedia mining of hidden links between political leaders","authors":["KM Frahm, K Jaffrès-Runser, DL Shepelyansky - arXiv preprint arXiv:1609.01948, 2016"],"snippet":"... At present directed networks of real systems can be very large (about 4.2 million articles for the English Wikipedia edition in 2013 [10] or 3.5 billion web pages for a publicly accessible web crawl that was gathered by the Common Crawl Foundation in 2012 [28]). ...","url":["http://arxiv.org/pdf/1609.01948"]}
{"year":"2016","title":"WOLVESAAR at SemEval-2016 Task 1: Replicating the Success of Monolingual Word Alignment and Neural Embeddings for Semantic Textual Similarity","authors":["H Bechara, R Gupta, L Tan, C Orasan, R Mitkov… - Proceedings of SemEval, 2016"],"snippet":"... 2We use the 300 dimensions vectors from the GloVe model trained on the Commoncrawl Corpus with 840B tokens, 2.2M vocabulary. distributions p and pθ using regularised KullbackLeibler (KL) divergence. J(θ) = 1 n n ∑ i=1KL(p(i)∣ ∣ ∣ ∣ p (i) θ ) + λ2||θ||2 2 (8) ...","url":["http://www.anthology.aclweb.org/S/S16/S16-1096.pdf"]}
{"year":"2016","title":"Word Representation on Small Background Texts","authors":["L Li, Z Jiang, Y Liu, D Huang - Chinese National Conference on Social Media …, 2016"],"snippet":"... For example, Pennington et al. (2014) used Wikipedia, Gigaword 5 and Common Crawl to learn word representations, each of which contained billions of tokens. There was not always a monotonic increase in performance as the amount of background texts increased. ...","url":["http://link.springer.com/chapter/10.1007/978-981-10-2993-6_12"]}
{"year":"2016","title":"Word2Vec vs DBnary: Augmenting METEOR using Vector Representations or Lexical Resources?","authors":["C Servan, A Berard, Z Elloumi, H Blanchon, L Besacier - arXiv preprint arXiv: …, 2016","C Servan, A Bérard, Z Elloumi, H Blanchon, L Besacier"],"snippet":"... German–English Europarl V7 + news commentary V10 2.1 M 57.2 M 59.7 M Russian–English Common Crawl + news commentary V10 + Yandex 2.0 M 47.2 M 50.3 M Table 2: Bilingual corpora used to train the word embeddings for each language pair. ...","url":["http://www.aclweb.org/anthology/C/C16/C16-1110.pdf","https://arxiv.org/pdf/1610.01291"]}
{"year":"2016","title":"Yandex School of Data Analysis approach to English-Turkish translation at WMT16 News Translation Task","authors":["A Dvorkovich, S Gubanov, I Galinskaya - Proceedings of the First Conference on …, 2016"],"snippet":"... 2.7 Data For training translation model, language models, and NMT reranker, we used only the provided constrained data (SETIMES 2 parallel TurkishEnglish corpus, and monolingual Turkish and En- glish Common Crawl corpora). ...","url":["http://www.aclweb.org/anthology/W/W16/W16-2311.pdf"]}
{"year":"2017","title":"$ k $-Nearest Neighbor Augmented Neural Networks for Text Classification","authors":["Z Wang, W Hamza, L Song - arXiv preprint arXiv:1708.07863, 2017"],"snippet":"... Table 1 shows the statistics of all the datasets. Experiment Settings We initialize word embeddings with the 300-dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus (Pennington, Socher, and Manning, 2014). ...","url":["https://arxiv.org/pdf/1708.07863"]}
{"year":"2017","title":"A Context-Aware Recurrent Encoder for Neural Machine Translation","authors":["B Zhang, D Xiong, J Su, H Duan - IEEE/ACM Transactions on Audio, Speech, and …, 2017"],"snippet":"Page 1. 2329-9290 (c) 2017 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This ...","url":["http://ieeexplore.ieee.org/abstract/document/8031316/"]}
{"year":"2017","title":"A Continuously Growing Dataset of Sentential Paraphrases","authors":["W Lan, S Qiu, H He, W Xu"],"snippet":"... We used 300-dimensional word vectors trained on Common Crawl and Twitter, summed the vectors for each sentence, and computed the cosine similarity. WMF/OrMF Weighted Matrix Factorization (WMF) (Guo and Diab, 2012) is an unsupervised latent space model. ...","url":["https://pdfs.semanticscholar.org/dfd2/bc4a55bfe59554ec1d5086e5c0f5a503c8a8.pdf"]}
{"year":"2017","title":"A Framework for Enriching Lexical Semantic Resources with Distributional Semantics","authors":["C Biemann, S Faralli, A Panchenko, SP Ponzetto - arXiv preprint arXiv:1712.08819, 2017"],"snippet":"… This makes our approach highly scalable: in recent experiments we have been accordingly able to apply our method at web scale on the CommonCrawl6, the largest existing public repository of web content … 6 https://commoncrawl.org Page 15 …","url":["https://arxiv.org/pdf/1712.08819"]}
{"year":"2017","title":"A Hybrid Framework for Text Modeling with Convolutional RNN","authors":["C Wang, F Jiang, H Yang - Proceedings of the 23rd ACM SIGKDD International …, 2017"],"snippet":"Page 1. A Hybrid Framework for Text Modeling with Convolutional RNN Chenglong Wang, Feijun Jiang, Hongxia Yang Alibaba Group 969 West Wenyi Road Hangzhou, China 310000 {chenglong.cl,feijun.jiang ,yang.yhx}@alibaba-inc.com ...","url":["http://dl.acm.org/citation.cfm?id=3098140"]}
{"year":"2017","title":"A Large Self-Annotated Corpus for Sarcasm","authors":["M Khodak, N Saunshi, K Vodrahalli - arXiv preprint arXiv:1704.05579, 2017"],"snippet":"... simple low-dimensional document representation. For word vectors we use normalized 300-dimensional GloVe representations trained on the Common Crawl corpus (Pennington et al., 2014). Since we are establishing baselines ...","url":["https://arxiv.org/pdf/1704.05579"]}
{"year":"2017","title":"A Lightweight Front-end Tool for Interactive Entity Population","authors":["H Oiwa, Y Suhara, J Komiya, A Lopatenko - arXiv preprint arXiv:1708.00481, 2017"],"snippet":"... We prepared models trained based on the CommonCrawl corpus and the Twitter corpus1. Note that the specification of the expansion algorithm is not limited to the algorithm described in this paper, as LUWAK considers the Expansion API as an external function. ...","url":["https://arxiv.org/pdf/1708.00481"]}
{"year":"2017","title":"A Memory-Augmented Neural Model for Automated Grading","authors":["S Zhao, Y Zhang, X Xiong, A Botelho, N Heffernan - 2017"],"snippet":"... networks. We used the publicly available pre-trained Glove word embeddings [23], which was trained on 42 billion tokens of web data, from Common Crawl (http://commoncrawl.org/). The dimension of each word vector is 300. ...","url":["https://siyuanzhao.github.io/pdf/L_S_2017.pdf"]}
{"year":"2017","title":"A Nested Attention Neural Hybrid Model for Grammatical Error Correction","authors":["J Ji, Q Wang, K Toutanova, Y Gong, S Truong, J Gao - arXiv preprint arXiv: …, 2017"],"snippet":"Page 1. A Nested Attention Neural Hybrid Model for Grammatical Error Correction Jianshu Ji†, Qinlong Wang†, Kristina Toutanova‡∗, Yongen Gong†, Steven Truong†, Jianfeng Gao§ †Microsoft AI & Research ‡Google Research ...","url":["https://arxiv.org/pdf/1707.02026"]}
{"year":"2017","title":"A Neural Approach to Source Dependency-Based Context Model for Statistical Machine Translation","authors":["K Chen, T Zhao, M Yang, L Liu, A Tamura, R Wang… - IEEE/ACM Transactions on …, 2017"],"snippet":"Page 1. 2329-9290 (c) 2017 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["http://ieeexplore.ieee.org/abstract/document/8105847/"]}
{"year":"2017","title":"A Neural Chatbot with Personality","authors":["H Nguyen, D Morales, T Chin"],"snippet":"... We also experiment with initializing embeddings for en- coder vocabulary and decoder vocabulary using GLoVe 300d Common Crawl [8]. To ensure that our model was implemented correctly, we trained it on a subset of data (3,000 pairs) and saw that the loss converged to 0 ...","url":["http://web.stanford.edu/class/cs224n/reports/2761115.pdf"]}
{"year":"2017","title":"A Query Log Analysis of Dataset Search","authors":["E Kacprzak, LM Koesten, LD Ibánez, E Simperl… - … 2017, Rome, Italy, June 5-8, …, 2017"],"snippet":"... Cafarella estimates more than one billion sources of data on the web as of February 2011, counting structured data extracted from Web pages [3]; and The Web Data Commons project recently extracted 233 million data tables from the Common Crawl [12]. ...","url":["http://books.google.de/books?hl=en&lr=lang_en&id=kiYmDwAAQBAJ&oi=fnd&pg=PA429&dq=commoncrawl&ots=a4pucnNpg8&sig=rhbU3CaLPahnItWRuhUlAHj8J9E"]}
{"year":"2017","title":"A Semi-supervised Framework for Image Captioning","authors":["W Chen, A Lucchi, T Hofmann"],"snippet":"... we use both the 2008-2010 News-CommonCrawl and Eu- roparl corpus 3 as out-of-domain training data. Combined, these two datasets contain ∼ 3M sentences, from which we removed sentences shorter than 7 words or longer than 30 words. ...","url":["https://pdfs.semanticscholar.org/4fa6/a688f350831503d158f8f618c58d1e06bc5d.pdf"]}
{"year":"2017","title":"A Shared Task on Bandit Learning for Machine Translation","authors":["A Sokolov, J Kreutzer, K Sunderland, P Danchenko"],"snippet":"... Data. For training initial or seed MT systems (the input to Algorithm 1), out-of-domain parallel data was restricted to DE-EN parts of Europarl v7, NewsCommentary v12, CommonCrawl and Rapid data from the WMT 2017 News Translation (constrained) task4. Furthermore ...","url":["http://www.cl.uni-heidelberg.de/~riezler/publications/papers/WMT2017.pdf"]}
{"year":"2017","title":"A simple sequence attention model for machine comprehension","authors":["M Hasegawa"],"snippet":"... The data was tokenized with a basic tokenizer and the Stanford GloVe [1] was used as trained word embeddings. To ensure a larger coverage among the extracted tokens the larger ”Common Crawl” with 840B tokens, 2.2M vocab and 300d vectors embeddings were used. ...","url":["https://web.stanford.edu/class/cs224n/reports/2761077.pdf"]}
{"year":"2017","title":"A Survey of Domain Adaptation for Statistical Machine Translation","authors":["H Cuong, K Sima'an"],"snippet":"... Surprisingly, degradation of translation quality is observed even when we train an MT system on large heterogeneous corpora (eg EuroParl, Common Crawl Corpus, UN Corpus, News Commentary) (Shah et al, 2012; Carpuat et al, 2014; Cuong et al, 2016). ...","url":["https://staff.fnwi.uva.nl/c.hoang/mt20172.pdf"]}
{"year":"2017","title":"A Teacher-Student Framework for Zero-Resource Neural Machine Translation","authors":["Y Chen, Y Liu, Y Cheng, VOK Li"],"snippet":"... For the WMT corpus, we evaluate our approach on a Spanish-French (Es-Fr) translation task with a zero-resource setting. We combine the following corpora to form the Es-En and En-Fr parallel corpora: Common Crawl, News Commentary, Europarl v7 and UN. ...","url":["http://nlp.csai.tsinghua.edu.cn/~ly/papers/acl2017_cy.pdf"]}
{"year":"2017","title":"A Unified Query-based Generative Model for Question Generation and Question Answering","authors":["L Song, Z Wang, W Hamza - arXiv preprint arXiv:1709.01058, 2017"],"snippet":"... The encoder and decoder share the same pre-trained word embeddings, which are the 300-dimensional GolVe (Pennington, Socher, and Manning, 2014) word vectors pre-trained from the 840B common crawl corpus, and the embeddings are not updated during training. ...","url":["https://arxiv.org/pdf/1709.01058"]}
{"year":"2017","title":"A Web Corpus for eCare: Collection, Annotation and Learning-Preliminary Results-DRAFT: 20 March 2017","authors":["M Santini, M Alirezai, M Nyström, A Jönsson"],"snippet":"... web, neither within the ”web as a corpus” experience, nor within the ”wacky” initiative, nor with Common Crawl corpus7. 5 See https://en.wikipedia.org/wiki/Fair_use 6 See https://www.jisc.ac. uk/guides/text-and-data-mining-copyright-exception 7 See http://commoncrawl.org/the ...","url":["https://www.researchgate.net/profile/Marina_Santini/publication/315390867_A_Web_Corpus_for_eCare_Collection_Annotation_and_Learning_-_Preliminary_Results_-/links/58cfb829458515b6ed8c1527/A-Web-Corpus-for-eCare-Collection-Annotation-and-Learning-Preliminary-Results.pdf"]}
{"year":"2017","title":"A Web Corpus for eCare: Collection, Lay Annotation and Learning-First Results","authors":["M Santini, A Jönsson, M Nyström, M Alirezai"],"snippet":"... from the web, neither within the \"web as a corpus\" experience, nor within the \"wacky\" initiative, nor with Common Crawl corpus9. ... for human language technology: introducing an LRE special section\" Lang Resources & Evaluation 2017 51 9See http://commoncrawl.org/the ...","url":["https://www.researchgate.net/profile/Marina_Santini/publication/318379265_A_Web_Corpus_for_eCare_Collection_Lay_Annotation_and_Learning_-First_Results-/links/596650de0f7e9b80917fea3e/A-Web-Corpus-for-eCare-Collection-Lay-Annotation-and-Learning-First-Results.pdf"]}
{"year":"2017","title":"A Web Page Distillation Strategy for Efficient Focused Crawling Based on Optimized Naïve Bayes (ONB) Classifier","authors":["AI Saleh, AE Abulwafa, MF Al Rahmawy - Applied Soft Computing, 2017"],"snippet":"The target of a focused crawler (FC) is to retrieve pages related to a specific domain of interest (DOI). However, FCs may be hasted if bad links were injected.","url":["http://www.sciencedirect.com/science/article/pii/S1568494616306536"]}
{"year":"2017","title":"Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks","authors":["M Hutchinson","W Foland, JH Martin - Proceedings of the 55th Annual Meeting of the …, 2017"],"snippet":"... The use of distributed word representations generated from large text corpora is pervasive in modern NLP. We start with 300 dimension GloVe representations (Pennington et al., 2014) trained on the 840 billion word common crawl (Smith et al., 2013). ...","url":["http://www.aclweb.org/anthology/P17-1043","https://zdoc.pub/abstract-meaning-representation-parsing-using-lstm-recurrent.html"]}
{"year":"2017","title":"Accelerating Innovation Through Analogy Mining","authors":["T Hope, J Chan, A Kittur, D Shahaf - arXiv preprint arXiv:1706.05585, 2017"],"snippet":"... In more formal terms, let wi = (w1 i ,w2 i ,...,wT i) be the sequence of GloVe [27] word vectors (pre-trained on Common Crawl web data), representing (x1 i ,x2 i ,...,xT i ). We select all xi word vectors for which ˜p j ik = 1(˜m j ik = 1) for some k, and concatenate them into one ...","url":["https://arxiv.org/pdf/1706.05585"]}
{"year":"2017","title":"Accurate Sentence Matching with Hybrid Siamese Networks","authors":["M Nicosia, A Moschitti - Proceedings of the 2017 ACM on Conference on …, 2017"],"snippet":"… Their training split contains 384,348 pairs, and the balanced development and test sets contain 10,000 pairs each. The embeddings are a subset of the 300-dimensional GloVe word vectors pretrained on the Common Crawl corpus, 3 covering the Quora dataset vocabulary …","url":["http://dl.acm.org/citation.cfm?id=3133156"]}
{"year":"2017","title":"Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates","authors":["G Collell, L Van Gool, MF Moens - arXiv preprint arXiv:1711.06821, 2017"],"snippet":"… 4.5 Word embeddings We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors' website.8 …","url":["https://arxiv.org/pdf/1711.06821"]}
{"year":"2017","title":"Adaptation and Combination of NMT Systems: The KIT Translation Systems for IWSLT 2016","authors":["E Cho, J Niehues, TL Ha, M Sperber, M Mediani… - Proceedings of the 13th …, 2016"],"snippet":"... to 1.0. We use a beam search for decoding, with the beam size of 12. The baseline systems were trained on the WMT parallel data. For both languages, this consists of the EPPS, NC, CommonCrawl corpus. In addition, we ...","url":["http://workshop2016.iwslt.org/downloads/IWSLT_2016_paper_17.pdf"]}
{"year":"2017","title":"Adapting Sequence Models for Sentence Correction","authors":["A Schmaltz, Y Kim, AM Rush, SM Shieber - arXiv preprint arXiv:1707.09067, 2017"],"snippet":"... provided access to SRILM (Stolcke, 2002) for running Junczys-Dowmunt and Grundkiewicz (2016) 7We found that including the features and data associated with the large language models of Junczys-Dowmunt and Grundkiewicz (2016), created from Common Crawl text ...","url":["https://arxiv.org/pdf/1707.09067"]}
{"year":"2017","title":"Adversarial Training for Cross-Domain Universal Dependency Parsing","authors":["M Sato, H Manabe, H Noji, Y Matsumoto"],"snippet":"... initialized POS tag embeddings. For the model 2The pre-trained word embeddings are provided by the CoNLL 2017 Shared Task organizers. These are trained with CommonCrawl and Wikipedia. with adversarial training, we ...","url":["http://universaldependencies.org/conll17/proceedings/pdf/K17-3007.pdf"]}
{"year":"2017","title":"Agree to Disagree: Improving Disagreement Detection with Dual GRUs","authors":["S Hiray, V Duppada"],"snippet":"... NLP tasks [24] [25]. For this task of (dis)agreement classification, we use GloVe embeddings of 300 dimensions trained on Common Crawl with 840 billion tokens, 2.2 million vocabulary. Page 3. 4.2. Lexicons We used affect, sentiment ...","url":["https://www.deepaffects.com/s/agree-to-disagree.pdf"]}
{"year":"2017","title":"All-but-the-Top: Simple and Effective Postprocessing for Word Representations","authors":["J Mu, S Bhat, P Viswanath - arXiv preprint arXiv:1702.01417, 2017"],"snippet":"... We test our observations on various word representations: four publicly available word representations (WORD2VEC1 (Mikolov et al., 2013) trained using Google News, GLOVE2 (Pennington et al., 2014) trained using Common Crawl, RAND-WALK (Arora et al., 2016 ...","url":["https://arxiv.org/pdf/1702.01417"]}
{"year":"2017","title":"An Empirical Analysis of NMT-Derived Interlingual Embeddings and their Use in Parallel Sentence Identification","authors":["C España-Bonet, ÁC Varga, A Barrón-Cedeño… - arXiv preprint arXiv: …, 2017","J van Genabith, A Barron-Cedeno, C España-Bonet…"],"snippet":"... context vectors. The parallel corpus includes data from United Na- tions (Rafalovitch and Dale, 2009), Common Crawl2, News Commentary3 and IWSLT4. We train system S1-w after cleaning and tokenising the texts. We ...","url":["https://arxiv.org/pdf/1704.05415","https://deepai.org/publication/an-empirical-analysis-of-nmt-derived-interlingual-embeddings-and-their-use-in-parallel-sentence-identification"]}
{"year":"2017","title":"An End-to-End Neural Architecture for Reading Comprehension","authors":["M Burkle, M Camacho, N Danyliw"],"snippet":"... referencing a fixed sized vocabulary using GloVe word embeddings from the Wikipedia 6B word dataset or the CommonCrawl 840B word ... Wikipedia corpus of ∼6 billion words, we moved to the 300-dimensional word embeddings trained on the Common Crawl vocabulary of ...","url":["http://web.stanford.edu/class/cs224n/reports/2761845.pdf"]}
{"year":"2017","title":"An In-Depth Experimental Comparison of RNTNs and CNNs for Sentence Modeling","authors":["Z Ahmadi, M Skowron, A Stier, S Kramer"],"snippet":"... On other datasets, we use the model trained on the web data from Common Crawl which contains a case-sensitive vocabulary of size 2.2 million. Experiments show that RNTNs work best when the word vector dimension is set between 25 and 35 [11]. ...","url":["http://www.ofai.at/~marcin.skowron/papers/DS2017.pdf"]}
{"year":"2017","title":"An overview of Lithuanian Internet media n-gram corpus","authors":["I Bumbuliene, L Boizou, J Mandravickaite, T Krilavicius - 2017"],"snippet":"... 19(1), pp. 61-93, 2013. [2] C. Buck, K. Heafield, B. Van Ooyen, “N-gram counts and language models from the common crawl,” in LREC, vol. 2. Citeseer, p. 4, 2014. [3] A. Pauls, D. Klein, “Faster and smaller n-gram language models,” in Proc. ...","url":["http://ceur-ws.org/Vol-1853/p05.pdf"]}
{"year":"2017","title":"Analogy Mining for Specific Design Needs","authors":["K Gilon, FY Ng, J Chan, HL Assaf, A Kittur, D Shahaf - arXiv preprint arXiv …, 2017"],"snippet":"… We use Glove pre-trained on the Common Crawl dataset (840B tokens, 300d vectors)1. We then normalize each document vector, and calculate cosine similarity (which is the same as Euclidean distance in this case) between the resulting vectors for each seed and all other …","url":["https://arxiv.org/pdf/1712.06880"]}
{"year":"2017","title":"Analysing and Improving embedded Markup of Learning Resources on the Web","authors":["S Dietze, D Taibi, R Yu, P Barker, M d'Aquin - 2017"],"snippet":"... 6 http://commoncrawl.org/ 7 http://grouper.ieee.org/groups/ltsc/wg12/20020612-Final-LOMDraft.html 8 https://www.imsglobal.org ... The Web Data Commons [1], a recent initiative investigating the Common Crawl, ie a Web crawl of approximately 2 billion HTML pages from over ...","url":["https://www.researchgate.net/profile/Stefan_Dietze/publication/313964715_Analysing_and_Improving_embedded_Markup_of_Learning_Resources_on_the_Web/links/58b05d1545851503be97ddfc/Analysing-and-Improving-embedded-Markup-of-Learning-Resources-on-the-Web.pdf"]}
{"year":"2017","title":"Analysis of semantic URLs to support automated linking of structured data on the web","authors":["S Lynden - Proceedings of the 7th International Conference on …, 2017"],"snippet":"... The Web Data Commons [13] effort to study the evolution of structured data on the web analyse the Common Crawl Web Corpus annually, most recently finding that about 38% of web pages contain some form of structured data. ...","url":["http://dl.acm.org/citation.cfm?id=3102265"]}
{"year":"2017","title":"Analyzing Movie Reviews Sentiment","authors":["D Sarkar, R Bali, T Sharma - Practical Machine Learning with Python, 2018"],"snippet":"In this chapter, we continue with our focus on case-study oriented chapters, where we will focus on specific real-world problems and scenarios and how we can use Machine Learning to solve them. We wil.","url":["https://link.springer.com/chapter/10.1007/978-1-4842-3207-1_7"]}
{"year":"2017","title":"Analyzing Neural MT Search and Model Performance","authors":["J Niehues, E Cho, TL Ha, A Waibel - ACL 2017, 2017"],"snippet":"... For the single models, we apply the early stopping based on the validation score. The baseline system is trained on the WMT parallel data, namely EPPS, NC, CommonCrawl and TED corpus. As validation data we used the newstest13 set from IWSLT evaluation campaign. ...","url":["http://www.aclweb.org/anthology/W/W17/W17-32.pdf#page=23"]}
{"year":"2017","title":"Analyzing the compositional properties of word embeddings","authors":["T Scheepers, E Gavves, E Kanoulas"],"snippet":"... 3For GloVe we used the representations from the Common Crawl which has 840B tokens and a vocabulary of 2.2M. ... trained on used news data, where fastText and GloVe use more definitional data, Wikipedia and Common Crawl respectively. ...","url":["https://thijs.ai/papers/scheepers-gavves-kanoulas-analyzing-compositional-properties.pdf"]}
{"year":"2017","title":"Any-gram Kernels for Sentence Classification: A Sentiment Analysis Case Study","authors":["R Kaljahi, J Foster - arXiv preprint arXiv:1712.07004, 2017"],"snippet":"… We use cosine similarity for word embedding similarities and the GloVe (Pennington et al., 2014) Common Crawl (1.9M vocabulary) word embeddings with a dimensionality of 300.6 … (2014). The GloVe Common Crawl vectors, however, performed better. Page 7 …","url":["https://arxiv.org/pdf/1712.07004"]}
{"year":"2017","title":"AraVec: A set of Arabic Word Embedding Models for use in Arabic NLP","authors":["AB Soliman, K Eissa, SR El-Beltagy - Linguistics, 2017"],"snippet":"... Here it is important to note that the Common Crawl project does not provide any technique for identifying or selecting the language of web pages to ... 5 http://www.internetworldstats.com/stats19. htm 6 http://www.internetworldstats.com/stats5.htm 7 http://commoncrawl.org 8 https ...","url":["https://www.researchgate.net/profile/Samhaa_El-Beltagy2/publication/319880027_AraVec_A_set_of_Arabic_Word_Embedding_Models_for_use_in_Arabic_NLP/links/59bfef730f7e9b48a29ba3a8/AraVec-A-set-of-Arabic-Word-Embedding-Models-for-use-in-Arabic-NLP.pdf"]}
{"year":"2017","title":"Architecting for Performance Clarity in Data Analytics Frameworks","authors":["K Ousterhout - 2017"],"snippet":"Page 1. Architecting for Performance Clarity in Data Analytics Frameworks Kay Ousterhout Electrical Engineering and Computer Sciences University of California at Berkeley Technical Report No. UCB/EECS-2017-158 http://www2 ...","url":["https://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-158.pdf"]}
{"year":"2017","title":"Archival Crawlers and JavaScript: Discover More Stuff but Crawl More Slowly","authors":["JF Brunelle, MC Weigle, ML Nelson - Digital Libraries (JCDL), 2017 ACM/IEEE Joint …, 2017"],"snippet":"... If our method was applied to the July 2015 Common Crawl dataset, a web-scale archival crawler will discover an additional 7.17 PB (5.12 times more) of information per year. This illustrates the significant increase in resources necessary for more thorough archival crawls. ...","url":["http://ieeexplore.ieee.org/abstract/document/7991554/"]}
{"year":"2017","title":"Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning","authors":["L Lucy, J Gauthier - arXiv preprint arXiv:1705.11168, 2017"],"snippet":"... attributive features. Collell and Moens (2016) find that word representations fail to pre- # word tokens # word types GloVe (Common Crawl) 840B 2.2M GloVe (Wiki+Gigaword) 6B 400K word2vec 100B 3M Table 1: Statistics ...","url":["https://arxiv.org/pdf/1705.11168"]}
{"year":"2017","title":"Assessing Convincingness of Arguments in Online Debates with Limited Number of Features","authors":["LA Chalaguine, C Schulz"],"snippet":"... 6http://nlp.stanford.edu/projects/glove/ 7http://commoncrawl.org/ 8because including stems, lemmas or both had no impact on the results we included stems only in our “top feature set” because they are less expensive to compute 80 Page 7. ture resulted in 66% in our case. ...","url":["https://www.aclweb.org/anthology/E/E17/E17-4008.pdf"]}
{"year":"2017","title":"ATOL: A Framework for Automated Analysis and Categorization of the Darkweb Ecosystem","authors":["SGPPV Yegneswaran, KNA Das - 2017"],"snippet":"... 2016), and an open repository of (non-onion) Web crawling data, called Common Crawl (Common Crawl Foundation 2016). Using these data sources as starting points, we developed tools to acquire additional onion addresses both from the onion Web and the open Web. ...","url":["http://www.csl.sri.com/users/shalini/atol_aics17_cameraready.pdf"]}
{"year":"2017","title":"Attention-based Dialog Embedding for Dialog Breakdown Detection","authors":["C Park, K Kim, S Kim"],"snippet":"… sentence. We used GloVe vectors of dimension 100 trained by the Twitter data. We used one from Twitter data rather than Common Crawl data be- cause it is more closely related to the general chat domain of our task. After …","url":["http://workshop.colips.org/dstc6/papers/track3_paper14_park.pdf"]}
{"year":"2017","title":"Attributes2Classname: A discriminative model for attribute-based unsupervised zero-shot learning","authors":["B Demirel, RG Cinbis, NI Cinbis - arXiv preprint arXiv:1705.01734, 2017"],"snippet":"... For each class and attribute name, we generate a 300-dimensional word embedding vector using GloVe [26] based on Common Crawl Data2 ... 2http://commoncrawl.org/the-data/ 3http://nlp.stanford. edu/projects/glove/ 4We will release our code and models upon publication. ...","url":["https://arxiv.org/pdf/1705.01734"]}
{"year":"2017","title":"Automated Categorization of Onion Sites for Analyzing the Darkweb Ecosystem","authors":["S Ghosh, A Das, P Porras, V Yegneswaran, A Gehani - 2017"],"snippet":"... Our sources of seed data include various published onion datasets( [32], [5], [25], [22]), .onion references from a large collection of recursive DNS resolvers [17], and an open repository of (non-onion) web crawling data, called Common Crawl [11]. ...","url":["http://www.csl.sri.com/users/gehani/papers/KDD-2017.Onions.pdf"]}
{"year":"2017","title":"Automatic Learning Content Sequence via Linked Open Data","authors":["R Manrique"],"snippet":"... in [14, 13]. We also plan to use a recent release dataset [5] that contains all embedded Learning Resource Metadata Initiative (LRMI)4 markup statements extracted from the Common Crawl releases 2013-2015. Each entity description ...","url":["https://iswc2017.ai.wu.ac.at/wp-content/uploads/papers/DC/paper_19.pdf"]}
{"year":"2017","title":"Automatic Threshold Detection for Data Selection in Machine Translation","authors":["MS Duma, W Menzel - WMT 2017, 2017"],"snippet":"... The in-domain corpora were made available by the competition and the general domain corpora we have chosen to select data from are the Wikipedia corpora (Wolk and Marasek, 2014) and the Commoncrawl corpora1. Experiments 1http://commoncrawl. org/ 483 Page 508. ...","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=507"]}
{"year":"2017","title":"Better Text Understanding Through Image-To-Text Transfer","authors":["K Kurach, S Gelly, M Jastrzebski, P Haeusser… - arXiv preprint arXiv: …, 2017"],"snippet":"... We used word embeddings obtained from three methods: • Glove: embeddings proposed in [20], trained on a Common Crawl dataset with 840 billion tokens. • M-Skip-Gram: embeddings proposed in [12], trained on Wikipedia and a set of images from ImageNet. ...","url":["https://arxiv.org/pdf/1705.08386"]}
{"year":"2017","title":"Biasing Attention-Based Recurrent Neural Networks Using External Alignment Information","authors":["T Alkhouli, H Ney"],"snippet":"... (1). We use the full bilingual data of the English→Romanian task. For the German→English task, we choose the common crawl, news commentary and European parliament bilingual data. ... This is to remove noisy sentence pairs that are frequent in the common crawl corpus. ...","url":["https://www-i6.informatik.rwth-aachen.de/publications/download/1036/Alkhouli-WMT%202017-2017.pdf"]}
{"year":"2017","title":"Big Data","authors":["R Womack"],"snippet":"Page 1. Topics in Data Science / Өгөгдлийн шинжлэх ухаан Rutgers University has made this article freely available. Please share how this access benefits you. Your story matters. [https://rucore.libraries.rutgers.edu/rutgers-lib/52378/story/] …","url":["https://rucore.libraries.rutgers.edu/rutgers-lib/52378/PDF/1/play/"]}
{"year":"2017","title":"Big Data: A Very Short Introduction","authors":["DE Holmes - 2017"]}
{"year":"2017","title":"Bilateral Multi-Perspective Matching for Natural Language Sentences","authors":["Z Wang, W Hamza, R Florian - arXiv preprint arXiv:1702.03814, 2017"],"snippet":"... 4.5. 4.1 Experiment Settings We initialize the word embeddings in the word representation layer with the 300-dimensional GloVe word vectors pretrained from the 840B Common Crawl corpus [Pennington et al., 2014]. For ...","url":["https://arxiv.org/pdf/1702.03814"]}
{"year":"2017","title":"Bilingual Word Embeddings for Bilingual Terminology Extraction from Specialized Comparable Corpora","authors":["A Hazem, E Morin - Proceedings of the Eighth International Joint …, 2017"],"snippet":"… and economic commentary crawled from the web (NC), Europarl corpus is a parallel corpus extracted from the proceedings of the European Parliament (EP7), JRC acquis corpus is a collection of legislative European Union documents (JRC) and Common Crawl corpus (CC …","url":["http://www.aclweb.org/anthology/I17-1069"]}
{"year":"2017","title":"BLEU2VEC: the Painfully Familiar Metric on Continuous Vector Space Steroids","authors":["A Tättar, M Fishel - WMT 2017, 2017"],"snippet":"... modifications. data from the WMT'2017 news translation shared task: we took a random 50 million sentences from the News Crawl corpora for each language (ex- cept Chinese, where we used a portion of Common Crawl). While ...","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=643"]}
{"year":"2017","title":"Bloom Filters for ReduceBy, GroupBy and Join in Thrill","authors":["A Noe, DIDMT Bingmann - 2017"],"snippet":"Page 1. Master thesis Bloom Filters for ReduceBy, GroupBy and Join in Thrill Alexander Noe Date: 12. January 2017 Supervisors: Prof. Dr. Peter Sanders Dipl. Inform. Dipl. Math. Timo Bingmann Institute of Theoretical Informatics, Algorithmics Department of Informatics ...","url":["https://pdfs.semanticscholar.org/bf34/8f2819a740ba3a473314b4eab616c421c9e1.pdf"]}
{"year":"2017","title":"Bootstrapping Chatbots for Novel Domains","authors":["P Babkin, MFM Chowdhury, A Gliozzo, M Hirzel… - Workshop at NIPS on …, 2017"],"snippet":"… between the corresponding dense vectors. We used 300-dimensional vectors pre-trained with the GloVe algorithm [19] on the Common Crawl corpus that come with the gensim Python library [20]. By applying this model, for …","url":["https://www.researchgate.net/profile/Avraham_Shinnar/publication/321664993_Bootstrapping_Chatbots_for_Novel_Domains/links/5a29ff24a6fdccfbbf81994a/Bootstrapping-Chatbots-for-Novel-Domains.pdf"]}
{"year":"2017","title":"Building a Web-Scale Dependency-Parsed Corpus from CommonCrawl","authors":["A Panchenko, E Ruppert, S Faralli, SP Ponzetto… - arXiv preprint arXiv: …, 2017"],"snippet":"Abstract: We present DepCC, the largest to date linguistically analyzed corpus in English including 365 million documents, composed of 252 billion tokens and 7.5 billion of named entity occurrences in 14.3 billion sentences from a web-scale crawl of the CommonCrawl","url":["https://arxiv.org/pdf/1710.01779"]}
{"year":"2017","title":"Building Lexical Vector Representations from Concept Definitions","authors":["DS Carvalho, M Le Nguyen"],"snippet":"... This parameter was adjusted using the training set for MEN, or inside each CV fold for the rest. • Both Word2Vec and GloVe were used with pre-trained, 300-dimensional models: 100 billion words GoogleNews corpus and Common Crawl 42 billion token corpus respectively. ...","url":["http://www.aclweb.org/anthology/E/E17/E17-1085.pdf"]}
{"year":"2017","title":"Byte-based Neural Machine Translation","authors":["MR Costa-jussà, C Escolano, JAR Fonollosa - Proceedings of the First Workshop on …, 2017"],"snippet":"... English. For the three language pairs, we used all data parallel data provided in the evaluation. For German-English, we used: europarl v.7, news commentary v.12, common crawl and rapid corpus of EU press re- leases. For ...","url":["http://www.aclweb.org/anthology/W17-4123"]}
{"year":"2017","title":"Can word vectors help corpus linguists?","authors":["G Desagulier - 2017"],"snippet":"… If we follow the distributional hypothesis, this means that the words have similar meanings. The quality of the vector representation is a function of the number 2It is sampled from a matrix of vectors obtained with GloVe (see below) on the basis of the Common Crawl dataset. 6 …","url":["https://halshs.archives-ouvertes.fr/halshs-01657591/document"]}
{"year":"2017","title":"Characterisation of mental health conditions in social media using Informed Deep Learning","authors":["G Gkotsis, A Oellrich, S Velupillai, M Liakata… - Scientific Reports, 2017"],"snippet":"... We considered pre-trained word vectors as input to the classifiers (eg using Glove's Common Crawl containing 840 Billion tokens 17 ), but the results did not improve. We attribute this to the size of our dataset which is adequate for representing the language within the corpus. ...","url":["http://www.nature.com/srep/2017/170322/srep45141/full/srep45141.html"]}
{"year":"2017","title":"Classification of keywords","authors":["I Prémont-schwarz, A Thakur, M Tober - US Patent 9,798,820, 2017"],"snippet":"… The resource contents module 320 may automatically acquire a plurality of resources. The resources may, for example, be Wikipedia articles and be acquired from Wikipedia.org, or an open repository of web crawl data such as CommonCrawl.org …","url":["http://www.freepatentsonline.com/9798820.html"]}
{"year":"2017","title":"Classification of search queries","authors":["A Thakur, M Tober - US Patent 9,767,182, 2017"],"snippet":"... The resource contents module 320 may automatically acquire a plurality of resources. The resources may, for example, be Wikipedia articles and be acquired from Wikipedia.org, or an open repository of web crawl data such as CommonCrawl.org. ...","url":["http://www.freepatentsonline.com/9767182.html"]}
{"year":"2017","title":"Classifier Stacking for Native Language Identification","authors":["W Li, L Zou - Bronze Sponsors, 2017"],"snippet":"... Word embeddings We use the Common Crawl (42B tokens, 1.9 M vocab, uncased, 300d vectors) in GloVe (global vectors for word representation)(Pennington et al., 2014) to produce feature vectors for each essay, with the help of Gensim (ˇRehurek and Sojka, 2010). ...","url":["http://www.aclweb.org/anthology/W/W17/W17-50.pdf#page=410"]}
{"year":"2017","title":"Classifying Phishing URLs Using Recurrent Neural Networks","authors":["AC Bahnsen, EC Bohorquez, S Villegas, J Vargas…"],"snippet":"... Sections 1PhishTank (https://www.phishtank.com/) 2Common Crawl (http://commoncrawl.org/) 978-1-5386-2701-3/17/$31.00 c 2017 IEEE Page 2. ... Half of them legitimate and half of them phishing. The legitimate URLs came from Common Crawl, a corpus of web crawl data. ...","url":["http://albahnsen.com/files/Classifying%20Phishing%20URLs%20Using%20Recurrent%20Neural%20Networks_cameraready.pdf"]}
{"year":"2017","title":"Cloud Computing Infrastructure for Data Intensive Applications","authors":["Y Demchenko, F Turkmen, C de Laat, CH Hsu… - Big Data Analytics for …, 2017"],"snippet":"This chapter describes the general architecture and functional components of the cloud-based big data infrastructure (BDI). The chapter starts with the analysis.","url":["https://www.sciencedirect.com/science/article/pii/B9780128093931000027"]}
{"year":"2017","title":"Coattention-Based Neural Network for Question Answering","authors":["J Andress, C Zanoci"],"snippet":"... We then proceed by embedding each word using the GloVe word vectors pretrained on the 840B Common Crawl corpus [6]. We found that switching from the default 100dimensional GloVe vectors to the larger 300-dimensional representation improved the performance ...","url":["https://web.stanford.edu/class/cs224n/reports/2762015.pdf"]}
{"year":"2017","title":"Common Crawl Mining","authors":["T Dean, A Pasha, B Clarke, CJ Butenhoff - 2017"],"snippet":"The main goal behind the Common Crawl Mining system is to improve Eastman Chemical Company's ability to use timely knowledge of public concerns to inform key business decisions. It provides information to Eastman Chemical Company that is valuable for","url":["https://vtechworks.lib.vt.edu/bitstream/handle/10919/77629/ccm_source_code.zip?sequence=5&isAllowed=y"]}
{"year":"2017","title":"Common Crawled Web Corpora: Constructing corpora from large amounts of web data","authors":["KB Kristoffersen - 2017"],"snippet":"… Additionally, by using data provided by the Common Crawl Foundation, I develop a new very large English corpus with more than 135 billion tokens … 3 Exploring the Common Crawl 27 3.1 The data . . . . . 27 3.1.1 A note on scale …","url":["https://www.duo.uio.no/bitstream/handle/10852/57836/Kristoffersen_MSc2.pdf?sequence=5"]}
{"year":"2017","title":"Composition of Compound Nouns Using Distributional Semantics","authors":["K Yee, J Kalita"],"snippet":"... word2vec 300 3,000,000 100.00 bn Google News GloVe 300 400,000 42.00 bn Common Crawl HPCA 200 178,080 1.65 bn enWiki+Reuters +WSJ CW 50 130,000 0.85 bn enWiki+Reuters RCV1 word2vec 500 30,025 100 mn BNC word2vec 500 19,679 120 mn esWiki ...","url":["http://www.cs.uccs.edu/~jkalita/papers/2016/KyraYeeICON2016.pdf"]}
{"year":"2017","title":"Compressed Nonparametric Language Modelling","authors":["E Shareghi, G Haffari, T Cohn"],"snippet":"Page 1. Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17) 2701 Compressed Nonparametric Language Modelling Ehsan Shareghi,♣ Gholamreza Haffari,♣ Trevor Cohn♠ ♣ Faculty ...","url":["http://static.ijcai.org/proceedings-2017/0376.pdf"]}
{"year":"2017","title":"Compressing Word Embeddings via Deep Compositional Code Learning","authors":["R Shu, H Nakayama - arXiv preprint arXiv:1711.01068, 2017","RSH Nakayama"],"snippet":"... purpose. We lowercase and tokenize all texts with the nltk package. We choose the 300-dimensional uncased Glove word vectors (trained on 42B tokens of Common Crawl data) as our baseline embeddings. The vocabulary ...","url":["https://arxiv.org/pdf/1711.01068","https://pdfs.semanticscholar.org/1713/d05f9d5861cac4d5ec73151667cb03a42bfc.pdf"]}
{"year":"2017","title":"Compression with the tudocomp Framework","authors":["P Dinklage, J Fischer, D Köppl, M Löbel, K Sadakane - arXiv preprint arXiv: …, 2017"],"snippet":"Page 1. Compression with the tudocomp Framework Patrick Dinklage1, Johannes Fischer1, Dominik Köppl1, Marvin Löbel1, and Kunihiko Sadakane2 1 Department of Computer Science, TU Dortmund, Germany, pdinklag@gmail ...","url":["https://arxiv.org/pdf/1702.07577"]}
{"year":"2017","title":"Concept/Theme Roll-Up","authors":["T Sahay, R Tadishetti, A Mehta, S Jadon - 2017"],"snippet":"... representation. For word embeddings, we used GloVe trained on a common crawl corpus, containing 1900000 words in its vocabulary. ... phrases. For words, the weights were initialized with GloVe embeddings trained on the common-crawl corpus. ...","url":["https://people.cs.umass.edu/~tsahay/lexalytics_report.pdf"]}
{"year":"2017","title":"ConceptNet at SemEval-2017 Task 2: Extending Word Embeddings with Multilingual Relational Knowledge","authors":["R Speer, J Lowry-Duda - arXiv preprint arXiv:1704.03560, 2017"],"snippet":"... The first source is the word2vec Google News embeddings2, and the second is the GloVe 1.2 embeddings that were trained on 840 billion tokens of the Common Crawl3. Because the input embeddings are only in En- glish, the vectors in other languages depended en- tirely on ...","url":["https://arxiv.org/pdf/1704.03560"]}
{"year":"2017","title":"CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies","authors":["D Zeman, M Popel, M Straka, J Hajic, J Nivre, F Ginter… - Proceedings of the CoNLL …, 2017"],"snippet":"... Page 3. Raw texts The supporting raw data was gathered from CommonCrawl, which is a publicly available web crawl created and maintained by the non-profit CommonCrawl foundation.2 The data is publicly available in the Amazon cloud both as raw HTML and as plain text. ...","url":["http://www.aclweb.org/anthology/K17-3001"]}
{"year":"2017","title":"Connecting the Dots: Towards Human-Level Grammatical Error Correction","authors":["S Chollampatt, HT Ng - Bronze Sponsors, 2017"],"snippet":"... Moreover, Junczys-Dowmunt and Grundkiewicz (2016) trained a web-scale language model (LM) using large corpora from the Common Crawl data (Buck et al., 2014). ... 2014. N-gram counts and language models from the Common Crawl...","url":["http://www.aclweb.org/anthology/W/W17/W17-50.pdf#page=347"]}
{"year":"2017","title":"Constructing and Evaluating a Novel Crowdsourcing-based Paraphrased Opinion Spam Dataset","authors":["S Kim, S Lee, D Park, J Kang - Proceedings of the 26th International Conference on …, 2017"],"snippet":"... the past and future context of an input are captured (Figure 3). We initialized the input word representations of our LSTM model using publicly available 300dimensional GloVe10 vectors (Pennington et al., 2014), which are trained on 840 billion tokens of Common Crawl data. ...","url":["http://dl.acm.org/citation.cfm?id=3052607"]}
{"year":"2017","title":"Context Similarity for Retrieval-Based Imputation","authors":["A Ahmadov, M Thiele, W Lehner, R Wrembel"],"snippet":"... an accurate imputation. We use Dresden Web Table Corpus (DWTC) which is comprised of more than 125 million web tables extracted from the Common Crawl as our knowledge source. The comprehensive experimental ...","url":["http://asonamdata.com/ASONAM2017_Proceedings/papers/165_1017_135.pdf"]}
{"year":"2017","title":"CONTEXT-AWARE CLUSTERING USING GLOVE AND K-MEANS","authors":["P Juneja, H Jain, T Deshmukh, S Somani, BK Tripathy"],"snippet":"... Gigaword5 + Page 7. International Journal of Software Engineering & Applications (IJSEA), Vol.8, No.4, July 2017 27 Wikipedia2014 which has 6 billion tokens, and on 42 billion tokens of web data, from Common Crawl. For ...","url":["https://www.researchgate.net/profile/BK_Tripathy/publication/318815506_Context_Aware_Clustering_Using_Glove_and_K-Means/links/598037d2458515687b4f9dfd/Context-Aware-Clustering-Using-Glove-and-K-Means.pdf"]}
{"year":"2017","title":"Continuous Learning from Human Post-Edits for Neural Machine Translation","authors":["M Turchi, M Negri, MA Farajian, M Federico - The Prague Bulletin of Mathematical …, 2017"],"snippet":"... For training the En_De NMT system, we merged the Europarl v7 (Koehn, 2005) and Common Crawl datasets released for the translation task at the 2016 Workshop on Statistical Machine Translation (WMT'16 (Bojar, 2016)) and random sampled 3.5 million sentence pairs. ...","url":["https://www.degruyter.com/downloadpdf/j/pralin.2017.108.issue-1/pralin-2017-0023/pralin-2017-0023.xml"]}
{"year":"2017","title":"Convolutional Encoding in Bidirectional Attention Flow for Question Answering","authors":["DR Miller"],"snippet":"... Language Processing (EMNLP), pp. 1532–1543, 2014. [9] “Common Crawl.” https://commoncrawl.org/. [10] RK Srivastava, K. Greff, and J. Schmidhuber, “Highway networks,” arXiv preprint arXiv:1505.00387, 2015. [11] P. Rajpurkar, J ...","url":["http://web.stanford.edu/class/cs224n/reports/2762032.pdf"]}
{"year":"2017","title":"Cost Weighting for Neural Machine Translation Domain Adaptation","authors":["B Chen, C Cherry, G Foster, S Larkin - ACL 2017, 2017"],"snippet":"... which contains 3003 sentence pairs. The training data contain 12 million sentence pairs, composed of various sub-domains, such as news commentary, Europarl, UN, common crawl web data, etc. In the corpus weighting adaptation ...","url":["http://www.aclweb.org/anthology/W/W17/W17-32.pdf#page=52"]}
{"year":"2017","title":"Counterfactual Learning for Machine Translation: Degeneracies and Solutions","authors":["C Lawrence, P Gajane, S Riezler - arXiv preprint arXiv:1711.08621, 2017"],"snippet":"… signal. Experiments are conducted on two language pairs. The first is German-to-English and its baseline system is trained on the concatenation of the Europarl corpus, the Common Crawl corpus and the News corpus. The …","url":["https://arxiv.org/pdf/1711.08621"]}
{"year":"2017","title":"Counterfactual learning from bandit feedback under deterministic logging: A case study in statistical machine translation","authors":["C Lawrence, A Sokolov, S Riezler - arXiv preprint arXiv:1707.09118, 2017"],"snippet":"... We conduct two SMT tasks with hypergraph re-decoding: The first is German-to-English and is trained using a concatenation of the Europarl corpus (Koehn, 2005), the Common Crawl corpus3 and the News Commentary corpus (Koehn and Schroeder, 2007). ...","url":["https://arxiv.org/pdf/1707.09118"]}
{"year":"2017","title":"Critical review of various near-duplicate detection methods in web crawl and their prospective application in drug discovery","authors":["L Pamulaparty, CVG Rao, MS Rao - International Journal of Biomedical Engineering …, 2017"],"snippet":"... Smith, JR, Saint-Amand, H., Plamada, M. and Lopez, A. (2013) 'Dirt cheap web-scale parallel text from the common crawl', Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), Association for Computational Linguistics, Sofia ...","url":["http://www.inderscienceonline.com/doi/abs/10.1504/IJBET.2017.087723"]}
{"year":"2017","title":"CS 224N Assignment 4: Question Answering on SQuAD","authors":["RA Ozturk, HA Inan, K Garbe"],"snippet":"... Fixing this problem partly by using all of the 400k words resulted in increased performance naturally, but there is still a performance gap between our dev performance and test performance. We should also note that 1 uses the Common Crawl dataset (2.2m words). ...","url":["https://web.stanford.edu/class/cs224n/reports/2761126.pdf"]}
{"year":"2017","title":"CS224N Project: Natural Language Inference for Quora Dataset","authors":["KHK Yoo, MM Almajid, ZY Wong"],"snippet":"... Most of the results were obtained using the smaller vocabulary of 6 billion tokens obtained from Wikipedia and Gigaword 5, while there exists a Common Crawl version which has 840B tokens with an embedding size of 300. ...","url":["https://web.stanford.edu/class/cs224n/reports/2755939.pdf"]}
{"year":"2017","title":"CUNI System for the WMT17 Multimodal Translation Task","authors":["J Helcl, J Libovický - arXiv preprint arXiv:1707.04550, 2017"],"snippet":"... By scoring the German part of several parallel corpora (EU Bookshop (Skadinš et al., 2014), News Commentary (Tiedemann, 2012) and CommonCrawl (Smith et al., 2013)), we were only able to retrieve a few hundreds of in-domain sentences. ...","url":["https://arxiv.org/pdf/1707.04550"]}
{"year":"2017","title":"D1. 1: Report on Building Translation Systems for Public Health Domain","authors":["O Bojar, B Haddow, D Marecek, R Sudarikov… - 2017"],"snippet":"Page 1. This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 644402. D1.1: Report on Building Translation Systems for Public Health Domain ...","url":["http://www.himl.eu/files/D1.1-report-on-building-translation-systems.pdf"]}
{"year":"2017","title":"Data Integration for Open Data on the Web","authors":["S Neumaier, A Polleres, S Steyskal, J Umbrich"],"snippet":"... However, some Web crawls have been made openly available, such as the Common Crawl corpus which contains “petabytes of data collected over the last 7 years”10. ... 10http://commoncrawl.org/, last accessed 30/03/2017 Page 5. Table 1: Top-10 portals, ordered by datasets. ...","url":["https://aic.ai.wu.ac.at/~polleres/publications/neum-etal-RW2017.pdf"]}
{"year":"2017","title":"Data Selection Strategies for Multi-Domain Sentiment Analysis","authors":["S Ruder, P Ghaffari, JG Breslin - arXiv preprint arXiv:1702.02426, 2017"],"snippet":"... a linear SVM classifier (Blitzer et al., 2006). We use GloVe vectors (Pennington et al., 2014) pre-trained on 42B tokens of the Common Crawl corpus7 for our word embeddings. For the auto-encoder representations, we use a ...","url":["https://arxiv.org/pdf/1702.02426"]}
{"year":"2017","title":"DCN+: Mixed Objective and Deep Residual Coattention for Question Answering","authors":["C Xiong, V Zhong, R Socher - arXiv preprint arXiv:1711.00106, 2017"],"snippet":"... Manning et al., 2014). For word embeddings, we use GloVe embeddings pretrained on the 840B Common Crawl corpus (Pennington et al., 2014) as well as character ngram embeddings by Hashimoto et al. (2017). In addition, we ...","url":["https://arxiv.org/pdf/1711.00106"]}
{"year":"2017","title":"Deep Almond: A Deep Learning-based Virtual Assistant","authors":["GCR Ramesh"],"snippet":"... Preprocessing was implemented in Java using CoreNLP [22]. We use the pretrained GloVe [23] vectors of size 300 trained on Common Crawl as our word vectors, and we do not train the word vectors. 5.1 Model Validation & Tuning ...","url":["https://web.stanford.edu/class/cs224n/reports/2748325.pdf"]}
{"year":"2017","title":"Deep Learning for User Comment Moderation","authors":["J Pavlopoulos, P Malakasiotis, I Androutsopoulos - arXiv preprint arXiv:1705.09993, 2017"],"snippet":"... 11See https://nlp.stanford.edu/projects/ glove/. We use 'Common Crawl' (840B tokens). 12For Gazzetta, words encountered only once in the training set (G-TRAIN-L or G-TRAIN-S) are also treated as OOV. ta : accept threshold tr : reject threshold 0.0 1.0 reject gray accept ...","url":["https://arxiv.org/pdf/1705.09993"]}
{"year":"2017","title":"Deep Neural Machine Translation with Linear Associative Unit","authors":["M Wang, Z Lu, J Zhou, Q Liu - arXiv preprint arXiv:1705.00861, 2017","MWZLJ Zhou, Q Liu"],"snippet":"... translation are presented in Table 2. We compare our NMT systems with various other systems including the winning system in WMT14 (Buck et al., 2014), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl corpus. ...","url":["http://www.aclweb.org/anthology/P/P17/P17-1013.pdf","https://arxiv.org/pdf/1705.00861"]}
{"year":"2017","title":"Deeper Attention to Abusive User Content Moderation","authors":["J Pavlopoulos, P Malakasiotis, I Androutsopoulos - 2017"],"snippet":"... (2017a). 14We implemented the methods of this sub-section using Keras (keras.io) and TensorFlow (tensorflow.org). 15See https://nlp.stanford.edu/projects/ glove/. We use 'Common Crawl' (840B tokens). Page 6. ta : accept threshold tr : reject threshold 0.0 1.0 reject gray accept ...","url":["http://nlp.cs.aueb.gr/pubs/emnlp2017.pdf"]}
{"year":"2017","title":"DeepSpace: Mood-Based Image Texture Generation for Virtual Reality from Music","authors":["M Sra, P Vijayaraghavan, O Rudovic, P Maes, D Roy - Computer Vision and Pattern …, 2017"],"snippet":"... task. We use the GloVe model trained on a common crawl dataset7 for the representation for words in the descriptive labels and mood. ... This approach of tractably modeling a joint distribution of 7http://commoncrawl.org/the-data/ pixels ...","url":["http://ieeexplore.ieee.org/abstract/document/8015017/"]}
{"year":"2017","title":"Denoising Clinical Notes for Medical Literature Retrieval with Convolutional Neural Model","authors":["L Soldaini, A Yates, N Goharian - 2017"],"snippet":"... Two source of evidence were used to obtain, for each term qi , its word embedding xi : GloVe vectors [10] pre-trained on the common crawl corpus4 and SkipGram vectors pre-trained on PubMed5. We found that concatenating domain-speci c with domain-agnostic embeddings ...","url":["http://ir.cs.georgetown.edu/downloads/cikm17-cds-notes.pdf"]}
{"year":"2017","title":"Deriving Neural Architectures from Sequence and Graph Kernels","authors":["T Lei, W Jin, R Barzilay, T Jaakkola - arXiv preprint arXiv:1705.09037, 2017"],"snippet":"Page 1. Deriving Neural Architectures from Sequence and Graph Kernels Tao Lei* 1 Wengong Jin* 1 Regina Barzilay 1 Tommi Jaakkola 1 Abstract The design of neural architectures for structured objects is typically guided by experimental in- sights rather than a formal process. ...","url":["https://arxiv.org/pdf/1705.09037"]}
{"year":"2017","title":"Determining Entailment of Questions in the Quora Dataset","authors":["A Tung, E Xu"],"snippet":"... We used the 840B common crawl GloVe pretrained embeddings https://nlp.stanford.edu/projects/ glove/, the starter code from CS224N http://web.stanford.edu/class/cs224n/, and tuned the hyper-parameters on these various models to achieve the optimal accuracy. ...","url":["https://web.stanford.edu/class/cs224n/reports/2748301.pdf"]}
{"year":"2017","title":"Distance-Aware Selective Online Query Processing Over Large Distributed Graphs","authors":["X Zhang, L Chen - Data Science and Engineering"],"url":["http://link.springer.com/article/10.1007/s41019-016-0023-z"]}
{"year":"2017","title":"Distinguishing “good” from “bad” Arguments in Online Debates & Feature Analysis using Feed-Forward Neural Networks","authors":["LA Chalaguine"],"snippet":"Page 1. Imperial College London Department of Computing Distinguishing “good” from “bad” Arguments in Online Debates & Feature Analysis using Feed-Forward Neural Networks Lisa Andreevna Chalaguine Supervisor: Claudia Schulz ...","url":["https://pdfs.semanticscholar.org/526f/468ebe630e10221dc77f21ce65aba72e0021.pdf"]}
{"year":"2017","title":"Distributed Algorithms on Exact Personalized PageRank","authors":["T Guo, X Cao, G Cong, J Lu, X Lin"],"snippet":"Page 1. Distributed Algorithms on Exact Personalized PageRank Tao Guo1 Xin Cao2 Gao Cong1 Jiaheng Lu3 Xuemin Lin2 1 School of Computer Science and Engineering, Nanyang Technological University, Singapore 2 School ...","url":["https://www.cs.helsinki.fi/u/jilu/documents/SIGMOD2017.pdf"]}
{"year":"2017","title":"Distributed Computing in Social Media Analytics","authors":["M Riemer - Distributed Computing in Big Data Analytics, 2017"],"snippet":"... For example, [16] the current state of the art Twitter sentiment analysis technique leverages knowledge from a Common Crawl of the internet, Movie Reviews, Emoticons, and a human defined rule logic model to drastically improve the performance of its recurrent neural network ...","url":["https://link.springer.com/chapter/10.1007/978-3-319-59834-5_8"]}
{"year":"2017","title":"Document Context Neural Machine Translation with Memory Networks","authors":["S Maruf, G Haffari - arXiv preprint arXiv:1711.03688, 2017"],"snippet":"Page 1. Document Context Neural Machine Translation with Memory Networks Sameen Maruf and Gholamreza Haffari Faculty of Information Technology, Monash University, VIC, Australia {firstname.lastname}@monash.edu.au Abstract …","url":["https://arxiv.org/pdf/1711.03688"]}
{"year":"2017","title":"Domain Adaptation for Multilingual Neural Machine Translation","authors":["AC Varga - 2017"],"snippet":"Page 1. Universität des Saarlandes Universidad del Pa´ıs Vasco/Euskal Herriko Unibertsitatea Domain Adaptation for Multilingual Neural Machine Translation Master's Thesis submitted in fulfillment of the degree requirements of the ...","url":["https://www.clubs-project.eu/assets/publications/other/MSc_thesis_AdamVarga.pdf"]}
{"year":"2017","title":"Don't Let One Rotten Apple Spoil the Whole Barrel: Towards Automated Detection of Shadowed Domains","authors":["D Liu, Z Li, K Du, H Wang, B Liu, H Duan - 2017"],"snippet":"Page 1. Don't Let One Rotten Apple Spoil the Whole Barrel: Towards Automated Detection of Shadowed Domains Daiping Liu University of Delaware dpliu@udel.edu Zhou Li ACM Member lzcarl@gmail.com Kun Du Tsinghua University dk15@tsinghua.edu.cn ...","url":["https://www.eecis.udel.edu/~dpliu/papers/ccs17.pdf"]}
{"year":"2017","title":"Doubly-Attentive Decoder for Multi-modal Neural Machine Translation","authors":["I Calixto, Q Liu, N Campbell - arXiv preprint arXiv:1702.01287, 2017"],"snippet":"... M sentence pairs (Bojar et al., 2015). These include the Eu- roparl v7 (Koehn, 2005), News Commentary and Common Crawl corpora, which are concatenated and used for pre-training. We use the scripts in the Moses SMT ...","url":["https://arxiv.org/pdf/1702.01287"]}
{"year":"2017","title":"Dynamic Coattention Networks for Reading Comprehension","authors":["H Tepanyan"],"snippet":"... using linear decoder. We use this final version with Common Crawl 840B glove vector embeddings from [2] to achieve the final scores of F1 = 58.2% and EM = 44.5% scores on the dev set. 1 Model 1. Simple Baseline Below we ...","url":["http://web.stanford.edu/class/cs224n/reports/2743745.pdf"]}
{"year":"2017","title":"Dynamic Coattention with Sentence Information","authors":["A Ruch"],"snippet":"... choice for document length. Embeddings: We used the GloVe word embeddings for the Common Crawl 840B dataset and explored initializing the embeddings of unseen words to zero or to a random vector. Intuitively using a ...","url":["https://pdfs.semanticscholar.org/eef0/e42394c625772f5b220797661aba893012f4.pdf"]}
{"year":"2017","title":"Dynamic Data Selection for Neural Machine Translation","authors":["M van der Wees, A Bisazza, C Monz - arXiv preprint arXiv:1708.00712, 2017"],"snippet":"... The WMT training corpus contains Commoncrawl, Europarl, and News Commentary but no in-domain news data. ... We train our systems on a mixture of domains, comprising Commoncrawl, Europarl, News Commentary, EMEA, Movies, and TED. ...","url":["https://arxiv.org/pdf/1708.00712"]}
{"year":"2017","title":"Dynamic Space Efficient Hashing","authors":["T Maier, P Sanders - arXiv preprint arXiv:1705.00997, 2017"],"snippet":"Page 1. Dynamic Space Efficient Hashing Tobias Maier and Peter Sanders Karlsruhe Institute of Technology, Karlsruhe, Germany {t.maier,sanders}@kit.edu Abstract We consider space efficient hash tables that can grow and ...","url":["https://arxiv.org/pdf/1705.00997"]}
{"year":"2017","title":"Effect of Data Imbalance on Unsupervised Domain Adaptation of Part-of-Speech Tagging and Pivot Selection Strategies","authors":["X Cui, F Coenen, D Bollegala - Journal of Machine Learning Research, 2017"],"snippet":"... (2016) to train the final adaptive classifier f only by projected features to reduce the dimensionality, where θx ∈ Rh. We use d = 300 dimensional GloVe (Pennington et al., 2014) embeddings (trained using 42B tokens from the Common Crawl) as word representations. ...","url":["https://cgi.csc.liv.ac.uk/~frans/PostScriptFiles/lidta2017.pdf"]}
{"year":"2017","title":"Energy-Efficient Data Transfer Algorithms for HTTP-Based Services","authors":["T Kosar, I Alan - arXiv preprint arXiv:1707.05730, 2017"],"snippet":"... Three different representative datasets were used during experiments in order to capture the throughput and power consumption differences based on the dataset type: (i) the HTML dataset is a set of raw HTML files from the Common Crawl project [3]; (ii) the image dataset is a ...","url":["https://arxiv.org/pdf/1707.05730"]}
{"year":"2017","title":"Entity linking across vision and language","authors":["AN Venkitasubramanian, T Tuytelaars, MF Moens - Multimedia Tools and …, 2017"],"snippet":"... 5.2 Using a hypernym database The second approach for detecting relevant mentions uses the WebIsADb database [40] containing more than 400 million hypernymy relations extracted from the CommonCrawl web corpus. ...","url":["http://link.springer.com/article/10.1007/s11042-017-4732-8"]}
{"year":"2017","title":"Estimating Missing Temporal Meta-Information using Knowledge-Based-Trust","authors":["Y Oulabi, C Bizer"],"snippet":"... We acquired data from the sources either by manually written crawlers and extractors, or through data dumps. 5.2 Web Table Corpus For our experiments we use the Web Data Commons Web Table Corpus from 20153, which was extracted from the July 2015 Common Crawl...","url":["https://pdfs.semanticscholar.org/9016/87f0f79efd175d6c3b9efba4e254e4bc410a.pdf"]}
{"year":"2017","title":"Evaluating Story Generation Systems Using Automated Linguistic Analyses","authors":["M Roemmele, AS Gordon, R Swanson"],"snippet":"... We specifically used the GloVe embedding vectors [45] trained on the Common Crawl corpus8. We computed the mean cosine similarity of the vectors for all pairs of content words between a generated sentence and its context (Metric 11). ...","url":["http://people.ict.usc.edu/~roemmele/publications/fiction_generation.pdf"]}
{"year":"2017","title":"Evaluating vector-space models of analogy","authors":["D Chen, JC Peterson, TL Griffiths - arXiv preprint arXiv:1705.04416, 2017"],"snippet":"... We used the 300-dimensional word2vec vectors trained on the Google News corpus that were provided by Google (Mikolov et al., 2013), and the 300-dimensional GloVe vectors trained on a Common Crawl web crawl corpus that were provided by Pennington et al. (2014). ...","url":["https://arxiv.org/pdf/1705.04416"]}
{"year":"2017","title":"Evaluation of a Feedback Algorithm inspired by Quantum Detection for Dynamic Search Tasks","authors":["E Di Buccio, M Melucci"],"snippet":"... Polar Domain and the Ebola Do- main. Each dataset is formatted using the Common Crawl Architecture schema from the DARPA MEMEX project, and stored as sequences of CBOR objects. The Ebola dataset refers to the outbreak ...","url":["http://trec.nist.gov/pubs/trec25/papers/UPD_IA-DD.pdf"]}
{"year":"2017","title":"Event Coreference Resolution by Iteratively Unfolding Inter-dependencies among Events","authors":["PK Choubey, R Huang - arXiv preprint arXiv:1707.07344, 2017"],"snippet":"Page 1. Event Coreference Resolution by Iteratively Unfolding Inter-dependencies among Events Prafulla Kumar Choubey and Ruihong Huang Department of Computer Science and Engineering Texas A&M University (prafulla.choubey, huangrh)@tamu.edu Abstract ...","url":["https://arxiv.org/pdf/1707.07344"]}
{"year":"2017","title":"Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis","authors":["R Cotterell, A Poliak, B Van Durme, J Eisner"],"snippet":"... distributional information. The embeddings, trained on extremely large text corpora, eg, Wikipedia and the Common Crawl, are claimed to encode semantic knowledge extracted from large text corpora. While numerous models ...","url":["https://ryancotterell.github.io/papers/cotterell+alb.eacl17.pdf"]}
{"year":"2017","title":"Exploiting Embedding in Content-Based Recommender systems","authors":["Y Huang - 2016"],"snippet":"Page 1. Multimedia Computing Group Exploiting Embedding in Content-Based Recommender Systems Yanbo Huang Master of Science Thesis Page 2. Page 3. Exploiting Embedding in Content-Based Recommender Systems Master of Science Thesis ...","url":["http://repository.tudelft.nl/islandora/object/uuid:cbec7bdd-4bab-4132-93cd-359587b9bf46/datastream/OBJ/view"]}
{"year":"2017","title":"Exploring Neural Transducers for End-to-End Speech Recognition","authors":["E Battenberg, J Chen, R Child, A Coates, Y Gaur, Y Li… - arXiv preprint arXiv: …, 2017"],"snippet":"... available for this benchmark from the Kaldi receipe [20]. The language model used by all models in Table 3 is built from a sample of the common crawl dataset [26]. Model specification. All models in Tables 1 and 3 are tuned ...","url":["https://arxiv.org/pdf/1707.07413"]}
{"year":"2017","title":"Extending the Scope of Co-occurrence Embedding","authors":["J Mi, Y Wang, J Zhu"],"snippet":"... Therefore, it is highly likely that our model ignores the n-grams with strong emotion, simply because they rarely occur in the training data. We expect a boost in the classification accuracy if we could train our model on a more comprehensive dataset, say, common crawl...","url":["https://web.stanford.edu/class/cs224n/reports/2758144.pdf"]}
{"year":"2017","title":"Extracting Conceptual Relationships and Inducing Concept Lattices from Unstructured Text","authors":["VS Anoop, S Asharaf - Journal of Intelligent Systems"],"snippet":"AbstractConcept and relationship extraction from unstructured text data plays a key role in meaning aware computing paradigms, which make computers intelligent by helping them learn, interpret, and synthesis information. These concepts and relationships leverage knowledge ...","url":["https://www.degruyter.com/view/j/jisys.ahead-of-print/jisys-2017-0225/jisys-2017-0225.xml"]}
{"year":"2017","title":"Extracting Parallel Paragraphs from Common Crawl","authors":["J Kúdela, I Holubová, O Bojar - The Prague Bulletin of Mathematical Linguistics, 2017"],"snippet":"Abstract Most of the current methods for mining parallel texts from the web assume that web pages of web sites share same structure across languages. We believe that there still exists a non-negligible amount of parallel data spread across sources not satisfying this","url":["https://www.degruyter.com/downloadpdf/j/pralin.2017.107.issue-1/pralin-2017-0003/pralin-2017-0003.xml"]}
{"year":"2017","title":"Extracting Visual Knowledge from the Web with Multimodal Learning","authors":["D Gong, DZ Wang"],"snippet":"... 5.1 Dataset We evaluate our approach based on a collection of web pages and images derived from the Common Crawl dataset [Smith et al., 2013] that is publicly available on Amazon S3. The entire Common Crawl dataset ...","url":["http://static.ijcai.org/proceedings-2017/0238.pdf"]}
{"year":"2017","title":"Fast Construction of Compressed Web Graphs","authors":["J Broß, S Gog, M Hauck, M Paradies - … on String Processing and Information Retrieval, 2017"],"snippet":"... Table 1). For experiments on a very large graph, we added a web graph originating from the CommonCrawl project. Table 1. ... Graphs are stored as set of adjacency lists. Each list entry occupies 4 bytes (8 bytes in case of CommonCrawl). ...","url":["https://link.springer.com/chapter/10.1007/978-3-319-67428-5_11"]}
{"year":"2017","title":"FBK's Participation to the English-to-German News Translation Task of WMT 2017","authors":["MA Di Gangi, N Bertoldi, M Federico - WMT 2017, 2017"],"snippet":"... Number of training sentences. original cleaned commoncrawl 2399123 2228833 europarl-v7 1920209 1719859 news-comm-v12 270769 255944 rapid2016 1329041 1277997 both English and German. We also filtered out ...","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=295"]}
{"year":"2017","title":"FilteredWeb: A Framework for the Automated Search-Based Discovery of Blocked URLs","authors":["A Darer, O Farnan, J Wright - arXiv preprint arXiv:1704.07185, 2017"],"snippet":"... Web search is a large and complicated business; most engines do not simply rank pages based on hyperlinks, but rather current trends and activity. One alternative to Bing is Common Crawl – an open data project that scrapes the web for pages. ...","url":["https://arxiv.org/pdf/1704.07185"]}
{"year":"2017","title":"Findings of the 2017 conference on machine translation (wmt17)","authors":["O Bojar, R Chatterjee, C Federmann, Y Graham… - WMT 2017, 2017"],"snippet":"... Some training corpora were identical from last year (Europarl4, Common Crawl, SETIMES2, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were updated (United Nations, CzEng v1. ...","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=193"]}
{"year":"2017","title":"Findings of the WMT 2017 Biomedical Translation Shared Task","authors":["A Jimeno Yepes, A Neveol, M Neves, K Verspoor…","AJ Yepes, A Névéol, M Neves, HPIU Potsdam… - WMT 2017, 2017"],"snippet":"… used. Tuning of the SMT systems was performed with MERT. Commoncrawl and Wikipedia were used as general domain data for all language pairs ex- cept for EN/PT, where no Commoncrawl data was provided by WMT. As …","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=258","https://www.research.ed.ac.uk/portal/files/40797681/123_1.pdf"]}
{"year":"2017","title":"From Segmentation to Analyses: A Probabilistic Model for Unsupervised Morphology Induction","authors":["T Bergmanis, S Goldwater"],"snippet":"... by our system and the MorphoChains baseline, we used word2vec (Mikolov et al., 2013) to train a Continuous Bag of Words model on a sub-sample of the Common Crawl (CC) corpus6 for ... 6Common Crawl http://commoncrawl.org 7Morpho Challenge 2010: http://research.ics. ...","url":["http://homepages.inf.ed.ac.uk/sgwater/papers/eacl17-morphAnalyses.pdf"]}
{"year":"2017","title":"Game, Set, Match-LSTM: Question Answering on SQUaD","authors":["I Torres, E Ehizokhale"],"snippet":"... The official test dataset is not publically available. It's kept by the authors of SQuAD to make model evaluation fair. We use GloVE word vectors trained on the 840B Common Crawl corpus. We limit the max context paragraph length to 300 and the max question length to 30. ...","url":["https://web.stanford.edu/class/cs224n/reports/2761956.pdf"]}
{"year":"2017","title":"Geographical Evaluation of Word Embeddings","authors":["M Konkol, T Brychcín, M Nykl, T Hercig - Proceedings of the Eighth International Joint …, 2017"],"snippet":"… We use two models provided by the authors of the model trained on Wikipedia and News Crawl (LexVec - w + nc), and Common Crawl (LexVec - cc). MetaEmbeddings is an ensemble method that combines several embeddings (Yin and Schütze, 2016) …","url":["http://www.aclweb.org/anthology/I17-1023"]}
{"year":"2017","title":"Global-Context Neural Machine Translation through Target-Side Attentive Residual Connections","authors":["L Miculicich, N Pappas, D Ram, A Popescu-Belis - arXiv preprint arXiv:1709.04849, 2017"],"snippet":"... Finally, we use the complete English- to-German set from WMT 2016 (Bojar and others 2016)3 which includes Europarl v7, Common Crawl, and News Commentary v11 with a total of ca. 4.5 million sentence pairs. ... N-gram counts and language models from the common crawl...","url":["https://arxiv.org/pdf/1709.04849"]}
{"year":"2017","title":"Globally Normalized Reader","authors":["J Raiman, J Miller - Proceedings of the 2017 Conference on Empirical …, 2017"],"snippet":"... tion. The hidden dimension of all recurrent layers is 200. We use the 300 dimensional 8.4B token Common Crawl GloVe vectors (Pennington et al., 2014). Words missing from the Common Crawl vocabulary are set to zero. In ...","url":["http://www.aclweb.org/anthology/D17-1112"]}
{"year":"2017","title":"Googleology as smart lexicography: Big messy data for better regional labels","authors":["S Dollinger - Dictionaries: Journal of the Dictionary Society of North …, 2016"],"snippet":"... help. Other services have other problems. Commoncrawl.org is one of the longest-running such projects and [End Page 72] offers big data for free. Accessing its data, however, requires serious programming expertise. Other ...","url":["https://muse.jhu.edu/article/645766/summary"]}
{"year":"2017","title":"Grammatical error correction in non-native English","authors":["Z Yuan - 2017"],"snippet":"Page 1. Technical Report Number 904 Computer Laboratory UCAM-CL-TR-904 ISSN 1476-2986 Grammatical error correction in non-native English Zheng Yuan March 2017 15 JJ Thomson Avenue Cambridge CB3 0FD United Kingdom phone +44 1223 763500 ...","url":["http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-904.pdf"]}
{"year":"2017","title":"Handling Homographs in Neural Machine Translation","authors":["F Liu, H Lu, G Neubig - arXiv preprint arXiv:1708.06510, 2017"],"snippet":"... side. For German and French, we use a combination of Europarl v7, Common Crawl, and News Commentary as training set. For development set, newstest2013 is used for German and newstest2012 is used for French. For ...","url":["https://arxiv.org/pdf/1708.06510"]}
{"year":"2017","title":"HCTI at SemEval-2017 Task 1: Use convolutional neural network to evaluate Semantic Textual Similarity","authors":["S Yang"],"snippet":"... 1) All punctuations are removed. 2) All words are lower-cased. 3) All sentences are tokenized by Natural Language Toolkit (NLTK) (Bird et al., 2009). 4) All words are replaced by pre-trained GloVe word vectors (Common Crawl, 840B tokens) (Pennington et al., 2014). ...","url":["http://nlp.arizona.edu/SemEval-2017/pdf/SemEval016.pdf"]}
{"year":"2017","title":"Hungarian Layer: Logics Empowered Neural Architecture","authors":["H Xiao, L Meng - arXiv preprint arXiv:1712.02555, 2017"],"snippet":"… for illustration. 4.1. Experimental Setting We initialize the word embedding with 300-dimensional GloVe (Pennington et al., 2014) word vectors pre-trained in the 840B Common Crawl corpus (Pennington et al., 2014). For the …","url":["https://arxiv.org/pdf/1712.02555"]}
{"year":"2017","title":"Hunter MT: A Course for Young Researchers in WMT17","authors":["J Xu, YZ Kuang, S Baijoo, H Lee, U Shahzad, M Ahmed… - WMT 2017, 2017"],"snippet":"... ing and tuning, including Europarl v7, News Commentary v12, Rapid Corpus of EU press releases, and parts of the Common Crawl corpus ... Rapid News Test 2016 2 German-English News 33.61 News Test 2016 3 English-Czech News 13.59 Europarl, CommonCrawl, News' 12 ...","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=446"]}
{"year":"2017","title":"IIIT-H at IJCNLP-2017 Task 4: Customer Feedback Analysis using Machine Learning and Neural Network Approaches","authors":["P Danda, P Mishra, S Kanneganti, S Lanka - … of the IJCNLP 2017, Shared Tasks, 2017"],"snippet":"… We used glove pre-trained embeddings1 (Pennington et al., 2014) for English while for the rest 1we used Common Crawl corpus with 840B tokens, 2.2M vocab, case-sensitive, 300-dimensional vectors available on https://nlp.stanford.edu/projects/glove/ 155 Page 2 …","url":["http://www.aclweb.org/anthology/I17-4026"]}
{"year":"2017","title":"IITP at EmoInt-2017: Measuring Intensity of Emotions using Sentence Embeddings and Optimized Features","authors":["MS Akhtar, P Sawant, A Ekbal, J Pawar… - EMNLP 2017, 2017"],"snippet":"... For this task, we use GloVe (Pennington et al., 2014) pre-trained word embedding trained on common crawl corpus. ... The choice of common crawl word embeddings for Twitter datasets is because of the normalization steps (Section 2.1). ...","url":["http://www.aclweb.org/anthology/W/W17/W17-52.pdf#page=228"]}
{"year":"2017","title":"IITP at SemEval-2017 Task 5: An Ensemble of Deep Learning and Feature Based Models for Financial Sentiment Analysis","authors":["D Ghosal, S Bhatnagar, MS Akhtar, A Ekbal…"],"snippet":"... billion and 400 million tweets respectively. For news headline we used GloVe common crawl model trained on 802 billion words and Word2Vec Google News model (Mikolov et al., 2013). We experimented with 200, 300 and ...","url":["https://www.aclweb.org/anthology/S/S17/S17-2154.pdf"]}
{"year":"2017","title":"Implementation and Analysis of Match-LSTM for SQuAD","authors":["M Graczyk"],"snippet":"... We used the Common Crawl 840B Glove vectors with 300 dimensions as our input embedding, and like the paper we did not train these vectors. Finally, we used softsign instead of tanh for a substantial improvement in training performance with no obvious change in metrics. ...","url":["https://web.stanford.edu/class/cs224n/reports/2761882.pdf"]}
{"year":"2017","title":"Implementation and Improvement of Match-LSTM in Question-Answering System","authors":["Y Zhang, H Peng"],"snippet":"... To initialize the word vector embeddings, we used the GloVe word embeddings of dimensionality o = 300 and vocabulary size of 2.2M that have been pre-trained on Common Crawl. We did not train the word embeddings, since our dataset is not very large. ...","url":["https://web.stanford.edu/class/cs224n/reports/2748656.pdf"]}
{"year":"2017","title":"Improving Machine Translation Quality Estimation with Neural Network Features","authors":["Z Chen, Y Tan, C Zhang, Q Xiang, L Zhang, M Li… - WMT 2017, 2017"],"snippet":"... To train the word embedding and the RNNLM, the source side and the target side of the bilingual parallel corpus for the translation task, publicly re- leased by the WMT evaluation campaign, are used; they include Europarl v7, Common Crawl corpus, News Commentary v8 and ...","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=575"]}
{"year":"2017","title":"Improving the Compositionality of Word Embeddings","authors":["MJ Scheepers - 2017"],"snippet":"… 2Real, natural and imaginary numbers are represented in computers as floating point numbers [49], which are not always exact but more often really close estimations of these numbers. 3The Common Crawl dataset can be found at: http://commoncrawl.org. Page 10. 4 …","url":["https://thijs.ai/papers/scheepers-msc-thesis-2017-improving-compositionality-word-embeddings.pdf"]}
{"year":"2017","title":"Induction of Latent Domains in Heterogeneous Corpora: A Case Study of Word Alignment","authors":["H Cuong, K Sima'an"],"snippet":"... resulting SMT systems. Going beyond the findings, we surmise that virtually any large corpus (eg Europarl, Hansards, Common Crawl) harbors an arbitrary diversity of hidden domains, unknown in advance. We address the ...","url":["https://staff.fnwi.uva.nl/c.hoang/mt20171.pdf"]}
{"year":"2017","title":"Inductive Representation Learning on Large Graphs","authors":["WL Hamilton, R Ying, J Leskovec - arXiv preprint arXiv:1706.02216, 2017"],"snippet":"... For features, we use off-the-shelf 300-dimensional GloVe CommonCrawl word vectors [25]; for each post, we concatenated (i) the average embedding of the post title, (ii) the average embedding of all the post's comments (iii) the post's score, and (iv) the number of comments ...","url":["https://arxiv.org/pdf/1706.02216"]}
{"year":"2017","title":"Information Extraction meets the Semantic Web: A Survey","authors":["JL Martinez-Rodriguez, A Hogan, I Lopez-Arevalo"],"snippet":"… Web [234]. Mika referred to this as the semantic gap [193], whereby the demand for structured data on the Web outstrips its supply. For example, in analysis of the 2013 Common Crawl dataset, Meusel et al. [189] found that …","url":["http://www.semantic-web-journal.net/system/files/swj1744.pdf"]}
{"year":"2017","title":"Integrating Knowledge from Latent and Explicit Features for Triple Scoring","authors":["LW Chen, B Mangipudi, J Bandlamudi, R Sehgal…"],"snippet":"... ford NLP Group. We use word embeddings of size 300 dimensions, which were pre-trained on the Common Crawl corpus 2. We integrate the learned vector representations of GloVe for nationality and profession. In a nutshell ...","url":["http://www.uni-weimar.de/medien/webis/events/wsdm-cup-17/wsdmcup17-papers-final/wsdmcup17-triple-scoring/chen17-notebook.pdf"]}
{"year":"2017","title":"Interstitial Content Detection","authors":["E Lucas - arXiv preprint arXiv:1708.04879, 2017"],"snippet":"... 'http://servo.org'. [7] I. Kreymer. Announcing the common crawl index, April 2015. 'http://commoncrawl. org/2015/04/announcing-the-common-crawl-index/'. [8] S. Rasheed, A. Naeem, and O. Ishaq. Automated number plate recognition using hough lines and template ...","url":["https://arxiv.org/pdf/1708.04879"]}
{"year":"2017","title":"ISSUES IN HUMAN AND AUTOMATIC TRANSLATION QUALITY ASSESSMENT","authors":["S Doherty"],"snippet":"... 2013; Bojar et al. 2014). These campaigns typically involve the provision of existing corpora (eg Europarl, News Commentary, Common Crawl, Gigaword, Wiki Headlines, and the UN Corpus) as well as WMT-commissioned translations with Page 6. 6 ...","url":["https://www.researchgate.net/profile/Stephen_Doherty3/publication/314261771_Issues_in_human_and_automatic_translation_quality_assessment/links/58bea025458515dcd28defdd/Issues-in-human-and-automatic-translation-quality-assessment.pdf"]}
{"year":"2017","title":"Iterative Attention Network for Question Answering","authors":["T Henighan"],"snippet":"... 3.6 future studies In this work only the 100-dimensional GloVe vectors trained on 6 billion tokens was used. It may be useful to try the 300-dimensional vectors trained over a 840 billion common-crawl corpus, which 6 Page 7. ...","url":["http://www.tomhenighan.com/pdfs/iterative-attention-network.pdf"]}
{"year":"2017","title":"Joint Learning of Structural and Textual Features for Web Scale Event Extraction","authors":["J Wiedmann - 2017"],"snippet":"... In a second expansion step, this seed data set is further extended automatically by identifying single event pages in the Common Crawl, a repository of crawled web data, based on Microdata annotations and the annotations derived from the seed data. ...","url":["http://www.cs.ox.ac.uk/files/8846/aaai17-wiedmann-eventextraction.pdf"]}
{"year":"2017","title":"Joint Training for Pivot-based Neural Machine Translation","authors":["Y Cheng, Q Yang, Y Liu, M Sun, W Xu"],"snippet":"... sets. The evaluation metric is case-insensitive BLEU [Papineni et al., 2002] as calculated by the multi-bleu.perl script. The WMT corpus is composed of the Common Crawl, News Commentary, Europarl v7 and UN corpora. The ...","url":["http://nlp.csai.tsinghua.edu.cn/~ly/papers/ijcai2017_cy.pdf"]}
{"year":"2017","title":"Killing Two Birds with One Stone: Malicious Domain Detection with High Accuracy and Coverage","authors":["I Khalil, B Guan, M Nabeel, T Yu - arXiv preprint arXiv:1711.00300, 2017"],"snippet":"Page 1. Killing Two Birds with One Stone: Malicious Domain Detection with High Accuracy and Coverage Issa Khalil, Bei Guan, Mohamed Nabeel, Ting Yu Qatar Computing Research Institute {ikhalil,bguan,mnabeel,tyu}@hbku.edu.qa ...","url":["https://arxiv.org/pdf/1711.00300"]}
{"year":"2017","title":"LDOW2017: 10th Workshop on Linked Data on the Web","authors":["J Lehmann, S Auer, S Capadisli, K Janowicz, C Bizer… - Proceedings of the 26th …, 2017"],"snippet":"... Wikidata, a collaborative knowledge-base designed to complement Wikipedia; LinkedGeoData, providing a structureddata export from OpenStreetMap; and Web Data Commons, collecting embedded meta-data extracted from billions of webpages found in the Common Crawl ...","url":["http://aidanhogan.com/docs/ldow2017.pdf"]}
{"year":"2017","title":"Learned in Translation: Contextualized Word Vectors","authors":["B McCann, J Bradbury, C Xiong, R Socher - arXiv preprint arXiv:1708.00107, 2017"],"snippet":"... When training an MT-LSTM, we used fixed 300-dimensional word vectors. We used the CommonCrawl-840B GloVe model for English word vectors, which were completely fixed during training, so that the MT-LSTM had to learn how to use the pretrained vectors for translation. ...","url":["https://arxiv.org/pdf/1708.00107"]}
{"year":"2017","title":"Learning bilingual word embeddings with (almost) no bilingual data","authors":["MAGLE Agirre"],"snippet":"... and Italian. Given that Finnish is not in- cluded in this collection, we used the 2.8 billion word Common Crawl corpus provided at WMT 20164 instead, which we tokenized using the Stanford Tokenizer (Manning et al., 2014). In ...","url":["http://www.aclweb.org/anthology/P/P17/P17-1042.pdf"]}
{"year":"2017","title":"Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext","authors":["J Wieting, J Mallinson, K Gimpel - arXiv preprint arXiv:1706.01847, 2017"],"snippet":"... EN, FR→EN, and DE→EN, respectively). The training data included: Eu- roparl v7 (Koehn, 2005), the Common Crawl corpus, the UN corpus (Eisele and Chen, 2010), News Commentary v10, the 109 French-English corpus, ...","url":["https://arxiv.org/pdf/1706.01847"]}
{"year":"2017","title":"Learning to Predict: A Fast Re-constructive Method to Generate Multimodal Embeddings","authors":["G Collell, T Zhang, MF Moens - arXiv preprint arXiv:1703.08737, 2017"],"snippet":"... 4 Experimental setup 4.1 Word embeddings We use 300-dimensional GloVe1 vectors [19] pre-trained on the Common Crawl corpus consisting of 840B tokens and a 2.2M words vocabulary. 4.2 Visual data and features We use ImageNet [17] as our source of labeled images. ...","url":["https://arxiv.org/pdf/1703.08737"]}
{"year":"2017","title":"Learning to select data for transfer learning with Bayesian Optimization","authors":["S Ruder, B Plank - arXiv preprint arXiv:1707.05246, 2017"],"snippet":"... We train an LDA model (Blei et al., 2003) with 50 topics and 10 iterations for topic distribution-based representations and use GloVe embeddings (Pennington et al., 2014) trained on 42B tokens of Common Crawl data6 for word embedding-based representations. ...","url":["https://arxiv.org/pdf/1707.05246"]}
{"year":"2017","title":"Length, Interchangeability, and External Knowledge: Observations from Predicting Argument Convincingness","authors":["P Potash, R Bhattacharya, A Rumshisky"],"snippet":"... and create the appropriate representation. For the embedding representation, we use GloVe (Pennington et al., 2014) 300 dimensions learned from the Common Crawl corpus with 840 billion tokens. Our Wikipedia data is from ...","url":["https://pdfs.semanticscholar.org/9785/f21ac0b33689dc3ae711a94383eda01785e9.pdf"]}
{"year":"2017","title":"Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search","authors":["C Hokamp, Q Liu - arXiv preprint arXiv:1704.07138, 2017"],"snippet":"Page 1. Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search Chris Hokamp ADAPT Centre Dublin City University chris.hokamp@computing.dcu.ie Qun Liu ADAPT Centre Dublin City University qun.liu@dcu.ie Abstract ...","url":["https://arxiv.org/pdf/1704.07138"]}
{"year":"2017","title":"LIG-CRIStAL Submission for the WMT 2017 Automatic Post-Editing Task","authors":["A Berard, L Besacier, O Pietquin - Proceedings of the Second Conference on …, 2017"],"snippet":"... To mitigate this, we decided to limit our use of external data to monolingual English (commoncrawl). ... PE side Similarly to Junczys-Dowmunt and Grundkiewicz (2016) we first performed a coarse filtering of well-formed sentences of commoncrawl...","url":["http://www.aclweb.org/anthology/W17-4772"]}
{"year":"2017","title":"LIG-CRIStAL System for the WMT17 Automatic Post-Editing Task","authors":["A Berard, O Pietquin, L Besacier - arXiv preprint arXiv:1707.05118, 2017"],"snippet":"... To mitigate this, we decided to limit our use of external data to monolingual English (commoncrawl). ... PE side Similarly to Junczys-Dowmunt and Grundkiewicz (2016) we first performed a coarse filtering of well-formed sentences of commoncrawl...","url":["https://arxiv.org/pdf/1707.05118"]}
{"year":"2017","title":"LIMSI submission for WMT'17 shared task on bandit learning","authors":["G Wisniewski - WMT 2017, 2017"],"snippet":"... At the end, our monolingual corpus contain 193292548 sentences. The translation model is estimated from the CommonCrawl, NewsCo, Europarl and Rapid corpora, resulting in a parallel corpus made of 5919142 sentences. ...","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=698"]}
{"year":"2017","title":"Linked Data is People: Building a Knowledge Graph to Reshape the Library Staff Directory","authors":["JA Clark, SWH Young"],"snippet":"... RDFa and JSON-LD are two other syntax encodings that enable structured data in HTML pages. In their analysis of structured data in the Common Crawl dataset, Bizer et al. (2013) note that the growth of machine-readable descriptions of web content continues to grow. ...","url":["http://journal.code4lib.org/articles/12320"]}
{"year":"2017","title":"LMU Munich's Neural Machine Translation Systems for News Articles and Health Information Texts","authors":["M Huck, F Braune, A Fraser"],"snippet":"... 1. Adding the News Commentary (NC) and Common Crawl (CC) parallel training data as provided for WMT17 by the organizers of the news translation shared task. We initialize the optimization on the larger corpus with the Europarl-trained baseline model. ...","url":["http://www.cis.uni-muenchen.de/~fraser/pubs/huck_wmt2017_system.pdf"]}
{"year":"2017","title":"Lump at SemEval-2017 Task 1: Towards an Interlingua Semantic Similarity","authors":["C España-Bonet, A Barrón-Cedeño - Proceedings of the 11th International Workshop …, 2017"],"snippet":"... 2http://commoncrawl.org/ 3http://www.casmacat.eu/corpus/ news-commentary.html 4https://sites.google.com/site/ iwsltevaluation2016/mt-track/ 5We built a version of the lemma translator with an extra language: Babel synsets (cf. ...","url":["http://www.aclweb.org/anthology/S17-2019"]}
{"year":"2017","title":"Machine Comprehension Using Multi-Perspective Context Matching and Co-Attention","authors":["A Bajenov, T Gupta"],"snippet":"... Embedding Layer Trainable or fixed embeddings Dropout post-embedding layer Gigaword (6B) or Common Crawl (840B) corpus Architecture Choices Number of layers Type of Layers Representation Sizes embedding size (100-300) lstm units (100-200) perspective units ...","url":["http://web.stanford.edu/class/cs224n/reports/2758309.pdf"]}
{"year":"2017","title":"Machine Comprehension with MMLSTM and Clustering","authors":["T Romero, Z Barnes, F Cipollone"],"snippet":"... 1.2 Data Our model is trained on the SQuAD dataset [1]. We split the SQuAD dataset up into 82k training questions, 5k validation questions, and 10k dev questions. The set of test questions is withheld. We use 300d Common Crawl GloVe [2] vectors for our word embeddings. ...","url":["https://web.stanford.edu/class/cs224n/reports/2761209.pdf"]}
{"year":"2017","title":"Machine Question and Answering","authors":["J Chang, M Jiang, D Le"],"snippet":"... 5 Page 6. were also initialized with 300-dimensional GloVe word vectors from the 840B Common Crawl corpus (Pennington et al., 2014). The above plots illustrate the cross entropy loss (left) and F1 score (right) vs epoch and clearly show overfitting. ...","url":["https://web.stanford.edu/class/cs224n/reports/2761996.pdf"]}
{"year":"2017","title":"Machine Translation Evaluation with Neural Networks","authors":["F Guzmán, S Joty, L Màrquez, P Nakov - Computer Speech & Language, 2016"],"snippet":"We present a framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a.","url":["http://www.sciencedirect.com/science/article/pii/S0885230816301693"]}
{"year":"2017","title":"Machine Translation: Phrase-Based, Rule-Based and Neural Approaches with Linguistic Evaluation","authors":["V Macketanz, E Avramidis, A Burchardt, J Helcl… - Cybernetics and Information …, 2017"],"snippet":"... difference). The generic parallel training data (Europarl [18], News Commentary, MultiUN [19], Commoncrawl [20]) are augmented with domain-specific data from the IT domain (Libreoffice, Ubuntu, Chromium Browser [21]). ...","url":["https://www.degruyter.com/downloadpdf/j/cait.2017.17.issue-2/cait-2017-0014/cait-2017-0014.xml"]}
{"year":"2017","title":"Massive Exploration of Neural Machine Translation Architectures","authors":["D Britz, A Goldie, T Luong, Q Le - arXiv preprint arXiv:1703.03906, 2017"],"snippet":"... 3 Experimental Setup 3.1 Datasets and Preprocessing We run all experiments on the WMT'15 English→German task consisting of 4.5M sentence pairs, obtained by combining the Europarl v7, News Commentary v10, and Common Crawl corpora. ...","url":["https://arxiv.org/pdf/1703.03906"]}
{"year":"2017","title":"Matching Web Tables To DBpedia-A Feature Utility Study","authors":["D Ritze, C Bizer - context, 2017"],"snippet":"... 215 Page 7. has been extracted from the CommonCrawl web corpus3. ... 3http://commoncrawl org/ Table 3 shows the results of the correlation analysis for the property and instance similarity matrices regarding precision, eg PPstdev, and recall, eg RPstdev. ...","url":["https://openproceedings.org/2017/conf/edbt/paper-148.pdf"]}
{"year":"2017","title":"Methods of sentence extraction, abstraction and ordering for automatic text summarization","authors":["MT Nayeem - 2017"],"snippet":"Page 1. METHODS OF SENTENCE EXTRACTION, ABSTRACTION AND ORDERING FOR AUTOMATIC TEXT SUMMARIZATION MIR TAFSEER NAYEEM Bachelor of Science, Islamic University of Technology, 2011 A Thesis …","url":["https://www.uleth.ca/dspace/bitstream/handle/10133/4993/NAYEEM_MIR_TAFSEER_MSC_2017.pdf?sequence=1"]}
{"year":"2017","title":"Modeling Target-Side Inflection in Neural Machine Translation","authors":["A Tamchyna, MWD Marco, A Fraser - arXiv preprint arXiv:1707.06012, 2017"],"snippet":"Page 1. Modeling Target-Side Inflection in Neural Machine Translation Aleš Tamchyna1,2 and Marion Weller-Di Marco1,3 and Alexander Fraser1 1LMU Munich, 2Memsource, 3University of Stuttgart ales.tamchyna@memsource ...","url":["https://arxiv.org/pdf/1707.06012"]}
{"year":"2017","title":"Modeling the Dynamic Framing of Controversial Topics in Online Communities","authors":["J Mendelsohn"],"snippet":"... After preprocessing, each post is a bag-of-words of variable length: p (n) ij = [w1,w2, ..., wl]. Each word in a post is represented by its 300-dimensional GloVe vector, trained on Common Crawl data (CITE GLOVE). Posts are then represented as the average of each word's vector. ...","url":["http://web.stanford.edu/class/cs224n/reports/2761128.pdf"]}
{"year":"2017","title":"Monotasks: Architecting for Performance Clarity in Data Analytics Frameworks","authors":["K Ousterhout, C Canel, S Ratnasamy, S Shenker - 2017"],"snippet":"Page 1. Monotasks: Architecting for Performance Clarity in Data Analytics Frameworks Kay Ousterhout UC Berkeley Christopher Canel⇤ Carnegie Mellon University Sylvia Ratnasamy UC Berkeley Scott Shenker UC Berkeley, ICSI ...","url":["http://kayousterhout.org/publications/sosp17-final183.pdf"]}
{"year":"2017","title":"Multi-channel Encoder for Neural Machine Translation","authors":["H Xiong, Z He, X Hu, H Wu - arXiv preprint arXiv:1712.02109, 2017"],"snippet":"… WMT'14 English-French. We use the full WMT' 14 parallel corpus as our training data. The detailed data sets are Europarl v7, Common Crawl, UN, News Commentary, Gi- gaword. In total, it includes 36 million sentence pairs …","url":["https://arxiv.org/pdf/1712.02109"]}
{"year":"2017","title":"Multi-Domain Neural Machine Translation through Unsupervised Adaptation","authors":["MA Farajian, M Turchi, M Negri, M Federico"],"snippet":"... PHP, Ubuntu, and translated UN documents (UN-TM).2 Since the size of these corpora is relatively small for training robust MT systems, in particular NMT solutions, we added the News Commentary data from WMT'133(WMT nc), as well as the CommonCrawl (CommonC.) and ...","url":["https://hermessvn.fbk.eu/svn/hermes/open/federico/papers/Amin_et.al-wmt2017.pdf"]}
{"year":"2017","title":"Multimodal Learning for Web Information Extraction","authors":["D Gong, DZ Wang, Y Peng - ACM International Conference​​ on​​ Multimedia, 2017"],"snippet":"… Collecting image corpus. The image corpus is not included in the Common Crawl data [25] where we derived text corpus … 5.1.2 Corpus. We derive our text and image corpus based on the Common Crawl dataset [25] that is publicly available on Amazon S3 …","url":["https://pdfs.semanticscholar.org/d2a5/815007832255a033759d25d771157ae9be16.pdf"]}
{"year":"2017","title":"Multimodal sentiment analysis with word-level fusion and reinforcement learning","authors":["M Chen, S Wang, PP Liang, T Baltrušaitis, A Zadeh… - Proceedings of the 19th …, 2017"],"snippet":"… For text inputs, we use pre-trained word embeddings (glove.840B.300d) [19] to convert the transcripts of videos in the CMU-MOSI dataset into word vectors. This is a 300 dimensional word embedding trained on 840 billion tokens from the common crawl dataset …","url":["http://dl.acm.org/citation.cfm?id=3136755.3136801"]}
{"year":"2017","title":"Multiple Turn Comprehension for the Bi-Directional Attention Flow Model","authors":["T Liu"],"snippet":"... The word embedding layer converts each word in the context and question into a dense vector word representation. We use the pre-trained GloVe (Pennington et al., 2014) vectors for this layer, in particular the Common Crawl 840B tokens, 300d vectors. ...","url":["http://web.stanford.edu/class/cs224n/reports/2761890.pdf"]}
{"year":"2017","title":"Named Entity Recognition in Twitter using Images and Text","authors":["D Esteves, R Peres, J Lehmann, G Napolitano"],"snippet":"... A disadvantage when using web search engines is that they are not open and free. This can be circumvented by indexing and searching on other large sources of information, such as Common Crawl and Flickr11. ... 11 http://commoncrawl.org/ and https://www.flickr.com/ Page 7. 7 ...","url":["https://www.researchgate.net/profile/Diego_Esteves/publication/317721565_Named_Entity_Recognition_in_Twitter_using_Images_and_Text/links/594a85dda6fdcc89090cb5f5/Named-Entity-Recognition-in-Twitter-using-Images-and-Text.pdf"]}
{"year":"2017","title":"Native Language Identification from i-vectors and Speech Transcriptions","authors":["B Ulmer, A Zhao, N Walsh"],"snippet":"... word (Pennington et al., 2014). The GloVe embeddings of words came from the Common Crawl 42B to- kens collection, and the 300 dimensional embeddings were used (Pennington et al., 2014). If no corresponding GloVe ...","url":["http://web.stanford.edu/class/cs224s/reports/Ben_Ulmer.pdf"]}
{"year":"2017","title":"Natural Language Question-Answering using Deep Learning","authors":["B Liu, F Lyu, R Roy"],"snippet":"... We experimented with both fixed 193 CommonCrawl.840B.300d pretrained word vectors and GLoVE.6B.100d pretrained word 194 vectors (Pennington, Socher, & Manning, 2015) 195 We enforce a fixed question length of 22 words, and fixed context length of 300 words. ...","url":["https://pdfs.semanticscholar.org/505a/ed7c751eb57bf5e59ab1cedc49448376b7d5.pdf"]}
{"year":"2017","title":"Neural Lie Detection with the CSC Deceptive Speech Dataset","authors":["S Desai, M Siegelman, Z Maurer"],"snippet":"... Each acoustic feature frame was 34 dimensional and each speaker-dependent frame was 68 dimensional. Lexical features were encoded using GloVe Wikipedia and CommonCrawl 100-dimensional embeddings[9] based on the transcripts provided with the dataset. ...","url":["http://web.stanford.edu/class/cs224s/reports/Shloka_Desai.pdf"]}
{"year":"2017","title":"Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search","authors":["L Dahlmann, E Matusov, P Petrushkov, S Khadivi - arXiv preprint arXiv:1708.03271, 2017"],"snippet":"... For development and test sets, two reference translations are used. The German→English system is trained on parallel corpora provided for the constrained WMT 2017 evaluation (Europarl, Common Crawl, and others). We ...","url":["https://arxiv.org/pdf/1708.03271"]}
{"year":"2017","title":"Neural Machine Translation Training in a Multi-Domain Scenario","authors":["H Sajjad, N Durrani, F Dalvi, Y Belinkov, S Vogel - arXiv preprint arXiv:1708.08712, 2017","HSNDF Dalvi, Y Belinkov, S Vogel"],"snippet":"... For German-English, we use the Europarl (EP), and the Common Crawl (CC) corpora made available for the 1st Conference on Statistical Machine Translation2 as out- of-domain corpus. ... EP = Europarl, CC = Common Crawl, UN = United Nations. ...","url":["https://arxiv.org/pdf/1708.08712","https://www.researchgate.net/profile/Nadir_Durrani/publication/319349687_Neural_Machine_Translation_Training_in_a_Multi-Domain_Scenario/links/59d0f2a3aca2721f43673f75/Neural-Machine-Translation-Training-in-a-Multi-Domain-Scenario.pdf"]}
{"year":"2017","title":"Neural Machine Translation with LSTM's","authors":["J Dhaliwal"],"snippet":"... 3. dev08 11 - old dev dat from 2008 to 2011 (0.3M) 4. crawl - data from common crawl (90M) 5. ccb2 - 109 parallel corpus (81M) ... 3. dev08 11 - old dev dat from 2008 to 2011 (0.3M) 4. crawl - data from common crawl (90M) 5. ccb2 pc30109 parallel corpus (81M) ...","url":["https://people.umass.edu/~jdhaliwal/files/s2s.pdf"]}
{"year":"2017","title":"Neural Networks and Spelling Features for Native Language Identification","authors":["J Bjerva, G Grigonyte, R Ostling, B Plank - Bronze Sponsors, 2017"],"snippet":"... PoS tags are represented by 64-dimensional embeddings, initialised randomly; word tokens by 300-dimensional embeddings, initialised with GloVe (Pennington et al., 2014) em- beddings trained on 840 billion words of English web data from the Common Crawl project. ...","url":["http://www.aclweb.org/anthology/W/W17/W17-50.pdf#page=255"]}
{"year":"2017","title":"Neural vs. Phrase-Based Machine Translation in a Multi-Domain Scenario","authors":["MA Farajian, M Turchi, M Negri, N Bertoldi, M Federico - EACL 2017, 2017"],"snippet":"... K PHP 38.4 K 259.0 K 9.7 K Ubuntu 9.0 K 47.7 K 8.6 K UN-TM 40.3 K 913.8 K 12.5 K CommonCrawl 2.6 M ... in particular NMT solutions, we used CommonCrawl and Europarl corpora as out-domain data in addition to the above-mentioned domain-specific corpora, resulting in ...","url":["http://www.aclweb.org/anthology/E/E17/E17-2.pdf#page=312"]}
{"year":"2017","title":"New Word Pair Level Embeddings to Improve Word Pair Similarity","authors":["A Shaukat, N Khan"],"snippet":"... Many previous approaches present embeddings for individual words [14, 15, 16, 27] using their distributional semantics (Common Crawl corpus1) and structured knowledge from ConceptNet and PPDB [31]. ... Figure 1 shows 1 http://commoncrawl.org/ ...","url":["http://faculty.pucit.edu.pk/nazarkhan/work/wps/wpe_icdar_wml17.pdf"]}
{"year":"2017","title":"NEWSQA: AMachine COMPREHENSION DATASET","authors":["A Trischler, T Wang, X Yuan, J Harris, A Sordoni…"],"snippet":"... Both mLSTM and BARB are implemented with the Keras framework (Chollet, 2015) using the Theano (Bergstra et al., 2010) backend. Word embeddings are initialized using GloVe vectors (Pennington et al., 2014) pre-trained on the 840-billion Common Crawl corpus. ...","url":["https://www.openreview.net/pdf?id=ry3iBFqgl"]}
{"year":"2017","title":"NoSQL Web Crawler Application","authors":["GC Deka - Advances in Computers, 2017"],"snippet":"With the advent of Web technology, the Web is full of unstructured data called Big Data. However, these data are not easy to collect, access, and process at lar.","url":["http://www.sciencedirect.com/science/article/pii/S0065245817300323"]}
{"year":"2017","title":"Novel Ranking-Based Lexical Similarity Measure for Word Embedding","authors":["J Dutkiewicz, C Jędrzejek - arXiv preprint arXiv:1712.08439, 2017"],"snippet":"… 4.1 Experimental setup We use the unmodified vector space model trained on 840 billion words from Common Crawl data with the GloVe algorithm introduced in Pennington et al. (2014). The model consists of 2.2 million unique vectors; Each vector consists of 300 components …","url":["https://arxiv.org/pdf/1712.08439"]}
{"year":"2017","title":"NRC Machine Translation System for WMT 2017","authors":["C Lo, S Larkin, B Chen, D Stewart, C Cherry, R Kuhn… - WMT 2017, 2017"],"snippet":"... 2 Russian-English news translation We used all the Russian-English parallel corpora available for the constrained news translation task. They include the CommonCrawl corpus, the NewsCommentary v12 corpus, the Yandex corpus and the Wikipedia headlines corpus. ...","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=354"]}
{"year":"2017","title":"On the Effective Use of Pretraining for Natural Language Inference","authors":["I Cases, MT Luong, C Potts - arXiv preprint arXiv:1710.02076, 2017"],"snippet":"... a 1We used the publicly released embeddings, trained with Common Crawl 840B tokens for GloVe (http:// nlp.stanford.edu/projects/glove/) and Google News 42B for word2vec https://code.google.com/ archive/p/word2vec/. Although ...","url":["https://arxiv.org/pdf/1710.02076"]}
{"year":"2017","title":"Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks","authors":["N Reimers, I Gurevych - arXiv preprint arXiv:1707.06799, 2017","NRI Gurevych"],"snippet":"... (2014) trained either on Wikipedia 2014 + Gigaword 5 (about 6 billion tokens) or on Common Crawl (about 840 billion tokens), and the Komninos and Manandhar (2016) embeddings11 trained on the Wikipedia August 2015 dump (about 2 billion tokens). ...","url":["https://arxiv.org/pdf/1707.06799","https://www.arxiv-vanity.com/papers/1707.06799v2/"]}
{"year":"2017","title":"Parallel Training Data Selection for Conversational Machine Translation","authors":["X Niu, M Carpuat"],"snippet":"... Corpus # Sentences # Words (en/fr) OpenSubtitles 33.5 M 284.0 M / 268.3 M MultiUN 13.2 M 367.1 M / 432.3 M Common Crawl 3.2 M 81.1 M / 91.3 M Europarl v7 2.0 M 55.7 M / 61.9 M Wikipedia 396 k 9.7 M / 8.7 M TED corpus 207 k 4.5 M / 4.8 M News Commentary v10 199 k ...","url":["https://pdfs.semanticscholar.org/fdf6/ae86229f51893dd6e33579511489af4a5eb7.pdf"]}
{"year":"2017","title":"Passfault: an Open Source Tool for Measuring Password Complexity and Strength","authors":["BA Rodrigues, JRB Paiva, VM Gomes, C Morris…"],"snippet":"... Wikipedia: The full text of Wikipedia in 2015. • Reddit: The corpus of Reddit comments through May 2015. • CCrawl: Text extracted from the Common Crawl and language-detected with cld2. Page 6. ACKNOWLEDGMENTS ...","url":["https://www.owasp.org/images/1/13/Artigo-Passfault.pdf"]}
{"year":"2017","title":"Predictor-Estimator using Multilevel Task Learning with Stack Propagation for Neural Quality Estimation","authors":["H Kim, JH Lee, SH Na - WMT 2017, 2017"],"snippet":"... allel corpora including the Europarl corpus, common crawl corpus, news commentary, rapid corpus of EU press releases for the WMT17 translation task3, and src-pe (source sentences-their target post-editions) pairs for the WMT17 QE task. ...","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=586"]}
{"year":"2017","title":"Predictor-Estimator: Neural Quality Estimation Based on Target Word Prediction for Machine Translation","authors":["H Kim, HY Jung, H Kwon, JH Lee, SH Na - ACM Transactions on Asian and Low- …, 2017"],"snippet":"... For training the word predictor, two parallel datasets of different sizes were used: a small dataset consisting of only the Europarl corpus (Koehn 2005) and a large dataset consisting of the Europarl corpus, common crawl corpus, and news commentary, which were provided for ...","url":["http://dl.acm.org/citation.cfm?id=3109480"]}
{"year":"2017","title":"Probabilistic Relation Induction in Vector Space Embeddings","authors":["Z Bouraoui, S Jameel, S Schockaert - arXiv preprint arXiv:1708.06266, 2017"],"snippet":"... data set1 (SG-GN). We also use two embeddings that have been learned with GloVe, one from the same Wikipedia dump (GloVe-Wiki) and one from the 840B words Common Crawl data set2 (GloVe-CC). For relations with at ...","url":["https://arxiv.org/pdf/1708.06266"]}
{"year":"2017","title":"Proposal for Automatic Extraction of Taxonomic Relations in Domain Corpus","authors":["HRL Chavez, MT Vidal - Advances in Pattern Recognition"],"snippet":"… His methodology is based on two sources of evidence, substring matches and Hearts patterns. They analyze all Wikipedia in search of the Hearts patterns and extract those relationships and make use of another corpus like GigaWord, ukWac and CommonCrawl. 30 …","url":["http://www.rcs.cic.ipn.mx/rcs/2017_133/Proposal%20for%20Automatic%20Extraction%20of%20Taxonomic%20Relations%20in%20Domain%20Corpus.pdf"]}
{"year":"2017","title":"ProvDS: Uncertain Provenance Management over Incomplete Linked Data Streams","authors":["Q Liu"],"snippet":"... These datasets will be used to evaluate our provenance computation over incomplete Linked Data Streams techniques. • The Web Data Commons project5 extracts structured data from the Common Crawl, the largest web corpus available to the public. ...","url":["https://iswc2017.semanticweb.org/wp-content/uploads/papers/DC/paper_2.pdf"]}
{"year":"2017","title":"Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations","authors":["J Wieting, K Gimpel - arXiv preprint arXiv:1711.05732, 2017"],"snippet":"… The model was trained on this data along with data from some smaller Czech sources ( 160k from common-crawl, 650k from Europarlm and 190k from News) … We compare with Eu- roparl, Common-crawl, and News for Czech …","url":["https://arxiv.org/pdf/1711.05732"]}
{"year":"2017","title":"Quantext: Analysing student responses to short-answer questions","authors":["J McDonald, ACM Moskal"],"snippet":"… 1 Similarity is calculated from a word2vec model of word embeddings using the GloVe algorithm (Pennington, Socher & Manning, 2014) and is pre-trained on the Common Crawl Corpus (Spiegler, 2013) … 1532-1543 Spiegler, S (2013) Statistics of the Common Crawl Corpus …","url":["https://www.researchgate.net/profile/Adon_Moskal/publication/321266093_Quantext_Analysing_student_responses_to_short-answer_questions/links/5a179890a6fdcc50ade61806/Quantext-Analysing-student-responses-to-short-answer-questions.pdf"]}
{"year":"2017","title":"Question Answering on SQuAD","authors":["C Yang, H Ishfaq"],"snippet":"... Then we use word embeddings from GloVe[6] to map words into embedding vectors. To decrease the out of vocabulary (OOV) error, we use the Common Crawl 840B 300d GloVe vectors. Words not found in GloVe are initialized randomly. ...","url":["https://web.stanford.edu/class/cs224n/reports/2749099.pdf"]}
{"year":"2017","title":"Question Answering on the SQuAD Dataset","authors":["DH Park, V Lakshman"],"snippet":"... Initially, we used 100-dimensional word embeddings pretrained on the Wikipedia corpus to train our model before fine-tuning our system by switching to 300-dimensional GloVe vectors trained on the Common Crawl corpus. ...","url":["https://web.stanford.edu/class/cs224n/reports/2761899.pdf"]}
{"year":"2017","title":"Question Answering with Multi-Perspective Context Matching","authors":["J Asperger"],"snippet":"... The word-level embeddings were taken from GloVe vectors that were pre-trained on the 840-billion-word Common Crawl Corpus. ... For my word representations, I used 300-dimensional GloVe vectors trained on the 840 billion word Common Crawl Corpus. ...","url":["https://pdfs.semanticscholar.org/599f/376502c61550fdd37011e0cb7157d281b493.pdf"]}
{"year":"2017","title":"Reading Comprehension on the SQuAD Dataset","authors":["FNU Budianto"],"snippet":"... The Glove version used is the Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors). Figure 1: Histogram of context length, question length, and answer length in the training set. 2 Page 3. ... I use the Glove840B300d Common Crawl for the word embedding layer. ...","url":["https://web.stanford.edu/class/cs224n/reports/2762006.pdf"]}
{"year":"2017","title":"Recurrent neural networks with specialized word embeddings for health-domain named-entity recognition","authors":["IJ Unanue, EZ Borzeshi, M Piccardi - arXiv preprint arXiv:1706.09569, 2017"],"snippet":"... Therefore, the training of the word embeddings only requires large, general-purpose text corpora such as Wikipedia (400K unique words) or Common Crawl (2.2M unique words), without the need for any manual annotation. ...","url":["https://arxiv.org/pdf/1706.09569"]}
{"year":"2017","title":"Regularizing neural networks by penalizing confident output distributions","authors":["G Pereyra, G Tucker, J Chorowski, Ł Kaiser, G Hinton - arXiv preprint arXiv: …, 2017"],"snippet":"... 535–541. ACM, 2006. Christian Buck, Kenneth Heafield, and Bas Van Ooyen. N-gram counts and language models from the common crawl. In LREC, volume 2, pp. 4. Citeseer, 2014. 8 Page 9. Under review as a conference paper at ICLR 2017 ...","url":["https://arxiv.org/pdf/1701.06548"]}
{"year":"2017","title":"Reinvestigating the Classification Approach to the Article and Preposition Error Correction","authors":["R Grundkiewicz, M Junczys-Dowmunt"],"snippet":"... Other than that, default options were used. We learnt word vectors from 75 millions of English sentences extracted from Common Crawl data4. ... 3 https://code.google.com/p/word2vec/ 4 https://commoncrawl.org/ 5 http://www.comp.nus.edu.sg/~nlp/conll14st.html Page 6. ...","url":["http://www.research.ed.ac.uk/portal/files/40342436/ltc_073_grundkiewicz_2.pdf"]}
{"year":"2017","title":"Report on the 2nd Workshop on Managing the Evolution and Preservation of the Data Web (MEPDaW 2016)","authors":["J Debattista, JD Fernández, J Umbrich"],"snippet":"... 1Slides of the talk: https://aic.ai.wu.ac.at/ polleres/presentations/20160530Keynote-MEPDaW2016. pdf 2http://commoncrawl.org/ 3http://internetmemory.org/ 4https://archive.org/index.php 5http://swse.deri.org/dyldo/ ACM SIGIR Forum 84 Vol. 50 No. 2 December 2016 Page 4. ...","url":["https://pdfs.semanticscholar.org/c1eb/93952ed5cc4bda08bdd75bed84332656d864.pdf"]}
{"year":"2017","title":"Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging","authors":["N Reimers, I Gurevych - arXiv preprint arXiv:1707.09861, 2017"],"snippet":"... (2014) trained either on Wikipedia 2014 + Gigaword 5 (GloVe1 with 100 dimensions and GloVe2 with 300 dimensions) or on Common Crawl (GloVe3), and the Komninos and Manandhar (2016) embeddings (Komn.)10. We also evaluate the approach of Bojanowski et al. ...","url":["https://arxiv.org/pdf/1707.09861"]}
{"year":"2017","title":"Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning","authors":["M Riemer, E Khabiri, R Goodwin - arXiv preprint arXiv:1704.03617, 2017"],"snippet":"... Our GRU model was fed a sequence of fixed 300 dimensional Glove vectors (Pennington et al., 2014), representing words based on analysis of 840 billion words from a common crawl of the internet, as the input xt for all tasks. ...","url":["https://arxiv.org/pdf/1704.03617"]}
{"year":"2017","title":"Representing Sentences as Low-Rank Subspaces","authors":["J Mu, S Bhat, P Viswanath - arXiv preprint arXiv:1704.05358, 2017"],"snippet":"... Due to the widespread use of word2vec and GloVe, we use their publicly available word representations – word2vec(Mikolov et al., 2013) trained us- ing Google News1 and GloVe (Pennington et al., 2014) trained using Common Crawl2 – to test our observations. ...","url":["https://arxiv.org/pdf/1704.05358"]}
{"year":"2017","title":"Retrieval, Crawling and Fusion of Entity-centric Data on the Web","authors":["S Dietze"],"snippet":"... Page 8. linked data world. However, the question to what extent this is due to the se- lective content of the Common Crawl or representative for schema.org adoption on the Web in general requires additional investigations. (a ...","url":["https://www.researchgate.net/profile/Stefan_Dietze/publication/312490472_Retrieval_Crawling_and_Fusion_of_Entity-centric_Data_on_the_Web/links/587e683808aed3826af45f18.pdf"]}
{"year":"2017","title":"Rule-based spreadsheet data transformation from arbitrary to relational tables","authors":["AO Shigarov, AA Mikhailov - Information Systems, 2017"],"snippet":"... These include about 50% of tables presented in 0.4M spreadsheets of ClueWeb09 Crawl 1 [5] and 147M (61%) of 233M web tables extracted from Common Crawl 2 [3]. They lack explicit semantics required for computer programs to interpret their layout and content. ...","url":["http://www.sciencedirect.com/science/article/pii/S0306437917304301"]}
{"year":"2017","title":"S3C: An Architecture for Space-Efficient Semantic Search over Encrypted Data in the Cloud","authors":["J Woodworth, MA Salehi, V Raghavan"],"snippet":"... To evaluate our system under Big data scale datasets, we utilized a second dataset, the Common Crawl Corpus from AWS, a web crawl composed of over five billion web pages We evaluated our system against the RFC using three types of metrics: Performance, Overhead ...","url":["http://hpcclab.org/paperPdf/bigdata16/bigdata16.pdf"]}
{"year":"2017","title":"Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ","authors":["JS Kessler - arXiv preprint arXiv:1703.00565, 2017"],"snippet":"Page 1. Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ Jason S. Kessler CDK Global jason.kessler@gmail.com Abstract Scattertext is an open source tool for visualizing linguistic variation between document categories in a language-independent way. ...","url":["https://arxiv.org/pdf/1703.00565"]}
{"year":"2017","title":"Scientific Literature Text Mining and the Case for Open Access","authors":["G Sarma"],"snippet":"… science and society of scientific literature text mining. We need a scientific analogue to CommonCrawl, an open respository of scientific articles for use in exploratory data analysis. Ironically, this argument is not new, and indeed …","url":["https://www.tjoe.org/pub/scientific-literature-text-mining-and-the-case-for-open-access"]}
{"year":"2017","title":"Secure Semantic Search Over Encrypted Big Data in the Cloud","authors":["JW Woodworth - 2017"],"snippet":"Page 1. Secure Semantic Search Over Encrypted Big Data in the Cloud A Dissertation Presented to the Graduate Faculty of the University of Louisiana at Lafayette In Partial Fulfillment of the Requirements for the Degree Master's of Science Jason W. Woodworth Spring 2017 ...","url":["http://hpcclab.org/theses/jasonwoodworth17.pdf"]}
{"year":"2017","title":"SEF@ UHH at SemEval-2017 Task 1: Unsupervised knowledge-free semantic textual similarity via paragraph vector","authors":["MS Duma, W Menzel - Proceedings of SemEval-2017. http://www. aclweb. org/ …, 2017"],"snippet":"... Track / Corpora AR-AR AR-EN ES-ES ES-EN EN-EN TR-EN Commoncrawl - - 1.84M - 2.39M - Wikipedia 151K 151K - 1.81M - 160K TED 152K 152K - 157K - 137K MultiUN 1M 1M - - - - EUBookshop - - - - - 23K SETIMES - - - - - 207K Tatoeba - - - - - 156K SNLI* - 150K - 150K ...","url":["https://www.aclweb.org/anthology/S/S17/S17-2024.pdf"]}
{"year":"2017","title":"Selective Decoding for Cross-lingual Open Information Extraction","authors":["S Zhang, K Duh, B Van Durme"],"snippet":"... The word embedding size is 300 for input tokens on both the encoder side and the decoder side. We use open-source GloVe vectors (Pennington et al., 2014) trained on Common Crawl 840B with 300 dimensions6 to initialize the word embeddings on the decoder side. ...","url":["https://www.cs.jhu.edu/~s.zhang/assets/pdf/selective-decoding.pdf"]}
{"year":"2017","title":"Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints","authors":["N Mrkšić, I Vulić, DÓ Séaghdha, I Leviant, R Reichart… - arXiv preprint arXiv: …, 2017"],"snippet":"... The first four languages are those of the Multilingual SimLex-999 dataset. For the four SimLex languages, we employ four well-known, high-quality word vector collections: a) The Common Crawl GloVe English vectors from Pennington et al. ...","url":["https://arxiv.org/pdf/1706.00374"]}
{"year":"2017","title":"Semantic vector evaluation and human performance on a new vocabulary MCQ test","authors":["JP Levy, JA Bullinaria, S McCormick"],"snippet":"... The 42B and 840B vectors were generated from 42 billion and 840 billion word corpora derived from Common Crawl archives (obtained by an automated process of systematically browsing the web). All the GloVe vectors used here have 300 dimensions. ...","url":["https://pdfs.semanticscholar.org/6506/d7783d2297f70c15a8caa07f022c36dfb168.pdf"]}
{"year":"2017","title":"Semantic-based Analysis of Javadoc Comments","authors":["A Blasi, K Kuznetsov, A Goffi, SD Castellanos, A Gorla…"],"snippet":"... In our preliminary tests we found that the publicly available pre-trained word vectors of the GloVe model based on Common Crawl dataset2 already produce good results, as they identify relations such as: “if vertex exists” ; graph.containsVertex(v) and “if the graph contains the ...","url":["http://sattose.wdfiles.com/local--files/2017:schedule/SATToSE_2017_paper_24.pdf"]}
{"year":"2017","title":"Semantics derived automatically from language corpora contain human-like biases","authors":["A Caliskan, JJ Bryson, A Narayanan - Science, 2017"],"snippet":"... We used the largest of the four corpora provided—the “Common Crawl” corpus obtained from a large-scale crawl of the Internet, containing 840 billion tokens (roughly, words). Tokens in this corpus are case sensitive, resulting in 2.2 million different ones. ...","url":["http://science.sciencemag.org/content/356/6334/183.abstract"]}
{"year":"2017","title":"Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation","authors":["D Cer, M Diab, E Agirre, I Lopez-Gazpio, L Specia - Proceedings of the 11th …, 2017"],"snippet":"Page 1. Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 1–14, Vancouver, Canada, August 3 - 4, 2017. cO2017 Association for Computational Linguistics SemEval-2017 Task ...","url":["http://nlp.arizona.edu/SemEval-2017/pdf/SemEval001.pdf"]}
{"year":"2017","title":"Sentence Embedding for Neural Machine Translation Domain Adaptation","authors":["R Wang, A Finch, M Utiyama, E Sumita"],"snippet":"... Out- of-domain corpora contained Common Crawl, Europarl v7, News Commentary v10 and United Nation (UN) EN-FR parallel corpora.4 • NIST 2006 Chinese (ZH) to English corpus 5 was used as the in-domain training corpus, following the settings of (Wang et al., 2014). ...","url":["https://www.aclweb.org/anthology/P/P17/P17-2089.pdf"]}
{"year":"2017","title":"SentiHeros at SemEval-2017 Task 5: An application of Sentiment Analysis on Financial Tweets","authors":["N Tabari, A Seyeditabari, W Zadrozny"],"snippet":"... In two separate experiments, we used vectors based on the Common Crawl (840B tokens, 2.2M vo- cab, cased, 300 dimensions), and the pre-trained word vectors for Twitter (2B tweets, 27B tokens, 1.2M vocab, 200 dimensions). ...","url":["http://nlp.arizona.edu/SemEval-2017/pdf/SemEval146.pdf"]}
{"year":"2017","title":"Shallow reading with Deep Learning: Predicting popularity of online content using only its title","authors":["K Marasek, P law Rokita","W Stokowiec, T Trzcinski, K Wolk, K Marasek, P Rokita - arXiv preprint arXiv: …, 2017"],"snippet":"... As a text embedding in our experiments, we use publicly available GloVe word vectors [Pennington et al., 2014] pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC)6. Since their output dimensionality can be modified, we show the ...","url":["http://ii.pw.edu.pl/~ttrzcins/papers/ISMIS_2017_paper_57.pdf","https://arxiv.org/pdf/1707.06806"]}
{"year":"2017","title":"Simple Dynamic Coattention Networks","authors":["W Wu"],"snippet":"... unk〉. This affected the accuracy of predicted answers, as seen from Table 3. To reduced the number of unknown words, the Common Crawl GloVe vectors, which has a larger vocabulary, should be used instead. Document ...","url":["https://pdfs.semanticscholar.org/6a79/6c1c9c30913cb24d64939f90dcb06fa82be7.pdf"]}
{"year":"2017","title":"Six Challenges for Neural Machine Translation","authors":["P Koehn, R Knowles - arXiv preprint arXiv:1706.03872, 2017"],"snippet":"... BLEU scores of 34.5 on the WMT 2016 news test set (for the NMT model, this reflects the BLEU score re- sulting from translation with a beam size of 1). We use a single corpus for computing our lexical frequency counts (a concatenation of Common Crawl, Europarl, and News ...","url":["https://arxiv.org/pdf/1706.03872"]}
{"year":"2017","title":"Sockeye: A Toolkit for Neural Machine Translation","authors":["F Hieber, T Domhan, M Denkowski, D Vilar, A Sokolov… - arXiv preprint arXiv …, 2017"],"snippet":"… 9 Page 10. EN→DE LV→EN Dataset Sentences Tokens Types Sentences Tokens Types Europarl v7/v8 1,905,421 91,658,252 862,710 637,687 27,256,803 437,914 Common Crawl 2,394,616 97,473,856 3,655,645 - - - News Comm. v12 270,088 11,990,594 460,220 …","url":["https://arxiv.org/pdf/1712.05690"]}
{"year":"2017","title":"Specialising Word Vectors for Lexical Entailment","authors":["I Vulić, N Mrkšić - arXiv preprint arXiv:1710.06371, 2017"],"snippet":"... experiment with a variety of well-known, publicly available English word vectors: 1) Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) trained on the Polyglot Wikipedia (Al-Rfou et al., 2013) by Levy and Goldberg (2014); 2) GLOVE Common Crawl (Pennington et ...","url":["https://arxiv.org/pdf/1710.06371"]}
{"year":"2017","title":"SpreadCluster: Recovering Versioned Spreadsheets through Similarity-Based Clustering","authors":["L Xu, W Dou, C Gao, J Wang, J Wei, H Zhong, T Huang"],"snippet":"Page 1. SpreadCluster: Recovering Versioned Spreadsheets through Similarity-Based Clustering Liang Xu1,2, Wensheng Dou1*, Chushu Gao1, Jie Wang1,2, Jun Wei1,2, Hua Zhong1, Tao Huang1 1State Key Laboratory of ...","url":["http://www.tcse.cn/~wsdou/papers/2017-msr-spreadcluster.pdf"]}
{"year":"2017","title":"SQuAD Question Answering using Multi-Perspective Matching","authors":["Z Maurer, S Desai, S Usmani"],"snippet":"... in some cases. In terms of future work to improve on our models, we can use 840B Common Crawl GloVe word vectors rather than the Glove word vectors pretrained on Wikipedia 2014 and Gigaword5. Given additional computational ...","url":["https://pdfs.semanticscholar.org/3b1a/a646bdc6daab268f6763b829686b00263333.pdf"]}
{"year":"2017","title":"Story Cloze Ending Selection Baselines and Data Examination","authors":["M Armstrong","T Mihaylov, A Frank - arXiv preprint arXiv:1703.04330, 2017"],"snippet":"... models. Using All features defined in Section 3.1, the word2vec vectors, trained on Google News 100B corpus perform best followed by ConcepNet enriched em- beddings and Glove trained on Common Crawl 840B. The ...","url":["https://arxiv.org/pdf/1703.04330","https://zdoc.pub/story-cloze-ending-selection-baselines-and-data-examination.html"]}
{"year":"2017","title":"Stronger Baselines for Trustable Results in Neural Machine Translation","authors":["M Denkowski, G Neubig - arXiv preprint arXiv:1706.09733, 2017"],"snippet":"... Scenario Size (sent) Sources WMT German-English 4,562,102 Europarl, Common Crawl, news commentary WMT English-Finnish 2,079,842 Europarl, Wikipedia titles WMT Romanian-English 612,422 Europarl, SETimes IWSLT English-French 220,400 TED talks IWSLT Czech ...","url":["https://arxiv.org/pdf/1706.09733"]}
{"year":"2017","title":"Structured Attention Networks","authors":["Y Kim, C Denton, L Hoang, AM Rush - arXiv preprint arXiv:1702.00887, 2017"],"snippet":"Page 1. Under review as a conference paper at ICLR 2017 STRUCTURED ATTENTION NETWORKS Yoon Kim∗ Carl Denton∗ Luong Hoang Alexander M. Rush {yoonkim@seas,carldenton@college,lhoang@g,srush@seas ...","url":["https://arxiv.org/pdf/1702.00887"]}
{"year":"2017","title":"Supervised Learning of Universal Sentence Representations from Natural Language Inference Data","authors":["A Conneau, D Kiela, H Schwenk, L Barrault, A Bordes - arXiv preprint arXiv: …, 2017"],"snippet":"... 512 hidden units. We use opensource GloVe vectors trained on Common Crawl 840B2 with 300 dimensions as fixed word embeddings and initialize other word vectors to random values sampled from U(-0.1,0.1). Input sen ...","url":["https://arxiv.org/pdf/1705.02364"]}
{"year":"2017","title":"SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks","authors":["K Shim, M Lee, I Choi, Y Boo, W Sung - Advances in Neural Information Processing …, 2017"],"snippet":"… 5, pp. 79–86. [29] Common Crawl Foundation, “Common crawl,” http://commoncrawl.org, 2016, Accessed: 2017-04-11. [30] Jorg Tiedemann, “Parallel data, tools and interfaces in OPUS,” in LREC, 2012, vol. 2012, pp. 2214–2218. 10 Page 11 …","url":["http://papers.nips.cc/paper/7130-svd-softmax-fast-softmax-approximation-on-large-vocabulary-neural-networks.pdf"]}
{"year":"2017","title":"SwissLink: High-Precision, Context-Free Entity Linking Exploiting Unambiguous Labels","authors":["R Prokofyev, M Luggen, DE Difallah, P Cudré-Mauroux - 2017"],"snippet":"… In order to understand how annotations are used on the Web, we crawled all entity links found on two large datasets, by processing the CommonCrawl 3 and the Wikipedia dumps 4. The output of our processing is a list of all words and phrases that were used as anchors in …","url":["https://exascale.info/assets/pdf/swisslink-semantics2017.pdf"]}
{"year":"2017","title":"Syntax-Directed Attention for Neural Machine Translation","authors":["K Chen, R Wang, M Utiyama, E Sumita, T Zhao - arXiv preprint arXiv:1711.04231, 2017"],"snippet":"… 4.1 Data sets The proposed methods were evaluated on two data sets. • For English (EN) to German (DE) translation task, 4.43 million bilingual sentence pairs of the WMT'14 data set was used as the training data, including Common Crawl, News Commentary and Europarl v7 …","url":["https://arxiv.org/pdf/1711.04231"]}
{"year":"2017","title":"SYSTRAN Purely Neural MT Engines for WMT2017","authors":["Y Deng, J Kim, G Klein, C Kobus, N Segal, C Servan… - WMT 2017, 2017"],"snippet":"... 3.1 Corpora We used the parallel corpora made available for the shared task: Europarl v7, Common Crawl corpus, News Commentary v12 and Rapid corpus of EU press releases. Both English and German texts were preprocessed with standard tokenisation tools. ...","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=289"]}
{"year":"2017","title":"Table Identification and Reconstruction in Spreadsheets","authors":["E Koci, M Thiele, O Romero, W Lehner - International Conference on Advanced …, 2017"],"snippet":"... This corpus is of a particular interest, since it provides access to real-world business spreadsheets used in industry. The third corpus is FUSE [3] that contains 249, 376 unique spreadsheets, extracted from Common Crawl 6 . ...","url":["http://link.springer.com/chapter/10.1007/978-3-319-59536-8_33"]}
{"year":"2017","title":"Tagging Patient Notes With ICD-9 Codes","authors":["S Ayyar"],"snippet":"... For every word we obtained pretrained word vectors from Glove (Common Crawl 840 billion tokens, 2.2 million vocab of dimension size 300)[7]. Since our text consists of translated text from clinical notes, there are several misrepresentations or errors in spellings of words ...","url":["https://web.stanford.edu/class/cs224n/reports/2744196.pdf"]}
{"year":"2017","title":"Taking into account Inter-sentence Similarity for Update Summarization","authors":["G de Chalendar, O Ferret - Proceedings of the Eighth International Joint …, 2017"],"snippet":"… MCL-GLOVE-ICSISumm. In this run, we used 2.2 million word vectors (300 dimensions) trained with GloVe (Pennington et al., 2014) on the 840 billion tokens from the Common Crawl repository. • MCL-ConceptNet-ICSISumm …","url":["http://www.aclweb.org/anthology/I17-2035"]}
{"year":"2017","title":"Taxonomy Induction using Hypernym Subsequences","authors":["A Gupta, R Lebret, H Harkous, K Aberer - arXiv preprint arXiv:1704.07626, 2017"],"snippet":"... A prominent ex- ample of such a resource is WebIsA [Seitner et al., 2016], a collection of more than 400 million hypernymy relations for English, extracted from the CommonCrawl web corpus using lexico-syntactic patterns. However ...","url":["https://arxiv.org/pdf/1704.07626"]}
{"year":"2017","title":"TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering","authors":["Y Jang, Y Song, Y Yu, Y Kim, G Kim - arXiv preprint arXiv:1704.04497, 2017"],"snippet":"... We then generate multiple choice options for each QA pair, selecting four phrases from our dataset. Specifically, we represent all verbs in our dictionary as a 300D vector using the GloVe word embedding [26] pretrained on the Common Crawl dataset. ...","url":["https://arxiv.org/pdf/1704.04497"]}
{"year":"2017","title":"The AFRL-MITLL WMT17 Systems: Old, New, Borrowed, BLEU","authors":["J Gwinnup, T Anderson, G Erdmann, K Young, M Kazi… - WMT 2017, 2017"],"snippet":"... 2.1 Data Used We utilized all available data sources provided for the language pairs we participated in, including the Commoncrawl (Smith et ... For Russian we conducted monolingual selection from provided Common Crawl, to match test sets from 2012-2016 (15K lines total). ...","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=327"]}
{"year":"2017","title":"The Effect of Translationese on Tuning for Statistical Machine Translation","authors":["S Stymne"],"snippet":"... For training we used Europarl and News commentary, provided by WMT, with a total of over 2M segments for German and French and .77M for Czech. For English→German we used additional data: bilingual Common Crawl (1.5M) and monolingual News (83M). ...","url":["http://www.ep.liu.se/ecp/131/030/ecp17131030.pdf"]}
{"year":"2017","title":"The Helsinki Neural Machine Translation System","authors":["R Östling, Y Scherrer, J Tiedemann, G Tang… - arXiv preprint arXiv: …, 2017"],"snippet":"... Another common outcome in SMT is the strong impact of language models. We can confirm this once again. Adding a second language model trained on common-crawl data (CC) has a strong influence on translation quality as we can see by the BLEU scores in Table 5. ...","url":["https://arxiv.org/pdf/1708.05942"]}
{"year":"2017","title":"The HIT-SCIR System for End-to-End Parsing of Universal Dependencies","authors":["W Che, J Guo, Y Wang, B Zheng, H Zhao, Y Liu… - CoNLL 2017, 2017"],"snippet":"... 4.1. 2 Data and Tools We use the provided 100-dimensional multilingual word embeddings5 in our tokenization, POS tagging and parsing models, and use the Wikipedia and CommonCrawl data for training Brown clusters. The number of clusters is set to 256. ...","url":["https://www.aclweb.org/anthology/K/K17/K17-3.pdf#page=64"]}
{"year":"2017","title":"The Karlsruhe Institute of Technology Systems for the News Translation Task in WMT 2017","authors":["NQ Pham, J Niehues, TL Ha, E Cho, M Sperber… - WMT 2017, 2017"],"snippet":"... 2.1 German↔ English As parallel data for our German↔ English systems, we used Europarl v7 (EPPS), News Commentary v12 (NC), Rapid corpus of EU press releases, Common Crawl corpus, and simulated data. Except ...","url":["http://www.aclweb.org/anthology/W/W17/W17-47.pdf#page=390"]}
{"year":"2017","title":"The RWTH Aachen University English-German and German-English Machine Translation System for WMT 2017","authors":["JT Peter, A Guta, T Alkhouli, P Bahar, J Rosendahl…"],"snippet":"... Both models are trained on all monolingual corpora, except the commoncrawl corpus, and the target side of the bilingual data (Section 4.2), which sums up to 365.44M sentences and 7230.15M running words, respectively. ...","url":["https://www-i6.informatik.rwth-aachen.de/publications/download/1048/PeterJan-ThorstenGutaAndreasAlkhouliTamerBaharParniaRosendahlJanRossenbachNickGra%E7aMiguelNeyHermann--TheRWTHAachenUniversityEnglish-GermanGerman-EnglishMachineTranslationSystemforWMT2017--2017.pdf"]}
{"year":"2017","title":"The TALP-UPC Neural Machine Translation System for German/Finnish-English Using the Inverse Direction Model in Rescoring","authors":["C Escolano, MR Costa-jussà, JAR Fonollosa - … of the Second Conference on Machine …, 2017"],"snippet":"... 4.1 Data and Preprocess For the three language pairs that we experimented with, we used all data parallel data available in the evaluation1. For German-English, we used: europarl v.7, news commentary v.12, common crawl and rapid corpus of EU press releases. ...","url":["http://www.aclweb.org/anthology/W17-4725"]}
{"year":"2017","title":"The UMD Machine Translation Systems at IWSLT 2016: English-to-French Translation of Speech Transcripts","authors":["X Niu, M Carpuat - Proceedings of the ninth International Workshop on …, 2016"],"snippet":"... Corpus # Sentences # Words (en/fr) OpenSubtitles 33.5 M 284.0 M / 268.3 M MultiUN 13.2 M 367.1 M / 432.3 M Common Crawl 3.2 M 81.1 M / 91.3 M Europarl v7 2.0 M 55.7 M / 61.9 M Wikipedia 396 k 9.7 M / 8.7 M TED corpus 207 k 4.5 M / 4.8 M News Commentary v10 199 k ...","url":["http://workshop2016.iwslt.org/downloads/IWSLT_2016_paper_26.pdf"]}
{"year":"2017","title":"The UMD Neural Machine Translation Systems [at WMT17 Bandit Learning Task","authors":["A Sharaf, S Feng, K Nguyen, K Brantley, H Daumé III - arXiv preprint arXiv: …, 2017"],"snippet":"... sider 40k sentences). Using this monolingual data, we use data selection on a large corpus of parallel out-of-domain data (Europarl, NewsCommentary, CommonCrawl, Rapid) to seed an initial translation model. Overall, the ...","url":["https://arxiv.org/pdf/1708.01318"]}
{"year":"2017","title":"The University of Edinburgh's Neural MT Systems for WMT17","authors":["R Sennrich, A Birch, A Currey, U Germann, B Haddow… - arXiv preprint arXiv: …, 2017"],"snippet":"... the whole of CzEng 1.6pre (Bojar et al., 2016), plus the latest WMT releases of Europarl, News-commentary and CommonCrawl... We use the following resources from the WMT parallel data: News Commentary v12, Common Crawl, Yandex Corpus and UN Parallel Corpus V1.0 ...","url":["https://arxiv.org/pdf/1708.00726"]}
{"year":"2017","title":"The University of Edinburgh's systems submission to the MT task at IWSLT","authors":["M Junczys-Dowmunt, A Birch - Proceedings of the ninth International Workshop on …, 2016"],"snippet":"... Commoncrawl [3] 2.3M 3.2M Europarl v7 [4] 1.9M 2.0M Giga Fr-En [3] – 22.5M News Commentary v11 [3] 0.2M 0.2M Opensubtitles 2016 [5] 13.4M 33.5M ... [7] C. Buck, K. Heafield, and B. van Ooyen, “N-gram counts and language models from the common crawl,” in Proceedings ...","url":["http://workshop2016.iwslt.org/downloads/IWSLT_2016_paper_27.pdf"]}
{"year":"2017","title":"The Web Data Commons Structured Data Extraction","authors":["A Primpeli, R Meusel, C Bizer, H Stuckenschmidt - 2017"],"snippet":"... for the year 2016. The Web Data Commons project extracts structured data from the web corpus provided by Common Crawl, the largest public web corpus, and offers the extracted data for public download. In order to process ...","url":["http://archiv.ub.uni-heidelberg.de/volltextserver/22891/"]}
{"year":"2017","title":"To Parse or Not to Parse: An Experimental Comparison of RNTNs and CNNs for Sentiment Analysis","authors":["Z Ahmadi, A Stier, M Skowron, S Kramer"],"snippet":"... On other datasets, we use the model trained on the web data from Common Crawl which contains a case-sensitive vocabulary of size 2.2 million. In all the experiments, the size of the word vector, the minibatch and the epochs were set to 25, 20 and 100, respectively. ...","url":["http://ceur-ws.org/Vol-1874/paper_1.pdf"]}
{"year":"2017","title":"TO THE METHODOLOGY OF CORPUS CONSTRUCTION FOR MACHINE LEARNING:“TAIGA” SYNTAX TREE CORPUS AND PARSER","authors":["TO Shavrina, O Shapovalova - КОРПУСНАЯ ЛИНГВИСТИКА–2017"],"snippet":"… For Russian language, large collections of web corpora are assembled—projects like RuTenTen and Aranea Russicum, which are not available for downloading, unlike resources based on Common Crawl, but all these corpora are crawled from an unbalanced set of links and …","url":["https://dspace.spbu.ru/bitstream/11701/8786/1/%D0%9A%D0%BE%D1%80%D0%BF%D1%83%D1%81%D0%BD%D0%B0%D1%8F%20%D0%BB%D0%B8%D0%BD%D0%B3%D0%B2%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B0-2017%20%28%D1%82%D1%80%D1%83%D0%B4%D1%8B%20%D0%BC%D0%B5%D0%B6%D0%B4.%20%D0%BA%D0%BE%D0%BD%D1%84%D0%B5%D1%80.%29.pdf#page=78"]}
{"year":"2017","title":"Topics in Data Science/Өгөгдлийн шинжлэх ухаан","authors":["R Womack - 2017"],"snippet":"Page 1. Topics in Data Science / Өгөгдлийн шинжлэх ухаан Rutgers University has made this article freely available. Please share how this access benefits you. Your story matters. [https://rucore.libraries.rutgers.edu/rutgers-lib/52378/story/] …","url":["https://rucore.libraries.rutgers.edu/rutgers-lib/52378/PDF/1/"]}
{"year":"2017","title":"Toward intrusion detection using belief decision trees for big data","authors":["I Boukhris, Z Elouedi, M Ajabi - Knowledge and Information Systems, 2017"],"snippet":"Page 1. Knowl Inf Syst DOI 10.1007/s10115-017-1034-4 REGULAR PAPER Toward intrusion detection using belief decision trees for big data Imen Boukhris1 · Zied Elouedi1 · Mariem Ajabi1 Received: 3 December 2015 / Accepted ...","url":["http://link.springer.com/article/10.1007/s10115-017-1034-4"]}
{"year":"2017","title":"Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques","authors":["J Deshmukh, S Podder, S Sengupta, N Dubash - Software Maintenance and …, 2017"],"snippet":"… Each word in the dictionary was then mapped to its corresponding embedding. We experimented with both GloVe vectors trained3 on Common Crawl dataset as well as Word2Vec vectors trained4 on Google news dataset. We …","url":["http://ieeexplore.ieee.org/abstract/document/8094414/"]}
{"year":"2017","title":"Towards Automatic Identification of Fake News: Headline-Article Stance Detection with LSTM Attention Models","authors":["S Chopra, S Jain, JM Sholar - 2017"],"snippet":"... 3 Page 4. of Wikipedia and Common Crawl. We further created a randomly initialized UNK vector of zeros, for words that were not found in the GloVe set. 5.4 LSTM Attention Architectures 5.4.1 Conditionally Encoded (CE) LSTMs ...","url":["https://pdfs.semanticscholar.org/eecc/5781c826a0af8229b8a24a6fca3d3e48b0fa.pdf"]}
{"year":"2017","title":"Towards Automatically Evaluating Security Risks and Providing Cyber Intelligence","authors":["X Liao - 2017"],"snippet":"Page 1. TOWARDS AUTOMATICALLY EVALUATING SECURITY RISKS AND PROVIDING CYBER INTELLIGENCE A Thesis Presented to The Academic Faculty by Xiaojing Liao In Partial Fulfillment of the Requirements for ...","url":["https://smartech.gatech.edu/bitstream/handle/1853/58679/LIAO-DISSERTATION-2017.pdf?sequence=1&isAllowed=y"]}
{"year":"2017","title":"Towards Document-Level Neural Machine Translation","authors":["L Miculicich Werlen - 2017"],"snippet":"Page 1. TROPE R HCRAESE R PAID I TOWARDS DOCUMENT-LEVEL NEURAL MACHINE TRANSLATION Lesly Miculicich Werlen Idiap-RR-25-2017 SEPTEMBER 2017 Centre du Parc, Rue Marconi 19, PO Box 592, CH ...","url":["https://infoscience.epfl.ch/record/231129/files/MiculicichWerlen_Idiap-RR-25-2017.pdf"]}
{"year":"2017","title":"Towards Semantic Query Segmentation","authors":["A Kale, T Taula, S Hewavitharana, A Srivastava - arXiv preprint arXiv:1707.07835, 2017"],"snippet":"... estimators. This process was repeated with pretrained GloVe vectors on common crawl [14] and facebook fasttext [2] pretrained model over Wikipedia corpus with 2.5M word vocabulary. 2 shows the experiment results. We ...","url":["https://arxiv.org/pdf/1707.07835"]}
{"year":"2017","title":"Towards the ImageNet-CNN of NLP: Pretraining Sentence Encoders with Machine Translation","authors":["B McCann, J Bradbury, C Xiong, R Socher - Advances in Neural Information …, 2017"],"snippet":"… When training an MT-LSTM, we used fixed 300-dimensional word vectors. We used the CommonCrawl-840B GloVe model for English word vectors, which were completely fixed during training, so that the MT-LSTM had to learn how to use the pretrained vectors for translation …","url":["http://papers.nips.cc/paper/7209-towards-the-imagenet-cnn-of-nlp-pretraining-sentence-encoders-with-machine-translation.pdf"]}
{"year":"2017","title":"TraininG towards a society of data-saVvy inforMation prOfessionals to enable open leadership INnovation","authors":["T Blume, F Böschen, L Galke, A Saleh, A Scherp - 2017"],"snippet":"Page 1. Deliverable 3.1: Technologies for MOVING data processing and visualisation v1.0 Till Blume, Falk Böschen, Lukas Galke, Ahmed Saleh, Ansgar Scherp, Matthias Schulte-Althoff/ZBW Chrysa Collyda, Vasileios Mezaris, Alexandros Pournaras, Christos Tzelepis/CERTH ...","url":["http://moving-project.eu/wp-content/uploads/2017/04/moving_d3.1_v1.0.pdf"]}
{"year":"2017","title":"Translation Quality and Productivity: A Study on Rich Morphology Languages","authors":["L Specia, K Harris, F Blain, A Burchardt, V Macketanz…"],"snippet":"... This process resulted in: • EN–DE: Over 20 million generic and in-domain sentence pairs obtained by merging the datasets available in the OPUS (Tiedemann, 2012), TAUS, WMT and JRC 3 repositories (eg Europarl, CDEP, CommonCrawl, etc.); ...","url":["https://fredblain.org/papers/pdf/specia_et_al_2017_translation_quality_and_productivity.pdf"]}
{"year":"2017","title":"Translation Quality Estimation Using only bilingual Corpora","authors":["L Liu, A Fujita, M Utiyama, A Finch, E Sumita - IEEE/ACM Transactions on Audio, …, 2017"],"snippet":"... languages. As the bilingual corpora for conducting M2LE training, we employed Europarl and Common Crawl provided by WMT13 for the WMT15 and WMT14 tasks and a Japanese–Chinese bilingual corpus [9] for the JA2ZH task. ...","url":["http://ieeexplore.ieee.org/abstract/document/7949019/"]}
{"year":"2017","title":"TSP: Learning Task-Specific Pivots for Unsupervised Domain Adaptation","authors":["X Cui, F Coenen, D Bollegala"],"snippet":"... We use the publicly available D = 300 dimensional GloVe4 (trained using 42B tokens from the Common Crawl) and CBOW5 (trained using 100B tokens from Google News) embeddings as the word representations required by TSP. ...","url":["https://cgi.csc.liv.ac.uk/~danushka/papers/Xia_ECML_2017.pdf"]}
{"year":"2017","title":"Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension","authors":["D Golub, PS Huang, X He, L Deng - arXiv preprint arXiv:1706.09789, 2017"],"snippet":"... We initialize word-embeddings for the BIDAF model, answer synthesis module, and question synthesis module with 300-dimensional-GloVe vectors (Pennington et al., 2014) trained on the 840B Common Crawl corpus. We set all embeddings of unknown word tokens to zero. ...","url":["https://arxiv.org/pdf/1706.09789"]}
{"year":"2017","title":"Two-Step MT: Predicting Target Morphology","authors":["F Burlot, E Knyazeva, T Lavergne, F Yvon - 2016"],"snippet":"... from TED training set Full TED set (117k) + QED (242k) + europarl (885k) + news-commentary (1M) Monolingual data (various subsets ranging from 5M to 200M): Target side of the biggest parallel corpus Czeng-1.6-pre subtitles news corpora (WMT'16) common-crawl (WMT'16 ...","url":["http://workshop2016.iwslt.org/downloads/IWSLT16_Burlot.pdf"]}
{"year":"2017","title":"Unbounded cache model for online language modeling with open vocabulary","authors":["E Grave, M Cisse, A Joulin - arXiv preprint arXiv:1711.02604, 2017"],"snippet":"... In the following, we refer to this dataset as commentary. • Common Crawl is a text dataset collected from diverse web sources. The dataset is shuffled at the sentence level. ... [9] C. Buck, K. Heafield, and B. van Ooyen. N-gram counts and language models from the common crawl...","url":["https://arxiv.org/pdf/1711.02604"]}
{"year":"2017","title":"Understanding and Predicting the Usefulness of Yelp Reviews","authors":["DZ Liu"],"snippet":"... I concatenate output from both RNNs to make the final prediction. (figure 1) [1] https://www.yelp. com/dataset_challenge [2] Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors): glove.42B.300d.zip from http://nlp.stanford.edu/projects/glove/ Page 4. ...","url":["https://web.stanford.edu/class/cs224n/reports/2760995.pdf"]}
{"year":"2017","title":"Understanding Regional Context of World Wide Web using Common Crawl Corpus","authors":["MA Mehmood, HM Shafiq, A Waheed"],"snippet":"Abstract—The World Wide Web has emerged as the most important and essential tool for the society. Today, people heavily rely on rich resources available in the web for communication, business, maps, and social networking etc. In addition, people seek web","url":["https://www.researchgate.net/profile/Amir_Mehmood/publication/321489200_Understanding_Regional_Context_of_World_Wide_Web_using_Common_Crawl_Corpus/links/5a251abaaca2727dd87e780a/Understanding-Regional-Context-of-World-Wide-Web-using-Common-Crawl-Corpus.pdf"]}
{"year":"2017","title":"Understanding Spreadsheet Evolution in Practice","authors":["L Xu - Software Maintenance and Evolution (ICSME), 2017 …, 2017"],"snippet":"… IEEE International Conference on Software Engineering (ICSE), 2015, pp. 7– 16. [28] “Common crawl data on AWS.” [Online]. Available: http://aws.amazon.com/datasets/ 41740. [29] C. Chambers, M. Erwig, and M. Luckey, “SheetDiff …","url":["http://ieeexplore.ieee.org/abstract/document/8094479/"]}
{"year":"2017","title":"Unsupervised Neural Machine Translation","authors":["M Artetxe, G Labaka, E Agirre, K Cho - arXiv preprint arXiv:1710.11041, 2017"],"snippet":"... For that purpose, we used the combination of all parallel corpora provided at WMT 2014, which comprise Europarl, Common Crawl and News Commentary for both language pairs plus the UN and the Gigaword corpus for FrenchEnglish. ...","url":["https://arxiv.org/pdf/1710.11041"]}
{"year":"2017","title":"Using Distributional Semantics for Automatic Taxonomy Induction","authors":["B Zafar, M Cochez, U Qamar"],"snippet":"... system. They used general and domain specific corpora such as GigaWord, ukWac etc. and the common crawl to extract lexico-syntactic patterns. Additionally, they applied pruning methods to refine the generated taxonomy. ...","url":["http://users.jyu.fi/~miselico/papers/distributional-semantics-taxonomy.pdf"]}
{"year":"2017","title":"Using images to improve machine-translating e-commerce product listings","authors":["I Calixto, D Stein, E Matusov, P Lohar, S Castilho… - EACL 2017, 2017"],"snippet":"... Table 2 we show the number of running words as well as the perplexity scores obtained with LMs trained on three sets of different German corpora: the Multi30k, eBay's in-domain data and a concatenation of the WMT 20152 Europarl (Koehn, 2005), Common Crawl and News ...","url":["https://www.aclweb.org/anthology/E/E17/E17-2.pdf#page=669"]}
{"year":"2017","title":"Using Recurrent Neural Network to Predict The Usefulness of Yelp Reviews","authors":["DZ Liu, G Singh"],"snippet":"... The frequency of alternation is a hyper-parameter Figure 2: MTL RNN structure with detailed input and output description [2] Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors): glove.42B.300d.zip from http://nlp.stanford.edu/projects/glove/ Page 4. ...","url":["https://web.stanford.edu/class/cs221/2017/restricted/p-final/dzliu/final.pdf"]}
{"year":"2017","title":"UWat-Emote at EmoInt-2017: Emotion Intensity Detection using Affect Clues, Sentiment Polarity and Word Embeddings","authors":["V John, O Vechtomova - EMNLP 2017, 2017"],"snippet":"... GloVe Model-Tweets (GV-T), Wikipedia + Gigaword (GV-WG), Common Crawl 42B tokens (GV-CC1), Common Crawl 840B tokens (GV-CC2): GloVe is similar to Word2Vec, in that it obtains dense vector representations of words. ...","url":["http://www.aclweb.org/anthology/W/W17/W17-52.pdf#page=265"]}
{"year":"2017","title":"Variable length word encodings for neural translation models","authors":["J Gao"],"snippet":"Page 1. Variable length word encodings for neural translation models Jiameng Gao Department of Engineering University of Cambridge This dissertation is submitted for the degree of Master of Philosophy Peterhouse August 11, 2016 Page 2. Page 3. Page 4. Page 5. ...","url":["http://www.mlsalt.eng.cam.ac.uk/foswiki/pub/Main/CurrentMPhils/Jiameng_Gao_8224881_assignsubmission_file_J_Gao_MPhil_dissertation.pdf"]}
{"year":"2017","title":"VecShare: A Framework for Sharing Word Representation Vectors","authors":["J Fernandez, Z Yu, D Downey"],"snippet":"... we utilize three sets of GloVe embeddings (Pennington et al., 2014): wik+, 100-dimensional embeddings trained on six billion tokens of Wikipedia and the Gigaword corpus; web, 300-dimensional embeddings trained on 42 billion tokens of the Common Crawl Web dataset ...","url":["http://www.cs.northwestern.edu/~ddowney/publications/vecshare_fernandez_2017.pdf"]}
{"year":"2017","title":"Vector Space Representations in Information Retrieval","authors":["V Novotný"],"snippet":"Page 1. Masaryk University Faculty of Informatics Vector Space Representations in Information Retrieval Master's Thesis Vít Novotný Brno, Fall 2017 Page 2. Page 3. Masaryk University Faculty of Informatics Vector Space Representations in Information Retrieval Master's Thesis …","url":["https://is.muni.cz/th/409729/fi_m/main.pdf"]}
{"year":"2017","title":"Visual Exploration of High-Dimensional Spaces Through Identification, Summarization, and Interpretation of Two-Dimensional Projections","authors":["S Liu - 2017"],"snippet":"Visual Exploration of High-Dimensional Spaces Through Identification, Summarization, and Interpretation of Two-Dimensional Projections. Abstract. With the ever-increasing amount of available computing resources and sensing ...","url":["http://search.proquest.com/openview/521292ce267e4e2b78aa24b8452c5a8d/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2017","title":"Visually Grounded Word Embeddings and Richer Visual Features for Improving Multimodal Neural Machine Translation","authors":["JB Delbrouck, S Dupont, O Seddati - arXiv preprint arXiv:1707.01009, 2017"],"snippet":"... to. As previously mentioned, the textual representation lwi is obtained with the word embeddings algorithm Glove. We use the pre-trained model on the Common Crawl corpus consisting of 840B tokens and a 2.2M words. The ...","url":["https://arxiv.org/pdf/1707.01009"]}
{"year":"2017","title":"Web-scale profiling of semantic annotations in HTML pages","authors":["R Meusel - 2017"],"snippet":"... In 2012, the Common Crawl Foundation (CC)8 started to continuously release crawled web corpora of a decent size and made them publicly available. Each of the corpora contains several tera-bytes of compressed HTML pages. ...","url":["https://ub-madoc.bib.uni-mannheim.de/41884/1/thesis_final_rm_20170322-1.pdf"]}
{"year":"2017","title":"Web-Scale Web Table to Knowledge Base Matching","authors":["D Ritze - 2017"],"snippet":"… 43 4.2.1 Common Crawl … Page 20. 12 CHAPTER 1. INTRODUCTION 1.4 Published Work Parts of the work presented in this thesis have been published previously: • The extraction of the WDC Web Table Corpus from the Common Crawl …","url":["https://ub-madoc.bib.uni-mannheim.de/43123/1/thesis.pdf"]}
{"year":"2017","title":"What's good for the goose is good for the GANder","authors":["C Hung, B Corcoran"],"snippet":"... To reduce the percentage of un- known words, we additionally brought down the size of our vocabulary to contain only the 10k most commonly used words in the training set; and used GloVe vectors (Pennington et al., 2014), pretrained on Common Crawl (having around 42B ...","url":["https://web.stanford.edu/class/cs224n/reports/2761035.pdf"]}
{"year":"2017","title":"Word Embeddings for Practical Information Retrieval","authors":["L Galke, A Saleh, A Scherp - INFORMATIK 2017, 2017"],"snippet":"... 2 zbw.eu/stw 3 A dataset of crawled web data from https://commoncrawl.org/ Word Embeddings for Similarity Scoring in Practical Information Retrieval 2161 Page 8. i “proceedings” — 2017/8/24 — 12:20 — page 2162 — #2162 i i i ...","url":["https://dl.gi.de/bitstream/handle/20.500.12116/3987/B29-2.pdf?sequence=1"]}
{"year":"2017","title":"Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes","authors":["N Garg, L Schiebinger, D Jurafsky, J Zou - arXiv preprint arXiv:1711.08412, 2017"],"snippet":"… nearly identical correlation. We further validate this association using different embeddings trained on Wikipedia and Common Crawl texts instead of Google News; see Appendix Section B.1 for details. Google News embedding …","url":["https://arxiv.org/pdf/1711.08412"]}
{"year":"2017","title":"Word Re-Embedding via Manifold Dimensionality Retention","authors":["S Hasan, E Curry - Proceedings of the 2017 Conference on Empirical …, 2017"],"snippet":"... Original Embedding Spaces. The original word embeddings used are pre-trained GloVe models: Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, 50d, 100d, 200d, & 300d vectors), and Common Crawl (42B tokens, 1.9M vocab, 300d vectors) (Pennington et al., 2014b). ...","url":["http://www.aclweb.org/anthology/D17-1033"]}
{"year":"2017","title":"Word vectors, reuse, and replicability: Towards a community repository of large-text resources","authors":["M Fares, A Kutuzov, S Oepen, E Velldal"],"snippet":"... Moreover, with an ac- curacy of 83.08 for the semantic analogies, the GloVe model trained on the lemmatized version of Wikipedia outperforms the GloVe model trained on 42 billion tokens of web data from the Common Crawl reported in (Pennington et al., 2014), which at an ...","url":["http://www.ep.liu.se/ecp/131/037/ecp17131037.pdf"]}
{"year":"2018","title":"“I think it might help if we multiply, and not add”: Detecting Indirectness in Conversation","authors":["P Goel, Y Matsuyama, M Madaio, J Cassell"],"snippet":"… We tried various available pre-trained models like Twitter word2vec [14] trained on 400 million Twitter tweets, GloVe representations [34] trained on Wikipedia articles (called GloVe wiki) and web content crawled via …","url":["http://articulab.hcii.cs.cmu.edu/wordpress/wp-content/uploads/2018/04/Goel-IWSDS2018_camera-ready_13Mar.pdf"]}
{"year":"2018","title":"A Bi-Encoder LSTM Model for Learning Unstructured Dialogs","authors":["D Shekhar - 2018"],"snippet":"Page 1. University of Denver Digital Commons @ DU Electronic Theses and Dissertations Graduate Studies 8-1-2018 A Bi-Encoder LSTM Model for Learning Unstructured Dialogs Diwanshu Shekhar University of Denver Follow …","url":["https://digitalcommons.du.edu/cgi/viewcontent.cgi?article=2508&context=etd"]}
{"year":"2018","title":"A Bidirectional LSTM-CRF Network with Subword Representations, Character Convolutions and Morphosyntactic Features for Named Entity Recognition in Polish","authors":["M Piotrowski, W Janowski, P Pezik - Proceedings ofthePolEval2018Workshop"],"snippet":"… Chiu and Nichols 2015, Ma and Hovy 2016). Secondly, the input token sequences are matched against 300-dimensional FastText subword embeddings vectors derived from a Common Crawl dump of Polish texts (Grave et al …","url":["http://poleval.pl/files/poleval2018.pdf#page=93"]}
{"year":"2018","title":"A Case Study of Closed-Domain Response Suggestion with Limited Training Data","authors":["L Galke, G Gerstenkorn, A Scherp"],"snippet":"… and return the known answer to this context. To compute the word vector centroids, we use German fastText [16] word vectors trained on CommonCrawl and Wikipedia6 [17]. For the experiment on English utterances, we use …","url":["http://www.lpag.de/p/response_suggestion.pdf"]}
{"year":"2018","title":"A Cognitive Assistant for Risk Identification and Modeling","authors":["D Subramanian, D Bhattachrajya, RR Torrado…"],"snippet":"… politics/government or alternative ways to describe the same risk. Specifically, we use Glove [17] trained on a Common Crawl corpus that has 1.9 million vocabulary words embedded in a 300-dimensional vector space. Table II …","url":["http://ieeexplore.ieee.org/iel7/8241556/8257893/08258091.pdf"]}
{"year":"2018","title":"A Comparative Analysis of Content-based Geolocation in Blogs and Tweets","authors":["K Pappas, M Azab, R Mihalcea - arXiv preprint arXiv:1811.07497, 2018"],"snippet":"… Page 13. We also experiment with a word embedding representation, where we use word vectors obtained with GloVe [47] trained on a Common Crawl and a Twitter dataset respectively, which are added up and averaged to create a word …","url":["https://arxiv.org/pdf/1811.07497"]}
{"year":"2018","title":"A Comparative Study of Embedding Models in Predicting the Compositionality of Multiword Expressions","authors":["N Nandakumar, B Salehi, T Baldwin - Australasian Language Technology Association …"],"snippet":"… The two character-level embedding models we experiment with are fastText (Bojanowski et al., 2017) and ELMo (Peters et al., 2018), as detailed below. fastText We used the 300-dimensional model pre-trained on Common Crawl and Wikipedia us- ing CBOW …","url":["http://alta2018.alta.asn.au/alta2018-draft-proceedings.pdf#page=81"]}
{"year":"2018","title":"A Comparison of Machine Translation Paradigms for Use in Black-Box Fuzzy-Match Repair","authors":["R Knowles, JE Ortega, P Koehn"],"snippet":"… We report scores with a beam size of 12. 4.3 Phrase-Based SMT (Moses) We use Moses (Koehn et al., 2007) to train our phrasebased statistical MT (SMT) system using the same parallel text as the NMT model, with the addition of Common Crawl,10 for phrase extraction …","url":["https://www.researchgate.net/profile/John_Ortega3/publication/324007613_A_Comparison_of_Machine_Translation_Paradigms_for_Use_in_Black-Box_Fuzzy-Match_Repair/links/5ab8cd010f7e9b68ef51fa23/A-Comparison-of-Machine-Translation-Paradigms-for-Use-in-Black-Box-Fuzzy-Match-Repair.pdf"]}
{"year":"2018","title":"A Comprehensive Course on Big Data for Undergraduate Students","authors":["JA Shamsi, SZ ul Hassan, N Bawany, N Shoaib"],"snippet":"… ' TABLE V PROJECTS S. No Project Title Big Data Analytics 1 Performance Analysis on EspnCricinfo data 2 YouTube Data Analysis Using Hadoop 3 Analysis of emails in Amazon Common Crawl Dataset using MapReduce …","url":["https://grid.cs.gsu.edu/~tcpp/curriculum/sites/default/files/paper%201_0.pdf"]}
{"year":"2018","title":"A course on big data analytics","authors":["J Eckroth - Journal of Parallel and Distributed Computing, 2018"],"snippet":"This report details a course on big data analytics designed for undergraduate junior and senior computer science students. The course is heavily focused on proj.","url":["https://www.sciencedirect.com/science/article/pii/S0743731518300972"]}
{"year":"2018","title":"A database of German definitory contexts from selected web sources.","authors":["A Barbaresi, L Lemnitzer, A Geyken - LREC, 2018"],"snippet":"… specific datasets, eg instructional texts compiled for teaching purposes (Borg, 2009); large encyclopedic re- sources such as Wikipedia (Kovár et al., 2016); or generalpurpose web corpora (Navigli et al., 2010); unfiltered …","url":["https://hal.archives-ouvertes.fr/hal-01798704/document"]}
{"year":"2018","title":"A Dataset and Reranking Method for Multimodal MT of User-Generated Image Captions","authors":["S Schamoni, J Hitschler, S Riezler"],"snippet":"… pairs: French-English We trained our French-English translation system on data made available for the WMT 2015 translation shared task.7 We used the Europarl, News Commentary and Common Crawl data for training. Russian …","url":["http://www.cl.uni-heidelberg.de/statnlpgroup/papers/AMTA2018a.pdf"]}
{"year":"2018","title":"A Dataset for Web-scale Knowledge Base Population","authors":["M Glass, A Gliozzo"],"snippet":"… Obtain the list of WARC paths from the Common Crawl website – Download the WARC files hosted … 8 http://commoncrawl.org 9 https://github.com/optimaize/language-detector Page 10. 4.2 DBpedia DBpedia [1] is a mature knowledge base built from the infoboxes of Wikipedia …","url":["https://2018.eswc-conferences.org/wp-content/uploads/2018/02/ESWC2018_paper_173.pdf"]}
{"year":"2018","title":"A Deep Learning Approach for Extracting Attributes of ABAC Policies","authors":["M Alohaly, H Takabi, E Blanco - Proceedings of the 23nd ACM on Symposium on …, 2018"],"snippet":"… IBM 2004. Course Registration Requirements. (2004). 3. 2017. Common Crawl. (September 2017). http://commoncrawl.org/. 4. Ryma Abassi, Michael Rusinowitch, Florent Jacquemard, and Sihem Guemara El Fatmi. 2010. XML …","url":["https://dl.acm.org/citation.cfm?id=3205984"]}
{"year":"2018","title":"A Deep Learning Architecture for De-identification of Patient Notes: Implementation and Evaluation","authors":["K Khin, P Burckhardt, R Padman - arXiv preprint arXiv:1810.01570, 2018"],"snippet":"… embeddings. The traditional word embeddings use the latest GloVe 3 [13] pre-trained word vectors that were trained on the Common Crawl with about 840 billion tokens. For every token input, ti,j,k, the GloVe 6 Page 7. system …","url":["https://arxiv.org/pdf/1810.01570"]}
{"year":"2018","title":"A Discriminative Latent-Variable Model for Bilingual Lexicon Induction","authors":["S Ruder, R Cotterell, Y Kementchedjhieva, A Søgaard - arXiv preprint arXiv …, 2018"],"snippet":"Page 1. A Discriminative Latent-Variable Model for Bilingual Lexicon Induction Sebastian Ruder @,H∗ Ryan Cotterell S,P∗ Yova Kementchedjhieva Z Anders Søgaard Z @ Insight Research Centre, National University of Ireland …","url":["https://arxiv.org/pdf/1808.09334"]}
{"year":"2018","title":"A Document Descriptor using Covariance of Word Vectors","authors":["M Torki"],"snippet":"… We tested the available different dimensionality 100, 200 and 300. We also used the 300 dimensions GloVe model that used commoncrawl with 42 Billion tokens We call the last one Lrg. This model provides word vectors of 300 dimensions for each word …","url":["https://www.researchgate.net/profile/Marwan_Torki/publication/325678643_A_Document_Descriptor_using_Covariance_of_Word_Vectors/links/5b1d9b5345851587f29f58a2/A-Document-Descriptor-using-Covariance-of-Word-Vectors.pdf"]}
{"year":"2018","title":"A Domain is only as Good as its Buddies: Detecting Stealthy Malicious Domains via Graph Inference","authors":["IM Khalil, B Guan, M Nabeel, T Yu - 2018"],"snippet":"… Thales scans DNS using a set of seed domain list compiled from multiple sources, including public blacklists (eg [19, 40, 47]), the Alexa list [13], the Common Crawl dataset [3], the domain feed from an undisclosed security","url":["https://www.researchgate.net/profile/Issa_Khalil/publication/323393061_A_Domain_is_only_as_Good_as_its_Buddies_Detecting_Stealthy_Malicious_Domains_via_Graph_Inference/links/5a93a507a6fdccecff05aa6f/A-Domain-is-only-as-Good-as-its-Buddies-Detecting-Stealthy-Malicious-Domains-via-Graph-Inference.pdf"]}
{"year":"2018","title":"A flexible space-time tradeoff on hybrid index with bicriteria optimization","authors":["X Song, Y Yang, Y Jiang - Tsinghua Science and Technology, 2019"],"snippet":"… sorting. Page 3. 108 Tsinghua Science and Technology, February 2019, 24(1): 106–122 (4) We execute a detailed experimental evaluation on two realistic datasets, GOV2 and Common Crawl, with AOL query log. Analysis shows …","url":["https://ieeexplore.ieee.org/iel7/5971803/8526498/08526502.pdf"]}
{"year":"2018","title":"A Framework to Discover Significant Product Aspects from E-commerce Product Reviews","authors":["S Indrakanti, G Singh - 2018"],"snippet":"… ranking. 3.4.3 Synonym-based Clustering. We use 300-dimensional word embeddings for one million vocabulary entries trained on the Common Crawl corpus using the GloVe algorithm [18] to compute word similarities. Synonym …","url":["https://sigir-ecom.github.io/ecom18Papers/paper19.pdf"]}
{"year":"2018","title":"A Genetic Algorithm for Combining Visual and Textual Embeddings Evaluated on Attribute Recognition","authors":["R Li, G Collell, MF Moens"],"snippet":"… Following Collell and Moens (2016), we employ 300dimensional GloVe vectors (Pennington et al., 2014) trained on the largest available corpus (840B tokens and a 2.2M words vocabulary from Common Crawl corpus) …","url":["http://ceur-ws.org/Vol-2226/paper7.pdf"]}
{"year":"2018","title":"A Geo-Tagging Framework for Address Extraction from Web Pages","authors":["J Efremova, I Endres, I Vidas, O Melnik - Industrial Conference on Data Mining, 2018"],"snippet":"… Common Crawl is a public corpus, mostly stored on Amazon Web Services 3 . A subset of the CommonCrawl dataset has schema information in the microdata format with manual annotation of the main content such as …","url":["https://link.springer.com/chapter/10.1007/978-3-319-95786-9_22"]}
{"year":"2018","title":"A Graph-to-Sequence Model for AMR-to-Text Generation","authors":["L Song, Y Zhang, Z Wang, D Gildea - arXiv preprint arXiv:1805.02473, 2018"],"snippet":"… 5.2 Settings We extract a vocabulary from the training set, which is shared by both the encoder and the de- coder. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on …","url":["https://arxiv.org/pdf/1805.02473"]}
{"year":"2018","title":"A Helping Hand: Transfer Learning for Deep Sentiment Analysis","authors":["X Dong, G Melo - Proceedings of the 56th Annual Meeting of the …, 2018"],"snippet":"… en es de ru α 0.0004 0.0008 0.003 0.003 cs it ja α 0.003 0.003 0.003 Common Crawl data1, while for other languages, we rely on the Facebook fastText Wikipedia em- beddings (Bojanowski et al., 2016) as input representations. All of these are 300-dimensional …","url":["http://www.aclweb.org/anthology/P18-1235"]}
{"year":"2018","title":"A Hybrid Deep Learning System for Machine Comprehension","authors":["G Wu"],"snippet":"… and therefore can suffer from the out-of-vocabulary words. Thus, I used glove840B300d Common Crawl as the word embedding database during my training. e Apply dropout and regularization: dropout and regularization help …","url":["http://web.stanford.edu/class/cs224n/reports/6879317.pdf"]}
{"year":"2018","title":"A Machine Learning Approach to Correlate Emotional Intelligence and Happiness Based on Twitter Data","authors":["SS Shravani, NK Jha, R Guha - 2018"],"snippet":"… The corpus in this case was the common crawl dataset [Mikolov, 2017] and the word vectors were obtained using fast text algorithm of Facebook AI research (FAIR) which gave us around 2 million word vectors. The data set …","url":["http://hci2018.bcs.org/prelim_proceedings/papers/Work-in-Progress%20Track/BHCI-2018_paper_115.pdf"]}
{"year":"2018","title":"A Multi-layer LSTM-based Approach for Robot Command Interaction Modeling","authors":["M Mensio, E Bastianelli, I Tiddi, G Rizzo - arXiv preprint arXiv:1811.05242, 2018"],"snippet":"… for word representation,” in Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014, pp. 1532–1543. [23] (2012) Common crawl. [Online]. Available: http://commoncrawl.org/","url":["https://arxiv.org/pdf/1811.05242"]}
{"year":"2018","title":"A Multi-task Ensemble Framework for Emotion, Sentiment and Intensity Prediction","authors":["MS Akhtar, D Ghosal, A Ekbal, P Bhattacharyya - arXiv preprint arXiv:1808.01216, 2018"],"snippet":"… Figure 1: Proposed Multi-task framework. 3.1 Deep Learning Models We employ the architecture of Figure 1a to train and tune all the deep learning models using pretrained GloVe (common crawl 840 billion) word embeddings (Pennington et al., 2014) …","url":["https://arxiv.org/pdf/1808.01216"]}
{"year":"2018","title":"A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction","authors":["S Chollampatt, HT Ng - arXiv preprint arXiv:1801.08831, 2018"],"snippet":"… Grundkiewicz 2016) employs a word-level SMT approach with task-specific features and a web-scale LM trained on the Common Crawl corpus … all subsidiary models that make use of additional English corpora such as the word-class LM and the web-scale Common Crawl LM …","url":["https://arxiv.org/pdf/1801.08831"]}
{"year":"2018","title":"A Multimodal Assessment Framework for Integrating Student Writing and Drawing in Elementary Science Learning","authors":["PAM Smith, S Leeman-Munk, A Shelton, BW Mott… - IEEE Transactions on …, 2018"],"snippet":"Page 1. 1939-1382 (c) 2018 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["http://ieeexplore.ieee.org/abstract/document/8274912/"]}
{"year":"2018","title":"A novel approach for phishing emails real time classifica-tion using k-means algorithm","authors":["V Mhaske-Dhamdhere, S Vanjale - International Journal of Engineering & …, 2017"],"snippet":"… to get to the two techniques utilizes RF (Random Forest) and LSTM (a long/here and now memory mastermind on datasets phish tank and Common Crawl, which gives result as precision rate of 93.5% and 98.7% .RF and LSTM utilizes 14 highlights of lexical and quantifiable …","url":["https://forum.sciencepubco.com/index.php/ijet/article/download/9018/3069"]}
{"year":"2018","title":"A novel approach for phishing emails real time classification using k-means algorithm","authors":["V Mhaske-Dhamdhere, S Vanjale - International Journal of Engineering & …, 2018"],"snippet":"… to get to the two techniques utilizes RF (Random Forest) and LSTM (a long/here and now memory mastermind on datasets phish tank and Common Crawl, which gives result as precision rate of 93.5% and 98.7% .RF and …","url":["http://bvucoepune.edu.in/wp-content/uploads/2018/BVUCOEP-DATA/Research_Publications/2017_18/168.pdf"]}
{"year":"2018","title":"A Pragmatic Guide to Geoparsing Evaluation","authors":["M Gritta, MT Pilehvar, N Collier - arXiv preprint arXiv:1810.12368, 2018"],"snippet":"… It 11 https://cloud.google.com/natural-language/ 12 https://spacy.io/usage/linguistic-features 13 https://github.com/jiesutd/NCRFpp 14 Common Crawl 42B - https://nlp.stanford.edu/ projects/glove/ Page 17. A Pragmatic Guide to Geoparsing Evaluation 17 …","url":["https://arxiv.org/pdf/1810.12368"]}
{"year":"2018","title":"A QUANTITATIVE ANALYSIS OF THE USE OF MICRODATA FOR SEMANTIC ANNOTATIONS ON EDUCATIONAL RESOURCES","authors":["R NAVARRETE, S LUJÁN-MORA - Journal of Web Engineering, 2018"],"snippet":"… This quantitative analysis was conducted on datasets extracted from the Common Crawl Corpus [17], as it is the largest corpus of web crawl … The CCF periodically releases the web corpus results from each crawling process for public use. This is the Common Crawl Corpus …","url":["http://www.rintonpress.com/xjwe17/jwe-17-12/045-072.pdf"]}
{"year":"2018","title":"A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings","authors":["M Artetxe, G Labaka, E Agirre - arXiv preprint arXiv:1805.06297, 2018"],"snippet":"… EnglishSpanish. More concretely, the dataset consists of 300-dimensional CBOW embeddings trained on WacKy crawling corpora (English, Italian, German), Common Crawl (Finnish) and WMT News Crawl (Spanish). The …","url":["https://arxiv.org/pdf/1805.06297"]}
{"year":"2018","title":"A Simple Machine Learning Method for Commonsense Reasoning? A Short Commentary on Trinh & Le (2018)","authors":["WS Saba - arXiv preprint arXiv:1810.00521, 2018"],"snippet":"… the suitcase”. T&L then compute, against the backdrop of training on a large corpus (trained on the language model LM-1-Billion, CommonCrawl, and SQuAD), the probabilities of s1 and s2 appearing in a large corpus. The …","url":["https://arxiv.org/pdf/1810.00521"]}
{"year":"2018","title":"A Simple Method for Commonsense Reasoning","authors":["TH Trinh, QV Le - arXiv preprint arXiv:1806.02847, 2018"],"snippet":"… We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training …","url":["https://arxiv.org/pdf/1806.02847"]}
{"year":"2018","title":"A Social Robot System for Modeling Children's Word Pronunciation: Socially Interactive Agents Track","authors":["S Spaulding, H Chen, S Ali, M Kulinski, C Breazeal - Proceedings of the 17th …, 2018"],"snippet":"… 3) The semantic distance between two words is the cosine distance between their word-embedding vector representation. We used word-embedding vectors from the pre-trained Common Crawl GloVe vectors [27], included with the SpaCy Python package …","url":["https://dl.acm.org/citation.cfm?id=3237946"]}
{"year":"2018","title":"A survey on optimal utilization of preemptible VM instances in cloud computing","authors":["AK Mishra, BK Umrao, DK Yadav - The Journal of Supercomputing, 2018"],"snippet":"Page 1. The Journal of Supercomputing https://doi.org/10.1007/s11227-018-2509-0 A survey on optimal utilization of preemptible VM instances in cloud computing Ashish Kumar Mishra1 · Brajesh Kumar Umrao1 · Dharmendra K. Yadav1 …","url":["https://link.springer.com/article/10.1007/s11227-018-2509-0"]}
{"year":"2018","title":"A Transfer Learning Based Hierarchical Attention Neural Network for Sentiment Classification","authors":["Z Qu, Y Wang, X Wang, S Zheng - International Conference on Data Mining and Big …, 2018"],"snippet":"… The large one consists of 209772 sentences pairs. While training the LSTM encoder, we obtain the fixed English word vectors using the CommonCrawl-840B GloVe model. The size of hidden units is 300 in the LSTM. We use SGD for training …","url":["https://link.springer.com/chapter/10.1007/978-3-319-93803-5_36"]}
{"year":"2018","title":"A Vector Worth a Thousand Counts-A Temporal Semantic Similarity Approach to Patent Impact Prediction","authors":["DS Hain, R Jurowetzki, T Buchmann, P Wolf - 2018"],"snippet":"… task convolutional neural network model trained on OntoNotes, with GloVe vectors (685k unique vectors with 300 dimensions) trained on Common Crawl. Given a patent abstract, spaCy predicts the meaning of each term in the document …","url":["http://conference.druid.dk/acc_papers/tgct2l45t4lahofdzb2w9uvbpan4hq.pdf"]}
{"year":"2018","title":"Absolute Orientation for Word Embedding Alignment","authors":["S Dev, S Hassan, JM Phillips - arXiv preprint arXiv:1806.01330, 2018"],"snippet":"… For instance, some groups have built word vector embeddings for enormous datasets (eg, GloVe embedding using 840 billion tokens from Common Crawl, or the word2vec embedding using 100 billion tokens of Google News), which …","url":["https://arxiv.org/pdf/1806.01330"]}
{"year":"2018","title":"Abstractive Summarization of Reddit Posts with Multi-level Memory Networks","authors":["B Kim, H Kim, G Kim - arXiv preprint arXiv:1811.00783, 2018"],"snippet":"… 4.1 Text Embedding Online posts include lots of morphologically similar words, which should be closely embedded. Thus, we use the fastText (Bojanowski et al., 2016) trained on the Common Crawl corpus, to initialize the word embedding matrix Wemb …","url":["https://arxiv.org/pdf/1811.00783"]}
{"year":"2018","title":"Abstractive Summarization Using Attentive Neural Techniques","authors":["J Krantz, J Kalita - arXiv preprint arXiv:1810.08838, 2018"],"snippet":"… Page 5. similarity sub-score uses GloVe embeddings2 pretrained on Common Crawl while the dissimilarity sub-score uses Word2Vec3 trained on the Google News dataset. Using different word embeddings provides …","url":["https://arxiv.org/pdf/1810.08838"]}
{"year":"2018","title":"Accurate semantic textual similarity for cleaning noisy parallel corpora using semantic machine translation evaluation metric: The NRC supervised submissions to the …","authors":["C Lo, M Simard, D Stewart, S Larkin, C Goutte, P Littell - Proceedings of the Third …, 2018"],"snippet":"… 2011, 2012; Bojar et al., 2013) as the basis of our development set, as we believe that all the test sets in the previous years are clean and highly parallel, as opposed to the “clean” training data where glitches may occur (especially …","url":["http://www.aclweb.org/anthology/W18-6481"]}
{"year":"2018","title":"Achieving Human Parity on Automatic Chinese to English News Translation","authors":["H Hassan, A Aue, C Chen, V Chowdhary, J Clark… - arXiv preprint arXiv …, 2018"],"snippet":"Page 1. Achieving Human Parity on Automatic Chinese to English News Translation Hany Hassan∗, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt …","url":["https://arxiv.org/pdf/1803.05567"]}
{"year":"2018","title":"Adapted TextRank for Term Extraction: A Generic Method of Improving Automatic Term Extraction Algorithms","authors":["Z Zhang, J Petrak, D Maynard"],"snippet":"… cosine similarity function between two vectors. For the word embeddings, we use the GloVe embeddings pre-trained on the general-purpose Common Crawl data2, but others could be investigated. For multi-word expressions, we …","url":["https://www.researchgate.net/profile/Ziqi_Zhang7/publication/326904600_A_Generic_Method_of_Improving_Automatic_Term_Extraction_Algorithms/links/5b6b3b73a6fdcc87df6da54b/A-Generic-Method-of-Improving-Automatic-Term-Extraction-Algorithms.pdf"]}
{"year":"2018","title":"Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification","authors":["R He, WS Lee, HT Ng, D Dahlmeier - arXiv preprint arXiv:1809.00530, 2018"],"snippet":"… 4.4 Training Details and Hyper-parameters We initialize word embeddings using the 300dimension GloVe vectors supplied by Pennington et al., (2014), which were trained on 840 billion tokens from the Common Crawl. For …","url":["https://arxiv.org/pdf/1809.00530"]}
{"year":"2018","title":"Addressing Age-Related Bias in Sentiment Analysis","authors":["M Díaz, I Johnson, A Lazar, AM Piper, D Gergle - 2018"],"snippet":"… 400K words, uncased WG-6B-100D WG-6B-200D WG-6B-300D CC-42B-300D Common Crawl of the Internet 1.9M works, uncased CC-840B-300D 2.2M words, cased TW-27B-25D 2 billion Twitter tweets 1.2M words, uncased TW-27B-50D TW-27B-100D TW-27B-200D …","url":["http://www-users.cs.umn.edu/~joh12041/Publications/AgeBiasSentimentAnalysis_CHI18.pdf"]}
{"year":"2018","title":"Advances in Pre-Training Distributed Word Representations","authors":["T Mikolov, E Grave, P Bojanowski, C Puhrsch, A Joulin - arXiv preprint arXiv …, 2017"],"snippet":"… problems. Training such models on massive data sources, like Common Crawl, can be cumbersome and many NLP practitioners prefer to use publicly available pre-trained word vectors over training the models by themselves …","url":["https://arxiv.org/pdf/1712.09405"]}
{"year":"2018","title":"Adversarial Example Generation with Syntactically Controlled Paraphrase Networks","authors":["M Iyyer, J Wieting, K Gimpel, L Zettlemoyer - arXiv preprint arXiv:1804.06059, 2018"],"snippet":"… The pretrained Czech-English model used for translation came from the Nematus NMT system (Sennrich et al., 2017). The training data of this system includes four sources: Common Crawl, CzEng 1.6, Europarl, and News Commentary …","url":["https://arxiv.org/pdf/1804.06059"]}
{"year":"2018","title":"Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization","authors":["EM Ponti, I Vulić, G Glavaš, N Mrkšić, A Korhonen - arXiv preprint arXiv:1809.04163, 2018"],"snippet":"… Polyglot Wikipedia (Al-Rfou et al., 2013) using Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) by Levy and Goldberg (2014) with bag-of- words contexts (window size is 2). 2) GLOVE-CC are GloVe vectors …","url":["https://arxiv.org/pdf/1809.04163"]}
{"year":"2018","title":"Aggression Identification Using Deep Learning and Data Augmentation","authors":["J Risch, R Krestel"],"snippet":"… More specifically, we use the common crawl embeddings5 for the English tasks and the Hindi Wikipedia embeddings6 for the Hindi tasks (Grave et al., 2018). In comparison to other embedding methods, fastText embeddings …","url":["https://hpi.de/fileadmin/user_upload/fachgebiete/naumann/publications/2018/Aggression_Identification_Using_Deep_Learning_and_Data_Augmentation.pdf"]}
{"year":"2018","title":"Aggressive Language Identification Using Word Embeddings and Sentiment Features","authors":["C Orasan - Proceedings of the First Workshop on Trolling …, 2018"],"snippet":"… representation of words. For the experiments presented in this paper, we used the pretrained word vectors obtained from the Common Crawl corpus containing 840 billion tokens and 2.2 million vocabulary entries. Each word …","url":["http://www.aclweb.org/anthology/W18-4414"]}
{"year":"2018","title":"Ain't Nobody Got Time For Coding: Structure-Aware Program Synthesis From Natural Language","authors":["J Bednarek, K Piaskowski, K Krawiec - arXiv preprint arXiv:1810.09717, 2018"],"snippet":"… Given that, we rely on a pretrained GloVe embedding, more specifically the Common Crawl, which has been trained on generic NL on 42B tokens by Pennington et al. (2014), has vocabulary size of 1.9M tokens, and embeds the words in a 300dimensional space …","url":["https://arxiv.org/pdf/1810.09717"]}
{"year":"2018","title":"Alibaba's Neural Machine Translation Systems for WMT18","authors":["Y Deng, S Cheng, J Lu, K Song, J Wang, S Wu, L Yao… - Proceedings of the Third …, 2018"],"snippet":"… For English ↔ Russian, we use the following resources from the WMT parallel data: News Commentary v13, CommonCrawl, ParaCrawl corpus … For EN → TR, about 6 million sentences are selected from the newscrawl2016, 2017 …","url":["http://www.aclweb.org/anthology/W18-6408"]}
{"year":"2018","title":"ALOD2Vec Matcher⋆","authors":["J Portisch, H Paulheim"],"snippet":"… like DBpedia [3] – but in- stead on the whole Web: The data set consists of hypernymy relations extracted from the Common Crawl1, a … 1 see http://commoncrawl.org/ 2 see http://webisa.webdatacommons.org/concept …","url":["http://dit.unitn.it/~pavel/om2018/papers/oaei18_paper3.pdf"]}
{"year":"2018","title":"Amortized Context Vector Inference for Sequence-to-Sequence Networks","authors":["S Chatzis, A Charalampous, K Tolias, SA Vassou - arXiv preprint arXiv:1805.09039, 2018"],"snippet":"Page 1. Amortized Context Vector Inference for Sequence-to-Sequence Networks Sotirios Chatzis Cyprus University of Technology sotirios. chatzis@cut.ac.cy Aristotelis Charalampous Cyprus University of …","url":["https://arxiv.org/pdf/1805.09039"]}
{"year":"2018","title":"AmritaNLP at SemEval-2018 Task 10: Capturing discriminative attributes using convolution neural network over global vector representation.","authors":["V Vinayan, KP Soman - Proceedings of The 12th International Workshop on …, 2018"],"snippet":"… ding (Pennington et al., 2014) of various dimensions are considered, which is learned over public data, available under the PDDL.2 (100, 300 di- mension word representation, embedded over 6B, 840B sizes common crawl corpus are considered) …","url":["http://www.aclweb.org/anthology/S18-1166"]}
{"year":"2018","title":"An Adaption of BIOASQ Question Answering dataset for Machine Reading systems by Manual Annotations of Answer Spans.","authors":["S Kamath, B Grau, Y Ma - Proceedings of the 6th BioASQ Workshop A challenge …, 2018"],"snippet":"… Several embedding spaces were tested as input vectors (Kamath et al., 2017) and the best performing ones which were the Glove embeddings trained on common crawl data with 840B tokens, were chosen as input to the system …","url":["http://www.aclweb.org/anthology/W18-5309"]}
{"year":"2018","title":"An Analysis of Hierarchical Text Classification Using Word Embeddings","authors":["RA Stein, PA Jaques, JF Valiati - Information Sciences, 2018"],"snippet":"Skip to main content …","url":["https://www.sciencedirect.com/science/article/pii/S0020025518306935"]}
{"year":"2018","title":"An Encoder-decoder Approach to Predicting Causal Relations in Stories","authors":["M Roemmele, AS Gordon"],"snippet":"… First, we encoded the words in each in- put segment as the sum of their GloVe embeddings3 (Pennington et al., 2014), which represent words according to a global log-bilinear regression model trained on word co-occurrence counts in the Common Crawl corpus …","url":["http://people.ict.usc.edu/~gordon/publications/NAACL-WS18A.PDF"]}
{"year":"2018","title":"An Experimental Analysis of Multi-Perspective Convolutional Neural Networks","authors":["Z Tu - 2018"],"snippet":"… nity to use. Popular pre-trained word vectors include word2vec trained on Google News, GloVe [31] trained on Common Crawl/Wikipedia/Twitter, and fastText [19] trained on Common Crawl/Wikipedia. Whereas word2vec captures …","url":["https://uwspace.uwaterloo.ca/bitstream/handle/10012/13297/Tu_Zhucheng.pdf?sequence=1"]}
{"year":"2018","title":"An Unsupervised Approach to Relatedness Analysis of Legal Language","authors":["Y Wang - 2018"],"snippet":"Page 1. An Unsupervised Approach to Relatedness Analysis of Legal Language by Ying Wang A thesis presented to the University of Waterloo in fulfillment of the thesis requirement for the degree of Master of Applied …","url":["https://uwspace.uwaterloo.ca/bitstream/handle/10012/13847/Wang_Ying.pdf?sequence=3&isAllowed=y"]}
{"year":"2018","title":"Analysis of DNS in cybersecurity","authors":["P Hudák"],"snippet":"Page 1. Masarykova univerzita Fakulta informatiky Analysis of DNS in cybersecurity Master's thesis Patrik Hudák Brno, 2017 Page 2. Page 3. Declaration Hereby I declare, that this paper is my original authorial work, which I have worked out by my own …","url":["https://is.muni.cz/th/byrdn/Thesis.pdf"]}
{"year":"2018","title":"Analysis of parallel iterative graph applications on shared memory systems","authors":["F Atik - 2018"],"snippet":"Page 1. ANALYSIS OF PARALLEL ITERATIVE GRAPH APPLICATIONS ON SHARED MEMORY SYSTEMS a thesis submitted to the graduate school of engineering and science of bilkent university in partial fulfillment of the requirements for the degree of master of science in …","url":["http://repository.bilkent.edu.tr/bitstream/handle/11693/35746/10178583.pdf?sequence=1"]}
{"year":"2018","title":"Analysis of Short Text Classification strategies using Out of-domain Vocabularies","authors":["D Roa - 2018"],"snippet":"Page 1. IN DEGREE PROJECT INFORMATION AND COMMUNICATION TECHNOLOGY, SECOND CYCLE, 30 CREDITS , STOCKHOLM SWEDEN 2018 Analysis of Short Text Classification strategies using Out- of-domain Vocabularies DIEGO ROA …","url":["http://www.diva-portal.org/smash/get/diva2:1259356/FULLTEXT01.pdf"]}
{"year":"2018","title":"Analysis of the Web Graph Aggregated by Host and Pay-Level Domain","authors":["A Funel - arXiv preprint arXiv:1802.05435, 2018"],"snippet":"… The web graph datasets, publicly available, have been released by the Common Crawl Foundation 1 and are based on a web crawl performed during the period May-June-July 2017 … 1http://commoncrawl.org … [7] from a crawl, provided by the Common Crawl Foundation, gath …","url":["https://arxiv.org/pdf/1802.05435"]}
{"year":"2018","title":"Analyzing conversations to automatically identify action items","authors":["R Raanani, R Levy, MY Breakstone, D Facher - US Patent App. 15/854,642, 2018"],"snippet":"… At the same time, natural language processing (NLP) approaches to both topic modeling and world-knowledge modeling, have become much more efficient due to the availability of large, freely accessible natural language corpora (eg, CommonCrawl), as well as freely …","url":["https://patents.google.com/patent/US20180122383A1/en"]}
{"year":"2018","title":"ANALYZING CONVERSATIONS TO AUTOMATICALLY IDENTIFY CUSTOMER PAIN POINTS","authors":["R Raanani, R Levy, MY Breakstone, D Facher - US Patent App. 15/902,808, 2018"],"snippet":"… At the same time, natural language processing (NLP) approaches to both topic modeling and world-knowledge modeling, have become much more efficient due to the availability of large, freely accessible natural language corpora (eg, CommonCrawl), as well as freely …","url":["http://www.freepatentsonline.com/y2018/0181561.html"]}
{"year":"2018","title":"Analyzing conversations to automatically identify deals at risk","authors":["R Raanani, R Levy, D Facher, MY Breakstone - US Patent App. 15/835,807, 2018"],"snippet":"… language processing (NLP) approaches to both topic modeling and i Insidesales.com “Market size 2013” study world-knowledge modeling, have become much more efficient due to the availability of large, freely accessible …","url":["https://patents.google.com/patent/US20180096271A1/en"]}
{"year":"2018","title":"ANALYZING CONVERSATIONS TO AUTOMATICALLY IDENTIFY PRODUCT FEATURE REQUESTS","authors":["R Raanani, R Levy, MY Beakstone, D Facher - US Patent App. 15/902,751, 2018"],"snippet":"… At the same time, natural language processing (NLP) approaches to both topic modeling and world-knowledge modeling, have become much more efficient due to the availability of large, freely accessible natural language corpora (eg, CommonCrawl), as well as freely …","url":["http://www.freepatentsonline.com/y2018/0183930.html"]}
{"year":"2018","title":"Analyzing conversations to automatically identify product features that resonate with customers","authors":["R Raanani, R Levy, MY Breakstone, D Facher - US Patent App. 15/937,494, 2018"],"snippet":"… At the same time, natural language processing (NLP) approaches to both topic modeling and world-knowledge modeling, have become much more efficient due to the availability of large, freely accessible natural language corpora (eg, CommonCrawl), as well as freely …","url":["https://patents.google.com/patent/US20180218733A1/en"]}
{"year":"2018","title":"Analyzing Uncertainty in Neural Machine Translation","authors":["M Ott, M Auli, D Granger, MA Ranzato - arXiv preprint arXiv:1803.00047, 2018"],"snippet":"Page 1. Analyzing Uncertainty in Neural Machine Translation Myle Ott Michael Auli David Granger Marc'Aurelio Ranzato Facebook AI Research Abstract Machine translation is a popular test bed for research in neural sequence …","url":["https://arxiv.org/pdf/1803.00047"]}
{"year":"2018","title":"Answering Factoid Questions with Recurrent Neural Networks","authors":["M Kim, K Paeng"],"snippet":"… one. The embedding matrix is initialized with 300-dimensional GloVe embeddings pre-trained on the 840B Common Crawl corpus [9]. We use case-sensitive embeddings, which results in ~140k usable GloVe embeddings …","url":["http://cs.umd.edu/~miyyer/data/answering-factoid-questions.pdf"]}
{"year":"2018","title":"Approaching Nested Named Entity Recognition with Parallel LSTM-CRFs","authors":["Ł Borchmann, A Gretkowski, F Gralinski - Proceedings ofthePolEval2018Workshop, 2018"],"snippet":"… 2014) were trained on a very large, freely available7 Common Crawl-based Web corpus of Polish (Buck et al. 2014) … N-gram Counts and Language Models from the Common Crawl.[in:] Proceedings of the Language Resources …","url":["http://poleval.pl/files/poleval2018.pdf#page=63"]}
{"year":"2018","title":"Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task","authors":["M Junczys-Dowmunt, R Grundkiewicz, S Guha… - arXiv preprint arXiv …, 2018"],"snippet":"… Current state-of-the-art GEC systems based on SMT, however, all include large-scale in- domain language models either following the steps outlined in Junczys-Dowmunt and Grundkiewicz (2016) or directly re-using their …","url":["https://arxiv.org/pdf/1804.05940"]}
{"year":"2018","title":"Approaching the largest 'API': extracting information from the Internet with Python","authors":["JE Germann"],"snippet":"… For every patron their dataset. For every dataset its patron. Bibliography. Common Crawl: Get Started[Internet]. [accessed 2017 Dec. 1] Available from http://commoncrawl.org/the-data/getstarted/. Imperva Incapsula's Bot Traffic Report [Internet]. [accessed 2017 Dec …","url":["http://journal.code4lib.org/articles/13197"]}
{"year":"2018","title":"Arabic Sentences Classification via Deep Learning","authors":["D Sagheer, F Sukkar"],"snippet":"… The research [21] presents AraVec Arabic word2vec models for the Arabic language using three different dataset resources: Wikipedia, Twitter and Common Crawl webpages crawl data, the models are built in the same strategies in Mikilov word2vec, skip-gram and CBOW …","url":["https://www.researchgate.net/profile/Dania_Sagheer/publication/326429992_Arabic_Sentences_Classification_via_Deep_Learning/links/5b5b5ecfa6fdccf0b2fa820a/Arabic-Sentences-Classification-via-Deep-Learning.pdf"]}
{"year":"2018","title":"Are Automatic Metrics Robust and Reliable in Specific Machine Translation Tasks?","authors":["M Chinea-Rios, A Peris, F Casacuberta"],"snippet":"… 5 Experimental setup Our experimental framework related a domain adaptation task, in the English to Spanish language direction. In our setup, we trained a PB-SMT and a NMT system on the same data, from a general corpus extracted from websites (Common Crawl) …","url":["https://www.researchgate.net/profile/Alvaro_Peris/publication/325059351_Are_Automatic_Metrics_Robust_and_Reliable_in_Specific_Machine_Translation_Tasks/links/5af4002e4585157136c96459/Are-Automatic-Metrics-Robust-and-Reliable-in-Specific-Machine-Translation-Tasks.pdf"]}
{"year":"2018","title":"Are we experiencing the Golden Age of Automatic Post-Editing?","authors":["M Junczys-Dowmunt - Proceedings of the AMTA 2018 Workshop on …, 2018"],"snippet":"… EN-DE bilingual data from the WMT-16 shared tasks on IT and news translation. ▶ German monolingual Common Crawl (CC) corpus. Proceedings for AMTA 2018 Workshop: Translation Quality Estimation …","url":["http://www.aclweb.org/anthology/W18-2105"]}
{"year":"2018","title":"ArgumenText: Searching for Arguments in Heterogeneous Sources","authors":["C Stab, J Daxenberger, C Stahlhut, T Miller, B Schiller… - Proceedings of the 2018 …, 2018"],"snippet":"… 3.1 Data As our objective is to search for arguments in any text domain, we build upon the English part of CommonCrawl,1 the largest Web corpus available to date. Before further processing, we followed 1http://commoncrawl.org/ Habernal et al …","url":["http://www.aclweb.org/anthology/N18-5005"]}
{"year":"2018","title":"Arretium or Arezzo? A Neural Approach to the Identification of Place Names in Historical Texts","authors":["R Sprugnoli"],"snippet":"… summarised below: • dropout: 0.25, 0.25 • classifier: CRF • LSTM-Size: 100 • optimizer: NADAM • word embeddings: GloVe Common Crawl 840B • character … namely: (i) GloVe embeddings, trained on a corpus of 840 billion …","url":["http://ceur-ws.org/Vol-2253/paper26.pdf"]}
{"year":"2018","title":"Associative Multichannel Autoencoder for Multimodal Word Representation","authors":["S Wang, J Zhang, C Zong - Proceedings of the 2018 Conference on Empirical …, 2018"],"snippet":"… representations. 4 Experimental Setup 4.1 Datasets Textual vectors. We use 300-dimensional GloVe vectors1 which are trained on the Common Crawl corpus consisting of 840B tokens and a vocabulary of 2.2M words2. Visual vectors. Our …","url":["http://www.aclweb.org/anthology/D18-1011"]}
{"year":"2018","title":"AttnConvnet at SemEval-2018 Task 1: Attention-based Convolutional Neural Networks for Multi-label Emotion Classification","authors":["Y Kim, H Lee, K Jung - arXiv preprint arXiv:1804.00831, 2018"],"snippet":"… dataset. Among those well-known word embeddings such as Word2Vec( Mikolov et al., 2013), GloVe(Pennington et al., 2014), fastText(Piotr et al., 2016), we adopt 300-dimension GloVe vectors for English ,which is trained …","url":["https://arxiv.org/pdf/1804.00831"]}
{"year":"2018","title":"Automated Detection of Adverse Drug Reactions in the Biomedical Literature Using Convolutional Neural Networks and Biomedical Word Embeddings","authors":["DS Miranda - arXiv preprint arXiv:1804.09148, 2018"],"snippet":"… 5.1 Embeddings GloVe 840B As in Huynh's work (Huynh et al., 2016), we use pre-trained word embeddings. Huynh focused mainly on the general purpose GloVe Common Crawl 840B, 300 dimensional word embeddings (Pennington et al., 2014). Pyysalo's Embeddings …","url":["https://arxiv.org/pdf/1804.09148"]}
{"year":"2018","title":"Automated discovery of privacy violations on the web","authors":["ST Englehardt - 2018"],"snippet":"Page 1. Automated discovery of privacy violations on the web Steven Tyler Englehardt A Dissertation Presented to the Faculty of Princeton University in Candidacy for the Degree of Doctor of Philosophy Recommended for …","url":["https://senglehardt.com/papers/princeton_phd_dissertation_englehardt.pdf"]}
{"year":"2018","title":"Automatic generation of playlists from conversations","authors":["R Raanani, R Levy, MY Breadstone - US Patent App. 15/793,691, 2018"],"snippet":"… At the same time, natural language processing (NLP) approaches to both topic modeling and world-knowledge modeling, have become much more efficient due to the availability of large, freely accessible natural language corpora (eg, CommonCrawl), as well as freely …","url":["https://patents.google.com/patent/US20180046710A1/en"]}
{"year":"2018","title":"Automatic Neural Question Generation using Community-based Question Answering Systems","authors":["T Baghaee - 2017"],"snippet":"Page 1. AUTOMATIC NEURAL QUESTION GENERATION USING COMMUNITYBASED QUESTION ANSWERING SYSTEMS TINA BAGHAEE Bachelor of Science, Shahid Beheshti University, 2011 A Thesis Submitted to the …","url":["https://www.uleth.ca/dspace/bitstream/handle/10133/5004/Baghaee_Tina_MSC_2017.pdf?sequence=1&isAllowed=y"]}
{"year":"2018","title":"AUTOMATIC PATTERN RECOGNITION IN CONVERSATIONS","authors":["R Raanani, R Levy, D Facher, MY Breakstone - US Patent App. 15/817,490, 2018"],"snippet":"… language processing (NLP) approaches to both topic modeling and world-knowledge modeling, have become much more efficient due to the i Insidesales.com “Market size 2013” study availability of large, freely …","url":["http://www.freepatentsonline.com/y2018/0077286.html"]}
{"year":"2018","title":"Automatic Post-Editing of Machine Translation: A Neural Programmer-Interpreter Approach","authors":["TT Vu, G Haffari - Proceedings of the 2018 Conference on Empirical …, 2018"],"snippet":"… Interestingly, training MT+AG and MT+AG+LM models on 23K data lead to better TER/BLEU than those trained on 500K+12K. This implies the importance of in-domain training data, as the synthetic corpus is created …","url":["http://www.aclweb.org/anthology/D18-1341"]}
{"year":"2018","title":"Automatic Question Tagging with Deep Neural Networks","authors":["B Sun, Y Zhu, Y Xiao, R Xiao, YG Wei - IEEE Transactions on Learning Technologies, 2018"],"snippet":"… Word2vec [41] trained on a large corpus. Some pre-trained word vectors are available, such as GloVe Common Crawl vectors1 and word2vec vectors2, which is trained on Google News. The word vectors can be divided into …","url":["http://ieeexplore.ieee.org/abstract/document/8295250/"]}
{"year":"2018","title":"Automatically Categorizing Software Technologies","authors":["M Nassif, C Treude, M Robillard - IEEE Transactions on Software Engineering, 2018","S Khan, WH Butt - 2022 2nd International Conference on Digital Futures …, 2022"],"snippet":"… All these approaches work by mining large text corpora. Among the latest such techniques is the WebIsA Database [32] from the Web Data Commons project, which extracts hypernyms from CommonCrawl,1 a corpus of over …","url":["https://ieeexplore.ieee.org/abstract/document/8359344/","https://ieeexplore.ieee.org/abstract/document/9787457/"]}
{"year":"2018","title":"Based Speech Recognition with Gated ConvNets","authors":["V Liptchinsky, G Synnaeve, R Collobert - arXiv preprint arXiv:1712.09444, 2017"],"snippet":"… Extra Resources Panayotov et al. (2015) HMM+DNN+pNorm phone fMLLR phone lexicon Amodei et al. (2016) 2D-CNN+RNN letter none 11.9Kh train set, Common Crawl LM Peddinti et al. (2015b) HMM+CNN phone iVectors phone lexicon Povey et al …","url":["https://arxiv.org/pdf/1712.09444"]}
{"year":"2018","title":"Belittling the Source: Trustworthiness Indicators to Obfuscate Fake News on the Web","authors":["D Esteves, AJ Reddy, P Chawla, J Lehmann - arXiv preprint arXiv:1809.00494, 2018"],"snippet":"… Social Tags: returns the frequency of social tags in wb: R ⋃ i=1 ϕ(i, wb) 11. OpenSources: returns the open-source classification (x) for a given website: x = { 1, if w ∈ O 0, if w ∈ O 12. PageRankCC: PageRank information …","url":["https://arxiv.org/pdf/1809.00494"]}
{"year":"2018","title":"Bi-Directional Differentiable Input Reconstruction for Low-Resource Neural Machine Translation","authors":["X Niu, W Xu, M Carpuat - arXiv preprint arXiv:1811.01116, 2018"],"snippet":"… data for Swahili↔English (SW↔EN), Tagalog↔English (TL↔EN) and Somali↔English (SO↔EN) contains a mixture of domains such as news and weblogs and is collected from the IARPA MATERIAL program2, the Global …","url":["https://arxiv.org/pdf/1811.01116"]}
{"year":"2018","title":"Bi-Directional Neural Machine Translation with Synthetic Parallel Data","authors":["X Niu, M Denkowski, M Carpuat - arXiv preprint arXiv:1805.11213, 2018"],"snippet":"… Page 3. Type Dataset # Sentences High-resource: German↔English Training Common Crawl + Europarl v7 + News Comm. v12 … 2.2). We use the training data as in-domain and either Common Crawl or ICWSM as out-of-domain …","url":["https://arxiv.org/pdf/1805.11213"]}
{"year":"2018","title":"Big Data Integration for Product Specifications","authors":["L Barbosa, V Crescenzi, XL Dong, P Merialdo, F Piai… - Data Engineering, 2018"],"snippet":"… Only 20% of our sources contained fewer pages than the same sources in Common Crawl, and a very small fraction of the pages in these sources were product pages: on a sample set of 12 websites where Common Crawl …","url":["http://sites.computer.org/debull/A18june/A18JUN-CD.pdf#page=73"]}
{"year":"2018","title":"Big Open Research Data","authors":["A PRIMPELI"],"snippet":"… Outline ▪ Requirements of a data scientist ▪ Open access datasets ▪ DBpedia ▪ Common Crawl ▪ The Web Data Commons project ▪ Why should we share data? ▪ How should we share data? 26/10/2017 … 5 Page 6. Common Crawl …","url":["https://pdfs.semanticscholar.org/presentation/a576/94b9a19b82cc219ae01004e5c30cbeea916e.pdf"]}
{"year":"2018","title":"Bigrams and BiLSTMs Two neural networks for sequential metaphor detection","authors":["Y Bizzoni, M Ghanimifard - NAACL HLT 2018, 2018"],"snippet":"… GloVe embeddings on British National Corpus (Consortium et al., 2007) from which the VUAMC corpus was sampled, and compared it with both pre-trained Word2Vec em- beddings on Google News corpus and standard …","url":["http://www.cl.cam.ac.uk/~es407/papers/Fig-Lang2018-proceedings.pdf#page=103"]}
{"year":"2018","title":"Bilingual Character Representation for Efficiently Addressing Out-of-Vocabulary Words in Code-Switching Named Entity Recognition","authors":["GI Winata, CS Wu, A Madotto, P Fung - arXiv preprint arXiv:1805.12061, 2018"],"snippet":"… We use 300-dimensional English (Mikolov et al., 2018) and Spanish (Grave et al., 2018) FastText pre-trained word vectors which comprise two million words vocabulary each and they are trained using Common Crawl and Wikipedia …","url":["https://arxiv.org/pdf/1805.12061"]}
{"year":"2018","title":"Biomedical Domain-Oriented Word Embeddings via Small Background Texts for Biomedical Text Mining Tasks","authors":["L Li, J Wan, D Huang - National CCF Conference on Natural Language …, 2017"],"snippet":"… For example, Pennington et al. used Wikipedia, Giga word 5 and Common Crawl to learn word embeddings, each of which contained billions of tokens [15]. However, there is not always a monotonic increase in performance as the amount of background texts increase …","url":["https://link.springer.com/chapter/10.1007/978-3-319-73618-1_46"]}
{"year":"2018","title":"BlogSet-BR: A Brazilian Portuguese Blog Corpus","authors":["H Santos, V Woloszyn, R Vieira - … of the Eleventh International Conference on …, 2018"],"snippet":"… For instance, the Common Crawl project maintains an open repository of web crawl data that can be accessed and analyzed by any research group2. This corpus has been used to build language models (Roziewski …","url":["http://www.aclweb.org/anthology/L18-1105"]}
{"year":"2018","title":"BomJi at SemEval-2018 Task 10: Combining Vector-, Pattern-and Graph-based Information to Identify Discriminative Attributes","authors":["E Santus, C Biemann, E Chersoni"],"snippet":"… 2The pre-trained vectors are available, respectively, at https://code.google.com/archive/ p/ word2vec/ (Google News, 300 dimensions) and at https://nlp.stanford.edu/projects/ glove/ (Common Crawl, 840B tokens, 300 dimensions) …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/publications/2018-santusetal-semeval-bomji.pdf"]}
{"year":"2018","title":"Bootstrapping Multilingual Intent Models via Machine Translation for Dialog Automation","authors":["N Ruiz, S Bangalore, J Chen - arXiv preprint arXiv:1805.04453, 2018"],"snippet":"… The NMT models were trained with parallel English-Spanish data from Europarl v7, CommonCrawl, and WMT News Commentary v8 from the WMT 2013 evaluation campaign (Bojar et al., 2013), as well as the TED talks from IWSLT 2014 (Cettolo et al., 2014) …","url":["https://arxiv.org/pdf/1805.04453"]}
{"year":"2018","title":"Bringing Order to Neural Word Embeddings with Embeddings Augmented by Random Permutations (EARP)","authors":["A Sharp","T Cohen, D Widdows - Proceedings of the 22nd Conference on Computational …, 2018"],"snippet":"Page 1. Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 465–475 Brussels, Belgium, October 31 - November 1, 2018. c 2018 Association for Computational Linguistics 465 …","url":["http://www.aclweb.org/anthology/K18-1045","https://zdoc.pub/bringing-order-to-neural-word-embeddings-with-embeddings-aug.html"]}
{"year":"2018","title":"Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis","authors":["A Moore, P Rayson - arXiv preprint arXiv:1806.05219, 2018"],"snippet":"… We found the best word vectors from SSWE and the common crawl 42B 300 dimension Glove vectors by five fold stratified cross validation for the NP methods and the highest accuracy on the validation set for the LSTM methods …","url":["https://arxiv.org/pdf/1806.05219"]}
{"year":"2018","title":"C) uestion Answering System with Deep Learning","authors":["JSRMI Schoenhals"],"snippet":"… The dataset contains more than 100k question-answer pairs on more than 500 articles, and it is split into 90k/10k train/dev question-context tuples with hidden test set. 3.2 Word Embeddings We use pre-trained GLoVe embeddings from the 840B Common Crawl corpus …","url":["http://web.stanford.edu/class/cs224n/reports/6908933.pdf"]}
{"year":"2018","title":"CAESAR: Context Awareness Enabled Summary-Attentive Reader","authors":["LH Chen, K Tripathi - arXiv preprint arXiv:1803.01335, 2018"],"snippet":"… Aftering experimenting with hyperparameters and various preprocessing settings, we settle on the following experiment details which gave the optimal result. We apply pretrained GloVe word embeddings trained on common","url":["https://arxiv.org/pdf/1803.01335"]}
{"year":"2018","title":"Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering","authors":["T Mihaylov, P Clark, T Khot, A Sabharwal"],"snippet":"… 11For all experiments we use d = 300 GloVe (Pennington et al., 2014) embeddings pre-trained on 840B tokens from Common Crawl (https://nlp.stanford.edu/projects/glove/). Page 7. Question Science Fact Common Knowledge (Type) Reasoning Challenge …","url":["http://ai2-website.s3.amazonaws.com/publications/Mihaylov-OpenBookQA-emnlp2018.pdf"]}
{"year":"2018","title":"Can Common Crawl reliably track persistent identifier (PID) use over time?","authors":["HS Thompson, J Tong - arXiv preprint arXiv:1802.01424, 2018"],"snippet":"Abstract: We report here on the results of two studies using two and four monthly web crawls respectively from the Common Crawl (CC) initiative between 2014 and 2017, whose initial goal was to provide empirical evidence for the changing patterns of use of so-called","url":["https://arxiv.org/pdf/1802.01424"]}
{"year":"2018","title":"Can Network Embedding of Distributional Thesaurus be Combined with Word Vectors for Better Representation?","authors":["A Jana, P Goyal - arXiv preprint arXiv:1802.06196, 2018"],"snippet":"… representation. For that purpose, we directly use very wellknown GloVe 1.2 embeddings (Pennington et al., 2014) trained on 840 billion words of the common crawl dataset having vector dimension of 300. As an instance …","url":["https://arxiv.org/pdf/1802.06196"]}
{"year":"2018","title":"Card-660: A Reliable Evaluation Framework for Rare Word Representation Models","authors":["MT Pilehvar, D Kartsaklis, V Prokhorov, N Collier - Proceedings of the 2018 …, 2018"],"snippet":"… Glove Wikipedia-Gigaword (300d) 400K 7% 55% 12% 74% 34.9 15.1 34.4 15.7 Glove Common Crawl - uncased (300d) 1.9M 1% 36% 1% 50% 36.5 29.2 37.7 27.6 Glove Common Crawl - cased (300d) 2.2M 1% 29% 2% 44% 44.0 33.0 45.1 27.3 …","url":["http://www.aclweb.org/anthology/D18-1169"]}
{"year":"2018","title":"Card-660: Cambridge Rare Word Dataset-a Reliable Benchmark for Infrequent Word Representation Models","authors":["MT Pilehvar, D Kartsaklis, V Prokhorov, N Collier - arXiv preprint arXiv:1808.09308, 2018"],"snippet":"… Glove Wikipedia-Gigaword (300d) 400K 7% 55% 12% 74% 34.9 15.1 34.4 15.7 Glove Common Crawl - uncased (300d) 1.9M 1% 36% 1% 50% 36.5 29.2 37.7 27.6 Glove Common Crawl - cased (300d) 2.2M 1% 29% 2% 44% 44.0 33.0 45.1 27.3 …","url":["https://arxiv.org/pdf/1808.09308"]}
{"year":"2018","title":"Categorization of Comparative Sentences for Argument Mining","authors":["M Franzek, A Panchenko, C Biemann - arXiv preprint arXiv:1809.06152, 2018"],"snippet":"… The re- maining objects were combined to pairs. For each object type as given by the Wikipedia List page or the seed word, all possible combinations were created. We drew sentences from the publicly available CommonCrawl3 index of Panchenko et al …","url":["https://arxiv.org/pdf/1809.06152"]}
{"year":"2018","title":"CERES: Distantly Supervised Relation Extraction from the Semi-Structured Web","authors":["C Lockard, XL Dong, A Einolghozati, P Shiralkar - arXiv preprint arXiv:1804.04635, 2018"],"snippet":"Page 1. CERES: Distantly Supervised Relation Extraction from the Semi-Structured Web Colin Lockard ∗ University of Washington lockardc@cs.washington.edu Xin Luna Dong Amazon lunadong@amazon.com Arash Einolghozati ∗ Facebook arashe@fb.com …","url":["https://arxiv.org/pdf/1804.04635"]}
{"year":"2018","title":"Character-based Neural Networks for Sentence Pair Modeling","authors":["W Lan, W Xu - arXiv preprint arXiv:1805.08297, 2018"],"snippet":"… word vectors (Pennington et al., 2014), trained on 27 billion words from Twitter (vocabulary size of 1.2 milion words) for social media datasets, and 300dimensional GloVe vectors, trained on 840 billion words (vocabulary …","url":["https://arxiv.org/pdf/1805.08297"]}
{"year":"2018","title":"Character-Level Feature Extraction with Densely Connected Networks","authors":["C Lee, YB Kim, D Lee, HS Lim - arXiv preprint arXiv:1806.09089, 2018"],"snippet":"… Our model uses the GloVe (Pennington et al., 2014) 300-dimensional vectors trained on the Common Crawl corpus with 42B tokens as word level features, as this resulted in the best performance in preliminary experiments …","url":["https://arxiv.org/pdf/1806.09089"]}
{"year":"2018","title":"Characterising dataset search—An analysis of search logs and data requests","authors":["E Kacprzak, L Koesten, LD Ibáñez, T Blount… - Journal of Web Semantics, 2018"],"snippet":"… In 2015 the Web Data Commons project extracted 233 million data tables from the Common Crawl [3]. The ability to generate business value from data analytics offers competitive advantage in virtually every industry worldwide …","url":["https://www.sciencedirect.com/science/article/pii/S1570826818300556"]}
{"year":"2018","title":"CIKM AnalytiCup 2017 Lazada Product Title Quality Challenge An Ensemble of Deep and Shallow Learning to predict the Quality of Product Titles","authors":["K Singh, V Sunder - arXiv preprint arXiv:1804.01000, 2018"],"snippet":"… calculate semantic similarity based features. For each pair of words in a title, we calculate the cosine distance between vectors of words generated from Common crawl google glove [Pennington et al. 2014]. Further, we normalize …","url":["https://arxiv.org/pdf/1804.01000"]}
{"year":"2018","title":"ClaiRE at SemEval-2018 Task 7-Extended Version","authors":["L Hettinger, A Dallmann, A Zehe, T Niebler, A Hotho"],"url":["https://www.arxiv-vanity.com/papers/1804.05825/"]}
{"year":"2018","title":"ClaiRE at SemEval-2018 Task 7: Classification of Relations using Embeddings","authors":["L Hettinger, A Dallmann, A Zehe, T Niebler, A Hotho - Proceedings of The 12th …, 2018"],"snippet":"… As a baseline, we employ a publicly available set of 300-dimensional word embeddings trained with GloVe (Pennington et al., 2014) on the Common Crawl data4 (CC) … 4http://commoncrawl.org/ 5https://arxiv.org 6https://arxiv.org/help/bulk_data …","url":["http://www.aclweb.org/anthology/S18-1134"]}
{"year":"2018","title":"Classifying Occupations According to Their Skill Requirements in Job Advertisements","authors":["J Djumalieva, A Lima, C Sleeman - 2018"],"snippet":"… There are publicly available pre-trained word embeddings models. We use a GloVe model, which contains a vocabulary of 2.2 million words and was trained using word to word co-occurrences in a Common Crawl corpus (Pennington, Socher, and Manning, 2014) …","url":["https://www.escoe.ac.uk/wp-content/uploads/2018/03/ESCoE-DP-2018-04.pdf"]}
{"year":"2018","title":"Cloud repository as a malicious service: challenge, identification and implication","authors":["X Liao, S Alrwais, K Yuan, L Xing, XF Wang, S Hao… - Cybersecurity, 2018"],"snippet":"… Running the scanner over all the data collected by the Common Crawl (Crawl 2015), which indexed five billion web pages, for those associated with all major cloud storage providers (including Amazon S3, Cloudfront, Google …","url":["https://link.springer.com/article/10.1186/s42400-018-0015-6"]}
{"year":"2018","title":"Clustering-Oriented Representation Learning with Attractive-Repulsive Loss","authors":["K Kenyon-Dean, A Cianflone, L Page-Caccia… - arXiv preprint arXiv …, 2018"],"snippet":"… Each sample contains the news article's title concatenated with its description. As input to our models, we use the 300-dimensional 840B Common Crawl pretrained Glove word embeddings (Pennington, Socher, and Manning 2014) provided by Stanford6 …","url":["https://arxiv.org/pdf/1812.07627"]}
{"year":"2018","title":"Code-Switched Named Entity Recognition with Embedding Attention","authors":["C Wang, K Cho, D Kiela - Proceedings of the Third Workshop on Computational …, 2018"],"snippet":"… We use high-quality FastText embeddings trained on Common Crawl (Grave et al., 2018; Mikolov et al., 2018) and employ shortcut-stacked sentence encoders (Nie and Bansal, 2017) to obtain deep token-level representations to feed into the CRF …","url":["http://www.aclweb.org/anthology/W18-3221"]}
{"year":"2018","title":"Collaborative Context-Aware Visual Question Answering","authors":["AS Toor - 2018"],"snippet":"Page 1. COLLABORATIVE CONTEXT-AWARE VISUAL QUESTION ANSWERING by Andeep Singh Toor A Dissertation Submitted to the Graduate Faculty of George Mason University In Partial fulfillment of The Requirements …","url":["http://search.proquest.com/openview/d4ca94e1edcc41c3401b171ab495c21c/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2018","title":"Combination of Statistical and Neural Machine Translation for Myanmar–English","authors":["B Marie, A Fujita, E Sumita"],"snippet":"… For English, we used the monolingual corpora provided by the WMT18 shared News Translation Task. As for Myanmar, we experimented with two monolingual corpora: Myanmar Wikipedia and Myanmar CommonCrawl … The CommonCrawl cor …","url":["http://benjaminmarie.com/pdf/WAT_2018_paper_7.pdf"]}
{"year":"2018","title":"Combining Convolution and Recursive Neural Networks for Sentiment Analysis","authors":["VD Van, T Thai, MQ Nghiem - Proceedings of the Eighth International Symposium on …, 2017"],"snippet":"… movie reviews. These words might not appear or not have good vector representations in the pre-trained Glove Common Crawl. 153 Page 4. SoICT '17, December 7–8, 2017, Nha Trang City, Viet Nam V. Van et al. Additionally …","url":["https://dl.acm.org/citation.cfm?id=3155158"]}
{"year":"2018","title":"Combining Shallow and Deep Learning for Aggressive Text Detection","authors":["V Golem, M Karan, J Šnajder - Proceedings of the First Workshop on Trolling …, 2018"],"snippet":"… Inputs to both models were GloVe (Pennington et al., 2014) 300-dimensional word embeddings trained on 840 billion tokens from the Common Crawl or 200-dimensional word embeddings trained on 20 billion tweets.5 Since …","url":["http://www.aclweb.org/anthology/W18-4422"]}
{"year":"2018","title":"Compact inverted index storage using general‐purpose compression libraries","authors":["M Petri, A Moffat - Software: Practice and Experience"],"snippet":"… We also develop (and make freely available) a new IR test collection based on the News sub-collection of the Common Crawl.6 The News sub-collection provides daily crawls of news websites in many languages. We refer to this collection as CC-NEWS-URL …","url":["http://onlinelibrary.wiley.com/doi/10.1002/spe.2556/full"]}
{"year":"2018","title":"Comparative Argument Mining","authors":["M Franzek"],"snippet":"… First, does a sentence compare two known objects? And secondly, if it does, is the first-mentioned object better or worse than the second-mentioned, according to the sentence context? The data set was created with data from …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2018-ma-mirco-franzek.pdf"]}
{"year":"2018","title":"Comparing Distributional and Frame Semantic Properties of Words","authors":["T Kleinbauer, TA Trost"],"snippet":"… G50 50 96.3% 6G Wikipedia & G100 100 96.3% Gigaword 5 G200 200 96.3% G300 300 96.3% G42 300 98.9% 42G Common Crawl G840 300 99.3% 840G Common Crawl GT25 25 90.2% 27G Twitter GT50 50 90.2% GT100 100 90.2% GT200 200 90.2 …","url":["https://www.oeaw.ac.at/fileadmin/subsites/academiaecorpora/PDF/konvens18_08.pdf"]}
{"year":"2018","title":"Comparing Open Source Search Engine Functionality, Efficiency and Effectiveness with Respect to Digital Forensic Search","authors":["J Hansen, K Porter, A Shalaginov, K Franke - NISK 2018, 2018"],"snippet":"… Malware ISCXAB Malware DAROA98/99 Network DARPA2000 Network MAWILab Network KDD Cup99 Network UNSW-NB15 Network NSA-CDX Network ADFA Network Name Catagory Kyoto data Network crawdad Network …","url":["http://nikt2018.ifi.uio.no/images/NISK2018_preproceedings.pdf#page=117"]}
{"year":"2018","title":"Comparing Theories of Speaker Choice Using a Model of Classifier Production in Mandarin Chinese","authors":["M Zhan, R Levy - Proceedings of the 2018 Conference of the North …, 2018"],"snippet":"Page 1. Proceedings of NAACL-HLT 2018, pages 1997–2005 New Orleans, Louisiana, June 1 - 6, 2018. c 2018 Association for Computational Linguistics Comparing Theories of Speaker Choice Using a Model of Classifier Production in Mandarin Chinese …","url":["http://www.aclweb.org/anthology/N18-1181"]}
{"year":"2018","title":"Comparison of Paragram and Glove Results for Similarity Benchmarks","authors":["PM Skłodowskiej-Curie - Multimedia and Network Information Systems, 2019"],"snippet":"… The TOEFL test set was introduced in [13]; the ESL test set was introduced in [28]. 4.1 Experimental Setup We use the unmodified vector space model trained on 840 billion words from Common Crawl data with the GloVe algorithm introduced in [21] …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=cf5sDwAAQBAJ&oi=fnd&pg=PA236&dq=commoncrawl&ots=Zl8zpSyQVF&sig=p1R4RPXAKZFFv4LxOE09nc5dejg"]}
{"year":"2018","title":"Comparison of Word Embeddings and Sentence Encodings as Generalized Representations for Crisis Tweet Classification Tasks","authors":["H Li, X Li, D Caragea, C Caragea"],"snippet":"… the other parameters. • SIF: denotes the SIF approach, which is considered to be a baseline for sentence embeddings. The original paper used GloVe embeddings pre-trained on the Common Crawl data. However, we used …","url":["https://www.cs.uic.edu/~cornelia/papers/iscram_asian18.pdf"]}
{"year":"2018","title":"Compositional Source Word Representations for Neural Machine Translation","authors":["D Ataman, MA Di Gangi, M Federico - 2018"],"snippet":"… For training the Czech–English and German– English NMT models, we use the available data sets from the WMT2 (Bojar et al., 2017) shared task on machine translation of news, which consist of Europarl (Koehn, 2005), Commoncrawl …","url":["http://rua.ua.es/dspace/bitstream/10045/76018/1/EAMT2018-Proceedings_05.pdf"]}
{"year":"2018","title":"Concatenated $ p $-mean Word Embeddings as Universal Cross-Lingual Sentence Representations","authors":["A Rücklé, S Eger, M Peyrard, I Gurevych - arXiv preprint arXiv:1803.01400, 2018"],"snippet":"… Word embeddings We use four diverse, po- tentially complementary types of word embeddings as basis for our sentence representation techniques: GloVe embeddings (GV) (Pennington et al., 2014) trained on Common Crawl;","url":["https://arxiv.org/pdf/1803.01400"]}
{"year":"2018","title":"Content Extraction and Lexical Analysis from Customer-Agent Interactions","authors":["S Nisioi, A Bucur, LP Dinu - Proceedings of the 2018 EMNLP Workshop W-NUT …, 2018"],"snippet":"… Somewhat surprising is the fact that a big majority of words from the Common Crawl vocabulary (approx … Furthermore, both the lexicons used in questions and answers present little overlap with Common Crawl, and in accordance …","url":["http://www.aclweb.org/anthology/W18-6118"]}
{"year":"2018","title":"Context and Copying in Neural Machine Translation","authors":["R Knowles, P Koehn - Proceedings of the 2018 Conference on Empirical …, 2018"],"snippet":"… are trained using the Marian toolkit (Junczys-Dowmunt et al., 2018).2 We use the WMT parallel text3 (Europarl, News Commentary, and CommonCrawl) along with … We consider both the full training data (including backtranslations …","url":["http://www.aclweb.org/anthology/D18-1339"]}
{"year":"2018","title":"Context and Embeddings in Language Modelling–an Exploration","authors":["M Nitsche, M Tropmann-Frick - 2018"],"snippet":"… alone. GloVe typically performs better than Word2Vec skip-gram, especially when the vocabulary is large. GloVe is also available on different corpora such as Twitter, Common Crawl or Wikipedia. 2.2 Bag of Tricks - fastText …","url":["http://ceur-ws.org/Vol-2277/paper24.pdf"]}
{"year":"2018","title":"Context is Everything: Finding Meaning Statistically in Semantic Spaces","authors":["E Zelikman - arXiv preprint arXiv:1803.08493, 2018"],"snippet":"… For example ”dog” and ”but” are both commonly used words, and in many spaces will have a smaller M-distance than ”Dachshund” and ”dog.” One solution to this issue is presented later in this paper 5300 dimensional word …","url":["https://arxiv.org/pdf/1803.08493"]}
{"year":"2018","title":"Contextual-CNN: A Novel Architecture Capturing Unified Meaning for Sentence Classification","authors":["J Shin, Y Kim, S Yoon, K Jung"],"snippet":"… For word representation, we use the publicly available 300-dimensional GloVe trained on the Common Crawl with 42B tokens [5], hence our embedding dimension d = 300. Word embedding is normalized to unit norm and is fixed in the experiments without fine-tuning …","url":["http://milab.snu.ac.kr/pub/BigComp2018.pdf"]}
{"year":"2018","title":"Convexification of Neural Graph","authors":["H Xiao - arXiv preprint arXiv:1801.02901, 2018"],"snippet":"… Specifically, we initialize the word embedding with 300-dimensional GloVe [Pennington et al., 2014] word vectors pre-trained in the 840B Common Crawl corpus [Pennington et al., 2014] and then set the hidden dimension as 100 for each LSTM …","url":["https://arxiv.org/pdf/1801.02901"]}
{"year":"2018","title":"Convolutional Spatial Attention Model for Reading Comprehension with Multiple-Choice Questions","authors":["Z Chen, Y Cui, W Ma, S Wang, G Hu - arXiv preprint arXiv:1811.08610, 2018"],"snippet":"… among two datasets. The word embeddings are initialized by the pre-trained GloVe word vectors (Common Crawl, 6B tokens, 100-dimension) (Pennington, Socher, and Manning 2014), and keep fixed during training. The words …","url":["https://arxiv.org/pdf/1811.08610"]}
{"year":"2018","title":"COVER: a linguistic resource combining common sense and lexicographic information","authors":["E Mensa, DP Radicioni, A Lieto - Language Resources and Evaluation"],"snippet":"… (2014)). In count based models, model vectors are learned by applying dimensionality reduction techniques to the co-occurrence counts matrix; in particular, GloVe embeddings have been acquired through a training …","url":["https://link.springer.com/article/10.1007/s10579-018-9417-z"]}
{"year":"2018","title":"Creation and optimization of resource contents","authors":["M Tober, D Neumann - US Patent App. 15/284,739, 2018"],"snippet":"… 310. The crawler module 310 may automatically crawl a network and acquire contents from one or more resources in the network, acquire the contents from an open repository of web crawl data such as CommonCrawl.org …","url":["https://patents.google.com/patent/US20180096067A1/en"]}
{"year":"2018","title":"Cross-lingual Decompositional Semantic Parsing Supplemental Material","authors":["S Zhang, X Ma, R Rudinger, K Duh, B Van Durme"],"snippet":"… Hidden states are zero initialized. All other parameters are sampled from U(−0.1,0.1). Decoder: Word embeddings are initialized by open-source GloVe vectors (Pennington et al., 2014) trained on Common Crawl 840B with 300 dimensions …","url":["http://anthology.aclweb.org/attachments/D/D18/D18-1194.Attachment.pdf"]}
{"year":"2018","title":"Cross-Lingual Propagation for Deep Sentiment Analysis","authors":["X Dong, G de Melo - Proceedings of the 32nd AAAI Conference on Artificial …, 2018"],"snippet":"… each mini-batch. 3.2 Embeddings The standard pre-trained word vectors used for English are the GloVe (Pennington, Socher, and Manning 2014) ones trained on 840 billion tokens of Common Crawl data2, 1https://www.irit …","url":["http://iiis.tsinghua.edu.cn/~weblt/papers/sentiment-propagation.pdf"]}
{"year":"2018","title":"Cross-Pair Text Representations for Answer Sentence Selection","authors":["K Tymoshenko, A Moschitti"],"snippet":"Page 1. Cross-Pair Text Representations for Answer Sentence Selection Kateryna Tymoshenko DISI, University of Trento (Adeptmind scholar) 38123 Povo (TN), Italy kateryna.tymoshenko@unitn.it Alessandro Moschitti∗ Amazon …","url":["https://m.media-amazon.com/images/G/01/amazon.jobs/Cross-PairTextRepresentationsforAnswerSentenceSelection._CB1538442919_.pdf"]}
{"year":"2018","title":"CrumbTrail: an Efficient Methodology to Reduce Multiple Inheritance in Knowledge Graphs","authors":["S Faralli, I Finocchi, SP Ponzetto, P Velardi - Knowledge-Based Systems, 2018"],"snippet":"… 5]. Finally, the availability of vast amounts of knowledge encoded in heterogeneous sources, including both structured – eg, Freebase [6], DBpedia [7] – semi-structured – eg, Wikipedia – as well as unstructured resources …","url":["https://www.sciencedirect.com/science/article/pii/S095070511830162X"]}
{"year":"2018","title":"CS224N Final SQuAD Improvements","authors":["RAV Misra - Context"],"snippet":"… U\"]. Page 5. 4 Experiments 4.1 Implementation and Metrics The model is trained and evaluated on the SQuAD dataset, and uses pretrained GloVe word vectors on the Common Crawl corpus (Pennington et al., 2014). Running …","url":["http://web.stanford.edu/class/cs224n/reports/6907927.pdf"]}
{"year":"2018","title":"CUNI team: CLEF eHealth Consumer Health Search Task 2018","authors":["S Saleh, P Pecina - CEUR Workshop Proceedings: Working Notes of CLEF, 2018"],"snippet":"… Document collection in the CLEF 2018 consumer health search task is created using CommonCrawl platform 1. First, the query set (described in Section 2.2) is submitted to Microsoft Bing APIs, and a list of domains is extracted from the top retrieved results …","url":["http://ceur-ws.org/Vol-2125/paper_201.pdf"]}
{"year":"2018","title":"CUNI Transformer Neural MT System for WMT18","authors":["M Popel - Proceedings of the Third Conference on Machine …, 2018"],"snippet":"… CzEng 1.7 57 065 618 424 543 184 Europarl v7 647 15 625 13 000 News Commentary v12 211 4 544 4 057 CommonCrawl 162 3 349 2 927 EN NewsCrawl 2016–17 47 483 934 981 CS NewsCrawl 2007–17 65 383 927 348 total 170 951 1 576 923 1 490 516 …","url":["http://www.aclweb.org/anthology/W18-6424"]}
{"year":"2018","title":"Cuttlefish: A Lightweight Primitive for Adaptive Query Processing","authors":["T Kaftan, M Balazinska, A Cheung, J Gehrke - arXiv preprint arXiv:1802.09180, 2018"],"snippet":"Page 1. Cuttlefish: A Lightweight Primitive for Adaptive Query Processing Tomer Kaftan University of Washington Magdalena Balazinska University of Washington Alvin Cheung University of Washington Johannes Gehrke Microsoft …","url":["https://arxiv.org/pdf/1802.09180"]}
{"year":"2018","title":"CytonMT: an Efficient Neural Machine Translation Open-source Toolkit Implemented in C++","authors":["X Wang, M Utiyama, E Sumita - arXiv preprint arXiv:1802.07170, 2018"],"snippet":"… Length penalty was applied through the formula in (Wu et al., 2016). Data Set # Sent. # Words Source Target CommonCrawl 2,399,123 54,572,703 58,869,785 Europarl v7 1,959,829 51,7061,34 54,327,972 News Comment. 223,153 5,689,117 5,660,789 Dev …","url":["https://arxiv.org/pdf/1802.07170"]}
{"year":"2018","title":"Dank Learning: Generating Memes Using Deep Neural Networks","authors":["V Peirson, L Abel, EM Tolunay - arXiv preprint arXiv:1806.04510, 2018"],"snippet":"… In this context, pretrained GloVe [3] word embeddings constitute the most suitable choice of word representation for our project, based on the fact that they have been trained on very large corpora, offering an option of words …","url":["https://arxiv.org/pdf/1806.04510"]}
{"year":"2018","title":"Data Augmentation and Deep Learning for Hate Speech Detection","authors":["K Hemker - 2018"],"snippet":"Page 1. IMPERIAL COLLEGE LONDON DEPARTMENT OF COMPUTING Data Augmentation and Deep Learning for Hate Speech Detection Author: Konstantin Hemker Supervisor: Dr. Björn Schuller Submitted in partial fulfillment …","url":["https://www.imperial.ac.uk/media/imperial-college/faculty-of-engineering/computing/public/1718-pg-projects/HemkerK-Data-Augmentation-and-Deep-Learning.pdf"]}
{"year":"2018","title":"Data Science with Vadalog: Bridging Machine Learning and Reasoning Data Science with Vadalog: Bridging Machine Learning and Reasoning","authors":["L Bellomarini, RR Fayzrakhmanov, G Gottlob…"],"snippet":"… including streams of structured or unstructured data from internal systems (eg, Enterprise Resource Planning, Workflow Management, and Supply Chain Management), external streams of unstructured data (eg, news and …","url":["https://www.groundai.com/project/data-science-with-vadalog-bridging-machine-learning-and-reasoning/"]}
{"year":"2018","title":"Data Science with Vadalog: Bridging Machine Learning and Reasoning","authors":["L Bellomarini, RR Fayzrakhmanov, G Gottlob… - arXiv preprint arXiv …, 2018"],"snippet":"… Workflow Management, and Supply Chain Management), external streams of unstructured data (eg, news and social media feeds, and Common Crawl1), publicly … 1 http://commoncrawl.org/ 2 http://www.cyc.com/researchcyc …","url":["https://arxiv.org/pdf/1807.08712"]}
{"year":"2018","title":"Data-Driven Language Understanding for Spoken Dialogue Systems","authors":["N Mrkšić - 2018"],"snippet":"Page 1. Data-Driven Language Understanding for Spoken Dialogue Systems Nikola Mrkšic Supervisor: Professor Steve Young Department of Engineering University of Cambridge This dissertation is submitted for the degree of Doctor …","url":["https://www.repository.cam.ac.uk/bitstream/handle/1810/276689/Mrksic-2018-PhD.pdf?sequence=1&isAllowed=y"]}
{"year":"2018","title":"Debunking Fake News One Feature at a Time","authors":["M Tosik, A Mallia, K Gangopadhyay - arXiv preprint arXiv:1808.02831, 2018"],"snippet":"… cosine tdidf Cosine similarity between headline/body TF-IDF vectors Real doc similarity Cosine similarity between averaged headline/body Common Crawl vectors Real doc similarity intro Cosine similarity between averaged …","url":["https://arxiv.org/pdf/1808.02831"]}
{"year":"2018","title":"Deep Extrofitting: Specialization and Generalization of Expansional Retrofitting Word Vectors using Semantic Lexicons","authors":["H Jo - arXiv preprint arXiv:1808.07337, 2018"],"snippet":"… The algorithm learns word vectors by making the dot products of word vectors equal to the logarithm of the words' probability of co-occurrence. We use glove.42B.300d trained on Common Crawl data, which has 1,917,493 unique words as 300-dimensional vectors …","url":["https://arxiv.org/pdf/1808.07337"]}
{"year":"2018","title":"Deep neural network architecture for sentiment analysis and emotion identification of Twitter messages","authors":["D Stojanovski, G Strezoski, G Madjarov, I Dimitrovski… - Multimedia Tools and …, 2018"],"snippet":"… SSWE 50 10M 137K Tweets word2vec 300 100B 3M GoogleNews GloVe Crawl 300 840B 2.2M Common Crawl GloVe Twitter 200 20B 1.2M Tweets Corpora size is expressed in token count with the exception of SSWE where only the number of tweets is provided …","url":["https://link.springer.com/article/10.1007/s11042-018-6168-1"]}
{"year":"2018","title":"Deep neural networks and distant supervision for geographic location mention extraction","authors":["A Magge, D Weissenbacher, A Sarker, M Scotch… - Bioinformatics, 2018"],"snippet":"AbstractMotivation. Virus phylogeographers rely on DNA sequences of viruses and the locations of the infected hosts found in public sequence databases like Gen.","url":["https://academic.oup.com/bioinformatics/article/34/13/i565/5045808"]}
{"year":"2018","title":"Deep Neural Networks for Query Expansion using Word Embeddings","authors":["A Imani, A Vakili, A Montazer, A Shakery - arXiv preprint arXiv:1811.03514, 2018"],"snippet":"… and newswire articles. This would make them more suitable for use in newswire collections. Using word embeddings pre-trained using common crawl data may yield better performance in web corpora. To summarize, the proposed …","url":["https://arxiv.org/pdf/1811.03514"]}
{"year":"2018","title":"Deep Semantic Learning for Conversational Agents","authors":["M Morisio, M Mensio - 2018"],"snippet":"Page 1. POLITECNICO DI TORINO Master of Science in Computer Engineering Master's Thesis Deep Semantic Learning for Conversational Agents Supervisor Prof. Maurizio Morisio Candidate Martino Mensio Tutor …","url":["https://www.researchgate.net/profile/Martino_Mensio/publication/324877915_Deep_Semantic_Learning_for_Conversational_Agents/links/5aeb205f45851588dd82cbc6/Deep-Semantic-Learning-for-Conversational-Agents.pdf"]}
{"year":"2018","title":"Deep-BGT at PARSEME Shared Task 2018: Bidirectional LSTM-CRF Model for Verbal Multiword Expression Identification","authors":["G Berk, B Erden, T Güngör - Proceedings of the Joint Workshop on Linguistic …, 2018"],"snippet":"… We chose the DEPREL tag as a feature in order to capture dependencies at sentence level. We use pre-trained word embeddings released by fastText (Grave et al., 2018), which were trained on Common Crawl and Wikipedia …","url":["http://www.aclweb.org/anthology/W18-4927"]}
{"year":"2018","title":"DeepPhish: Simulating Malicious AI","authors":["AC Bahnsen, I Torroledo, LD Camacho, S Villegas"],"snippet":"… Forest (RF) algorithm. Using a database comprised of one million legitimate URLs from the Common Crawl database, and one million phishing URLs from Phishtank, both models showed great statistical results. On one hand …","url":["https://albahnsen.com/wp-content/uploads/2018/05/deepphish-simulating-malicious-ai_submitted.pdf"]}
{"year":"2018","title":"DEPARTMENT OF INFORMATICS M. Sc. IN COMPUTER SCIENCE","authors":["II Koutsikakis, P Malakasiotis, J Pavlopoulos"],"snippet":"Page 1. DEPARTMENT OF INFORMATICS M.Sc. IN COMPUTER SCIENCE M.Sc. Thesis “Toxicity Detection in User Generated Content” Ioannis-Ion Koutsikakis ΕΥ1606 Supervisors: Ion Androutsopoulos Prodromos Malakasiotis …","url":["http://nlp.cs.aueb.gr/theses/koutsikakis_msc_thesis.pdf"]}
{"year":"2018","title":"Department of Informatics MSc in Computer Science","authors":["E Kyriakakis"],"snippet":"… tasks. Pretrained word vectors are usually trained in large corpora like Wikipedia or/and Common Crawl. Dozat et al. use 100D word2vec (Mikolov et al., 2013) pretrained (on Wikipedia and Common Crawl) word embeddings1. Dozat …","url":["http://www2.aueb.gr/users/ion/docs/kyriakakis_msc_thesis.pdf"]}
{"year":"2018","title":"Deriving Word Embeddings Using Multilingual Transfer Learning for Opinion Mining","authors":["K Stavridis, G Koloniari, E Keramopoulos - 2018 South-Eastern European Design …, 2018"],"snippet":"… A word's context refers to its neighboring words in a text corpus. The Word2Vec model was trained on a huge corpus from Google (news), while the GloVe model has several variations based on the training input such as Wikipedia, Twitter and Common Crawl …","url":["https://ieeexplore.ieee.org/abstract/document/8544940/"]}
{"year":"2018","title":"Detecting Misogynous Tweets","authors":["R Ahluwalia, E Shcherbinina, E Callow, A Nascimento… - Proceedings of the Third …, 2018"],"snippet":"… For comparison, we trained the same LSTM-networks using a fastText model trained on Common Crawl and Wikipedia text that maps words to 300-dimensional vectors [8]. Document-level embedding is an extension …","url":["http://ceur-ws.org/Vol-2150/AMI_paper3.pdf"]}
{"year":"2018","title":"Detecting Signs of Dementia Using Word Vector Representations","authors":["B Mirheidari, D Blackburn, T Walker, A Venneri… - Proc. Interspeech 2018, 2018"],"snippet":"… We initially used the 'w2vec' model pre-trained on the Google News dataset (3 million vocabulary size, 100 billions words and 300 vector size) and the 'GloVe' model pre-trained on the Common Crawl (2.2 million vocabulary …","url":["https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1764.pdf"]}
{"year":"2018","title":"Detection of mergeable Wikipedia articles based on overlapping topics","authors":["R Wang, M Iwaihara"],"snippet":"… Google News 3M vocabulary 100 billion tokens Skip-gram Common Crawl 2.2M vocabulary 840 billion tokens Glove … The results are shown in Table 3 Table 3: Combination model result Combining embedding result(Common Crawl and Google News) accuracy …","url":["http://db-event.jpn.org/deim2018/data/papers/157.pdf"]}
{"year":"2018","title":"Determination of content score","authors":["A Kagoshima, K Londenberg, F Xu - US Patent App. 10/325,033, 2019","A Kagoshima, K Londenberg, F Xu - US Patent App. 15/337,268, 2018"],"snippet":"… 310. The crawler module 310 may automatically crawl a network and acquire contents from one or more resources in the network, or acquire the contents from an open repository of web crawl data such as CommonCrawl.org …","url":["https://patentimages.storage.googleapis.com/56/46/d7/219015bf015761/US10325033.pdf","https://patents.google.com/patent/US20180121430A1/en"]}
{"year":"2018","title":"Developing Chatbots","authors":["NK Manaswi - Deep Learning with Applications Using Python, 2018"],"snippet":"… 200. 6B. 400,000. GloVe. 10+10. GloVe. Wikipedia + Gigaword 5. 300. 6B. 400,000. GloVe. 10+10. GloVe. Common Crawl 42B. 300. 42B. 1.9M. GloVe. AdaGrad. GloVe. Common Crawl 840B. 300. 840B. 2.2M. GloVe. AdaGrad …","url":["https://link.springer.com/chapter/10.1007/978-1-4842-3516-4_11"]}
{"year":"2018","title":"Di-LSTM Contrast: A Deep Neural Network for Metaphor Detection","authors":["K Swarnkar, AK Singh - NAACL HLT 2018, 2018"],"snippet":"… variations of our model. We use 300-dimensional GloVe vectors (Pennington et al., 2014) trained on 6B Common Crawl corpus as word embeddings, setting the embeddings of out- of-vocabulary words to zero. To prevent overfit …","url":["http://www.cl.cam.ac.uk/~es407/papers/Fig-Lang2018-proceedings.pdf#page=127"]}
{"year":"2018","title":"Differential Approach to Web-Corpus Construction","authors":["T Shavrina"],"snippet":"… This approach does not allow saving maximum of metatext information, since all the boilerplate is deleted, but it allows to gather a lot in a short time (example – Common Crawl). 2) fitted – all the materials from the listed thousands of URLs are crawled …","url":["http://www.dialog-21.ru/media/4261/shavrina.pdf"]}
{"year":"2018","title":"Discovering Implicit Knowledge with Unary Relations","authors":["M Glass, A Gliozzo - Proceedings of the 56th Annual Meeting of the …, 2018"],"snippet":"… As part of the contributions for this paper, we release the benchmark to the re- search community providing the software needed to generate it from Common Crawl and DBpedia as an open source project2. As a baseline, we adapt a state of the art deep learning based …","url":["http://www.aclweb.org/anthology/P18-1147"]}
{"year":"2018","title":"Discovering Novel Emergency Events in Text Streams","authors":["D Deviatkin, A Shelmanov, D Larionov - 2018"],"snippet":"… of emergency related messages by incorporating more labeled data from CrisisLex corpora [24] and by exploring: ● Various embeddings: word-level: fastText [16] (trained on our own corpus / pre-trained on English Wikipedia) …","url":["http://ceur-ws.org/Vol-2277/paper36.pdf"]}
{"year":"2018","title":"Discovering Phonesthemes with Sparse Regularization","authors":["NF Liu, GA Levow, NA Smith"],"snippet":"… 3 Data For our experiments, we use 300-dimensional GloVe (Pennington et al., 2014) English word em- beddings trained on the cased Common Crawl. Many of the terms in the set of pretrained vectors are not English words …","url":["https://homes.cs.washington.edu/~nfliu/papers/liu+levow+smith.sclem2018.pdf"]}
{"year":"2018","title":"Discriminator at SemEval-2018 Task 10: Minimally Supervised Discrimination","authors":["A Kulmizev, M Abdou, V Ravishankar, M Nissim - Proceedings of The 12th …, 2018"],"snippet":"… The VSM used in our final submission consisted of an av- erage of three sets of embeddings: GloVe word embeddings trained on Common Crawl (840B to- kens) (Pennington et al., 2014), the same GloVe embeddings post …","url":["http://www.aclweb.org/anthology/S18-1167"]}
{"year":"2018","title":"Distinguishing attributes using text corpora and relational knowledge","authors":["R Speer, J Lowry-Duda"],"snippet":"… Unicode CLDR emoji data • word2vec, precomputed on Google News • GloVe, precomputed on the Common Crawl • fastText, customized to learn from parallel text, trained on OpenSubtitles 2016 We used the embeddings …","url":["http://blog.conceptnet.io/2018/06/naacl2018-poster.pdf"]}
{"year":"2018","title":"Distributed Evaluation of Subgraph Queries Using Worstcase Optimal LowMemory Dataflows","authors":["K Ammar, F McSherry, S Salihoglu, M Joglekar - arXiv preprint arXiv:1802.03760, 2018"],"snippet":"Page 1. Distributed Evaluation of Subgraph Queries Using Worst-case Optimal Low-Memory Dataflows Khaled Ammar†, Frank McSherry‡, Semih Salihoglu†, Manas Joglekar♯ †University of Waterloo, ‡ETH Zürich,♯Google …","url":["https://arxiv.org/pdf/1802.03760"]}
{"year":"2018","title":"Distributed Representations of Tuples for Entity Resolution","authors":["MESTS Joty, MON Tang - Proceedings of the VLDB Endowment, 2018","MESTS Joty, MON Tang - Proceedings of the VLDB Endowment,(11), 2018"],"snippet":"… is not always possible or feasible. For example, the popular GloVe dictionary is trained on the Common Crawl corpus, which is almost 2 TB requiring exorbitant computing re- sources. Given an unseen word, another simplistic …","url":["http://da.qcri.org/ntang/pubs/vldb18-deeper.pdf","https://pdfs.semanticscholar.org/334e/9eb88738671a5c9a53dea174586e885ec00b.pdf"]}
{"year":"2018","title":"DL Team at SemEval-2018 Task 1: Tweet Affect Detection using Sentiment Lexicons and Embeddings","authors":["D Kravchenko, L Pivovarova - Proceedings of The 12th International Workshop on …, 2018"],"snippet":"… We use the following two models: 1. Common Crawl: 300-dimensional vectors trained on huge Internet corpus of 840 billion tokens and 2.2 million distinct words … GloVe Common Crawl 46.93 53.98 43.66 56.31 66.38 59.26 Google …","url":["http://www.aclweb.org/anthology/S18-1025"]}
{"year":"2018","title":"DMCB at SemEval-2018 Task 1: Transfer Learning of Sentiment Classification Using Group LSTM for Emotion Intensity prediction","authors":["Y Kim, H Lee - Proceedings of The 12th International Workshop on …, 2018"],"snippet":"… We try five pre-trained word embeddings to choose the best one for the target model. Two are trained with GloVe (Pennington et al., 2014) using different data sets: one1 is trained with very large data in Common crawl, and the …","url":["http://www.aclweb.org/anthology/S18-1044"]}
{"year":"2018","title":"Domain Adapted Word Embeddings for Improved Sentiment Classification","authors":["PK Sarma, YI Liang, WA Sethares - arXiv preprint arXiv:1805.04576, 2018","PKSYL William, A Sethares"],"snippet":"Generic word embeddings are trained on large-scale generic corpora; Domain Specific (DS) word embeddings are trained only on data from a domain of interest. This paper proposes a method to combine the breadth of generic embeddings with …","url":["https://ar5iv.labs.arxiv.org/html/1805.04576","https://arxiv.org/pdf/1805.04576"]}
{"year":"2018","title":"Dropout during inference as a model for neurological degeneration in an image captioning network","authors":["B Li, R Zhang, F Rudzicz - arXiv preprint arXiv:1808.03747, 2018"],"snippet":"… The neural network model is implemented us- ing PyTorch (Paszke et al., 2017). We use GLoVe embeddings from SpaCy (Pennington et al., 2014; Honnibal and Johnson, 2015) trained on Common Crawl. The network was trained using Adam (Kingma and Ba, 2014) …","url":["https://arxiv.org/pdf/1808.03747"]}
{"year":"2018","title":"Dynamic Meta-Embeddings for Improved Sentence Representations","authors":["D Kiela, C Wang, K Cho - Proceedings of the 2018 Conference on Empirical …, 2018"],"snippet":"… Figure 4 shows a similar breakdown for open versus closed class. The analysis allows us to make several interesting observations: it appears that this model prefers GloVe embeddings, followed by the two FastText …","url":["http://www.aclweb.org/anthology/D18-1176"]}
{"year":"2018","title":"Effective Strategies for Combining Attention Mechanism with LSTM for Aspect-Level Sentiment Classification","authors":["K Shuang, X Ren, H Guo, J Loo, P Xu - Proceedings of SAI Intelligent Systems …, 2018"],"snippet":"… 346. Neg. 807. 196. 870. 128. 1560. 173. Total. 3608. 1120. 2328. 638. 6248. 692. 4.2 Experimental Settings. In the experiments, pre-trained word vectors are trained by Glove on Common Crawl Corpus size and they are used to …","url":["https://link.springer.com/chapter/10.1007/978-3-030-01057-7_62"]}
{"year":"2018","title":"Efficient and Robust Question Answering from Minimal Context over Documents","authors":["S Min, V Zhong, R Socher, C Xiong - arXiv preprint arXiv:1805.08092, 2018"],"snippet":"Page 1. Efficient and Robust Question Answering from Minimal Context over Documents Sewon Min1∗, Victor Zhong2, Richard Socher2, Caiming Xiong2 Seoul National University1, Salesforce Research2 shmsw25@snu …","url":["https://arxiv.org/pdf/1805.08092"]}
{"year":"2018","title":"Efficient Unsupervised Word Sense Induction, Disambiguation and Embedding","authors":["BH Soleimani, H Naderi, S Matwin"],"snippet":"… as sentiment analysis, translation, etc. 2 Proposed Method Similar to word embeddings, sense embeddings should also be learned from a large text corpus such as Wikipedia or Common Crawl data. In our method, we first calculate …","url":["https://bigdata1.research.cs.dal.ca/behrouz/publication/nipsw2018/NIPSW2018_EfficientWordSenseDisambiguation.pdf"]}
{"year":"2018","title":"Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl","authors":["J Bevendorff, B Stein, M Hagen, M Potthast - European Conference on Information …, 2018"],"snippet":"Abstract Elastic ChatNoir (Search: www. chatnoir. eu Code: www. github. com/chatnoir-eu) is an Elasticsearch-based search engine offering a freely accessible search interface for the two ClueWeb corpora and the Common Crawl …","url":["https://link.springer.com/chapter/10.1007/978-3-319-76941-7_83"]}
{"year":"2018","title":"EmbNum: Semantic labeling for numerical values with deep metric learning","authors":["P Nguyen, K Nguyen, R Ichise, H Takeda - arXiv preprint arXiv:1807.01367, 2018"],"snippet":"… In a study of Lehmberg et al. [1], 233 million table data resources have extracted from the July 2015 version of the Common Crawl3. Additionally, Mitlohner et al … Table schema is 3 http://commoncrawl.org/ arXiv:1807.01367 …","url":["https://arxiv.org/pdf/1807.01367"]}
{"year":"2018","title":"Emergency Vocabulary","authors":["DM Nemeskey, A Kornai - Information Systems Frontiers"],"snippet":"… 2004) and CommonCrawl.1 In the normal course of events, emergencies like natural disasters, military escalation, epidemic outbreaks etc … 5 Originally, the embedding contains 2,196,017 words with the associated 300-dimension …","url":["https://link.springer.com/article/10.1007/s10796-018-9843-x"]}
{"year":"2018","title":"Emotion Detection and Classification in a Multigenre Corpus with Joint Multi-Task Deep Learning","authors":["S Tafreshi, M Diab - Proceedings of the 27th International Conference on …, 2018"],"snippet":"… Common training set for word embedding models are wiki+news, news, tweets, and common crawl, and the methods are Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fasText (Bojanowski et al., 2016) …","url":["http://www.aclweb.org/anthology/C18-1246"]}
{"year":"2018","title":"EmotionLines: An Emotion Corpus of Multi-Party Conversations","authors":["SY Chen, CC Hsu, CC Kuo, LW Ku - arXiv preprint arXiv:1802.08379, 2018"],"snippet":"… Given a utterance of M words, the one-hot encoding for utterance words is denoted by U = {w1, w2, w3,..., wM }. We first embed the words to the word embedding , which is publicly available 300-dimensional GloVe pre-trained on Common Crawl data (Pennington et al., 2014) …","url":["https://arxiv.org/pdf/1802.08379"]}
{"year":"2018","title":"EmotionX-JTML: Detecting emotions with Attention","authors":["J Torres - Proceedings of the Sixth International Workshop on …, 2018"],"snippet":"… Each column of the matrix stores the em- beddings of the corresponding word, resulting in d dimensional input matrix M ∈ RM×d. The weights of the word embeddings use the 300dimensional GloVe Embeddings …","url":["http://www.aclweb.org/anthology/W18-3510"]}
{"year":"2018","title":"End-to-End Retrieval in Continuous Space","authors":["D Gillick, A Presta, GS Tomar - arXiv preprint arXiv:1811.08008, 2018"],"snippet":"… embeddings using Inverse Document Frequency (IDF)4, and try 3 different settings for pre-training: standard word2vec, word2vec trained with the Paralex dataset (closer to the question domain), and Glove (Pennington …","url":["https://arxiv.org/pdf/1811.08008"]}
{"year":"2018","title":"Energy-Aware Data Throughput Optimization for Next Generation Internet","authors":["T Kosar, I Alan, MF Bulut - Information Sciences, 2018"],"snippet":"… Three different representative datasets were used during experiments in order to capture the throughput and power consumption differences based on the dataset type: (i) the HTML dataset is a set of raw HTML files from the …","url":["https://www.sciencedirect.com/science/article/pii/S0020025518307874"]}
{"year":"2018","title":"Engaging English Speaking Facebook Users in an Anti-ISIS Awareness Campaign","authors":["A Shajkovci - Journal of Strategic Security, 2018"],"snippet":"… STRONG VERY LOW 0 133 able, act, adult, advantage, aggressive, agreement, army, assistance, attack, aware, benefit, b . . . WEAK LOW 16 41 admit, apologize, ashamed, bum, common, crawl, death, depression, desperate, die, disgrace, dis …","url":["https://scholarcommons.usf.edu/cgi/viewcontent.cgi?article=1679&context=jss"]}
{"year":"2018","title":"English-Basque Statistical and Neural Machine Translation","authors":["IJ Unanue, LG Arratibel, EZ Borzeshi, M Piccardi - Proceedings of the Eleventh …, 2018"],"snippet":"… For English, we have used the available CommonCrawl pre-trained embeddings3. We have evaluated the use of these embeddings in two different ways: maintaining them fixed during both training and testing (f-emb) …","url":["http://www.aclweb.org/anthology/L18-1141"]}
{"year":"2018","title":"Entity-Oriented Search","authors":["K Balog - 2018"],"snippet":"Page 1. The Information Retrieval Series Krisztian Balog EntityOriented Search Page 2. The Information Retrieval Series Volume 39 Series Editors ChengXiang Zhai Maarten de Rijke Editorial Board Nicholas J. Belkin Charles …","url":["https://link.springer.com/content/pdf/10.1007/978-3-319-93935-3.pdf"]}
{"year":"2018","title":"ERASeD: Exposing Racism And Sexism using Deep Learning","authors":["S Paul, J Bhaskaran","SPJ Bhaskaran"],"snippet":"… We tried out the following methods: • Pretrained Embeddings – GloVe (Twitter) – GloVe (Common Crawl) The biggest drawback of using pretrained embeddings is that 20% of the words we see in the dataset from Twitter do not have embeddings …","url":["http://web.stanford.edu/class/cs224n/reports/6908288.pdf","https://pdfs.semanticscholar.org/7d21/50c3d9028b6b58fb7fb01d70e4ffe08d2c58.pdf"]}
{"year":"2018","title":"eSCAPE: a Large-scale Synthetic Corpus for Automatic Post-Editing","authors":["M Negri, M Turchi, R Chatterjee, N Bertoldi - arXiv preprint arXiv:1803.07274, 2018"],"snippet":"… It consists of 4.3 million instances created by first filtering a subset of IT-related sentences from the German Common Crawl corpus6, and then by using two English–German and German–English PBMT systems trained on …","url":["https://arxiv.org/pdf/1803.07274"]}
{"year":"2018","title":"Evaluating MT for massive open online courses","authors":["S Castilho, J Moorkens, F Gaspari, R Sennrich, A Way… - Machine Translation, 2018"],"snippet":"… 2012) as distributed in OPUS (Tiedemann 2012) – OPUS European Central Bank (ECB) – OPUS European Medicines Agency (EMEA) – OPUS EU Bookshop – OPUS OpenSubtitles5 – WMT News Commentary – WMT …","url":["https://link.springer.com/article/10.1007/s10590-018-9221-y"]}
{"year":"2018","title":"Evaluating the psychological plausibility of word2vec and GloVe distributional semantic models","authors":["I Kajic, C Eliasmith"],"snippet":"… In this work, we use pre-trained GloVe and word2vec vectors. GloVe vectors were trained using the Common Crawl dataset containing approximately 840 billion word tokens. Word2vec vectors were trained on the Google News …","url":["http://compneuro.uwaterloo.ca/files/publications/kajic.2018a.pdf"]}
{"year":"2018","title":"Evaluation of High-Dimensional Word Embeddings using Cluster and Semantic Similarity Analysis","authors":["S Atakishiyev - 2018"],"snippet":"Page 1. Evaluation of High-Dimensional Word Embeddings using Cluster and Semantic Similarity Analysis by Shahin Atakishiyev A thesis submitted in partial fulfillment of the requirements for the degree of Master of Science …","url":["https://www.researchgate.net/profile/Shahin_Atakishiyev/publication/325945300_Evaluation_of_High-Dimensional_Word_Embeddings_using_Cluster_and_Semantic_Similarity_Analysis/links/5b2de0ac0f7e9b0df5be7f0d/Evaluation-of-High-Dimensional-Word-Embeddings-using-Cluster-and-Semantic-Similarity-Analysis.pdf"]}
{"year":"2018","title":"Evaluation of sentence embeddings in downstream and linguistic probing tasks","authors":["CS Perone, R Silveira, TS Paula - arXiv preprint arXiv:1806.06259, 2018"],"snippet":"… 1024 Word2Vec (BoW, Google news) Self-supervised 300 p-mean (monolingual) – 3600 FastText (BoW, Common Crawl) Self-supervised 300 GloVe (BoW, Common Crawl) Self-supervised 300 USE (DAN) Supervised 512 USE (Transformer) Supervised 512 …","url":["https://arxiv.org/pdf/1806.06259"]}
{"year":"2018","title":"Evidence of semantic processing difficulty in naturalistic reading","authors":["C Shain, R Futrell, M van Schijndel, E Gibson…"],"snippet":"… a novel measure — incremental semantic relatedness — for three naturalistic reading time corpora: Dundee [12], UCL [7], and Natural Stories [9]. In particular, we embedded all three corpora using GloVe vectors [20] pretrained …","url":["https://vansky.github.io/assets/pdf/shain_etal-2018-cuny.pdf"]}
{"year":"2018","title":"Existing Web Archives","authors":["P Webster - The SAGE Handbook of Web History. London: Sage, 2018"]}
{"year":"2018","title":"Explicit Ensemble Attention Learning for Improving Visual Question Answering","authors":["V Lioutas, N Passalis, A Tefas - Pattern Recognition Letters, 2018"],"snippet":"Visual Question Answering (VQA) is among the most difficult multi-modal problems as it requires a machine to be able to properly understand a question about ar.","url":["https://www.sciencedirect.com/science/article/pii/S0167865518301600"]}
{"year":"2018","title":"Explicit Retrofitting of Distributional Word Vectors","authors":["G Glavaš, I Vulić - Proceedings of the 56th Annual Meeting of the …, 2018"],"snippet":"… al., 2013) using the Skip-Gram algorithm with Negative Sampling (SGNS) (Mikolov et al., 2013b) by Levy and Goldberg (2014b), using the context windows of size 2; (2) GLOVE-CC – vectors trained with the GloVe (Pennington …","url":["http://www.aclweb.org/anthology/P18-1004"]}
{"year":"2018","title":"Exploiting Efficient and Effective Lazy Semi-Bayesian Strategies for Text Classification","authors":["F Viegas, L Rocha, E Resende, T Salles, W Martins… - Neurocomputing, 2018"],"snippet":"Automatic Document Classification (ADC) has become the basis of many important applications, eg, authorship identification, opinion mining, spam filtering, co.","url":["https://www.sciencedirect.com/science/article/pii/S0925231218304636"]}
{"year":"2018","title":"Exploring and Mitigating Gender Bias in GloVe Word Embeddings","authors":["MFLC Vera"],"snippet":"… and the cosine similarity. Finally, to show that gender 100 biases exist across different types of embeddings, we use both the GloVe word embeddings 101 pretrained on Wiki and pretrained on Common Crawl. 102 Plotting all …","url":["https://pdfs.semanticscholar.org/512f/ccca0d3a5f454d5360d32d48e3d4d5ddd578.pdf"]}
{"year":"2018","title":"Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks","authors":["L Song, Z Wang, M Yu, Y Zhang, R Florian, D Gildea - arXiv preprint arXiv …, 2018"],"snippet":"Page 1. Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks Linfeng Song1∗, Zhiguo Wang2†, Mo Yu2†, Yue Zhang3, Radu Florian2 and Daniel Gildea1 …","url":["https://arxiv.org/pdf/1809.02040"]}
{"year":"2018","title":"Exploring Neural Methods for Parsing Discourse Representation Structures","authors":["R van Noord, L Abzianidze, A Toral, J Bos - arXiv preprint arXiv:1810.12579, 2018"],"snippet":"… We simply add the word embedding vector after the sequence of character-embeddings for each word in the input and still initialise these embeddings using the pre-trained GloVe embeddings. 6The Common …","url":["https://arxiv.org/pdf/1810.12579"]}
{"year":"2018","title":"Exploring Optimism and Pessimism in Twitter Using Deep Learning","authors":["C Caragea, LP Dinu, B Dumitru"],"snippet":"… We used mini-batches of 40 samples. Dropout rate was set to 0.5 and the classifier's last three layers have 300, 200, and 100 neurons. We used GloVe vectors (Pennington et al., 2014) trained on Common Crawl …","url":["https://www.cs.uic.edu/~cornelia/papers/emnlp18_op.pdf"]}
{"year":"2018","title":"Exploring speed and memory trade-offs for achieving optimum performance on SQuAD dataset","authors":["R Aksitov"],"snippet":"… bidirectional in this architecture was not producing any measurable gains for some time, until I also switched to Common Crawl embedding with … When I switched to CommonCrawl embeddings, I have found out that Tensorflow …","url":["http://web.stanford.edu/class/cs224n/reports/6909117.pdf"]}
{"year":"2018","title":"Exploring the importance of context and embeddings in neural NER models for task-oriented dialogue systems","authors":["P Jayarao, C Jain, A Srivastava - arXiv preprint arXiv:1812.02370, 2018"],"snippet":"… We used the publicly available Glove embeddings1 (Pennington et al., 2014) trained on Wikipedia 2014 corpus with dimension sizes 50 (G50W) and 300 (G300W) and another 300 di- mensional embeddings (G300C) trained on …","url":["https://arxiv.org/pdf/1812.02370"]}
{"year":"2018","title":"ExtRA: Extracting Prominent Review Aspects from Customer Feedback","authors":["Z Luo, S Huang, FF Xu, BY Lin, H Shi, K Zhu - … of the 2018 Conference on Empirical …, 2018"],"snippet":"Page 1. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3477–3486 Brussels, Belgium, October 31 - November 4, 2018. c 2018 Association for Computational Linguistics 3477 …","url":["http://www.aclweb.org/anthology/D18-1384"]}
{"year":"2018","title":"Extrofitting: Enriching Word Representation and its Vector Space with Semantic Lexicons","authors":["H Jo, SJ Choi - arXiv preprint arXiv:1804.07946, 2018"],"snippet":"… 3 Experiment Data 3.1 Pretrained Word Vectors GloVe (Pennington et al., 2014) has lots of variations in respect to word dimension, number of to- kens, and train sources. We used glove.6B trained on Wikipedia+Gigawords …","url":["https://arxiv.org/pdf/1804.07946"]}
{"year":"2018","title":"Facebook Use of Sensitive Data for Advertising in Europe","authors":["JG Cabañas, Á Cuevas, R Cuevas - arXiv preprint arXiv:1802.05030, 2018"],"snippet":"Page 1. arXiv:1802.05030v1 [cs.SI] 14 Feb 2018 Facebook Use of Sensitive Data for Advertising in Europe José González Caba˜nas Universidad Carlos III de Madrid jgcabana@it.uc3m.es ´Angel Cuevas Universidad Carlos III de Madrid acrumin@it.uc3m.es …","url":["https://arxiv.org/pdf/1802.05030"]}
{"year":"2018","title":"Facilitating mapping of control policies to regulatory documents","authors":["SB Tirumala, A Jagmohan, E Khabiri, TH Li… - US Patent App. 15/349,766, 2018"],"snippet":"… dictionary 206). The global corpora 203 can comprise a general internet-based collection of texts derived from various sources (eg, GUTENBERG®, REUTERS®, COMMON CRAWL®, and/or GOOGLE NEWS®). The regulatory …","url":["https://patents.google.com/patent/US20180137107A1/en"]}
{"year":"2018","title":"Fast Dictionary-Based Compression for Inverted Indexes","authors":["GE Pibiri, M Petri, A Moffat - 2019"],"snippet":"… We use the standard Gov2 collection containing 426 GiB of text; and CCNEWS, an English subset of the freely available NEWS subset of the CommonCrawl1, consisting of news articles in the period 09/01/16 to …","url":["http://pages.di.unipi.it/pibiri/papers/WSDM19.pdf"]}
{"year":"2018","title":"Filtering and Mining Parallel Data in a Joint Multilingual Space","authors":["H Schwenk - arXiv preprint arXiv:1805.09822, 2018"],"snippet":"… only a limited amount of parallel training data is available (4.5M sentences). 2.1M are high quality human translations and 2.4M are crawled and aligned sentences (Common Crawl corpus). As in other works, we use …","url":["https://arxiv.org/pdf/1805.09822"]}
{"year":"2018","title":"Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds","authors":["S Zhang, K Duh, B Van Durme - arXiv preprint arXiv:1804.08000, 2018"],"snippet":"… 3.2 Hyperparameters We use open-source GloVe vectors (Pennington et al., 2014) trained on Common Crawl 840B with 300 dimensions to initialize word embeddings used in all encoders. All weight parameters are sampled from U(−0.01,0.01) …","url":["https://arxiv.org/pdf/1804.08000"]}
{"year":"2018","title":"Focused Crawling Through Reinforcement Learning","authors":["M Han, PH Wuillemin, P Senellart - International Conference on Web Engineering, 2018"],"snippet":"… efficiently crawl relevant pages. In future work, we hope to evaluate our method in larger and various datasets, such as full English Wikipedia and dataset from the site http://commoncrawl.org/, etc. Another challenging possibility …","url":["https://link.springer.com/chapter/10.1007/978-3-319-91662-0_20"]}
{"year":"2018","title":"Follow The Money: Online Piracy and Self-Regulation in the Advertising Industry","authors":["M Batikas, J Claussen, C Peukert - 2018"],"snippet":"… Get HTML source code files from Commoncrawl and parse them (∼500,000 webpages) … 10 Page 14. Figure 2: Timing of data snapshots 2016w7 2016w15 2016w23 2016w31 2016w39 2016w47 2017w3 2017w11 Note: Available data snapshots from Common Crawl …","url":["http://www.cesifo-group.de/DocDL/cesifo1_wp6852.pdf"]}
{"year":"2018","title":"Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities: Supplementary Material","authors":["H Pham, PP Liang, T Manzini, LP Morency, B Poczós"],"snippet":"… visual and acoustic modalities. Language: We used 300 dimensional Glove word embeddings trained on 840 billion tokens from the common crawl dataset (Pennington, Socher, and Manning, 2014). These word embeddings …","url":["http://www.cs.cmu.edu/~pliang/papers/aaai2019_seq2seq_supp.pdf"]}
{"year":"2018","title":"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation","authors":["B Thompson, H Khayrallah, A Anastasopoulos… - arXiv preprint arXiv …, 2018"],"snippet":"… 1 For De–En and Ru– En, we also use data from WMT 2017 (Bojar et al., 2017),2 which contains data from several sources: Europarl (parliamentary proceedings) (Koehn, 2005),3 News Commentary (political and economic …","url":["https://arxiv.org/pdf/1809.05218"]}
{"year":"2018","title":"From Mad Men to Maths Men: Concentration and Buyer Power in Online Advertising","authors":["F Decarolis, G Rovigatti"],"snippet":"Page 1. From Mad Men to Maths Men: Concentration and Buyer Power in Online Advertising ∗ Francesco Decarolis Gabriele Rovigatti PRELIMINARY AND INCOMPLETE PLEASE DO NOT CIRCULATE Abstract This paper …","url":["https://www.tse-fr.eu/sites/default/files/TSE/documents/ChaireJJL/Digital-Economics-Conference/Conference/decarolis_franceso.pdf"]}
{"year":"2018","title":"FROM TEXT CLASSIFICATION TO IMAGE CLUSTERING, PROBLEMS LESS OPTIMIZED","authors":["A Herandi - 2018"],"snippet":"… We have used similar models for our BiLSTM and BiGRU models. We are using two layers stacked on top of each other and use only the last output from each direction from the second layer for our inference, because we are 1 http://commoncrawl.org/ Page 18. 10 …","url":["https://uta-ir.tdl.org/uta-ir/bitstream/handle/10106/27492/HERANDI-THESIS-2018.pdf?sequence=1"]}
{"year":"2018","title":"Full text document","authors":["Z Bouraoui, S Jameel, S Schockaert"],"snippet":"… set3 (SG-GN). We also use two embeddings that have been learned with GloVe, one from the same english Wikipedia dump (GloVe-Wiki) and one from the 840B words Common Crawl data set4 (GloVe-CC). For relations with …","url":["https://kar.kent.ac.uk/67263/1/Shoaib-COLING-2018.pdf"]}
{"year":"2018","title":"Fully Convolutional Speech Recognition","authors":["N Zeghidour, Q Xu, V Liptchinsky, N Usunier… - arXiv preprint arXiv …, 2018"],"snippet":"… 5.83 12.69 (12k training hours AM, common crawl LM) Sequence-to- sequence [9] 3.54 11.52 3.82 12.76 … (speaker adaptation, 3k acoustic states) DeepSpeech 2 [7] 5 3.6 (12k training hours AM, common crawl LM) Frontend …","url":["https://arxiv.org/pdf/1812.06864"]}
{"year":"2018","title":"Garbage In, Garbage Out","authors":["H Sanders"],"snippet":"… VirusTotal 10 million URLs from January 2017 ≈ 4% malware CommonCrawl & PhishTank Sophos 10 million internal URLs from January … VirusTotal 10 million URLs from January 2017 ≈ 4% malware CommonCrawl & Phishtank …","url":["https://pdfs.semanticscholar.org/c940/a2144b1e5cc89bbeb9003b5f3541e50fc9e1.pdf"]}
{"year":"2018","title":"Gated Convolutional Neural Network for Sentence Matching","authors":["P Chen, W Guo, Z Chen, J Sun, L You - memory, 2018"],"snippet":"… 3.2. Training We implement our models on Tensorflow [26] and train them on an Nvidia GeForce GTX 1080 GPU. We initialize the word em- beddings with 300-dimensional GloVe word vectors pretrained from 840B Common Crawl corpus [19] …","url":["https://www.isca-speech.org/archive/Interspeech_2018/pdfs/0070.pdf"]}
{"year":"2018","title":"Generalizing and Improving Bilingual Word Embedding Mappings with a Multi-Step Framework of Linear Transformations","authors":["M Artetxe, G Labaka, E Agirre - 2018"],"snippet":"… The corpora used consisted of 2.8 billion words for English (ukWaC + Wikipedia + BNC), 1.6 billion words for Italian (itWaC), 0.9 billion words for German (SdeWaC), and 2.8 billion words for Finnish (Common Crawl from WMT 2016) …","url":["http://ixa.eus/sites/default/files/dokumentuak/11455/aaai18.pdf"]}
{"year":"2018","title":"Generating Wikipedia by Summarizing Long Sequences","authors":["PJ Liu, M Saleh, E Pot, B Goodrich, R Sepassi, L Kaiser… - arXiv preprint arXiv …, 2018"],"snippet":"… To encourage further research on large-scale summarization, we will release the URLs used in our experiments (the Wikipedia URL as well as the URLs of its references) that are available as part of the CommonCrawl dataset4, which is freely available for download …","url":["https://arxiv.org/pdf/1801.10198"]}
{"year":"2018","title":"Generation of code from text description with syntactic parsing and Tree2Tree model","authors":["A Stehnii - 2017"],"snippet":"Page 1 …","url":["http://www.er.ucu.edu.ua/bitstream/handle/1/1191/Stehnii-master-thesis.pdf?sequence=1&isAllowed=y"]}
{"year":"2018","title":"Global normalized reader systems and methods","authors":["J RAIMAN, J Miller - US Patent App. 15/706,486, 2018"],"snippet":"… prediction. The hidden dimensions of all recurrent layers were 200. In embodiments, the 300 dimensional 8.4B token Common Crawl GloVe vectors were used. Words missing from the Common Crawl vocabulary were set to zero. In …","url":["https://patentimages.storage.googleapis.com/de/9c/d5/dabff582992ff9/US20180300312A1.pdf"]}
{"year":"2018","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding","authors":["A Wang, A Singh, J Michael, F Hill, O Levy… - arXiv preprint arXiv …, 2018"],"snippet":"Page 1. arXiv:1804.07461v1 [cs.CL] 20 Apr 2018 GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding Alex Wang1, Amanpreet Singh1, Julian Michael2, Felix Hill3, Omer Levy2, and Samuel R. Bowman1 …","url":["https://arxiv.org/pdf/1804.07461"]}
{"year":"2018","title":"GraphIt-A High-Performance DSL for Graph Analytics","authors":["Y Zhang, M Yang, R Baghdadi, S Kamil, J Shun… - arXiv preprint arXiv …, 2018"],"snippet":"Page 1. GraphIt – A High-Performance DSL for Graph Analytics YUNMING ZHANG, MIT CSAIL MENGJIAO YANG, MIT CSAIL RIYADH BAGHDADI, MIT CSAIL SHOAIB KAMIL, Adobe Research JULIAN SHUN, MIT CSAIL SAMAN AMARASINGHE, MIT CSAIL …","url":["https://arxiv.org/pdf/1805.00923"]}
{"year":"2018","title":"Harnessing NLG to Create Finnish Poetry Automatically","authors":["M Hämäläinen"],"snippet":"… This is 3The semantic repository can be browsed and downloaded on https://mikakalevi. com/semfi/ 4http://commoncrawl.org/ due to the Finnish agreement rule which requires the case of an adjective attribute to agree with the case of the noun …","url":["http://computationalcreativity.net/iccc2018/sites/default/files/papers/ICCC_2018_paper_6.pdf"]}
{"year":"2018","title":"Heuristics-based Query Reordering for Federated Queries in SPARQL 1.1 and SPARQL-LD","authors":["T Yannakis, P Fafalios, Y Tzitzikas"],"snippet":"… The question is: how 3 http://commoncrawl.org/ 4 http://webdatacommons. org/structureddata/2016-10/stats/stats.html Page 2. 2 Thanos Yannakis, Pavlos Fafalios, and Yannis Tzitzikas can we efficiently query this large, distributed …","url":["http://l3s.de/~fafalios/files/pubs/fafalios2018_QuWeDa.pdf"]}
{"year":"2018","title":"HFL-RC System at SemEval-2018 Task 11: Hybrid Multi-Aspects Model for Commonsense Reading Comprehension","authors":["Z Chen, Y Cui, W Ma, S Wang, T Liu, G Hu - arXiv preprint arXiv:1803.05655, 2018"],"snippet":"… Page 4. 3 Experiments 3.1 Experimental Setups We listed the main hyper-parameters of our model in Table 1. The word embeddings are initialized by the pre-trained GloVe word vectors (Common Crawl, 6B tokens, 100-dimension) (Pennington et al., 2014) …","url":["https://arxiv.org/pdf/1803.05655"]}
{"year":"2018","title":"Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts","authors":["D Jin, P Szolovits - arXiv preprint arXiv:1808.06161, 2018"],"snippet":"… pus of Wikipedia 2014 + Gigaword 57 (denoted as “Glove-wiki”), fastText embeddings pre-trained on Wikipedia8 (denoted as “FastText-wiki”), and fastText embeddings initialized with the standard GloVe Common Crawl …","url":["https://arxiv.org/pdf/1808.06161"]}
{"year":"2018","title":"Historical Web as a Tool for Analyzing Social Change","authors":["R Schroeder, N Brügger, J Cowls - Second International Handbook of Internet …, 2018"],"snippet":"… are themselves responsible for the curation and selection of what to archive (eg, Archive-It), and, second, publicly available collections, archived by “amateurs,” with no cultural heritage obligations (eg, The Archive Team …","url":["https://link.springer.com/content/pdf/10.1007/978-94-024-1202-4_24-1.pdf"]}
{"year":"2018","title":"Hourglass-Incremental Graph Processing on Heterogeneous Infrastructures","authors":["P Joaquim"],"snippet":"… On June 2016 Facebook reportes[13] an average of 1.13 billion daily active users. Another example is the largest World Wide Web hyperlink graph made publicly available by Common Crawl[14] that has over 3.5 billion web pages and 128 billion hyperlinks between them …","url":["https://pdfs.semanticscholar.org/92bf/f88f18c632327f7d503c9a7ab72cb433b7c1.pdf"]}
{"year":"2018","title":"How to Do Things with Translations","authors":["J Jacobs - 2018"],"snippet":"… 17 Page 18. as the Common Crawl corpus19 often have vocabulary sizes v exceeding 2 million, thus re … 19http://commoncrawl.org/ 20For a comprehensive survey of the most popular embedding algorithms, see Goldberg (2017). 18 Page 19 …","url":["https://www-cs.stanford.edu/~jjacobs3/Jacobs-How_to_Do_Things_with_Translations.pdf"]}
{"year":"2018","title":"Human versus automatic quality evaluation of NMT and PBSMT","authors":["D Shterionov, R Superbo, P Nagle, L Casanellas… - Machine Translation, 2018"],"snippet":"… 2015) is built with TED talks, EPPS, news commentary,8 and Common Crawl data; the NMT system they compare to (Luong and Manning 2015) is a pre-trained NMT system that was further improved with data provided by the IWSLT2015 organizers …","url":["https://link.springer.com/article/10.1007/s10590-018-9220-z"]}
{"year":"2018","title":"Hybrid Self-Attention Network for Machine Translation","authors":["K Song, T Xu, F Peng, J Lu - arXiv preprint arXiv:1811.00253, 2018"],"snippet":"… 2016) shared vocabulary. WMT14 English-German WMT14 EnglishGerman dataset (Buck, Heafield, and van Ooyen 2014) comprises about 4.5 million sentence pairs that are extracted from three corpora: Common …","url":["https://arxiv.org/pdf/1811.00253"]}
{"year":"2018","title":"Hypothesis Only Baselines in Natural Language Inference","authors":["A Poliak, J Naradowsky, A Haldar, R Rudinger… - arXiv preprint arXiv …, 2018"],"snippet":"… Following Conneau et al. (2017), we map the resulting to- kens to 300-dimensional GloVe vectors (Pennington et al., 2014) trained on 840 billion tokens from the Common Crawl, using the GloVe OOV vector for unknown words …","url":["https://arxiv.org/pdf/1805.01042"]}
{"year":"2018","title":"Identifying Semantic Divergences in Parallel Text without Annotations","authors":["Y Vyas, X Niu, M Carpuat - arXiv preprint arXiv:1803.11112, 2018","YVXNM Carpuat"],"snippet":"… On the CommonCrawl test set, the examples with disagreement are more diverse, ranging from Page 7. Divergence Detection OpenSubtitles Common Crawl Approach +P +R +F -P -R -F Overall F +P +R +F -P -R …","url":["https://arxiv.org/pdf/1803.11112","https://deeplearn.org/arxiv/30360/identifying-semantic-divergences-in-parallel-text-without-annotations"]}
{"year":"2018","title":"Identifying the Most Effective Feature Category in Machine Learning-based Phishing Website Detection","authors":["CL Tan, KL Chiew, N Musa, DHA Ibrahim - International Journal of Engineering & …, 2018"],"snippet":"… [31] “Common Crawl”, available online: http://commoncrawl.org/, last visit: 10.01.2017. [32] Selenium Project (2017), “Selenium WebDriver”, available online: http://www.seleniumhq.org/projects/webdriver/, last visit: 10.01.2017 …","url":["https://www.researchgate.net/profile/Choon_Lin_Tan/publication/329554643_Identifying_the_Most_Effective_Feature_Category_in_Machine_Learning-based_Phishing_Website_Detection/links/5c0f183e299bf139c74fb929/Identifying-the-Most-Effective-Feature-Category-in-Machine-Learning-based-Phishing-Website-Detection.pdf"]}
{"year":"2018","title":"Improved Text Analytics Transfer Learning","authors":["M Riemer, E Khabiri, R Goodwin - 2018"],"snippet":"… Our GRU model was fed a sequence of fixed 300 dimensional Glove vectors (Pennington et al., 2014), representing words based on analysis of 840 billion words from a common crawl of the internet, as the input xt for all tasks …","url":["https://openreview.net/pdf?id=HyggjiiMz6m"]}
{"year":"2018","title":"Improving Cross-Lingual Word Embeddings by Meeting in the Middle","authors":["Y Doval, J Camacho-Collados, L Espinosa-Anke… - arXiv preprint arXiv …, 2018"],"snippet":"… For Italian and German, we use the itWaC and sdeWaC corpora from the WaCky project (Baroni et al., 2009), containing 2 and 0.8 billion words, respectively.2 Lastly, for Finnish, we use the Common Crawl monolingual …","url":["https://arxiv.org/pdf/1808.08780"]}
{"year":"2018","title":"Improving Machine Translation of Educational Content via Crowdsourcing","authors":["M Behnke, AVM Barone, R Sennrich, V Sosoni…"],"snippet":"… et al., 2012) as distributed in OPUS (Tiedemann, 2012); ● OPUS European Central Bank (ECB); ● OPUS European Medicines Agency (EMEA); ● OPUS EU Bookshop; ● OPUS OpenSubtitles 7; ● WMT News Commentary; ● …","url":["http://www.lrec-conf.org/proceedings/lrec2018/pdf/855.pdf"]}
{"year":"2018","title":"Improving Neural Machine Translation with External Information","authors":["J Helcl"],"snippet":"… image descriptions, we selected similar sentence pairs from the parallel SDEWAC corpus (Faaß and Eckart, 2013) and German parts of WMT parallel corpora, such as EU Bookshop (Skadinš et al., 2014), News Commentary (Tiedemann, 2012), and CommonCrawl (Smith et al …","url":["https://pdfs.semanticscholar.org/5449/aa3ca855cee1a80fe5f6bc97f7c0742e3e99.pdf"]}
{"year":"2018","title":"Improving Personalized Consumer Health Search: Notebook for eHealth at CLEF 2018","authors":["H Yang, T Gonçalves - CEUR Workshop Proceedings: Working Notes of CLEF, 2018"],"snippet":"… Page 4. 3.2 Dataset The dataset used in CLEF 2018 CHS task consisted of web pages acquired from the CommonCrawl. By submitting the task queries to the Microsoft Bing APIs repeatedly over a period of time, an initial …","url":["http://ceur-ws.org/Vol-2125/paper_195.pdf"]}
{"year":"2018","title":"Improving the learning of chemical-protein interactions from literature using transfer learning and word embeddings.","authors":["P Corbett, J Boyle"],"snippet":"… GloVe offers both some public pre-trained embeddings (based on Wikipedia and Common Crawl), and also the software to compile your own – in previous work (6) we had success with the public embeddings, but here, we compiled our own …","url":["http://www.biocreative.org/media/store/files/2018/BC6_track5_10.pdf"]}
{"year":"2018","title":"Improving Word Embedding Compositionality using Lexicographic Definitions","authors":["T Scheepers, E Gavves, E Kanoulas - 2017"],"snippet":"… We used GloVe representations that where trained on the Common Crawl which has 840B tokens and a vocabulary of 2.2M. fastText uses a training algorithm that is based on skip-gram, however it represents each word as a bag of character n-grams …","url":["https://thijs.ai/papers/scheepers-gavves-kanoulas-improving-embedding-lexicographic.pdf"]}
{"year":"2018","title":"Improving Word Embedding Coverage in Less-Resourced Languages Through Multi-Linguality and Cross-Linguality: A Case Study with Aspect-Based Sentiment …","authors":["MS Akhtar, P Sawant, S Sen, A Ekbal, P Bhattacharyya - ACM Transactions on Asian …, 2018","S SEN, A EKBAL"],"snippet":"… However, these systems require a large corpus for training and building the models (eg, the pretrained Google News Word2Vec model was trained on 100 billion words, pretrained common-crawl GloVe model was trained on 840 billion words) …","url":["https://dl.acm.org/citation.cfm?id=3273931","https://dl.acm.org/doi/fullHtml/10.1145/3273931"]}
{"year":"2018","title":"Incorporating Statistical Machine Translation Word Knowledge into Neural Machine Translation","authors":["X Wang, Z Tu, M Zhang - IEEE/ACM Transactions on Audio, Speech, and …, 2018"],"snippet":"Page 1. 2329-9290 (c) 2018 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/8421063/"]}
{"year":"2018","title":"Incorporating the Structure of the Belief State in End-to-End Task-Oriented Dialogue Systems","authors":["L Shu, P Molino, M Namazifar, B Liu, H Xu, H Zheng…"],"snippet":"Page 1. Incorporating the Structure of the Belief State in End-to-End Task-Oriented Dialogue Systems Lei Shu∗1, Piero Molino2, Mahdi Namazifar3, Bing Liu1, Hu Xu1, Huaixiu Zheng3, and Gokhan Tur3 1University …","url":["http://alborz-geramifard.com/workshops/nips18-Conversational-AI/Papers/18convai-Incorporating%20the%20Structure.pdf"]}
{"year":"2018","title":"Inducing Grammars with and for Neural Machine Translation","authors":["K Tran, Y Bisk - arXiv preprint arXiv:1805.10850, 2018","Y Bisk, K Tran - Proceedings of the 2nd Workshop on Neural Machine …, 2018"],"snippet":"… Table 1 shows the statistics of the data. For En↔De, we use a concatenation of Europarl, Common Crawl, Rapid corpus of EU press releases, and News Commentary v12 … For En↔Ru, we use Common Crawl, News Commentary v12, and Yandex Corpus …","url":["http://www.aclweb.org/anthology/W18-2704","https://arxiv.org/pdf/1805.10850"]}
{"year":"2018","title":"Inducing Implicit Relations from Text Using Distantly Supervised Deep Nets","authors":["M Glass, A Gliozzo, O Hassanzadeh… - International Semantic Web …, 2018"],"snippet":"… We also evaluate on a web-scale knowledge base population benchmark that we called CC-DBP 5 . It combines the text of Common Crawl 6 with the triples from 298 frequent relations in DBpedia [1]. Mentions of DBpedia entities …","url":["https://link.springer.com/chapter/10.1007/978-3-030-00671-6_3"]}
{"year":"2018","title":"InferLite: Simple Universal Sentence Representations from Natural Language Inference Data","authors":["J Kiros, W Chan - Proceedings of the 2018 Conference on Empirical …, 2018"],"snippet":"… (2018) describe a contextual gating Feature dataset dim method Glove Common Crawl 300 News Google News 500 CBOW Query Google Search 800 CBOW Table 1: Comparison of word representations used. method for word embedding selection …","url":["http://www.aclweb.org/anthology/D18-1524"]}
{"year":"2018","title":"Inferring gender of Reddit users","authors":["E Vasilev - 2018"],"snippet":"Page 1. People and Knowledge Networks WeST Fachbereich 4: Informatik Institute for Web Science and Technologies Inferring gender of Reddit users Masterarbeit zur Erlangung des Grades einer Master of Science (M.Sc.) im Studiengang Web Science …","url":["https://kola.opus.hbz-nrw.de/files/1619/Master_thesis_Vasilev.pdf"]}
{"year":"2018","title":"Inferring Missing Categorical Information in Noisy and Sparse Web Markup","authors":["N Tempelmeier, E Demidova, S Dietze - arXiv preprint arXiv:1803.00446, 2018"],"snippet":"… information. For instance, from 26 million nodes describing events within the Common Crawl in 2016, 59% of nodes provide less than six statements and only 257,000 nodes (0.96%) are typed with more specific event subtypes …","url":["https://arxiv.org/pdf/1803.00446"]}
{"year":"2018","title":"Integrating Multiplicative Features into Supervised Distributional Methods for Lexical Entailment","authors":["T Vu, V Shwartz - arXiv preprint arXiv:1804.08845, 2018"],"snippet":"… We used 300-dimensional pre-trained word em- beddings, namely, GloVe (Pennington et al., 2014b) containing 1.9M word vectors trained on a corpus of web data from Common Crawl (42B to- kens),1 and Word2vec (Mikolov …","url":["https://arxiv.org/pdf/1804.08845"]}
{"year":"2018","title":"Intent Generation for Goal-Oriented Dialogue Systems based on Schema. org Annotations","authors":["U Şimşek, D Fensel - arXiv preprint arXiv:1807.01292, 2018"],"snippet":"… These vectors are trained based on existing ConceptNet 5.5 knowledge graph3 data, word2vec embeddings [11] trained with 300B words from Google News Dataset and GloVe embeddings [15] trained with 840B words from …","url":["https://arxiv.org/pdf/1807.01292"]}
{"year":"2018","title":"Interactions and influence of world painters from the reduced Google matrix of Wikipedia networks","authors":["SE Zant, K Jaffrès-Runser, KM Frahm… - arXiv preprint arXiv …, 2018"],"snippet":"… At present, directed networks of real systems can be very large (about 4.2 million articles for the English Wikipedia edition in 2013 [16] or 3.5 billion web pages for a publicly accessible web crawl that was gathered by the Common Crawl Foundation in 2012 [25]) …","url":["https://arxiv.org/pdf/1807.01255"]}
{"year":"2018","title":"Investigating open data portals automatically: a methodology and some illustrations","authors":["AS Correa, PO Zander, FSC da Silva - Proceedings of the 19th Annual International …, 2018"],"snippet":"Page 1. Investigating open data portals automatically: a methodology and some illustrations Andreiwid Sheffer Correa Federal Institute of Sao Paulo Campinas, Sao Paulo, Brazil University of Sao Paulo Sao Paulo, Brazil andreiwid@ifsp.edu.br …","url":["https://dl.acm.org/citation.cfm?id=3209292"]}
{"year":"2018","title":"Iterative reasoning with bi-directional attention flow for machine comprehension","authors":["ADA Gupta"],"snippet":"… [2016] pre-trained on the common crawl dataset … 4.2 Implementation Details We used pre-trained case sensitive 300-dimensional pre-trained word embeddings trained on Common Crawl and Wikipedia using fastText …","url":["http://web.stanford.edu/class/cs224n/reports/6908250.pdf"]}
{"year":"2018","title":"Iterative Recursive Attention Model for Interpretable Sequence Classification","authors":["M Tutek, J Šnajder - arXiv preprint arXiv:1808.10503, 2018"],"snippet":"… We clip the global norm of the gradients to 1.0 and set weight decay to 0.00003. We use 300-dimensional GloVe word embeddings trained on the Common Crawl corpus and 100-dimensional character embeddings. We follow the recommendation of Mu et al …","url":["https://arxiv.org/pdf/1808.10503"]}
{"year":"2018","title":"KnowMore-Knowledge Base Augmentation with Structured Web Markup","authors":["R Yua, U Gadirajua, B Fetahua, O Lehmbergb…"],"snippet":"Page 1. Semantic Web 0 (0) 1 1 IOS Press KnowMore - Knowledge Base Augmentation with Structured Web Markup Editor(s): Name Surname, University, Country Solicited review(s): Name Surname, University, Country Open review(s): Name Surname, University, Country …","url":["http://www.semantic-web-journal.net/system/files/swj1773.pdf"]}
{"year":"2018","title":"L'optimisation des plongements de mots pour le français: une application de la classification des phrases","authors":["J Park - Actes de la conférence Traitement Automatique de la …"],"snippet":"… c ATALA 2018 283 Page 299. Wikipedia Europarl New crawls Common crawl Giga in-domain total 675M 64M 223M 89M 793M 664M 2.5 B TABLE 1–The size (# of token) of corpora for embedding 2.3 Lemma-aware …","url":["https://hal.archives-ouvertes.fr/hal-01843560/document#page=296"]}
{"year":"2018","title":"LA3: A Scalable Link-and Locality-Aware Linear Algebra-Based Graph Analytics System","authors":["Y Ahmad, O Khattab, A Malik, A Musleh, M Hammoud… - Proceedings of the VLDB …, 2018"],"snippet":"Page 1. LA3: A Scalable Linkand Locality-Aware Linear Algebra-Based Graph Analytics System Yousuf Ahmad, Omar Khattab, Arsal Malik, Ahmad Musleh, Mohammad Hammoud Carnegie Mellon University in Qatar 1myahmad …","url":["http://www.vldb.org/pvldb/vol11/p920-ahmad.pdf"]}
{"year":"2018","title":"Language Modeling at Scale","authors":["M Patwary, M Chabbi, H Jun, J Huang, G Diamos… - arXiv preprint arXiv …, 2018"],"snippet":"… The figure shows four datasets: 1-Billion word [6] (1b), Gutenberg [7] (gb), Common crawl [8] (cc), and Amazon review [9] (ar) … The 4 lines correspond to 4 datasets: one Billion word (1b), Gutenberg (gb), Common Crawl (cc), and Amazon Review (ar). TABLE I DATASETS …","url":["https://arxiv.org/pdf/1810.10045"]}
{"year":"2018","title":"Language use shapes cultural norms: Large scale evidence from gender","authors":["M Lewis, G Lupyan"],"snippet":"… Results Figure 2 shows the effect size measures derived from the English Wikipedia corpus plotted against effect size estimates reported by CBN from two different models (trained on the Common Crawl and Google News corpora) … model Common Crawl (GloVe) …","url":["http://home.uchicago.edu/~mollylewis/papers/gender_cogsci_2018.pdf"]}
{"year":"2018","title":"Large scale distributed neural network training through online distillation","authors":["AT Passos, G Pereyra, G Hinton, G Dahl, R Ormandi… - 2018","R Anil, G Pereyra, A Passos, R Ormandi, GE Dahl… - arXiv preprint arXiv …, 2018"],"snippet":"… stochastic gradient descent. We have experiments on Criteo clickthrough rate, and the largest to-date dataset used for neural language modeling, based on Common Crawl and containing $6\\times 10^{11}$ tokens. In these …","url":["https://arxiv.org/pdf/1804.03235","https://research.google.com/pubs/pub46642.html"]}
{"year":"2018","title":"Large-Scale Analysis of Style Injection by Relative Path Overwrite","authors":["S Arshad, SA Mirheidari, T Lauinger, B Crispo, E Kirda… - 2018"],"snippet":"… using RPO. We extract pages using relativepath stylesheets from the Common Crawl dataset [9], automatically test if style directives can be injected using RPO, and determine whether they are interpreted by the browser. Out …","url":["http://www.ccs.neu.edu/home/arshad/publications/www2018rpo.pdf"]}
{"year":"2018","title":"Latent Question Interpretation Through Parameter Adaptation Using Stochastic Neuron","authors":["T Parshakova, DS Kim"],"snippet":"… In effect, it led to maximally effective behaviour in the question-answering task. 6 Experiments Implementation Details For the word embeddings we use GloVe embeddings pretrained on the 840B Common Crawl corpus [Pennington et al., 2014] …","url":["http://ceur-ws.org/Vol-2134/paper07.pdf"]}
{"year":"2018","title":"Latent Semantic Analysis Approach for Document Summarization Based on Word Embeddings","authors":["K Al-Sabahi, Z Zuping, Y Kang - arXiv preprint arXiv:1807.02748, 2018"],"snippet":"… Training is performed on aggregated global word-word co-occurrence statistics from a corpus. In this work, we use the one trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip. 3.2. LSA algorithm …","url":["https://arxiv.org/pdf/1807.02748"]}
{"year":"2018","title":"LATEX GLOVES: Protecting Browser Extensions from Probing and Revelation Attacks","authors":["A Sjösten, S Van Acker, P Picazo-Sanchez, A Sabelfeld - Power, 2018"],"snippet":"Page 1. LATEX GLOVES: Protecting Browser Extensions from Probing and Revelation Attacks Alexander Sjösten∗, Steven Van Acker∗, Pablo Picazo-Sanchez and Andrei Sabelfeld Chalmers University of Technology …","url":["http://www.cse.chalmers.se/~andrei/ndss19.pdf"]}
{"year":"2018","title":"Learning Emotion-enriched Word Representations","authors":["A Agrawal, A An, M Papagelis - Proceedings of the 27th International Conference on …, 2018"],"snippet":"… GloVe 6B Wiki + Gigaword 6B tokens 400K GloVe 42B Common Crawl 42B tokens 1.9M … Paired t-tests using the results on all four datasets indicate EWE is significantly better than all the other methods with p-values < 0.02. uncased …","url":["http://www.aclweb.org/anthology/C18-1081"]}
{"year":"2018","title":"Learning Multimodal Word Representation via Dynamic Fusion Methods","authors":["S Wang, J Zhang, C Zong - arXiv preprint arXiv:1801.00532, 2018"],"snippet":"… Experimental Setup Datasets We use 300-dimensional GloVe vectors3 which are trained on the Common Crawl corpus consisting of 840B tokens and a vocabulary of 2.2M words. Our source of visual vectors are collected from ImageNet (Russakovsky et al …","url":["https://arxiv.org/pdf/1801.00532"]}
{"year":"2018","title":"Learning Neural Emotion Analysis from 100 Observations: The Surprising Effectiveness of Pre-Trained Word Representations","authors":["S Buechel, J Sedoc, HA Schwartz, L Ungar - arXiv preprint arXiv:1810.10949, 2018"],"snippet":"… For ANET, we rely on the FastText embeddings trained on Common Crawl (Mikolov et al., 2018). For ANPST and MAS, we use the FastText embeddings by Grave et al. (2018) trained on the respective Wikipedias … ANET English …","url":["https://arxiv.org/pdf/1810.10949"]}
{"year":"2018","title":"Learning Patent Speak: Investigating Domain-Specific Word Embeddings","authors":["J Risch, R Krestel"],"snippet":"… used to train word embeddings. It contains more than twice the number of tokens of the English Wikipedia (16 billion) and is only exceeded by the Common Crawl dataset, which consists of 600 billion tokens. We assume that the …","url":["https://www.researchgate.net/profile/Julian_Risch/publication/327317674_Learning_Patent_Speak_Investigating_Domain-Specific_Word_Embeddings/links/5b87ea484585151fd13c55ce/Learning-Patent-Speak-Investigating-Domain-Specific-Word-Embeddings.pdf"]}
{"year":"2018","title":"Learning representations for sentiment classification using Multi-task framework","authors":["H Meisheri, H Khadilkar - Proceedings of the 9th Workshop on Computational …, 2018"],"snippet":"… First set of embeddings are generated by using three different pre-trained embeddings. • Pre-trained embeddings which are generated from common crawl corpus have 6 Billion to- kens which help in a better encoding of the …","url":["http://www.aclweb.org/anthology/W18-6244"]}
{"year":"2018","title":"Learning Robust Joint Representations for Multimodal Sentiment Analysis","authors":["H Pham, PP Liang, T Manzini, LP Morency, B Póczos"],"snippet":"Page 1. Learning Robust Joint Representations for Multimodal Sentiment Analysis Hai Pham*, Paul Pu Liang*, Thomas Manzini, Louis-Philippe Morency, Barnabás Póczos School of Computer Science, Carnegie Mellon University …","url":["http://www.cs.cmu.edu/~pliang/papers/nips2018ws_translations.pdf"]}
{"year":"2018","title":"Learning semantic similarity in a continuous space","authors":["M Deudon - Advances in Neural Information Processing Systems, 2018"],"snippet":"… that connect those parts. Word2Vec [5] and GloVe [6] are semantic embeddings of words based on their context and frequency of co-occurence in a text corpus such as Wikipedia, Google News or Common-Crawl. The key idea …","url":["http://papers.nips.cc/paper/7377-learning-semantic-similarity-in-a-continuous-space.pdf"]}
{"year":"2018","title":"Learning to Rank Query Graphs for Complex Question Answering over Knowledge Graphs","authors":["G Maheshwari, P Trivedi, D Lukovnikov, N Chakraborty… - arXiv preprint arXiv …, 2018"],"snippet":"… All our models pe- formed poorly on QALD-7, with the best being DAM dot. 4in keeping with the train-test splits suggested by the authors. 5Trained over the common Crawl corpus. 300 dimensions, and with 1.9M tokens in the …","url":["https://arxiv.org/pdf/1811.01118"]}
{"year":"2018","title":"Learning Word Embeddings for Data Sparse and Sentiment Rich Data Sets","authors":["PK Sarma - Proceedings of the 2018 Conference of the North …, 2018"],"snippet":"… a very different distribution from generic corpora, so pre-trained generic word embeddings such as those trained on Common Crawl or Wikipedia … d2 be the matrix of generic word embeddings (obtained by, eg, running the GloVe …","url":["http://www.aclweb.org/anthology/N18-4007"]}
{"year":"2018","title":"Learning Word Meta-Embeddings by Autoencoding","authors":["C Bao, D Bollegala"],"snippet":"… (2014). The released version we used contains 1,917,494 word embeddings with dimensionality of 300 that is trained on Common Crawl dataset. The intersection of two vocabularies contains 154,076 words for which we create meta-embeddings …","url":["http://danushka.net/papers/aeme.pdf"]}
{"year":"2018","title":"Learning Word Vectors for 157 Languages","authors":["E Grave, P Bojanowski, P Gupta, A Joulin, T Mikolov - arXiv preprint arXiv …, 2018"],"snippet":"… In this work, we contribute word vectors trained on Wikipedia and the Common Crawl, as well as three new analogy datasets to evaluate these … We also show that using the common crawl data, while being noisy, can lead to models with larger coverage, and better models for …","url":["https://arxiv.org/pdf/1802.06893"]}
{"year":"2018","title":"Lecture Video Search Engine Using Hadoop MapReduce","authors":["PP Deolikar - 2017"],"snippet":"… While doing so, they dont need to bother. about crawling the data. Gil Elbaz, former Google employee funded Common CrawlCommonCrawl is implemented using Hadoop MapReduce Technology. Their strategy is to crawl. broadly and frequently, to cover all the web pages …","url":["http://search.proquest.com/openview/94314da495ab40e1ea0d1c2490d6d62a/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2018","title":"Legal Deposit Web Archives and the Digital Humanities: A Universe of Lost Opportunity?","authors":["P Gooding, M Terras, L Berube - 2018"],"snippet":"… Restricted deposit library ac- cess requires researchers to look elsewhere for portable web data: by undertaking their own web crawls, or by utilising datasets from Common Crawl (http://commoncrawl. org/) and the Internet Archive (https://archive.org) …","url":["http://eprints.gla.ac.uk/168229/1/168229.pdf"]}
{"year":"2018","title":"Lessons from Natural Language Inference in the Clinical Domain","authors":["A Romanov, C Shivade - arXiv preprint arXiv:1808.06752, 2018"],"snippet":"… fastText[Wiki]: fastText embeddings (Bo- janowski et al., 2017), trained on Wikipedia. 7http://commoncrawl.org/ Page 7. • fastText[CC]: fastText embeddings, trained on Common Crawl. Furthermore, we trained fastText …","url":["https://arxiv.org/pdf/1808.06752"]}
{"year":"2018","title":"Lessons Learned and Research Agenda for Big Data Integration of Product Specifications (Discussion Paper)","authors":["L Barbosa, V Crescenzi, XL Dong, P Merialdo, F Piai…"],"snippet":"… About 68% of the sources discovered by our approach were not present in Common Crawl … these sources were product pages: on a sample set of 12 websites where Common Crawl presented … DEXTER) is an extension …","url":["http://ceur-ws.org/Vol-2161/paper29.pdf"]}
{"year":"2018","title":"Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora","authors":["A Hazem, E Morin - Proceedings of the 27th International Conference on …, 2018"],"snippet":"… Common crawl corpus (CC) is an open repository of data collected over 7 years of web crawling sets of raw web page data and text extracts7. 4 www.sciencedirect.com/ 5 www.ttc-project.eu/index.php/releases-publications …","url":["http://www.aclweb.org/anthology/C18-1080"]}
{"year":"2018","title":"Leveraging textual properties of bug reports to localize relevant source files","authors":["R Gharibi, AH Rasekh, MH Sadreddini… - Information Processing & …, 2018"],"snippet":"Skip to main content …","url":["https://www.sciencedirect.com/science/article/pii/S0306457318301092"]}
{"year":"2018","title":"Lifelong Domain Word Embedding via Meta-Learning","authors":["H Xu, B Liu, L Shu, PS Yu - arXiv preprint arXiv:1805.09991, 2018"],"snippet":"… Page 6. GloVe.6B: This is the lower-cased embeddings pre-trained from Wikipedia and Gigaword 5, which has 6 billion tokens. GloVe.840B: This is the cased embeddings pre-trained from Common Crawl corpus, which has 840 billion tokens …","url":["https://arxiv.org/pdf/1805.09991"]}
{"year":"2018","title":"Linguistic Features of Helpfulness in Automated Support for Creative Writing","authors":["M Roemmele, AS Gordon"],"snippet":"… tween the context and suggestion in terms of their Jaccard similarity (proportion of overlapping words; Metric 8), GloVe word embeddings7 trained on the Common Crawl corpus (Metric 9), and sentence (skip-thought) …","url":["http://people.ict.usc.edu/~gordon/publications/NAACL-WS18B.PDF"]}
{"year":"2018","title":"List Intersection for Web Search: Algorithms, Cost Models, and Optimizations","authors":["S Kim, T Lee, S Hwang, S Elnikety - Proceedings of the VLDB Endowment"],"snippet":"Page 1. List Intersection for Web Search: Algorithms, Cost Models, and Optimizations Sunghwan Kim POSTECH sunghwan08@gmail.com Taesung Lee IBM Research AI Taesung.Lee@ibm.com Seung-won …","url":["http://www.vldb.org/pvldb/vol12/p1-kim.pdf"]}
{"year":"2018","title":"LMU Munich's Neural Machine Translation Systems at WMT 2018","authors":["M Huck, D Stojanovski, V Hangya, A Fraser - Proceedings of the Third Conference on …, 2018"],"snippet":"… BLEU (Pa- pineni et al., 2002)) of different system setups after several development stages is presented in the top section of Table 1. WMT_parallel de- notes the Europarl, News Commentary, and Common Crawl parallel …","url":["http://www.aclweb.org/anthology/W18-6446"]}
{"year":"2018","title":"Local Homology of Word Embeddings","authors":["T Temčinas - arXiv preprint arXiv:1810.10136, 2018"],"snippet":"Page 1. Local Homology of Word Embeddings Tadas Temcinas 1 Abstract Topological data analysis (TDA) has been widely used to make progress on a number of problems. However, it seems that TDA application in natural …","url":["https://arxiv.org/pdf/1810.10136"]}
{"year":"2018","title":"Local-Global Graph Clustering with Applications in Sense and Frame Induction","authors":["D Ustalov, A Panchenko, C Biemann, SP Ponzetto - arXiv preprint arXiv:1808.06696, 2018"],"snippet":"Page 1. Local-Global Graph Clustering with Applications in Sense and Frame Induction Dmitry Ustalov∗ University of Mannheim Alexander Panchenko∗∗ University of Hamburg Chris Biemann University of Hamburg Simone Paolo Ponzetto University of Mannheim …","url":["https://arxiv.org/pdf/1808.06696"]}
{"year":"2018","title":"Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion","authors":["A Joulin, P Bojanowski, T Mikolov, H Jégou, E Grave - Proceedings of the 2018 …, 2018"],"snippet":"… word vectors of Mikolov et al. (2018) to Spanish. It leads to an improvement of 1% both for vectors trained on Common Crawl (85% to 86%) and Wikipedia + News (87% to 88%). 5 Conclusion This paper shows that minimizing …","url":["http://www.aclweb.org/anthology/D18-1330"]}
{"year":"2018","title":"M1. 2–Corpora for the Machine Translation Engines","authors":["C Espana-Bonet, J Stiller, S Henning - 2018"],"snippet":"… Table 5: Size of the parallel corpora obtained from the different sources described in Section 2.1: United Nations (UN), Europarl V7 (EP), Common Crawl (ComCrawl), EMEA and Scielo. Figures of the PubPsych corpus are …","url":["https://www.clubs-project.eu/assets/publications/project/CLUBS_MTcorpora.v3.pdf"]}
{"year":"2018","title":"MAC: Mining Activity Concepts for Language-based Temporal Localization","authors":["R Ge, J Gao, K Chen, R Nevatia - arXiv preprint arXiv:1811.08925, 2018"],"snippet":"… We only keep the twoword tuple as our VO. ie open door, wash hands. After lemmatizing every word, we use the GloVe [22] word en- coder which is pre-trained on the Common Crawl data to get each word a 300-dimensional word embedding …","url":["https://arxiv.org/pdf/1811.08925"]}
{"year":"2018","title":"Machine Learning with and for Semantic Web Knowledge Graphs","authors":["H Paulheim - Reasoning Web International Summer School, 2018"],"snippet":"… To that end, it scans the Common Crawl Web corpus 20 for so-called Hearst patterns, eg X such as Y, to infer relations like X skos:broader Y. The graph comes with very detailed provenance data, including the patterns used …","url":["https://link.springer.com/chapter/10.1007/978-3-030-00338-8_5"]}
{"year":"2018","title":"Machine Reading Comprehension on SQuAD","authors":["DMT Tan"],"snippet":"… The end-to-end neural architecture consists of the following components. Page 3. Input vectors. For each word in context (C) and question (Q), we use the 300-dim GloVe embeddings pretrained on the 840B Common Crawl corpus [5] as input vectors …","url":["http://web.stanford.edu/class/cs224n/reports/6909266.pdf"]}
{"year":"2018","title":"MAJE Submission to the WMT2018 Shared Task on Parallel Corpus Filtering","authors":["M Fomicheva, J González-Rubio - Proceedings of the Third Conference on Machine …, 2018"],"snippet":"… We also conducted some initial experiments us- ing the Common Crawl corpus, under the rationale that it would be closer to the domain of the noisy data from the Paracrawl corpus. However, Common Crawl data …","url":["http://www.aclweb.org/anthology/W18-6476"]}
{"year":"2018","title":"Making Caches Work for Graph Analytics","authors":["Y Zhang, V Kiriansky, C Mendis, S Amarasinghe…"],"snippet":"… The system has 128GB of DDR3-1600 memory. The machine runs with Transparent Huge Pages (THP) enabled. Data Sets: We used the social networks, including LiveJournal [10] and Twitter [8], the SD web graph from 2012 common crawl [11], and the RMAT graphs …","url":["http://groups.csail.mit.edu/commit/papers/2017/zhang-bigdata17-cagra.pdf"]}
{"year":"2018","title":"Manifold Scholarship: Hybrid Publishing in a Print/Digital Era","authors":["MK Gold, J Karlin, Z Davis, P Gooding, M Terras… - Digital Humanities 2018 …, 2018"],"snippet":"… Restricted deposit library ac- cess requires researchers to look elsewhere for portable web data: by undertaking their own web crawls, or by utilising datasets from Common Crawl (http://commoncrawl. org/) and the Internet Archive (https://archive. org) …","url":["https://pdfs.semanticscholar.org/7cf9/f2db34ac1b5791de374b78b2fdbe97dc27af.pdf#page=590"]}
{"year":"2018","title":"Manual vs Automatic Bitext Extraction","authors":["A Makazhanov, B Myrzakhmetov, Z Assylbekov - Proceedings of the Eleventh …, 2018"],"snippet":"… Biometrika, 52(3/4):591–611. Smith, JR, Saint-Amand, H., Plamada, M., Koehn, P., Callison-Burch, C., and Lopez, A. (2013). Dirt cheap web-scale parallel text from the common crawl. In ACL (1), pages 1374–1383. Tiedemann, J. (2007) …","url":["http://www.aclweb.org/anthology/L18-1606"]}
{"year":"2018","title":"Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings","authors":["M Artetxe, H Schwenk - arXiv preprint arXiv:1811.01136, 2018"],"snippet":"Page 1. Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings Mikel Artetxe University of the Basque Country (UPV/EHU)∗ mikel.artetxe@ehu.eus Holger Schwenk Facebook AI Research schwenk@fb.com Abstract …","url":["https://arxiv.org/pdf/1811.01136"]}
{"year":"2018","title":"MEADE: Towards a Malicious Email Attachment Detection Engine","authors":["EM Rudd, R Harang, J Saxe - arXiv preprint arXiv:1804.08162, 2018"],"snippet":"… To this end, we collected a dataset of over 5 million malicious/benign Microsoft Office documents from VirusTotal for evaluation as well as a dataset of benign Microsoft Office documents from the Common Crawl …","url":["https://arxiv.org/pdf/1804.08162"]}
{"year":"2018","title":"Measuring Semantic Coherence of a Conversation","authors":["S Vakulenko, M de Rijke, M Cochez, V Savenkov… - arXiv preprint arXiv …, 2018"],"snippet":"… We utilise two publicly available word embedding models: GloVe embeddings pretrained on the Common Crawl corpus (2.2M words, 300 dimensions)14 and Word2Vec model trained on the Google News corpus …","url":["https://arxiv.org/pdf/1806.06411"]}
{"year":"2018","title":"Measuring the Effects of Data Parallelism on Neural Network Training","authors":["CJ Shallue, J Lee, J Antognini, J Sohl-Dickstein… - arXiv preprint arXiv …, 2018"],"snippet":"… For LM1B, we chose the Transformer model and an LSTM model. For Common Crawl, we chose the Transformer model … Common Crawl Text Language modeling ∼25.8 billion sentences Cross entropy error Table …","url":["https://arxiv.org/pdf/1811.03600"]}
{"year":"2018","title":"Meet the Data","authors":["K Balog - Entity-Oriented Search, 2018"],"snippet":"… Size refers to compressed data ahttps://lemurproject.org/clueweb09/ bhttps://lemurproject.org/clueweb12/ chttp://commoncrawl.org/2017/06/may … Common Crawl Common Crawl5 is a nonprofit organization that regularly …","url":["https://link.springer.com/content/pdf/10.1007/978-3-319-93935-3_2.pdf"]}
{"year":"2018","title":"Memory Fusion Network for Multi-view Sequential Learning","authors":["A Zadeh, PP Liang, N Mazumder, S Poria, E Cambria… - arXiv preprint arXiv …, 2018"],"snippet":"… The Glove embeddings used are 300 dimensional word embeddings trained on 840 billion tokens from the common crawl dataset, resulting in a sequence of dimension T × dxtext = T × 300 after alignment. The timing of word utterances is extracted using P2FA forced aligner …","url":["https://arxiv.org/pdf/1802.00927"]}
{"year":"2018","title":"Merging Search Results Generated by Multiple Query Variants Using Data Fusion","authors":["N Motlogelwa, T Leburu-Dingalo, E Thuma - CEUR Workshop Proceedings: Working …, 2018"],"snippet":"… In Section 3.2 we describe the queries used for retrieval. 3.1 Document Collection “The document collection used in CLEF 2018 consists of web pages acquired from the CommonCrawl. An initial list of websites was identified for acquisition …","url":["http://ceur-ws.org/Vol-2125/paper_194.pdf"]}
{"year":"2018","title":"METHODS AND SYSTEMS FOR QUERY SEGMENTATION","authors":["AG Kale, T Taula, A Srivastava, S Hewavitharana - US Patent App. 15/681,663, 2018"],"snippet":"… This process may be repeated but using pre-trained GloVe vectors on common crawl and facebook fasttext pretrained model over a Wikipedia corpus with 2.5 M word vocabulary. Table 2 shows the experimental results of this analysis …","url":["http://www.freepatentsonline.com/y2018/0329999.html"]}
{"year":"2018","title":"microNER: A Micro-Service for German Named Entity Recognition based on BiLSTM-CRF","authors":["G Wiedemann, R Jindal, C Biemann - arXiv preprint arXiv:1811.02902, 2018"],"snippet":"… It is rather determined by the fact that it was trained on several billion tokens of the Common Crawl corpus.2 To evaluate the contribution of different German word … 2http://commoncrawl org 3For fastText, we use the publicly available German model from Bojanowski et al …","url":["https://arxiv.org/pdf/1811.02902"]}
{"year":"2018","title":"Mining Training Data for Language Modeling Across the World's Languages","authors":["M Prasad, T Breiner, D van Esch - 2018"],"snippet":"… 64 Page 5. [16] Common crawlCommon Crawl Foundation. Ac- cessed: June 4, 2018. [Online]. Available: https://www.commoncrawl.org [17] UL Abteilung Automatische Sprachverarbeitung. Corpora collection. Deutscher …","url":["https://www.isca-speech.org/archive/SLTU_2018/pdfs/Manasa.pdf"]}
{"year":"2018","title":"Miracl at Clef 2018: Consumer Health Search Task","authors":["S ZAYANI, N KSENTINI, M TMAR, F GARGOURI - CEUR Workshop Proceedings …, 2018"],"snippet":"… concepts. Page 6. 4 Resources employed 4.1 Datasets The document collection used in CLEFeHealth 2018 is composed of web pages acquired from the CommonCrawl An initial list of websites was identified for acquisition. The …","url":["http://ceur-ws.org/Vol-2125/paper_141.pdf"]}
{"year":"2018","title":"MIsA: Multilingual\" IsA\" Extraction from Corpora","authors":["S Faralli, E Lefever, S Paolo Ponzetto - the Eleventh International Conference on …, 2018"],"snippet":"… 2016). Our MIsA is an extension of the WebIsADb framework (Seitner et al., 2016) - a publicly available database with more than 400 million English hypernymy relations ex- tracted from the CommonCrawl web corpus - where: 1 …","url":["https://biblio.ugent.be/publication/8562721/file/8562722"]}
{"year":"2018","title":"Mitigating Bottlenecks in Wide Area Data Analytics via Machine Learning","authors":["H Wang, B Li - IEEE Transactions on Network Science and …, 2018"],"snippet":"Page 1. 2327-4697 (c) 2018 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["http://ieeexplore.ieee.org/abstract/document/8319505/"]}
{"year":"2018","title":"Mixture of Expert/Imitator Networks: Scalable Semi-supervised Learning Framework","authors":["S Kiyono, J Suzuki, K Inui - arXiv preprint arXiv:1810.05788, 2018"],"snippet":"… in DNNs has been gaining the performance. For example, several studies have utilized unlabeled 1http://commoncrawl.org arXiv:1810.05788v1 [cs.CL] 13 Oct 2018 Page 2. data as additional training data, which essentially …","url":["https://arxiv.org/pdf/1810.05788"]}
{"year":"2018","title":"Modeling Empathy and Distress in Reaction to News Stories","authors":["S Buechel, A Buffone, B Slaff, L Ungar, J Sedoc - arXiv preprint arXiv:1808.10399, 2018"],"snippet":"… problems. The input to our models is based on word embeddings, namely the publicly available FastText embeddings which were trained on Common Crawl (≈600B tokens) (Bojanowski et al., 2017; Mikolov et al., 2018). Ridge …","url":["https://arxiv.org/pdf/1808.10399"]}
{"year":"2018","title":"Modeling voice calls to improve an outcome of a call between a representative and a customer","authors":["R Raanani, R Levy, MY Breakstone - US Patent App. 16/017,646, 2018","R Raanani, R Levy, MY Breakstone - US Patent App. 16/689,688, 2020"],"snippet":"… language processing (NLP) approaches to both topic modeling and i Insidesales.com “Market size 2013” study world-knowledge modeling, have become much more efficient due to the availability of large, freely accessible …","url":["https://patentimages.storage.googleapis.com/89/79/27/c037a0d312fd5d/US20180309873A1.pdf","https://patents.google.com/patent/US20200092420A1/en"]}
{"year":"2018","title":"Multi-task Projected Embedding for Igbo","authors":["I Ezeani, M Hepple, I Onyenwe, C Enemuo - International Workshop on Temporal …, 2018"],"snippet":"… igSwproj from same as igWkproj but with subword information. igCrlproj from fastText Common Crawl dataset. Table 3 shows the vocabulary lengths (\\(Vocabs^L\\)), and the dimensions (Dimension) of each of the models used in our experiments …","url":["https://link.springer.com/chapter/10.1007/978-3-030-00794-2_31"]}
{"year":"2018","title":"Multi-turn QA: A RNN Contextual Approach to Intent Classification for Goal-oriented Systems","authors":["M Mensio, G Rizzo, M Morisio - Companion of the The Web Conference 2018 on The …, 2018"],"snippet":"… 8. Christian Buck, Kenneth Heafield, and Bas van Ooyen. 2014. N-gram Counts and Language Models from the Common Crawl Proceedings of the Language Resources and Evaluation Conference (LREC), Vol. Vol. 2. Citeseer, Reykjavik, Iceland, 4 …","url":["https://dl.acm.org/citation.cfm?id=3191539"]}
{"year":"2018","title":"Multilingual word embeddings and their utility in cross-lingual learning","authors":["A Kulmizev - 2018"],"snippet":"… language corpus. When said corpus is large enough (eg Wikipedia, Common Crawl, or the concatenation of the two), the resulting DSM can be assumed to represent the distributional semantics of an entire language. The algorithms …","url":["https://addi.ehu.es/bitstream/handle/10810/29083/TFM_Artur_Kulmizev.pdf?sequence=1"]}
{"year":"2018","title":"Multimodal Language Analysis with Recurrent Multistage Fusion","authors":["PP Liang, Z Liu, A Zadeh, LP Morency - arXiv preprint arXiv:1808.03920, 2018"],"snippet":"Page 1. Multimodal Language Analysis with Recurrent Multistage Fusion Paul Pu Liang1, Ziyin Liu2, Amir Zadeh2, Louis-Philippe Morency2 1Machine Learning Department, 2Language Technologies Institute Carnegie Mellon …","url":["https://arxiv.org/pdf/1808.03920"]}
{"year":"2018","title":"Multimodal Language Analysis with Recurrent Multistage Fusion: Supplementary Material","authors":["PP Liang, Z Liu, A Zadeh, LP Morency"],"snippet":"… 1.1 Multimodal Features Here we present extra details on feature extraction for the language, visual and acoustic modalities. Language: We used 300 dimensional Glove word embeddings trained on 840 billion tokens from …","url":["http://www.cs.cmu.edu/~pliang/papers/emnlp2018-recurrent-fusion-supp.pdf"]}
{"year":"2018","title":"Natural language processing using a neural network","authors":["B McCann, C Xiong, R Socher - US Patent App. 16/000,638, 2018"],"snippet":"… in the second language. In some examples, training of an MT-LSTM of the encoder 310 uses fixed 300-dimensional word vectors, such as the CommonCrawl-840B GloVe model for English word vectors. These word vectors …","url":["https://patentimages.storage.googleapis.com/f0/42/0e/084fa3f0799a39/US20180349359A1.pdf"]}
{"year":"2018","title":"Navigating Online Semantic Resources for Entity Set Expansion","authors":["WT Adrian, M Manna - International Symposium on Practical Aspects of …, 2018"],"snippet":"… the given entry (synset) in BabelNet. WebIsADatabase [25] is a publicly available database containing more than 400 million hypernymy relations extracted from the CommonCrawl web corpus. The tuples of the database are …","url":["https://link.springer.com/chapter/10.1007/978-3-319-73305-0_12"]}
{"year":"2018","title":"Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation","authors":["R Grundkiewicz, M Junczys-Dowmunt - arXiv preprint arXiv:1804.05945, 2018"],"snippet":"… ops). All systems use a 5-gram Language Model (LM) and OSM (Durrani et al., 2011) both estimated from the target side of the training data, and a 5-gram LM and 9-gram WCLM trained on Common Crawl data (Buck et al., 2014) …","url":["https://arxiv.org/pdf/1804.05945"]}
{"year":"2018","title":"Neural Adaptation Layers for Cross-domain Named Entity Recognition","authors":["BY Lin, W Lu - arXiv preprint arXiv:1810.06368, 2018"],"snippet":"… ter stream grab4); 3) general emb, which is pretrained on CommonCrawl containing both formal and user-generated content.5 We obtain the intersection of the top 5K words from source and target vocabularies sorted by frequency to build P1 …","url":["https://arxiv.org/pdf/1810.06368"]}
{"year":"2018","title":"Neural Cross-Lingual Named Entity Recognition with Minimal Resources","authors":["J Xie, Z Yang, G Neubig, NA Smith, J Carbonell - arXiv preprint arXiv:1808.09861, 2018"],"snippet":"Page 1. Neural Cross-Lingual Named Entity Recognition with Minimal Resources Jiateng Xie,1 Zhilin Yang,1 Graham Neubig,1 Noah A. Smith,2,3 and Jaime Carbonell1 1Language Technologies Institute, Carnegie Mellon University …","url":["https://arxiv.org/pdf/1808.09861"]}
{"year":"2018","title":"Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss","authors":["P Xu, D Barbosa - arXiv preprint arXiv:1803.03378, 2018"],"snippet":"… appearing in the training set. For this purpose, we used the freely available 300-dimensional cased word embedding trained on 840 billion tokens from the Common Crawl supplied by Pennington et al. (2014). For both datasets, we …","url":["https://arxiv.org/pdf/1803.03378"]}
{"year":"2018","title":"Neural Learning for Question Answering in Italian","authors":["D Croce, A Zelenanska, R Basili - International Conference of the Italian Association …, 2018"],"snippet":"… In [5], both the paragraph and the question encodings use \\(d=128\\) hidden units. The best performing system reported in [5] uses GloVe embeddings [14] trained on Common Crawl of dimension \\(n=300\\) with more than 2 millions of tokens …","url":["https://link.springer.com/chapter/10.1007/978-3-030-03840-3_29"]}
{"year":"2018","title":"Neural Machine Translation with Decoding History Enhanced Attention","authors":["M Wang, J Xie, Z Tan, J Su, D Xiong, C Bian - … of the 27th International Conference on …, 2018"],"snippet":"… translation are presented in Table 2. We compare our NMT systems with various other systems including the winning system in WMT14 (Buck et al., 2014), a phrase-based system whose language models were trained on …","url":["http://www.aclweb.org/anthology/C18-1124"]}
{"year":"2018","title":"Neural models of factuality","authors":["R Rudinger, AS White, B Van Durme - arXiv preprint arXiv:1804.02472, 2018"],"snippet":"Page 1. Neural models of factuality Rachel Rudinger Johns Hopkins University Aaron Steven White University of Rochester Benjamin Van Durme Johns Hopkins University Abstract We present two neural models for event fac …","url":["https://arxiv.org/pdf/1804.02472"]}
{"year":"2018","title":"NEURAL NETWORKS FOR NARRATIVE CONTINUATION","authors":["M Roemmele - 2018"],"snippet":"Page 1. NEURAL NETWORKS FOR NARRATIVE CONTINUATION by Melissa Roemmele A Ph.D. Dissertation Presented to the FACULTY OF THE GRADUATE SCHOOL UNIVERSITY OF SOUTHERN CALIFORNIA …","url":["http://people.ict.usc.edu/~roemmele/publications/dissertation.pdf"]}
{"year":"2018","title":"Neural Networks for Semantic Textual Similarity","authors":["DS Prijatelj, J Ventura, J Kalita"],"snippet":"… tors. This specific set of word vectors have 300 dimensions and were pre-trained on 840 billion tokens taken from Common Crawl3. Different pre-trained word vectors may be used in place of this specific pre-trained set. After …","url":["http://cs.uccs.edu/~jkalita/papers/2017/DerekPrijateljICON2017.pdf"]}
{"year":"2018","title":"Neural Quality Estimation of Grammatical Error Correction","authors":["S Chollampatt, HT Ng - Proceedings of the 2018 Conference on Empirical …, 2018"],"snippet":"Page 1. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2528–2539 Brussels, Belgium, October 31 - November 4, 2018. c 2018 Association for Computational Linguistics 2528 …","url":["http://www.aclweb.org/anthology/D18-1274"]}
{"year":"2018","title":"Next Utterance Ranking Based On Context Response Similarity","authors":["BEA Boussaha, N Hernandez, C Jacquin, E Morin"],"snippet":"… E. Parameter tuning Word embeddings were initialized with Glove [21] pretrained on Common Crawl Corpus4 then fine tuned during training5 … 4http://commoncrawl.org/the-data/ 5Note that we trained word embeddings …","url":["https://basma-b.github.io/assets/publications/nextUtterance/paper.pdf"]}
{"year":"2018","title":"NICT's Neural and Statistical Machine Translation Systems for the WMT18 News Translation Task","authors":["B Marie, R Wang, A Fujita, M Utiyama, E Sumita - arXiv preprint arXiv:1809.07037, 2018"],"snippet":"… As English monolingual data, we used all the available data except the “Common Crawl” and “News Discussions” corpora.2 For all other languages, we used all the available monolingual corpora, except for Turkish for which we …","url":["https://arxiv.org/pdf/1809.07037"]}
{"year":"2018","title":"NL-FIIT at IEST-2018: Emotion Recognition utilizing Neural Networks and Multi-level Preprocessing","authors":["S Pecar, M Farkaš, M Simko, P Lacko, M Bielikova - Proceedings of the 9th Workshop …, 2018"],"snippet":"… input words, we used various pre-trained word embeddings available online like GloVe em- beddings1. With GloVe embeddings, we experimented with domain specific embeddings trained on Twitter data but also with embeddings …","url":["http://www.aclweb.org/anthology/W18-6231"]}
{"year":"2018","title":"Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction","authors":["Z Xie, G Genthial, S Xie, A Ng, D Jurafsky - Proceedings of the 2018 Conference of …, 2018"],"snippet":"… h) (Wu et al., 2016). The final scoring function also incorporates a 5-gram language model trained on a subset of Common Crawl, estimated with Kneser-Ney smoothing using KenLM (Heafield, 2011). We in- corporate the …","url":["http://www.aclweb.org/anthology/N18-1057"]}
{"year":"2018","title":"Not just about size-A Study on the Role of Distributed Word Representations in the Analysis of Scientific Publications","authors":["A Garcia, JM Gomez-Perez - arXiv preprint arXiv:1804.01772, 2018","A Garcia-Silva, JM Gomez-Perez"],"snippet":"… our evaluation task, embeddings from a scientific publication corpus consistently generate classifiers with a top performance that is only matched by classifiers learned from embeddings from very large document corpora …","url":["http://ceur-ws.org/Vol-2106/paper3.pdf","https://arxiv.org/pdf/1804.01772"]}
{"year":"2018","title":"Novelty Goes Deep. A Deep Neural Solution To Document Level Novelty Detection","authors":["T Ghosal, V Edithal, A Ekbal, P Bhattacharyya…"],"snippet":"… Training is done with a mini-batch size of 64. GloVe vectors trained on Common Crawl 840B (https://nlp.stanford.edu/projects/glove/) with 300 dimensions are used as fixed word embeddings. 7SNLI has 3 classes of …","url":["https://www.cse.iitb.ac.in/~pb/papers/coling18-novelty.pdf"]}
{"year":"2018","title":"NTT's Neural Machine Translation Systems for WMT 2018","authors":["M Morishita, J Suzuki, M Nagata - Proceedings of the Third Conference on Machine …, 2018"],"snippet":"… This year's submission includes the following features: • Noisy data filtering for Common Crawl and ParaCrawl corpora (Section 3.1) … This year, ParaCrawl and Common Crawl corpora, which were created by crawling parallel websites, were provided for training …","url":["http://www.aclweb.org/anthology/W18-6421"]}
{"year":"2018","title":"NTU NLP Lab System at SemEval-2018 Task 10: Verifying Semantic Differences by Integrating Distributional Information and Expert Knowledge","authors":["YT Shiue, HH Huang, HH Chen - Proceedings of The 12th International Workshop on …, 2018"],"snippet":"… al., 2017). We use the pre-trained embeddings of English concepts3. 4. GloVe(Common Crawl): The GloVe model (Pennington et al., 2014) obtains word representation according to global co-occurrence statistics. We use …","url":["http://www.aclweb.org/anthology/S18-1171"]}
{"year":"2018","title":"On Link Stability Detection for Online Social Networks","authors":["J Zhang, X Tao, L Tan, JCW Lin, H Li, L Chang - International Conference on …, 2018"],"snippet":"… Since the social network we obtain from the repositories of common crawl contains missing links and partial information, stochastic estimations are used to measure the accuracy and reliability of our experimental MVVA results [12] …","url":["https://link.springer.com/chapter/10.1007/978-3-319-98809-2_20"]}
{"year":"2018","title":"On NDN and (“lack of”) Measurement","authors":["T Silverston"],"snippet":"… URLs Dataset • Web Crawling of 7 main organizations – Amazon, Ask, Stackoverflow, BBC, CNN, Google, Yahoo – Common Crawl Data Set repository • 1.73B URLs -> 7M for each organization /(Organization) …","url":["https://pdfs.semanticscholar.org/presentation/08d7/0e3f0b27b03a5f99f22bfeebeafa47c9bbb7.pdf"]}
{"year":"2018","title":"On the Compressed Sensing Properties of Word Embeddings","authors":["M Khodak - 2018"],"snippet":"… word2vec embeddings trained on Google News and GloVe vectors trained on Common Crawl were obtained from public repositories [20, 23] while Amazon and Wikipedia embeddings were trained for 100 iterations using …","url":["ftp://ftp.cs.princeton.edu/techreports/2018/008.pdf"]}
{"year":"2018","title":"On the Design and Tuning of Machine Learning Models for Language Toxicity Classification in Online Platforms","authors":["M Rybinski, W Miller, J Del Ser, MN Bilbao… - International Symposium on …, 2018"],"snippet":"… For (B) we have used vectors pre-trained on Common Crawl corpus with GloVe algorithm [9]. A more complete description of the text representations involved in our experiments is given below … For pre-trained embeddings …","url":["https://link.springer.com/chapter/10.1007/978-3-319-99626-4_29"]}
{"year":"2018","title":"Ontology Augmentation Through Matching with Web Tables","authors":["O Lehmberg, O Hassanzadeh"],"snippet":"… 3 http://commoncrawl.org … The used PageRank values are obtained from the publicly available Common Crawl WWW Ranking.4 For each partition of columns, we use the maximum PageRank of all source web pages and …","url":["http://disi.unitn.it/~pavel/om2018/papers/om2018_LTpaper4.pdf"]}
{"year":"2018","title":"Ontology Driven Extraction of Research Processes","authors":["V Pertsas, P Constantopoulos, I Androutsopoulos"],"snippet":"… Our experiments with other general-purpose, publicly available embeddings, such as those trained on the Common Crawl corpus using GloVe9, or those trained on Wikipedia articles with word2vec, showed inferior performance …","url":["http://www2.aueb.gr/users/ion/docs/iswc2018.pdf"]}
{"year":"2018","title":"Open Bibliometrics and Undiscovered Public Knowledge","authors":["D Stuart - Online Information Review, 2018"],"snippet":"… Whether altmetrics is really any more open than traditional citation analysis is a matter of debate, although services such as Common Crawl (http://commoncrawl.org), an open repository of web crawl data, provides …","url":["https://www.emeraldinsight.com/doi/abs/10.1108/OIR-07-2017-0209"]}
{"year":"2018","title":"OpenSeq2Seq: extensible toolkit for distributed and mixed precision training of sequence-to-sequence models","authors":["O Kuchaiev, B Ginsburg, I Gitman, V Lavrukhin, C Case… - arXiv preprint arXiv …, 2018"],"snippet":"… training). In our experiments, we used WMT 2016 English→German data set obtained by combining the Europarlv7, News Commentary v10, and Common Crawl corpora and resulting in roughly 4.5 million sentence pairs …","url":["https://arxiv.org/pdf/1805.10387"]}
{"year":"2018","title":"Optimizing Automatic Evaluation of Machine Translation with the ListMLE Approach","authors":["M Li, M Wang - ACM Transactions on Asian and Low-Resource …, 2018"],"snippet":"… Bilingual parallel data comprising Europarl v7, Common Crawl corpus, and News Commentary v10, released by the WMT'2015 Machine Translation Shared Task [39] were employed to train the bidirectional lexical translation probability …","url":["https://dl.acm.org/citation.cfm?id=3226045"]}
{"year":"2018","title":"Out-of-Distribution Detection using Multiple Semantic Label Representations","authors":["G Shalev, Y Adi, J Keshet - arXiv preprint arXiv:1808.06664, 2018"],"snippet":"… respectively. The third and forth representations were based on GloVe [36], where the third one was trained using both Wikipedia corpus and Gigawords [34] dataset, the fourth was trained using Common Crawl dataset. The …","url":["https://arxiv.org/pdf/1808.06664"]}
{"year":"2018","title":"Pangloss: Fast Entity Linking in Noisy Text Environments","authors":["M Conover, M Hayes, S Blackburn, P Skomoroch… - arXiv preprint arXiv …, 2018"],"snippet":"… For each surface form Pangloss calls a RocksDB key-value store to retrieve candidate entries (represented by circles) based on associations between hyperlink anchor text and Wikipedia URLs in Wikipedia and Common Crawl (Section 3.5) … 3.3.4 Common Crawl …","url":["https://arxiv.org/pdf/1807.06036"]}
{"year":"2018","title":"ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations","authors":["J Wieting, K Gimpel - Proceedings of the 56th Annual Meeting of the …, 2018"],"snippet":"… Dataset Avg. Length Avg. IDF Avg. Para. Score Vocab. Entropy Parse Entropy Total Size Common Crawl 24.0±34.7 7.7±1.1 0.83±0.16 7.2 3.5 0.16M … (2017). Its training data includes four sources: Common Crawl, CzEng 1.6 (Bojar …","url":["http://www.aclweb.org/anthology/P18-1042"]}
{"year":"2018","title":"Periodizing Web Archiving: Biographical, Event-Based, National and Autobiographical Traditions","authors":["R Rogers"],"snippet":"Page 1. Periodizing Web Archiving: Biographical, Event-Based, National and Autobiographical Traditions Richard Rogers INTRODUCTION: HISTORIOGRAPHIES BUILT INTO WEB ARCHIVES The purpose of this chapter is …","url":["https://www.researchgate.net/profile/Richard_Rogers13/publication/327403018_Periodizing_Web_Archiving_Biographical_Event-Based_National_and_Autobiographical_Traditions/links/5b8d511e299bf114b7eeea4e/Periodizing-Web-Archiving-Biographical-Event-Based-National-and-Autobiographical-Traditions.pdf"]}
{"year":"2018","title":"Phrase-Level Metaphor Identification using Distributed Representations of Word Meaning","authors":["O Zayed, JP McCrae, P Buitelaar - NAACL HLT 2018, 2018"],"snippet":"… 10. –GloVe Common Crawl5: We used a pretrained model on the Common Crawl dataset containing 840 billion tokens of web data (about 2 million words). The vectors are 300dimensional using 100 training iteration. For …","url":["http://www.cl.cam.ac.uk/~es407/papers/Fig-Lang2018-proceedings.pdf#page=93"]}
{"year":"2018","title":"Phrase-level Self-Attention Networks for Universal Sentence Encoding","authors":["W Wu, H Wang, T Liu, S Ma - Proceedings of the 2018 Conference on Empirical …, 2018"],"snippet":"… and sentence textual similarity. 3.1 Model Configuration 300-dimensional GloVe (Pennington et al., 2014) word embeddings (Common Crawl, uncased) are used to represent words. Following Parikh et al. (2016), out-of-vocabulary …","url":["http://www.aclweb.org/anthology/D18-1408"]}
{"year":"2018","title":"Platypus–A Multilingual Question Answering Platform for Wikidata","authors":["TP Tanon, MD de Assunçao, E Caron, FM Suchanek"],"snippet":"… The template analyzer is implemented using RasaNLU [34]. We used the Glove [35] word vectors trained on Common Crawl provided by Spacy and the RasaNLU entity extractor based on the CRFsuite library [36]. Our system can be accessed in three ways …","url":["https://2018.eswc-conferences.org/wp-content/uploads/2018/02/ESWC2018_paper_130.pdf"]}
{"year":"2018","title":"Pointer-CNN for Visual Question Answering","authors":["J Svidt, JS Jepsen - 2018"],"snippet":"Page 1. Pointer-CNN for Visual Question Answering Jakob Svidt Aalborg University jsvidt13@student.aau.dk Jens Søholm Jepsen Aalborg University jjepse12@student. aau June 14, 2018 Abstract Visual Question Answering …","url":["https://projekter.aau.dk/projekter/files/281551620/SvidtJepsen.pdf"]}
{"year":"2018","title":"Pooling Word Vector Representations Across Models","authors":["AC Graesser, V Rus - … Linguistics and Intelligent Text Processing: 18th …"],"snippet":"… The model was trained on non-zero elements in a global word co-occurrence matrix. We used the pre-trained model GloVe-42B which was trained on 42 billion words of Common Crawl corpus and it contains about 1.9 million unique tokens …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=dDNyDwAAQBAJ&oi=fnd&pg=PA17&dq=commoncrawl&ots=qmDDIrUYu6&sig=KN6CvMWGvERnEsXe5NqWJlRnqnM"]}
{"year":"2018","title":"Predicting and Generating Discussion Inspiring","authors":["YJWJ Park"],"snippet":"… response time before being presented to the human. • An LSTM model [2] [4] using pretrained 300 dimensional GloVe word embeddings [5] on Common Crawl to embed the comments. A self-attention layer was added on top …","url":["http://web.stanford.edu/class/cs224n/reports/6879446.pdf"]}
{"year":"2018","title":"Predicting Company Ratings through Glassdoor","authors":["TE Whittle"],"snippet":"… This dataset has 300-dimnesional vectors for 3 million words and phrases. The GloVe pre-trained embeddings come from a model trained on 42 billion tokens encountered by Common Crawl, a program designed to crawl the web and extract text …","url":["http://web.stanford.edu/class/cs224n/reports/6880837.pdf"]}
{"year":"2018","title":"Predictive Embeddings for Hate Speech Detection on Twitter","authors":["R Kshirsagar, T Cukuvac, K McKeown, S McGregor - arXiv preprint arXiv:1809.10644, 2018"],"snippet":"… 5 Experimental Setup We tokenize the data using Spacy (HonnibalandJohnson, 2015). We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) (Pennington et al., 2014) and fine tune them for the task. We …","url":["https://arxiv.org/pdf/1809.10644"]}
{"year":"2018","title":"Preferred Answer Selection in Stack Overflow: Better Text Representations... and Metadata, Metadata, Metadata","authors":["X Xu, A Bennett, D Hoogeveen, JH Lau, T Baldwin"],"snippet":"… Word embeddings were set to 150 di- mensions. The co-occurrence weighting function's maximum value xmax was kept at the default of 10. For SEMEVAL, we used pretrained Common Crawl cased embeddings with 840G tokens …","url":["http://noisy-text.github.io/2018/pdf/W-NUT201819.pdf"]}
{"year":"2018","title":"Preserved Structure Across Vector Space Representations","authors":["A Amatuni, E He, E Bergelson - arXiv preprint arXiv:1802.00840, 2018"],"snippet":"… We use the set of vectors pretrained by the GloVe authors on the Common Crawl corpus with 42 billion tokens, resulting in 300 dimensional vectors for 1.9 million unique words1. Such vectors have shown promise in modeling early semantic networks (Amatuni & Bergelson …","url":["https://arxiv.org/pdf/1802.00840"]}
{"year":"2018","title":"Probabilistic German Morphosyntax","authors":["R Schäfer"],"snippet":"Page 1. Probabilistic German Morphosyntax HABILITATIONSSCHRIFT zur Erlangung der Lehrbefähigung für das Fach Germanistische und Allgemeine Sprachwissenschaft vorgelegt der Philosophischen Fakultät II der …","url":["http://rolandschaefer.net/wp-content/uploads/RolandSchaefer_2018_ProbabilisticGermanMorphosyntax_Habil_DRAFT.pdf"]}
{"year":"2018","title":"PROMT Systems for WMT 2018 Shared Translation Task","authors":["A Molchanov - Proceedings of the Third Conference on Machine …, 2018"],"snippet":"… The CommonCrawl and (especially) ParaCrawl corpora were heavily filtered and normalized using the PROMT tools and algorithms (including language recognition, removal of meaningless sentences, in-house tools for parallel …","url":["http://www.aclweb.org/anthology/W18-6420"]}
{"year":"2018","title":"Pseudo Descriptions for Meta-Data Retrieval","authors":["T Gollub, E Genc, N Lipka, B Stein - Proceedings of the 2018 ACM SIGIR International …, 2018"],"snippet":"… As reference collections, the TREC collections themselves as well as Wikipedia and the CommonCrawl are used … 2Not considered were lists, disambiguations, and short (#words < 250) articles. 3http://commoncrawl …","url":["https://dl.acm.org/citation.cfm?id=3234957"]}
{"year":"2018","title":"QED: A fact verification system for the FEVER shared task","authors":["J Luken, N Jiang, MC de Marneffe - Proceedings of the First Workshop on Fact …, 2018"],"snippet":"… described below. 4.2 Embedding We used GloVe word embeddings (Pennington et al., 2014) with 300 dimensions pre-trained us- ing CommonCrawl to get a vector representation of the evidence sentence. We also experimented …","url":["http://www.aclweb.org/anthology/W18-5526"]}
{"year":"2018","title":"Quantifying macroeconomic expectations in stock markets using Google Trends","authors":["J Bock - arXiv preprint arXiv:1805.00268, 2018"],"snippet":"… trends.google.com/trends/, March 5, 2018. 3 Common Crawl (42B tokens) GloVe word embeddings, retrieved from Stanford University, https://nlp.stanford.edu/projects/ glove/, March 4, 2018. 4 GloVe word embeddings are vector …","url":["https://arxiv.org/pdf/1805.00268"]}
{"year":"2018","title":"Quantitative Web History Methods","authors":["A Cocciolo - The SAGE Handbook of Web History, 2018"]}
{"year":"2018","title":"Quantum-like Generalization of Complex Word Embedding: a lightweight approach for textual classification","authors":["H Liu"],"snippet":"… (B) - crawl-300d-2M-vec ‡ 2 Million 600 Billion (C) - GloVe.Common Crawl.840B.300d † 2.2 Million 840 Billion Table 1. The pre-trained word embedding models selected for this experiment, where †= GloVe algorithm embeddings, ‡= Fasttext algorithm embeddings …","url":["http://ceur-ws.org/Vol-2191/paper19.pdf"]}
{"year":"2018","title":"Quester: A Speech-Based Question Answering Support System for Oral Presentations","authors":["R Asadi, H Trinh, HJ Fell, TW Bickmore - … of the 23rd International Conference on …, 2018"],"snippet":"… ( , ) = √ 2 =0 (2) ( ) is the word vector representation of keyword k. We used a pre-trained GloVe [14] vector representation with 1.9 million uncased words and vectors with 300 elements, trained using 42 billion tokens of web data from Common Crawl …","url":["http://relationalagents.com/publications/IUI18.pdf"]}
{"year":"2018","title":"Question Answering on SQuAD Dataset","authors":["ZDJ Dong, J Geng"],"snippet":"… We use the 300-dimensional case-insensitive Common Crawl GloVe word embeddings [7], and do not retrain the embeddings during training … We believe this is mainly because the Common Crawl version has a much larger …","url":["http://web.stanford.edu/class/cs224n/reports/6878267.pdf"]}
{"year":"2018","title":"Question Answering System with Question Type Modelling","authors":["K Ponomareva"],"snippet":"… and bug fixing. It might be also useful to consider additional training data sets with more variety of question types present and using a larger pretrained word vectors set, such as CommonCrawl.840B.300d. Acknowledgments I …","url":["http://web.stanford.edu/class/cs224n/reports/6904810.pdf"]}
{"year":"2018","title":"Ranking Documents by Answer-Passage Quality","authors":["E Yulianti, RC Chen, F Scholer, WB Croft, M Sanderson - 2018"],"snippet":"… We use the same set of word embeddings learned from the Y!A data (as with EmbYA), but the effectiveness is roughly comparable to a pre-trained model learned on the Common Crawl data [9]. For all methods tested in …","url":["http://rueycheng.com/paper/answer-passages.pdf"]}
{"year":"2018","title":"Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study","authors":["T Ge, F Wei, M Zhou - arXiv preprint arXiv:1807.01270, 2018"],"snippet":"Page 1. Microsoft Research Technical Report REACHING HUMAN-LEVEL PERFORMANCE IN AUTOMATIC GRAMMATICAL ERROR CORRECTION: AN EMPIRICAL STUDY Tao Ge, Furu Wei, Ming Zhou Natural Language …","url":["https://arxiv.org/pdf/1807.01270"]}
{"year":"2018","title":"Reasoning with Sarcasm by Reading In-between","authors":["Y Tay, LA Tuan, SC Hui, J Su - arXiv preprint arXiv:1805.02856, 2018"],"snippet":"… We use the GloVe model trained on 2B Tweets for the Tweets and Reddit dataset. The Glove model trained on Common Crawl is used for the Debates corpus. The size of the word em- beddings is fixed at d = 100 and are fine-tuned during training …","url":["https://arxiv.org/pdf/1805.02856"]}
{"year":"2018","title":"Recommendation System Developer","authors":["E Nandini, J Neal, T Olson, C Prater-Lee"],"snippet":"… This matrix is mapped to a vector space; GloVe uses a least squares regression model to minimize the dot product between word vectors. spaCy trains GloVe on Common Crawl corpus by default, which offers free web page data …","url":["https://taylorlolson.com/docs/recommendation-system-developer.pdf"]}
{"year":"2018","title":"Refining Word Embeddings Using Intensity Scores for Sentiment Analysis","authors":["LC Yu, J Wang, KR Lai, X Zhang"],"snippet":"Page 1. 2329-9290 (c) 2017 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://www.researchgate.net/profile/Jin_Wang115/publication/322143064_Refining_Word_Embeddings_Using_Intensity_Scores_for_Sentiment_Analysis/links/5a4d9ad50f7e9b8284c4e442/Refining-Word-Embeddings-Using-Intensity-Scores-for-Sentiment-Analysis.pdf"]}
{"year":"2018","title":"Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation","authors":["H Khayrallah, B Thompson, K Duh, P Koehn - Proceedings of the 2nd Workshop on …, 2018"],"snippet":"… large, out-of-domain corpus we utilize bi- text from WMT2017 (Bojar et al., 2017),4 which contains data from several sources: Europarl parliamentary proceedings (Koehn, 2005),5 News Commentary (political and economic …","url":["http://www.aclweb.org/anthology/W18-2705"]}
{"year":"2018","title":"Report on the Third Quality Translation Shared Task","authors":["P Williams, P Koehn, O Bojar, T Kocmi, L Specia… - 2018"],"snippet":"Page 1. This document is part of the Research and Innovation Action “Quality Translation 21 (QT21)”. This project has received funding from the European Union's Horizon 2020 program for ICT under grant agreement no. 645452 …","url":["http://www.qt21.eu/wp-content/uploads/2018/08/QT21-D4.3.pdf"]}
{"year":"2018","title":"Representativeness of Latent Dirichlet Allocation Topics Estimated from Data Samples with Application to Common Crawl","authors":["Y Du, A Herzog, A Luckow, R Nerella, C Gropp, A Apon"],"snippet":"Abstract—Common Crawl is a massive multi-petabyte dataset hosted by Amazon. It contains archived HTML web page data from 2008 to date. Common Crawl has been widely used for text mining purposes. Using data extracted from Common Crawl has several advantages","url":["https://www.researchgate.net/profile/Yuheng_Du/publication/322512712_Representativeness_of_latent_dirichlet_allocation_topics_estimated_from_data_samples_with_application_to_common_crawl/links/5a67b4980f7e9b76ea8f086e/Representativeness-of-latent-dirichlet-allocation-topics-estimated-from-data-samples-with-application-to-common-crawl.pdf"]}
{"year":"2018","title":"Reproducible Web Corpora: Interactive Archiving with Automatic Quality Assessment","authors":["J KIESEL, F KNEIST, M ALSHOMARY, B STEIN… - 2018"],"snippet":"… since disappeared. The Common Crawl [36] is also missing many of such resources. Other … the web. As a population of web pages to draw a sample from, we resort to the recent billion-page Common Crawl 2017-04 [36]. From …","url":["https://webis.de/downloads/publications/papers/stein_2018q.pdf"]}
{"year":"2018","title":"Research Data Management","authors":["S Kühne"],"snippet":"… the web as graph data - 886 m. nodes (07/2018) - 5.4 bn. edges (07/2018) THE COMMON CRAWL ARCHIVE, http://commoncrawl.org Some research questions - prevalence of Web advertising - etymologies of words …","url":["https://www.ral.uni-leipzig.de/fileadmin/user_upload/dokumente/Schleyer/Kuehne_Introduction.pdf"]}
{"year":"2018","title":"Research Frontiers in Information Retrieval Report from the Third Strategic Workshop on Information Retrieval in Lorne (SWIRL 2018)","authors":["JS Culpepper, F Diaz, MD Smucker"],"snippet":"Page 1. WORKSHOP REPORT Research Frontiers in Information Retrieval Report from the Third Strategic Workshop on Information Retrieval in Lorne (SWIRL 2018) Editors J. Shane Culpepper, Fernando Diaz, and Mark D. Smucker …","url":["http://www.damianospina.com/wp-content/uploads/2018/04/swirl3-report.pdf"]}
{"year":"2018","title":"RI-Match: Integrating Both Representations and Interactions for Deep Semantic Matching","authors":["L Chen, Y Lan, L Pang, J Guo, J Xu, X Cheng - Asia Information Retrieval Symposium, 2018"],"snippet":"… First, we introduce our experimental settings, including parameter setting, and evaluation metrics. Parameter Settings. We initialize word embeddings in the word embedding layer with 300-dimensional Glove word vectors pre-trained in the 840B Common Crawl corpus …","url":["https://link.springer.com/chapter/10.1007/978-3-030-03520-4_9"]}
{"year":"2018","title":"Rigging Research Results by Manipulating Top Websites Rankings","authors":["VL Pochat, T Van Goethem, W Joosen - arXiv preprint arXiv:1806.01156, 2018"],"snippet":"Page 1. Rigging Research Results by Manipulating Top Websites Rankings Victor Le Pochat, Tom Van Goethem and Wouter Joosen imec-DistriNet, KU Leuven 3001 Leuven, Belgium Email: firstname.lastname@cs.kuleuven.be …","url":["https://arxiv.org/pdf/1806.01156"]}
{"year":"2018","title":"Risk Analysis of Information-Leakage Through Interest Packets in NDN","authors":["D Kondo, T Silverston, H Tode, T Asami, O Perrin"],"snippet":"… We collected URLs from the data repository provided by Common Crawl and we evaluate the performances of our per-packet filters … All the URLs in our data set provided by Common Crawl did not necessarily have <fragment> part as defined in RFC 1808 …","url":["http://ieeexplore.ieee.org/iel7/8106907/8116300/08116403.pdf"]}
{"year":"2018","title":"Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots","authors":["Y Yoon, WR Ko, M Jang, J Lee, J Kim, G Lee - arXiv preprint arXiv:1810.12541, 2018"],"snippet":"… In the space of word embedding, words of similar meaning have similar representations, so understanding natural language is easier. We used the pretrained word embedding model GloVe, trained on the Common Crawl corpus [21] …","url":["https://arxiv.org/pdf/1810.12541"]}
{"year":"2018","title":"S3BD: Secure Semantic Search over Encrypted Big Data in the Cloud","authors":["J Woodworth, MA Salehi - arXiv preprint arXiv:1809.07927, 2018"],"snippet":"Page 1. CONCURRENCY AND COMPUTATION: PRACTICE AND EXPERIENCE Concurrency Computat.: Pract. Exper. 0000; 00:1–22 Published online in Wiley InterScience (www.interscience.wiley.com). DOI: 10.1002/cpe …","url":["https://arxiv.org/pdf/1809.07927"]}
{"year":"2018","title":"Sandpiper: Scaling Probabilistic Inferencing to Large Scale Graphical Models","authors":["A Ulanov, M Marwah, M Kim, R Dathathri, C Zubieta…"],"snippet":"… It is the hyperlinked graph obtained from a web crawl conducted by Common Crawl in August 2012 [32] … We used a real, large-scale web graph for this use case. We derived it from the web graph available at Web Data Commons [30], based on the Common Crawl data [32] …","url":["http://marwah.org/publications/papers/bigdata2017.pdf"]}
{"year":"2018","title":"Scheduled Multi-Task Learning: From Syntax to Translation","authors":["E Kiperwasser, M Ballesteros - arXiv preprint arXiv:1804.08915, 2018"],"snippet":"Page 1. Scheduled Multi-Task Learning: From Syntax to Translation Eliyahu Kiperwasser∗ Computer Science Department Bar-Ilan University Ramat-Gan, Israel elikip@gmail.com Miguel Ballesteros IBM Research 1101 …","url":["https://arxiv.org/pdf/1804.08915"]}
{"year":"2018","title":"SDC: structured data collection by yourself","authors":["T Ohshima, M Toyama - Proceedings of the 8th International Conference on …, 2018"],"snippet":"… PVLDB 1, 1 (2008), 538--549. http://www.vldb.org/pvldb/171453916.pdf. 2. Common Crawl [nd]. Common Crawl. http://commoncrawl.org/. ([nd]). 3. DataHub - Frictionless Data [nd]. DataHub - Frictionless Data. http://datahub.io/. ([nd]) …","url":["https://dl.acm.org/citation.cfm?id=3200849"]}
{"year":"2018","title":"Searching Arguments in German with ArgumenText","authors":["C Stahlhut"],"snippet":"… for arguments on a topic such as “nuclear energy”, it first retrieves relevant documents via Elasticsearch from a large collection of documents, such as Common Crawl2. In the … 1A demonstrator is publicly available at …","url":["http://desires.dei.unipd.it/papers/paper20.pdf"]}
{"year":"2018","title":"Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings","authors":["M Tkachenko, CC Chia, H Lauw - Proceedings of the 56th Annual Meeting of the …, 2018"],"snippet":"… embeddings. The previous preoccupation centers around corpus size, ie, a larger corpus is perceived to be richer in statistical information. For instance, popular corpora include Wikipedia, Common Crawl, and Google News. We …","url":["http://www.aclweb.org/anthology/P18-1112"]}
{"year":"2018","title":"SEGBOT: A Generic Neural Text Segmentation Model with Pointer Network","authors":["J Li, A Sun, S Joty - IJCAI. Under Review, 2018"],"snippet":"Page 1. SEGBOT: A Generic Neural Text Segmentation Model with Pointer Network Jing Li, Aixin Sun and Shafiq Joty School of Computer Science and Engineering, Nanyang Technological University, Singapore …","url":["https://www.researchgate.net/profile/Aixin_Sun/publication/325168535_SEGBOT_A_Generic_Neural_Text_Segmentation_Model_with_Pointer_Network/links/5afb9f710f7e9b3b0bf2a964/SEGBOT-A-Generic-Neural-Text-Segmentation-Model-with-Pointer-Network.pdf"]}
{"year":"2018","title":"Semantic Term\" Blurring\" and Stochastic\" Barcoding\" for Improved Unsupervised Text Classification","authors":["RF Martorano III - arXiv preprint arXiv:1811.02456, 2018"],"snippet":"… trained on millions of documents from Google News. In the case of later models, they have trained on common crawl, a dataset of billions of web pages. The high level intuition with these models, is that terms used in similar contexts, likely have similar semantics …","url":["https://arxiv.org/pdf/1811.02456"]}
{"year":"2018","title":"Semi-Supervised Neural System for Tagging, Parsing and Lemmatization","authors":["P Rybak, A Wróblewska - CoNLL 2018, 2018"],"snippet":"… For Uyghur language only 3M words are available. The provided data sets come either from Wikipedia or Commom Crawl. Where it is possible we choose the sentences from Common Crawl, due to longer (on average) sentence sizes …","url":["http://universaldependencies.org/conll18/proceedings/K18-2.pdf#page=53"]}
{"year":"2018","title":"Sentence Classification for Investment Rules Detection","authors":["Y Mansar, S Ferradans - Proceedings of the First Workshop on Economics and …, 2018"],"snippet":"… embedding. This is justified by the fact that some words used in prospectuses are uncommon in the general use of language and thus are not included in available word vectors pre-trained on Wikipedia or common crawl alone …","url":["http://www.aclweb.org/anthology/W18-3106"]}
{"year":"2018","title":"Sentence Encoding with Tree-constrained Relation Networks","authors":["L Yu, CM d'Autume, C Dyer, P Blunsom, L Kong… - arXiv preprint arXiv …, 2018"],"snippet":"Page 1. SENTENCE ENCODING WITH TREE-CONSTRAINED RE- LATION NETWORKS Lei Yu Cyprien de Masson d'Autume Chris Dyer Phil Blunsom Lingpeng Kong Wang Ling DeepMind {leiyu, cyprien, cdyer, pblunsom, lingpenk, lingwang}@google.com ABSTRACT …","url":["https://arxiv.org/pdf/1811.10475"]}
{"year":"2018","title":"Sentence Modeling via Multiple Word Embeddings and Multi-level Comparison for Semantic Textual Similarity","authors":["HN Tien, MN Le, Y Tomohiro, I Tatsuya - arXiv preprint arXiv:1805.07882, 2018"],"snippet":"… The em- bedding representations in fastText are 300dimensional vectors. • GloVe is a 300-dimensional word embedding model learned on aggregated global wordword co-occurrence statistics from Common Crawl (840 billion tokens) …","url":["https://arxiv.org/pdf/1805.07882"]}
{"year":"2018","title":"Sentence Selection and Weighting for Neural Machine Translation Domain Adaptation","authors":["R Wang, M Utiyama, A Finch, L Liu, K Chen, E Sumita - IEEE/ACM Transactions on …, 2018"],"snippet":"Page 1. 2329-9290 (c) 2018 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/8360031/"]}
{"year":"2018","title":"Sentence Similarity Learning Method based on Attention Hybrid Model","authors":["Y Wang, X Di, J Li, H Yang, L Bi - Journal of Physics: Conference Series, 2018"],"snippet":"… with our method. 4.3. Experimental Setup We initialize word representation in the word embedding layer with the 300-dimensional GloVe word vectors pre-trained from the Common Crawl Corpus [18]. Embeddings for words …","url":["http://iopscience.iop.org/article/10.1088/1742-6596/1069/1/012119/pdf"]}
{"year":"2018","title":"Sentence Simplification with Memory-Augmented Neural Networks","authors":["T Vu, B Hu, T Munkhdalai, H Yu - arXiv preprint arXiv:1804.07445, 2018"],"snippet":"… We used the same hyperparameters across all datasets. Word embeddings were initialized either randomly or with Glove vectors (Pennington et al., 2014) pre-trained on Common Crawl data (840B tokens), and fine-tuned during training …","url":["https://arxiv.org/pdf/1804.07445"]}
{"year":"2018","title":"SentEval: An Evaluation Toolkit for Universal Sentence Representations","authors":["A Conneau, D Kiela - arXiv preprint arXiv:1803.05449, 2018"],"snippet":"… Continuous bag-of-words embeddings (average of word vectors). We consider the most commonly used pretrained word vectors available, namely the fastText (Mikolov et al., 2017) and the GloVe (Pennington et al., 2014) vectors trained on CommonCrawl …","url":["https://arxiv.org/pdf/1803.05449"]}
{"year":"2018","title":"Sentiment Bias in Predictive Text Recommendations Results in Biased Writing","authors":["KC Arnold, K Chauncey, KZ Gajos"],"snippet":"… lending, or law enforcement—if the data sets used to train the algorithms are bi- ased [2]. Such biased data sets are more common than initially suspected: Recent work demonstrated that two popular text corpora, the …","url":["https://www.eecs.harvard.edu/~kgajos/papers/2018/arnold18sentiment.pdf"]}
{"year":"2018","title":"Sentiment Expression Boundaries in Sentiment Polarity Classification","authors":["R Kaljahi, J Foster - Proceedings of the 9th Workshop on Computational …, 2018"],"snippet":"… The input layer for these systems is the concatenation of an embedding layer, which uses pre-trained GloVe (Pennington et al., 2014) word embeddings 2 (1.9M vocabulary Common Crawl), concatenated with a one-hot vector …","url":["http://www.aclweb.org/anthology/W18-6222"]}
{"year":"2018","title":"Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples","authors":["M Cheng, J Yi, H Zhang, PY Chen, CJ Hsieh - arXiv preprint arXiv:1803.01128, 2018"],"snippet":"… one is trained from scratch. For the machine translation task, we train our model using 453k pairs from the Europal corpus of German-English WMT 157, common crawl and news-commentary. We use the hyper-parameters suggested …","url":["https://arxiv.org/pdf/1803.01128"]}
{"year":"2018","title":"Sequence-to-sequence Models for Cache Transition Systems","authors":["X Peng, L Song, D Gildea, G Satta"],"snippet":"… Hidden state sizes for both encoder and decoder are set to 100. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training …","url":["https://www.cs.rochester.edu/u/gildea/pubs/peng-acl18.pdf"]}
{"year":"2018","title":"Shortcutting Label Propagation for Distributed Connected Components","authors":["S Stergiou, D Rughwani, K Tsioutsiouliklis - … Conference on Web Search and Data …, 2018"],"snippet":"… Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references. 1. 2016. Common Crawl. (2016). http://commoncrawl.org/. 2. 2016. Twitter Graph. (2016) …","url":["https://dl.acm.org/citation.cfm?id=3159696"]}
{"year":"2018","title":"Simple Algorithms For Sentiment Analysis On Sentiment Rich, Data Poor Domains.","authors":["PK Sarma, W Sethares - Proceedings of the 27th International Conference on …, 2018"],"snippet":"… Furthermore, in some of these domains, representing words from off-the-shelf word embeddings such as ones obtained from training word2vec, GloVe on Wikipedia or common-crawl may not be efficient. This is because the …","url":["http://www.aclweb.org/anthology/C18-1290"]}
{"year":"2018","title":"SLIND: Identifying Stable Links in Online Social Networks","authors":["J Zhang, L Tan, X Tao, X Zheng, Y Luo, JCW Lin - International Conference on …, 2018"],"snippet":"… The dataset chosen for this study, as well as for the demo, was crawled from Facebook and obtained from the repositories of the Common Crawl (August 2016) 1 . It is de-anonymized to reveal the following relational …","url":["https://link.springer.com/chapter/10.1007/978-3-319-91458-9_54"]}
{"year":"2018","title":"Smart Focused Web Crawler for Hidden Web","authors":["S Kaur, G Geetha - Information and Communication Technology for …, 2019"],"snippet":"… The number of partitions will depend on the number of URLs in site database. Tel-8 and common crawl datasets will be used. MapReduce function will be called, and the input will split into 64 MB plus copies of this on other clusters …","url":["https://link.springer.com/chapter/10.1007/978-981-13-0586-3_42"]}
{"year":"2018","title":"SocialLink: exploiting graph embeddings to link DBpedia entities to Twitter profiles","authors":["Y Nechaev, F Corcoglioniti, C Giuliano - Progress in Artificial Intelligence, 2018"],"snippet":"Page 1. Progress in Artificial Intelligence https://doi.org/10.1007/s13748-018-0160-x REGULAR PAPER SocialLink: exploiting graph embeddings to link DBpedia entities to Twitter profiles Yaroslav Nechaev1,2 · Francesco Corcoglioniti1 · Claudio Giuliano1 …","url":["https://link.springer.com/article/10.1007/s13748-018-0160-x"]}
{"year":"2018","title":"Software Requirements Classification Using Word Embeddings and Convolutional Neural Networks","authors":["VL Fong - 2018"],"snippet":"Page 1. SOFTWARE REQUIREMENTS CLASSIFICATION USING WORD EMBEDDINGS AND CONVOLUTIONAL NEURAL NETWORKS A Thesis presented to the Faculty of California Polytechnic State University, San Luis Obispo In Partial Fulfillment …","url":["https://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=3249&context=theses"]}
{"year":"2018","title":"SOLVENT: A Mixed Initiative System for Finding Analogies between Research Papers","authors":["J CHAN, JC CHANG, TOM HOPE, D SHAHAF… - 2018"],"snippet":"… We originally used pre-trained GloVe [35] vectors trained on the Common Crawl dataset 5. However, baseline performance was very poor … Finally, we used an initial prototype GloVe model (with Common Crawl) to suggest new matches we might have missed …","url":["http://joelchan.me/assets/pdf/2018-cscw-schema-highlighter.pdf"]}
{"year":"2018","title":"Speech-Based Real-Time Presentation Tracking Using Semantic Matching","authors":["R Asadi - 2017"],"snippet":"Speech-Based Real-Time Presentation Tracking Using Semantic Matching. Abstract. Oral presentations are an essential yet challenging aspect of academic and professional life. To date, many commercial and research products …","url":["http://search.proquest.com/openview/a91c85d71f1e130a00bee5b6f95d90e3/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2018","title":"Stop Illegal Comments: A Multi-Task Deep Learning Approach","authors":["A Elnaggar, B Waltl, I Glaser, J Landthaler… - arXiv preprint arXiv …, 2018"],"snippet":"… 12] and our Glove). The Fast Text is 2 million word vectors trained on Common Crawl with dimension 300, while Glove is 2.2 million word vectors trained on Common Crawl with dimension 300. Furthermore, we trained a custom …","url":["https://arxiv.org/pdf/1810.06665"]}
{"year":"2018","title":"Studio Ousia's Quiz Bowl Question Answering System at NIPS HCQA 2017","authors":["I Yamada"],"snippet":"… Moreover, we use the GloVe word embeddings [10] trained on the 840 billion Common Crawl corpus to initialize the word representations. We randomly select 10% questions from the dataset as a validation set and use the remaining questions to train the model …","url":["http://www.cs.umd.edu/~miyyer/data/Ikuya.pdf"]}
{"year":"2018","title":"Studio Ousia's Quiz Bowl Question Answering System","authors":["I Yamada, R Tamaki, H Shindo, Y Takefuji"],"snippet":"… 1,000. We use filter window sizes of 2, 3, 4, and 5, and 1,000 feature maps for each filter. We use the GloVe word embeddings [12] trained on the 840 billion Common Crawl corpus to initialize the word representations. As in …","url":["https://www.researchgate.net/profile/Yoshiyasu_Takefuji/publication/323535360_Studio_Ousia%27s_Quiz_Bowl_Question_Answering_System/links/5a9a6cde45851586a2aa0ade/Studio-Ousias-Quiz-Bowl-Question-Answering-System.pdf"]}
{"year":"2018","title":"Studying the Difference Between Natural and Programming Language Corpora","authors":["C Casalnuovo, K Sagae, P Devanbu - arXiv preprint arXiv:1806.02437, 2018"],"snippet":"… The German and Spanish corpora were selected from a sample of files from the unlabeled datasets from the ConLL 2017 Shared Task (Ginter et al, 2017), which consist of web text obtained from CommonCrawl.8 Like the 1 billion …","url":["https://arxiv.org/pdf/1806.02437"]}
{"year":"2018","title":"Style Transfer Through Back-Translation","authors":["S Prabhumoye, Y Tsvetkov, R Salakhutdinov, AW Black - arXiv preprint arXiv …, 2018"],"snippet":"… We used data from Workshop in Statistical Machine Translation 2015 (WMT15) (Bojar et al., 2015) to train our translation models. We used the French– English data from the Europarl v7 corpus, the news commentary …","url":["https://arxiv.org/pdf/1804.09000"]}
{"year":"2018","title":"SumeCzech: Large Czech News-Based Summarization Dataset","authors":["M Straka, N Mediankin, T Kocmi, Z Žabokrtský… - Proceedings of the Eleventh …, 2018"],"snippet":"… The raw data for the dataset was collected from the Common Crawl project2 using the Common Crawl API. Initially, five Czech news websites were selected to create the dataset: novinky.cz, lidovky.cz, denik.cz, idnes.cz, and ihned.cz …","url":["http://www.aclweb.org/anthology/L18-1551"]}
{"year":"2018","title":"Supplementary Material for “Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering”","authors":["DK Nguyen, T Okatani"],"snippet":"… All the questions were tokenized using Python Natural Language Toolkit (nltk) [2]. We used the vocabulary provided by the CommonCrawl-840B Glove model for English word vectors [11], and set out-of-vocabulary words to unk …","url":["http://openaccess.thecvf.com/content_cvpr_2018/Supplemental/3586-supp.pdf"]}
{"year":"2018","title":"Survey of Simple Neural Networks in Semantic Textual Similarity Analysis","authors":["DS Prijatelj, J Ventura, J Kalita"],"snippet":"… vectors. This specific set of word vectors have 300 dimensions and were pre-trained on 840 billion tokens taken from Common Crawl3. Different pretrained word vectors may be used in-place of this specific pretrained set …","url":["http://cs.uccs.edu/~jkalita/work/reu/REU2017/11Prijatelj.pdf"]}
{"year":"2018","title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference","authors":["R Zellers, Y Bisk, R Schwartz, Y Choi - arXiv preprint arXiv:1808.05326, 2018"],"snippet":"… We consider three different types of word representations: 300d GloVe vectors from Common Crawl (Pennington et al., 2014), 300d Numberbatch vectors retrofitted using ConceptNet relations (Speer et al., 2017), and …","url":["https://arxiv.org/pdf/1808.05326"]}
{"year":"2018","title":"Systems and methods for improved user interface","authors":["Z Wei, T Nguyen, I Chan, KM Liou, H Wang, H Lu - US Patent App. 15/621,647, 2018"],"snippet":"Aspects of the present disclosure relate to systems and methods for a voice-centric virtual or soft keyboard (or keypad). Unlike other keyboards, embodiments of the present disclosure prioritize the voice keyboard, meanwhile providing users with a quick and uniform navigation to …","url":["https://patents.google.com/patent/US20180011688A1/en"]}
{"year":"2018","title":"T2S: An Encoder-Decoder Model for Topic-Based Natural Language Generation","authors":["W Ou, C Chen, J Ren - International Conference on Applications of Natural …, 2018"],"snippet":"… We initialize our word embeddings with publicly available 300-dimensional Glove vectors [13], which is trained on 840 billion tokens of Common Crawl data 2 . Words that do not exist in the pretrained Glove vectors are replaced by “<unk>” token …","url":["https://link.springer.com/chapter/10.1007/978-3-319-91947-8_15"]}
{"year":"2018","title":"TabVec: Table Vectors for Classification of Web Tables","authors":["M Ghasemi-Gol, P Szekely - arXiv preprint arXiv:1802.06290, 2018"],"snippet":"… They evaluated their system on the common crawl dataset, and reported signi cant improvement compared to previous feature based methods … Three of these datasets are from unusual domains, and one is a sample from Common Crawl …","url":["https://arxiv.org/pdf/1802.06290"]}
{"year":"2018","title":"TCS Research at SemEval-2018 Task 1: Learning Robust Representations using Multi-Attention Architecture","authors":["H Meisheri, L Dey - Proceedings of The 12th International Workshop on …, 2018"],"snippet":"… corpus which results in parallel attention mechanism - one set from the twitter space and another from a common crawl corpus … 2014) trained over common crawl corpus with 300 dimension vector, Character1 level embeddings trained …","url":["http://www.aclweb.org/anthology/S18-1043"]}
{"year":"2018","title":"Temporal Modular Networks for Retrieving Complex Compositional Activities in Videos","authors":["L Fei-Fei, JC Niebles"],"snippet":"… 4.2. The Stanford Parser [27] is used to obtain the initial parse trees for the compositional structure. For word vectors as part of the base module input, we use the 300-dimensional GloVe [36] vectors pretrained on Common Crawl (42 billion tokens) …","url":["http://svl.stanford.edu/assets/papers/liu2018eccv.pdf"]}
{"year":"2018","title":"Ten Years of WebTables","authors":["M Cafarella, A Halevy, H Lee, J Madhavan, C Yu… - Proceedings of the VLDB …, 2018"],"snippet":"… Several researchers produced web tables from the public Common Crawl [1, 24, 15], thereby making them available to a broad audience outside the large Web companies. Wang, et al. [36] improved ex- traction quality by leveraging curated knowledge bases …","url":["http://www.vldb.org/pvldb/vol11/p2140-cafarella.pdf"]}
{"year":"2018","title":"Text Embeddings for Retrieval From a Large Knowledge Base","authors":["T Cakaloglu, C Szegedy, X Xu - arXiv preprint arXiv:1810.10176, 2018"],"snippet":"… We specially utilized the ”glove-840B-300d” pre-trained word vectors where it was trained on using the common crawl within 840B tokens, 2.2M vocab, cased, 300d vectors. We created the GloVe representation of our corpus …","url":["https://arxiv.org/pdf/1810.10176"]}
{"year":"2018","title":"Text-based Sentiment Analysis and Music Emotion Recognition","authors":["E Çano - 2018"],"snippet":"… 38 3.4 Confusion matrix of lexicon-generated song labels . . . . . 42 5.1 Listofwordembeddingcorpora . . . . . 65 5.2 Google News compared with Common Crawl . . . . . 69 5.3 Propertiesofself_w2vmodels . . . . . 70 …","url":["https://www.researchgate.net/profile/Erion_Cano/publication/325651523_Text-based_Sentiment_Analysis_and_Music_Emotion_Recognition/links/5b1a8d640f7e9b68b429cdae/Text-based-Sentiment-Analysis-and-Music-Emotion-Recognition.pdf"]}
{"year":"2018","title":"Text-Driven Head Motion Synthesis Using Neural Networks","authors":["BTS Bojlén"],"snippet":"… We also compared embeddings trained on Common Crawl (a large collection of websites), and on Wikipedia and the Gigaword corpus of news articles. The model trained was a baseline RNN with the architecture specified in Table 4.2 …","url":["https://btao.org/static/dissertation.pdf"]}
{"year":"2018","title":"TEXTBUGGER: Generating Adversarial Text Against Real-world Applications","authors":["J Li, S Ji, T Du, B Li, T Wang"],"snippet":"… This is because we observe that the stop-words also have impact on the prediction results. In particular, our experiments utilize the 300-dimension GloVe embeddings7 trained on 840 billion tokens of Common Crawl. Words …","url":["https://nesa.zju.edu.cn/download/TEXTBUGGER%20Generating%20Adversarial%20Text%20Against%20Real-world%20Applications.pdf"]}
{"year":"2018","title":"The ADAPT System Description for the IWSLT 2018 Basque to English Translation Task","authors":["A Poncelas, A Way, K Sarasola - International Workshop on Spoken Language …, 2018"],"snippet":"… pair (see Table 4)[12]. In particular, we use the CommonCrawl, Europarl V7, NewsCommentary V12 and UN datasets for training, 5 the NewsTest 2008-2012 corpora for validation and NewsTest 2013 for testing. We did not use …","url":["https://workshop2018.iwslt.org/downloads/Proceedings_IWSLT_2018.pdf#page=91"]}
{"year":"2018","title":"The AFRL WMT18 Systems: Ensembling, Continuation and Combination","authors":["J Gwinnup, T Anderson, G Erdmann, K Young - … of the Third Conference on Machine …, 2018"],"snippet":"… We took the Russian and English monolingual CommonCrawl (Smith et al., 2013) data provided by the organizers and applied tokenization and BPE with our common, joint model … 2013. Dirt cheap web-scale parallel text from the common crawl …","url":["http://www.aclweb.org/anthology/W18-6411"]}
{"year":"2018","title":"The Geometry of Culture: Analyzing Meaning through Word Embeddings","authors":["AC Kozlowski, M Taddy, JA Evans - arXiv preprint arXiv:1803.09288, 2018"],"snippet":"Page 1. The Geometry of Culture: Analyzing Meaning through Word Embeddings Austin C. Kozlowski​1 Matt Taddy​2,3 James A. Evans​1 1 ​University of Chicago, Department of Sociology 2 ​University of Chicago, Booth School of Business 3 ​Amazon …","url":["https://arxiv.org/pdf/1803.09288"]}
{"year":"2018","title":"The Importance of Subword Embeddings in Sentence Pair Modeling","authors":["W Lan, W Xu"],"snippet":"… GloVe word vectors (Pennington et al.), trained on 27 billion words from Twitter (vocabulary size of 1.2 milion words) for social media datasets, and 300-dimensional GloVe vectors, trained on 840 billion words (vocabulary …","url":["https://pdfs.semanticscholar.org/c99f/e106e7d1cc62f7cb73ea6fc745b8679e4d2f.pdf"]}
{"year":"2018","title":"The Knowledge and Language Gap in Medical Information Seeking","authors":["L Soldaini - 2018"],"snippet":"The Knowledge and Language Gap in Medical Information Seeking. Abstract. Interest in medical information retrieval has risen significantly in the last few years. The Internet has become a primary source for consumers looking …","url":["http://search.proquest.com/openview/e669cd1478b33d52fa4cc71e8393c639/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2018","title":"The MLLP-UPV German-English Machine Translation System for WMT18","authors":["J Iranzo-Sánchez, P Baquero-Arnal, GVG Dıaz-Munıo…"],"snippet":"… 422 Page 2. Table 1: Size by corpus of the WMT18 parallel dataset Corpus Sentences (M) News Commentary v13 0.3 Rapid (press releases) 1.3 Common Crawl 1.9 Europarl v7 2.4 ParaCrawl 36.4 WMT18 total 42.3 the rest of the WMT corpora …","url":["http://www.statmt.org/wmt18/pdf/WMT041.pdf"]}
{"year":"2018","title":"The Natural Language Decathlon: Multitask Learning as Question Answering","authors":["B McCann, NS Keskar, C Xiong, R Socher - arXiv preprint arXiv:1806.08730, 2018"],"snippet":"Page 1. The Natural Language Decathlon: Multitask Learning as Question Answering Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher Salesforce Research {bmccann,nkeskar,cxiong,rsocher}@salesforce.com Abstract …","url":["https://arxiv.org/pdf/1806.08730"]}
{"year":"2018","title":"The RWTH Aachen Machine Translation Systems for IWSLT 2017","authors":["P Bahar, J Rosendahl, N Rossenbach, H Ney"],"snippet":"… The majority of removed sentence pairs are part of the Common Crawl (300k sentences ie 14% of Common Crawl) and the OpenSubtitles corpora (1000k sentences ie 8% of OpenSubtitles) … Common Crawl Europarl UN News Comment OpenSub QED TED Wiki Total …","url":["https://www-i6.informatik.rwth-aachen.de/publications/download/1061/BaharParniaRosendahlJanRossenbachNickNeyHermann--TheRWTHAachenMachineTranslationSystemsforIWSLT2017--2017.pdf"]}
{"year":"2018","title":"The RWTH Aachen University Filtering System for the WMT 2018 Parallel Corpus Filtering Task","authors":["N Rossenbach, J Rosendahl, Y Kim, M Graça… - Proceedings of the Third …, 2018"],"snippet":"… We train IBM1 models for both directions (s2t and t2s) using the bilingual data from the WMT 2018 German↔English task namely the Europarl, CommonCrawl, NewsCommentary and Rapid corpus. 4.3 Neural Network Language Model …","url":["http://www.aclweb.org/anthology/W18-6487"]}
{"year":"2018","title":"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018","authors":["J Schamper, J Rosendahl, P Bahar, Y Kim, A Nix… - Proceedings of the Third …, 2018"],"snippet":"… 1.4% BLEU. The Transformer model was trained using the standard parallel WMT 2018 data sets (namely Europarl, CommonCrawl, NewsCommentary and Rapid, in total 5.9M sentence pairs) as well as the 4.2M sen3http://www …","url":["http://www.aclweb.org/anthology/W18-6426"]}
{"year":"2018","title":"The Speechmatics Parallel Corpus Filtering System for WMT18","authors":["T Ash, R Francis, W Williams - Proceedings of the Third Conference on Machine …, 2018"],"snippet":"… This data comprises the data for the WMT 2018 news translation task data for German-English without the Paracrawl parallel corpus. This data is approximately 130M words, drawn from Europarl, Common Crawl, News …","url":["http://www.aclweb.org/anthology/W18-6472"]}
{"year":"2018","title":"The study of keyword search in open source search engines and digital forensics tools with respect to the needs of cyber crime investigations","authors":["J Hansen - 2017"],"snippet":"Page 1. The study of keyword search in open source search engines and digital forensics tools with respect to the needs of cyber crime investigations Joachim Hansen Master in Information Security Supervisor: Katrin Franke …","url":["https://brage.bibsys.no/xmlui/bitstream/handle/11250/2479196/18187_FULLTEXT.pdf?sequence=1"]}
{"year":"2018","title":"The University of Cambridge's Machine Translation Systems for WMT18","authors":["F Stahlberg, A de Gispert, B Byrne - arXiv preprint arXiv:1808.09465, 2018"],"snippet":"… Page 3. Corpus Over-sampling #Sentences Common Crawl 2x 4.43M Europarl v7 2x 3.76M News Commentary v13 2x 0.57M Rapid 2016 2x 2.27M ParaCrawl 1x 11.16M Synthetic (news-2017) 1x 20.00M Total 42.19M Table …","url":["https://arxiv.org/pdf/1808.09465"]}
{"year":"2018","title":"The University of Edinburgh's Submissions to the WMT18 News Translation Task","authors":["B Haddow, N Bogoychev, D Emelin, U Germann… - Proceedings of the Third …, 2018"],"snippet":"… closely Corpus % Back translations1 50% CommonCrawl 5% Europarl 15% News-commentary 10% ParaCrawl 10% Rapid 10% Table 1: Blend of data for training the DE↔EN en- semble models (40M sentence pairs total) …","url":["http://www.aclweb.org/anthology/W18-6412"]}
{"year":"2018","title":"The USTC-NEL Speech Translation system at IWSLT 2018","authors":["D Liu, J Liu, W Guo, S Xiong, Z Ma, R Song, C Wu… - arXiv preprint arXiv …, 2018"],"snippet":"… Page 2. Table 2: text training data. Corpus raw filtered commoncrawl 2.39M 1.80M rapid 1.32M 1.00M europal 1.92M 1.81M commentary 0.284M 0.233M paracrawl 36.35M 12.35M opensubtitles 22.51M 14.24M WIT3(in domain) 0.209M 0.207M …","url":["https://arxiv.org/pdf/1812.02455"]}
{"year":"2018","title":"Topic coherence analysis for the classification of Alzheimer's disease}}","authors":["A Pompili, A Abad, DM de Matos, IP Martins - Proc. IberSPEECH 2018, 2018"],"snippet":"… regularities among sentences. To this purpose, we rely on a pre-trained model of word vector representations containing 2 million word vectors, in 300 dimensions, trained with fastText on Common Crawl [27]. In the process …","url":["https://www.isca-speech.org/archive/IberSPEECH_2018/pdfs/IberS18_O5-1_Pompili.pdf"]}
{"year":"2018","title":"Topic Modeling for Analyzing Open-Ended Survey Responses","authors":["AS Pietsch, S Lessmann"],"snippet":"… Word2Vec [32] and the second one on Common Crawl web data via Global Vectors for Word Representation (GloVe) [33] … The set is trained on 42 billion tokens of Common Crawl web data and contains 300-dimensional vectors …","url":["https://www.wiwi.hu-berlin.de/de/forschung/irtg/results/discussion-papers/discussion-papers-2017-1/irtg1792dp2018-054.pdf"]}
{"year":"2018","title":"Toward better reasoning from natural language","authors":["A Purtee - 2018"],"snippet":"Page 1. Toward Better Reasoning from Natural Language by Adam Lee Purtee Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Supervised by Professor Lenhart Schubert and …","url":["https://urresearch.rochester.edu/fileDownloadForInstitutionalItem.action?itemId=34810&itemFileId=186239"]}
{"year":"2018","title":"Towards Automated Factchecking: Developing an Annotation Schema and Benchmark for Consistent Automated Claim Detection","authors":["L Konstantinovskiy, O Price, M Babakar, A Zubiaga - arXiv preprint arXiv:1809.08193, 2018"],"snippet":"… The method provided by InferSent involves words be- ing converted to their common crawl GloVe implementations before being passed through a bidirectional long-short-term memory (BiLSTM) network (Hochreiter & Schmidhuber, 1997) …","url":["https://arxiv.org/pdf/1809.08193"]}
{"year":"2018","title":"Towards Knowledge Graph Construction from Entity Co-occurrence","authors":["N Heist"],"snippet":"… patterns. 2 https://www.mturk.com/ 3 Pages starting with List of in http://downloads. dbpedia.org/2016-10/corei18n/en/labels en.ttl.bz2 4 http://commoncrawl.org/ 5 http://webdatacommons.org/structureddata/#results-2017-1 Page 7 …","url":["https://people.kmi.open.ac.uk/francesco/wp-content/uploads/2018/11/EKAWDC2018_3.pdf"]}
{"year":"2018","title":"Towards Linear Time Neural Machine Translation with Capsule Networks","authors":["M Wang, J Xie, Z Tan, J Su - arXiv preprint arXiv:1811.00287, 2018"],"snippet":"… French translation are presented in Table 1. We compare CAPSNMT with various other systems including the winning system in WMT'14 (Buck et al., 2014), a phrase-based system whose language models were trained on …","url":["https://arxiv.org/pdf/1811.00287"]}
{"year":"2018","title":"Towards Personalized Learning using Counterfactual Inference for Randomized Controlled Trials","authors":["S Zhao - 2018"],"snippet":"Page 1. Towards Personalized Learning using Counterfactual Inference for Randomized Controlled Trials by Siyuan Zhao A Dissertation Submitted to the Faculty of the WORCESTER POLYTECHNIC INSTITUTE …","url":["https://web.wpi.edu/Pubs/ETD/Available/etd-042618-010745/unrestricted/szhao.pdf"]}
{"year":"2018","title":"Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation","authors":["P Bahar, C Brix, H Ney - arXiv preprint arXiv:1810.03975, 2018"],"snippet":"… 5 Experiments We have done the experiments on the WMT 2017 German→English and English→German news tasks consisting of 4.6M training samples collected from the well-known data sets Europarl-v7, News-Commentary-v10 and Common-Crawl …","url":["https://arxiv.org/pdf/1810.03975"]}
{"year":"2018","title":"Training a Neural Network in a Low-Resource Setting on Automatically Annotated Noisy Data","authors":["MA Hedderich, D Klakow - arXiv preprint arXiv:1807.00745, 2018"],"snippet":"… Page 4. tries other than Britain until the scientific” where ”Britain” is the target word with label y = LOC. Sentence boundaries are padded. We encode the words using the 300-dimensional GloVe vectors trained on cased text …","url":["https://arxiv.org/pdf/1807.00745"]}
{"year":"2018","title":"Training Tips for the Transformer Model","authors":["M Popel, O Bojar - arXiv preprint arXiv:1804.00247, 2018"],"snippet":"… commoncrawl 161 k 3.3 M 2.9 M … Most of our training data comes from the CzEng parallel treebank, version 1.7 (57M sentence pairs), and the rest (1M sentence pairs) comes from three smaller sources (Europarl, News …","url":["https://arxiv.org/pdf/1804.00247"]}
{"year":"2018","title":"Transfer Learning from LDA to BiLSTM-CNN for Offensive Language Detection in Twitter","authors":["G Wiedemann, E Ruppert, R Jindal, C Biemann - Austrian Academy of Sciences …, 2018"],"snippet":"… classification labels per task. have not been seen during training the embedding model. We use a model pre-trained with German language data from Wikipedia and Common Crawl provided by Mikolov et al.(2018). First, we unify all …","url":["https://www.oeaw.ac.at/fileadmin/subsites/academiaecorpora/PDF/GermEval2018_Proceedings.pdf#page=91"]}
{"year":"2018","title":"Transferred Embeddings for Igbo Similarity, Analogy and Diacritic Restoration Tasks","authors":["IEMHI Onyenwe, C Enemuo - COLING 2018, 2018"],"snippet":"… org news dataset. • igWkSbwd from same as igWkNews but with subword information. • igWkCrl from fastText Common Crawl dataset Table 1 shows the vocabulary lengths (vocabs), and the dimensions (vectors) of each of the models used in our experiments …","url":["http://www.aclweb.org/anthology/W18-40#page=40"]}
{"year":"2018","title":"Transferred Embeddings for Igbo Similarity, Analogy, and Diacritic Restoration Tasks","authors":["I Ezeani, I Onyenwe, M Hepple - Proceedings of the Third Workshop on Semantic …, 2018"],"snippet":"… igWkSbwd from same as igWkNews but with subword information. • igWkCrl from fastText Common Crawl dataset Table 1 shows the vocabulary lengths (vocabs), and the dimensions (vectors) of each of the models used in our experiments. 3 Model Evaluation …","url":["http://www.aclweb.org/anthology/W18-4004"]}
{"year":"2018","title":"Translation of Biomedical Documents with Focus on Spanish-English","authors":["MS Duma, W Menzel - Proceedings of the Third Conference on Machine …, 2018"],"snippet":"… 2http://commoncrawl.org/ 3https://paracrawl.eu/index.html … Track / Corpora EN-ES EN-PT EN-RO Commoncrawl 1.8M - - Paracrawl - 2.1M 2.4M Wikipedia 1.6M 1.6M - EMEA 678K 1.08M 994K Scielo-gma 2016 166K 613K - Table 1: Corpora used for DSTF 3.2 Tools …","url":["http://www.aclweb.org/anthology/W18-6444"]}
{"year":"2018","title":"Two-Step Multi-factor Attention Neural Network for Answer Selection","authors":["P Zhang, Y Hou, Z Su, Y Su - Pacific Rim International Conference on Artificial …, 2018"],"snippet":"… 3.3 Experimental Settings. We initialize word embeddings with 300-dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus [20]. For out of vocabulary (OOV) words, their embeddings are initialized randomly …","url":["https://link.springer.com/chapter/10.1007/978-3-319-97304-3_50"]}
{"year":"2018","title":"UBC-NLP at IEST 2018: Learning Implicit Emotion With an Ensemble of Language Models","authors":["H Alhuzali, M Elaraby, M Abdul-Mageed"],"snippet":"… morphology like English. Additionally, fastText partially solves issues with out-of-vocabulary words since it exploits character sequences. FastText is trained on the Common Crawl dataset, consisting of 600B tokens. For this and the …","url":["https://mageed.sites.olt.ubc.ca/files/2018/09/emnlp18_IEST_WASSA_2018.pdf"]}
{"year":"2018","title":"Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation","authors":["M Artetxe, G Labaka, I Lopez-Gazpio, E Agirre - arXiv preprint arXiv:1809.02094, 2018"],"snippet":"… analogies. We use the largest pre-trained model published by the authors9, which was trained on 840 billion words of the Common Crawl corpus and contains 300dimensional vectors for 2.2 million words. Fasttext (Bojanowski …","url":["https://arxiv.org/pdf/1809.02094"]}
{"year":"2018","title":"Understanding Back-Translation at Scale","authors":["S Edunov, M Ott, M Auli, D Grangier - arXiv preprint arXiv:1808.09381, 2018"],"snippet":"… It also allows us to estimate the value of BT data for domain adaptation since the newscrawl corpus (BT-news) is pure news whereas the bitext is a mixture of eu- roparl and commoncrawl with only a small newscommentary portion …","url":["https://arxiv.org/pdf/1808.09381"]}
{"year":"2018","title":"Understanding Search Queries in Natural Language","authors":["Z Neverilová, M Kvaššay - RASLAN 2018 Recent Advances in Slavonic Natural …, 2018"],"snippet":"… stop-words are removed. Tokens are mapped to 300-dimensional word embeddings using publicly available vocabulary of FastText [2] vectors trained on CommonCrawl dataset. Missing words are ignored. Vectors are then …","url":["http://nlp.fi.muni.cz/raslan/raslan18.pdf#page=93"]}
{"year":"2018","title":"Unsupervised Disambiguation of Abstract Syntax","authors":["O KALLDAL, M LUDVIGSSON"],"snippet":"Page 1. NoPConj : PConj AAnter : Ant TTAnt : Temp whoever_NP : NP NoVoc : Voc PPos : Pol is_right_VP : VP PhrUtt : Phr TPast : Tense PredVP : Cl UseCl : S UttS : Utt Unsupervised Disambiguation of Abstract Syntax A Language …","url":["http://publications.lib.chalmers.se/records/fulltext/255307/255307.pdf"]}
{"year":"2018","title":"Unsupervised Domain Adaptation by Adversarial Learning for Robust Speech Recognition","authors":["P Denisov, NT Vu, MF Font - arXiv preprint arXiv:1807.11284, 2018"],"snippet":"… Summary of the used corpora is given in Tab. 1. In addition to that, 197 millions words of Italian Deduplicated CommonCrawl Text are used to build Italian language model. Italian dictionary ILE with pronunciations for 588k words is used as a lexicon. 3.2 Baseline …","url":["https://arxiv.org/pdf/1807.11284"]}
{"year":"2018","title":"Unsupervised Mining of Analogical Frames by Constraint Satisfaction","authors":["L De Vine, S Geva, P Bruza - Australasian Language Technology Association …"],"snippet":"… 2 a3, 3 Figure 4: Determining an analogy completion from a larger frame We conducted experiments with embeddings constructed by ourselves as well as with publicly accessible embeddings from the fastText web site2 trained …","url":["http://alta2018.alta.asn.au/alta2018-draft-proceedings.pdf#page=44"]}
{"year":"2018","title":"Unsupervised Neural Machine Translation Initialized by Unsupervised Statistical Machine Translation","authors":["B Marie, A Fujita - arXiv preprint arXiv:1810.12703, 2018"],"snippet":"… These methods usually exploit existing accurate translation models and have shown to be useful especially when targeting 1See for instance the Common Crawl project: http:// commoncrawl.org/ low-resource language pairs and domains …","url":["https://arxiv.org/pdf/1810.12703"]}
{"year":"2018","title":"Unsupervised Post-processing of Word Vectors via Conceptor Negation","authors":["T Liu, L Ungar, J Sedoc - arXiv preprint arXiv:1811.11001, 2018"],"snippet":"… We use the publicly available pre-trained Google News Word2Vec (Mikolov et al. 2013)5 and Common Crawl GloVe6 (Pennington, Socher, and Manning 2014) to perform lexical-level experiments. For CN, we fix α = 2 for …","url":["https://arxiv.org/pdf/1811.11001"]}
{"year":"2018","title":"Unsupervised semantic frame induction using triclustering","authors":["D Ustalov, A Panchenko, A Kutuzov, C Biemann… - arXiv preprint arXiv …, 2018"],"snippet":"… In our evaluation, we use triple frequencies from the DepCC dataset (Panchenko et al., 2018) , which is a dependency-parsed version of the Common Crawl corpus, and the standard 300-dimensional word embeddings …","url":["https://arxiv.org/pdf/1805.04715"]}
{"year":"2018","title":"Unsupervised Sense-Aware Hypernymy Extraction","authors":["D Ustalov, A Panchenko, C Biemann, SP Ponzetto - arXiv preprint arXiv:1809.06223, 2018"],"snippet":"… Recent approaches to hypernym extraction went into three directions: (1) unsupervised methods based on such huge corpora as CommonCrawl1 to ensure extraction coverage using Hearst (1992) patterns (Seitner et al …","url":["https://arxiv.org/pdf/1809.06223"]}
{"year":"2018","title":"User-Centric Ontology Population","authors":["K Clarkson, AL Gentile, D Gruhl, P Ristoski, J Terdiman…"],"snippet":"… Ristoski et al. [29] use standard word embeddings and graph embeddings to align instances extracted from the Common Crawl4 to the DBpedia ontology. The use of deep learning models has also been explored for this task. Dong et al …","url":["https://2018.eswc-conferences.org/wp-content/uploads/2018/02/ESWC2018_paper_10.pdf"]}
{"year":"2018","title":"Using a Stacked Residual LSTM Model for Sentiment Intensity Prediction","authors":["J Wang, B Peng, X Zhang - Neurocomputing, 2018"],"snippet":"… To enhance performance of LSTM layers, we also introduce a bi-directional strategy [34]. The word embeddings used in this experiment was respectively pre-trained on Common Crawl 840B 2 (English) and wiki dumps 3 (Chinese) by GloVe [55] …","url":["https://www.sciencedirect.com/science/article/pii/S0925231218311226"]}
{"year":"2018","title":"Using context to identify the language of face-saving","authors":["N Naderi, G Hirst"],"snippet":"… For all our Neural Network models, we initialized our word representations using the publicly available GloVe pre-trained word embeddings (Pennington et al., 2014)8 (300-dimensional vectors trained on Common Crawl data) …","url":["ftp://ftp.db.toronto.edu/public_html/cs/ftp/public_html/pub/gh/Naderi+Hirst-ArgMining-2018.pdf"]}
{"year":"2018","title":"Using Deep Learning For Title-Based Semantic Subject Indexing To Reach Competitive Performance to Full-Text","authors":["F Mai, L Galke, A Scherp - arXiv preprint arXiv:1801.06717, 2018"],"snippet":"… We adopt the preprocessing and tokenization scheme of Galke et. al [5]. For the LSTM and CNN, we use 300-dimensional pretrained word embeddings obtained from training GloVe [28] on Common Crawl with 840 billion tokens7. Out-of-vocabulary words are discarded …","url":["https://arxiv.org/pdf/1801.06717"]}
{"year":"2018","title":"Using Machine Learning to Detect Malicious Websites","authors":["R Elsaleh - 2018"],"snippet":"… Benign Data Benign data was obtained from the Common Crawl6. The Common Crawl is a massive, continuously updated collection of crawled websites available for download … PhishTank 36,485 65,000 VirusTotal 5,036 0 6 http://commoncrawl.org/ Page 21. 12 …","url":["http://search.proquest.com/openview/da712bc2891c9bddbdc64e287a72dcc1/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2018","title":"Using Monolingual Data in Neural Machine Translation: a Systematic Study","authors":["F Burlot, F Yvon - Proceedings of the Third Conference on Machine …, 2018"],"snippet":"… For German, we use samples from News-Commentary-11, Rapid, Common-Crawl (WMT 2017) and Multi- UN (see table 1). Bilingual BPE units (Sennrich et al., 2016b) are learned with 50k merge operations, yielding …","url":["http://www.aclweb.org/anthology/W18-6315"]}
{"year":"2018","title":"Using Wikipedia Edits in Low Resource Grammatical Error Correction","authors":["A Boyd"],"snippet":"… 2.4 Language Model For reranking, we train a language model on the first one billion lines (~12 billion tokens) of the deduplicated German Common Crawl corpus (Buck et al., 2014). 3 Method … 2014. N-gram counts and language models from the Common Crawl …","url":["http://noisy-text.github.io/2018/pdf/W-NUT201811.pdf"]}
{"year":"2018","title":"Using Word Embeddings for Information Retrieval: How Collection and Term Normalization Choices Affect Performance","authors":["D Roy, D Ganguly, S Bhatia, S Bedathur, M Mitra - 2018"],"snippet":"… In future, we plan to solidify these observations to offer general best practices for a range of different neural IR methods (eg DRRM[7]) as well as experiment using large datasets (eg Common Crawl). REFERENCES [1] Qingyao Ai …","url":["http://sumitbhatia.net/papers/cikm18.pdf"]}
{"year":"2018","title":"Utilizing Neural Networks and Linguistic Metadata for Early Detection of Depression Indications in Text Sequences","authors":["M Trotzek, S Koitka, CM Friedrich - arXiv preprint arXiv:1804.07000, 2018"],"snippet":"Page 1. SUBMITTED FOR PUBLICATION TO THE IEEE, 2018 1 Utilizing Neural Networks and Linguistic Metadata for Early Detection of Depression Indications in Text Sequences Marcel Trotzek, Sven Koitka, and Christoph M. Friedrich, Member, IEEE …","url":["https://arxiv.org/pdf/1804.07000"]}
{"year":"2018","title":"UWB at SemEval-2018 Task 10: Capturing Discriminative Attributes from Word Distributions","authors":["T Brychcín, T Hercig, J Steinberger, M Konkol - … of The 12th International Workshop on …, 2018"],"snippet":"… SS-GloVe 6B, Wikipedia + Gigaword 5 n = 300 62.0% 62.5% SS-GloVe 42B, Common Crawl n = 300 62.6% 62.7% SS-GloVe 840B, Common Crawl n = 300 62.1% 62.6 … SS-LDA 1-5B, Wikipedia n = 200 60.5% 63.1 …","url":["http://www.aclweb.org/anthology/S18-1153"]}
{"year":"2018","title":"Vecsigrafo: Corpus-based Word-Concept Embeddings","authors":["R Denaux, JM Gomez-Perez"],"snippet":"… To compare our embeddings to those trained on a very large corpus, we use pre-calculated GloVe embeddings that were trained on CommonCrawl7. Besides the text corpora, the tested embeddings con …","url":["http://semantic-web-journal.net/system/files/swj1864.pdf"]}
{"year":"2018","title":"Visual and affective grounding in language and mind","authors":["S De Deyne, DJ Navarro, G Collell, A Perfors"],"snippet":"… We also included an extremely large corpus consisting of 840 billion words from the Common Crawl project.5 As before, the language vectors were combined in a multimodal visual or affective model and the correlations were optimized by fitting values of β …","url":["https://compcogscisydney.org/publications/DeDeyneNCP_grounding.pdf"]}
{"year":"2018","title":"Visual Concept Selection with Textual Knowledge for Understanding Activities of Daily Living and Life Moment Retrieval","authors":["TH Tang12, MH Fu, HH Huang, KT Chen, HH Chen13"],"snippet":"… Page 8. GloVe [9] trained on Common Crawl with 840B tokens and ConceptNet Numberbatch [8]. The comparison in percentage dissimilarity [1] is shown in Table 1, where (G) and (N) denote GloVe and ConceptNet Numberbatch word vectors, respectively …","url":["http://ceur-ws.org/Vol-2125/paper_124.pdf"]}
{"year":"2018","title":"Visual Question Answering using Explicit Visual Attention","authors":["V Lioutas, N Passalis, A Tefas - Circuits and Systems (ISCAS), 2018 IEEE …, 2018"],"snippet":"… For extracting textual representations we used pre-trained GloVe embedding vectors (Common Crawl (42B tokens), 300d) [1]. Note that the GloVe embeddings were used only for initialization and then they were optimized during the training …","url":["https://ieeexplore.ieee.org/abstract/document/8351158/"]}
{"year":"2018","title":"Visual Relationship Detection Based on Guided Proposals and Semantic Knowledge Distillation","authors":["F Plesse, A Ginsca, B Delezoide, F Prêteux - arXiv preprint arXiv:1805.10802, 2018"],"snippet":"… iterations. The word embeddings used by the semantic knowledge introduced in Section 2.1 were obtained from the publicly available Glove model [19] trained on the Common Crawl corpus, consisting of 42B tokens. 4.2. Results …","url":["https://arxiv.org/pdf/1805.10802"]}
{"year":"2018","title":"Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs","authors":["S Kumar, Y Tsvetkov - arXiv preprint arXiv:1812.04616, 2018"],"snippet":"… target word embeddings for English and French on corpora constructed using WMT'16 (Bojar et al., 2016) monolingual datasets containing data from Europarl, News Commentary, News Crawl from 2007 to 2015 and News …","url":["https://arxiv.org/pdf/1812.04616"]}
{"year":"2018","title":"WBI at CLEF eHealth 2018 Task 1: Language-independent ICD-10 coding using multi-lingual embeddings and recurrent neural networks","authors":["J Ševa, M Sänger, U Leser - 2018"],"snippet":"… Each token is represented using pre-trained fastText5 word embeddings [4]. We utilize fastText embedding models for French, Italian and Hungarian trained on Common Crawl and Wikipedia articles6. Independently from …","url":["http://ceur-ws.org/Vol-2125/paper_118.pdf"]}
{"year":"2018","title":"Weaver: Deep Co-Encoding of Questions and Documents for Machine Reading","authors":["M Raison, PE Mazaré, R Das, A Bordes - arXiv preprint arXiv:1804.10490, 2018"],"snippet":"… Unless otherwise noted, we use 300dimensional FastText word embeddings trained on Common Crawl (Mikolov et al., 2017) and keep them fixed during training. Out-of-vocabulary words are represented with a fixed randomly initialized vector …","url":["https://arxiv.org/pdf/1804.10490"]}
{"year":"2018","title":"Web archives and Knowledge organisation","authors":["NO Finnemann, D Phil"],"snippet":"… Internet Archive, established in 1996, and Common Crawl (commoncrawl.org) established in 2007.12 Since 2006 the Internet Archive also provide a subscriptionbased archive service, Archive-it (archive-it.org) allowing anybody …","url":["https://curis.ku.dk/ws/files/189392223/Web_Archives_Manuscript.pdf"]}
{"year":"2018","title":"What can we learn from Semantic Tagging?","authors":["M Abdou, A Kulmizev, V Ravishankar, L Abzianidze… - arXiv preprint arXiv …, 2018"],"snippet":"… sets of experiments: we optimized using Adam with a learning rate of 0.00005; we weight the auxiliary semantic tagging loss with λ = 0.1; the pre-trained word embeddings we use are GloVe embeddings of dimension 300 trained …","url":["https://arxiv.org/pdf/1808.09716"]}
{"year":"2018","title":"What's Cached is Prologue: Reviewing Recent Web Archives Research Towards Supporting Scholarly Use","authors":["E Maemura"],"snippet":"… Internet Archive. Samar et al. (2016) analyze coverage of trending topics for the Netherlands in 2014 by comparing the National Library of the Netherlands' web archive to the Common Crawl dataset. Milligan et al. (2016) use …","url":["https://tspace.library.utoronto.ca/bitstream/1807/89426/1/Maemura%20AM2018%20Paper-Postprint.pdf"]}
{"year":"2018","title":"When data permutations are pathological: the case of neural natural language inference","authors":["N Schluter, D Varab - Proceedings of the 2018 Conference on Empirical …, 2018"],"snippet":"… via an LSTM. Other hyperparameters. We use 300 dimensional GloVe embeddings trained on the Common Crawl 840B tokens dataset (Pennington et al., 2014), which remain fixed during training. Out of vocabulary (OOV …","url":["http://www.aclweb.org/anthology/D18-1534"]}
{"year":"2018","title":"Who gets held accountable when a facial recognition algorithm fails?","authors":["E Broad - IQ: The RIM Quarterly, 2018"],"snippet":"… In that experiment, the machine learning tool was trained on what's called a “common crawl” corpus: a list of 840 billion words in material published on the Web. Training AI on historical data can freeze our society in its current setting, or even turn it back …","url":["https://search.informit.com.au/documentSummary;dn=965944620566147;res=IELBus"]}
{"year":"2018","title":"WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse Supplementary Material","authors":["M Faruqui, E Pavlick, I Tenney, D Das"],"snippet":"… 3.3 Experimental Setting We use FastText (Mikolov et al., 2018; Grave et al., 2018)3 word vectors of length 300, originally trained on more than 600 billion word to- kens each from Common Crawl corpus for each language …","url":["http://anthology.aclweb.org/attachments/D/D18/D18-1028.Attachment.pdf"]}
{"year":"2018","title":"Wikipedia Text Reuse: Within and Without","authors":["M Alshomary, M Völske, T Licht, H Wachsmuth, B Stein… - arXiv preprint arXiv …, 2018"],"snippet":"… Abstract. We study text reuse related to Wikipedia at scale by compiling the first corpus of text reuse cases within Wikipedia as well as without (ie, reuse of Wikipedia text in a sample of the Common Crawl) … 3 …","url":["https://arxiv.org/pdf/1812.09221"]}
{"year":"2018","title":"WikiRef: Wikilinks as a route to recommending appropriate references for scientific Wikipedia pages","authors":["A Jana, P Kanojiya, A Mukherjee, P Goyal - arXiv preprint arXiv:1806.04092, 2018"],"snippet":"… proposed by Conneau et al. (2017). Note that, for applying this architecture we use the GloVe vectors trained on Common Crawl data (840B tokens)7as seeds for representing words in a document. We name these variants of …","url":["https://arxiv.org/pdf/1806.04092"]}
{"year":"2018","title":"Will it Blend? Blending Weak and Strong Labeled Data in a Neural Network for Argumentation Mining","authors":["E Shnarch, C Alzate, L Dankin, M Gleize, Y Hou… - Proceedings of the 56th …, 2018"],"snippet":"… maximum global norm of 1.0. Words are represented using the 300 dimensional GloVe embeddings learned on 840B Common Crawl tokens and are left untouched during training (Pennington et al., 2014). We note that even …","url":["http://www.aclweb.org/anthology/P18-2095"]}
{"year":"2018","title":"Word embedding for French natural language in healthcare: a comparative study","authors":["E DYNOMANT, R LELONG, B DAHAMNA…"],"snippet":"… [30] compared the three word embedding methods but the three models were trained on different datasets (Word2Vec on news data, while FastText and GloVe trained on more definitional data, Wikipedia and Common Crawl respectively) …","url":["https://preprints.jmir.org/preprint/download/12310/pdf"]}
{"year":"2018","title":"Word embeddings for monolingual and cross-lingual domain-specific information retrieval","authors":["C Wigder - 2018"],"snippet":"Page 1. Word embeddings for monolingual and cross-lingual domain-specific information retrieval CHAYA WIGDER Master in Computer Science Date: June 4, 2018 Supervisor: Johan Boye Examiner: Viggo Kann Swedish title …","url":["http://www.nada.kth.se/~ann/exjobb/chaya_wigder.pdf"]}
{"year":"2018","title":"Word Emotion Induction for Multiple Languages as a Deep Multi-Task Learning Problem","authors":["S Buechel, U Hahn"],"snippet":"… experiments, we rely on the following widely used, publicly available embedding models trained on very large corpora (summarized in Table 3): the SGNS model trained on the Google News corpus2 (GOOGLE), the …","url":["https://www.researchgate.net/profile/Sven_Buechel/publication/325019685_Word_Emotion_Induction_for_Multiple_Languages_as_a_Deep_Multi-Task_Learning_Problem/links/5af1b275aca272bf425628a9/Word-Emotion-Induction-for-Multiple-Languages-as-a-Deep-Multi-Task-Learning-Problem.pdf"]}
{"year":"2018","title":"Word2Bits-Quantized Word Vectors","authors":["M Lam - arXiv preprint arXiv:1803.05651, 2018"],"snippet":"… complete picture of the relative performance of the two. We would also like to train quantized word vectors on much larger corpuses of data such as Common Crawl or Google News. Another task is to validate that overfitting occurs …","url":["https://arxiv.org/pdf/1803.05651"]}
{"year":"2018","title":"XNLI: Evaluating Cross-lingual Sentence Representations","authors":["A Conneau, G Lample, R Rinott, A Williams… - arXiv preprint arXiv …, 2018"],"snippet":"… on the word translation task. In this paper, we pretrain our embeddings using the common-crawl word embeddings (Grave et al., 2018) aligned with the MUSE library of Conneau et al. (2018b). 4.2.2 Universal Multilingual Sentence …","url":["https://arxiv.org/pdf/1809.05053"]}
{"year":"2018","title":"YouTube AV 50K: an Annotated Corpus for Comments in Autonomous Vehicles","authors":["T Li, L Lin, M Choi, K Fu, S Gong, J Wang - arXiv preprint arXiv:1807.11227, 2018"],"snippet":"Page 1. YouTube AV 50K: an Annotated Corpus for Comments in Autonomous Vehicles Tao Li Department of Computer Science Purdue University West Lafayette, IN 47907 Email: taoli@purdue.edu Kaiming Fu Weldon School …","url":["https://arxiv.org/pdf/1807.11227"]}
{"year":"2018","title":"Zero-Shot Object Detection by Hybrid Region Embedding","authors":["B Demirel, RG Cinbis, N Ikizler-Cinbis - arXiv preprint arXiv:1805.06157, 2018"],"snippet":"… 4.2 Class Embeddings For the Fashion-ZSD dataset, we generate 300-dimensional GloVe word embedding vectors [31] for each class name, using Common Crawl Data1. For the class names that contain multiple words, we take the average of the word vectors …","url":["https://arxiv.org/pdf/1805.06157"]}
{"year":"2018","title":"Zewen at SemEval-2018 Task 1: An Ensemble Model for Affect Prediction in Tweets","authors":["Z Chi, H Huang, J Chen, H Wu, R Wei - … of The 12th International Workshop on …, 2018"],"snippet":"… GloVe (Pennington et al., 2014) trained by Common Crawl … the same model hyperparameters which are listed in Table 1 and Table 2. Also, the four methods use the same word em- beddings, which is a pre-trained …","url":["http://www.aclweb.org/anthology/S18-1046"]}
{"year":"2019","title":"% 0 Conference Proceedings% TA Large DataBase of Hypernymy Relations Extracted from the Web.% A Seitner, Julian% A Bizer, Christian% A Eckert, Kai","authors":["A Ponzetto, S Paolo"],"snippet":"… for many word understanding applications. We present a publicly available database containing more than 400 million hypernymy relations we extracted from the CommonCrawl web corpus. We describe the infrastructure we …","url":["https://www.aclweb.org/anthology/papers/L/L16/L16-1056.endf"]}
{"year":"2019","title":"A 6-month Analysis of Factors Impacting Web Browsing Quality for QoE Prediction","authors":["A Saverimoutou, B Mathieu, S Vaton - Computer Networks, 2019"],"snippet":"… Fourth Party [15] instruments the Mozilla-Firefox browser and Web Xray [16] is a PhantomJS based tool for measuring HTTP traffic. XRay [17] and AdFisher [18] run automated personalization detection experiments and …","url":["https://www.sciencedirect.com/science/article/pii/S1389128619307546"]}
{"year":"2019","title":"A Bilingual Adversarial Autoencoder for Unsupervised Bilingual Lexicon Induction","authors":["X Bai, H Cao, K Chen, T Zhao - IEEE/ACM Transactions on Audio, Speech, and …, 2019"],"snippet":"… [29]. This dataset consists of gold dictionaries and 300-dimensional CBOW5 embeddings trained on WacKy crawling corpora (English, Italian, German), Common Crawl (Finish) and WMT News Crawl (Spanish). We report the results 2We set k = 10 …","url":["https://ieeexplore.ieee.org/abstract/document/8754809/"]}
{"year":"2019","title":"A Combined Approach to Automatic Taxonomy Extraction","authors":["S Pecar, M Simko"],"snippet":"… [10] presented publicly available database of hypernym relations called WebIsA. This database was created using Hearst-like patterns on CommonCrawl web corpus. They extracted more than 400 million hypernymy relations …","url":["https://ieeexplore.ieee.org/iel7/8859030/8864801/08864911.pdf"]}
{"year":"2019","title":"A Common Semantic Space for Monolingual and Cross-Lingual Meta-Embeddings","authors":["I García Ferrero - 2019","I García, R Agerri, G Rigau - arXiv preprint arXiv:2001.06381, 2020"],"snippet":"… From GloVe (GV) [34], the Common Crawl vectors (600 billion words) … The WS353 dataset is di- vided in two subsets [1]. In this section all the meta-embeddings have been mapped to the vector space of the English FastText (Common Crawl, 600B tokens) …","url":["https://addi.ehu.eus/bitstream/handle/10810/36183/MAL-Iker_Garcia.pdf?sequence=1&isAllowed=y","https://arxiv.org/pdf/2001.06381"]}
{"year":"2019","title":"A COMPARATIVE STUDY ON END-TO-END SPEECH TO TEXT TRANSLATION","authors":["P Bahar, T Bieschke, H Ney"],"snippet":"… We select our checkpoints based on the dev set. For the MT training, we use the TED, OpenSubtitles2018, Europarl, ParaCrawl, CommonCrawl, News Commentary, and Rapid corpora resulting in 32M sentence pairs after filtering noisy samples …","url":["https://www-i6.informatik.rwth-aachen.de/publications/download/1121/BaharParniaBieschkeTobiasNeyHermann--Acomparativestudyonend-to-endspeechtotexttranslation--2019.pdf"]}
{"year":"2019","title":"A Comparison of Context-sensitive Models for Lexical Substitution","authors":["AG Soler, A Cocos, M Apidianaki, C Callison-Burch"],"snippet":"… to two context-insensitive baselines that solely rely on the target-to-substitute similarity of standard, pre-trained word embeddings: 300-dimensional GloVe vectors (Pennington et al., 2014)5 and 300-dimensional FastText vectors …","url":["http://www.cis.upenn.edu/~ccb/publications/comparison-of-context-sensitive-models-for-lexical-substitution.pdf"]}
{"year":"2019","title":"A Comparison of Neural Document Classification Models","authors":["M Nitsche, S Halbritter"],"snippet":"… Bojanowski et al. (2016). It is available for 294 languages. An updated version, trained on Common Crawl in addition to Wikipedia and with adapted parameters is available for 157 languages (Grave et al., 2018). These are the …","url":["https://users.informatik.haw-hamburg.de/~ubicomp/projekte/master2019-proj/nitsche-halbritter2.pdf"]}
{"year":"2019","title":"A Comparison on Fine-grained Pre-trained Embeddings for the WMT19Chinese-English News Translation Task","authors":["Z Li, L Specia - Proceedings of the Fourth Conference on Machine …, 2019"],"snippet":"… In addition, the Common Crawl Corpus from WMT is used as monolingual data to pre-train the embeddings … We trained the embeddings on the Common Crawl Corpus provided by WMT19 and fine-tuned them on the task data when training the RNN …","url":["https://www.aclweb.org/anthology/W19-5324"]}
{"year":"2019","title":"A Contextualized Word Representation Approach for Irony Detection","authors":["L Garcıa, D Moctezuma, V Muniz - Proceedings of the Iberian Languages Evaluation …, 2019"],"snippet":"… 2.2 Word Embeddings We use the ELMo pre-trained word embeddings provided by [4], which were trained with a corpus of 20 million-words randomly sampled from the raw text released by the CoNLL 2018 shared …","url":["http://ceur-ws.org/Vol-2421/IroSvA_paper_5.pdf"]}
{"year":"2019","title":"A Dataset for Content Error Detection in Web Archives","authors":["J Kiesel, F Hubricht, B Stein, M Potthast"],"snippet":"… The Webis Web Archive 2017 [9] contains 10,000 web pages sampled from the Common Crawl [11] in a way which ensured that both well-known and less … January 2017 Common Crawl Archive, 2017. http: //commoncrawl.org …","url":["https://webis.de/downloads/publications/papers/stein_2019e.pdf"]}
{"year":"2019","title":"A Deep Dive into Supervised Extractive and Abstractive Summarization from Text","authors":["M Dey, D Das - Data Visualization and Knowledge Engineering, 2020"],"snippet":"… 7.1. As mentioned in the algorithm, estimated frequency p(w) of every words have been found from datasets (enwiki, poliblogs, commoncrawl, text8) [1]. The parameter “a” for our task is fixed at \\(3 * 10^{-3}\\). The vector of all …","url":["https://link.springer.com/chapter/10.1007/978-3-030-25797-2_5"]}
{"year":"2019","title":"A Deep Learning Approach for Identification of Confusion in Unstructured Crowdsourced Annotations","authors":["R Gardner, M Varma, C Zhu"],"snippet":"… improvements in model performance. We also compared the VQR binary classification model based on GLoVe embeddings with a model based on FastText embeddings trained on Common Crawl (3; 14). Since the FastText …","url":["https://pdfs.semanticscholar.org/ad70/7fc8b36a8f3daf8742cf92fcf099de434cec.pdf"]}
{"year":"2019","title":"A Dynamic Evolutionary Framework for Timeline Generation based on Distributed Representations","authors":["D Liang, G Wang, J Nie - arXiv preprint arXiv:1905.05550, 2019"],"snippet":"… To learn the distributed representations, we use pre-trained word vectors2, trained on Common Crawl and Wikipedia by fastText tookit [2]. Furthermore, each v(q) is only embedded by the name of the topic q as a experimental control …","url":["https://arxiv.org/pdf/1905.05550"]}
{"year":"2019","title":"A Framework to Estimate the Nutritional Value of Food in Real Time Using Deep Learning Techniques","authors":["R Yunus, O Arif, H Afzal, MF Amjad, H Abbas… - IEEE Access, 2019"],"snippet":"… First is Common-Crawl, which is an archive hosted on an Amazon S3 bucket … is more relevant as it return pages corresponding to precise labels while the text from Common-Crawl is more generic. The raw text data obtained …","url":["https://ieeexplore.ieee.org/iel7/6287639/8600701/08590712.pdf"]}
{"year":"2019","title":"A Language Invariant Neural Method for TimeML Event Detection","authors":["S Prabhu, P Goel, A Debnath, M Shrivastava"],"snippet":"… The CNN uses 40 filters with a window size of 3. For our contextual word embeddings, we use fastText embeddings for English (Bojanowski et al., 2017) which are pretrained on commonCrawl and the Wikipedia corpus. FastText …","url":["https://www.researchgate.net/profile/Pranav_Goel/publication/337387464_A_Language_Invariant_Neural_Method_for_TimeML_Event_Detection/links/5dd4c5ec299bf11ec8629470/A-Language-Invariant-Neural-Method-for-TimeML-Event-Detection.pdf"]}
{"year":"2019","title":"A Massive Collection of Cross-Lingual Web-Document Pairs","authors":["A El-Kishky, V Chaudhary, F Guzman, P Koehn - arXiv preprint arXiv:1911.06154, 2019"],"snippet":"… Other works (Smith et al., 2013) have mined Common Crawl for bitexts for machine … and restrictions, we mined 54 million aligned documents across 12 Common Crawl snapshots … 2: NMT performance on comparable directions …","url":["https://arxiv.org/pdf/1911.06154"]}
{"year":"2019","title":"A Multi-Task Approach for Disentangling Syntax and Semantics in Sentence Representations","authors":["S Wiseman, K Gimpel, Q Tang, M Chen"],"snippet":"04/02/19 - We propose a generative model for a sentence that uses two latent variables, with one intended to represent the syntax of the sent...","url":["https://deepai.org/publication/a-multi-task-approach-for-disentangling-syntax-and-semantics-in-sentence-representations"]}
{"year":"2019","title":"A New Corpus for Low-Resourced Sindhi Language with Word Embeddings","authors":["W Ali, J Kumar, J Lu, Z Xu - arXiv preprint arXiv:1911.12579, 2019"],"snippet":"… 3Available online at https://rdrr.io/cran/wordspace/man/WordSim353.html 4We denote Sindhi word representations as (SdfastText) recently revealed by fastText, available at (https://fasttext.cc/docs/en/crawl-vectors.html) trained …","url":["https://arxiv.org/pdf/1911.12579"]}
{"year":"2019","title":"A New Hybrid Ensemble Feature Selection Framework for Machine Learning-based Phishing Detection System","authors":["KL Chiew, CL Tan, KS Wong, KSC Yong, WK Tiong - Information Sciences, 2019"],"snippet":"… June 2017. Specifically, we selected 5000 phishing webpages based on URLs from PhishTank 2 and OpenPhish 3 , and another 5000 legitimate webpages based on URLs from Alexa 4 and the Common Crawl 5 archive. The …","url":["https://www.sciencedirect.com/science/article/pii/S0020025519300763"]}
{"year":"2019","title":"A Question-Entailment Approach to Question Answering","authors":["AB Abacha, D Demner-Fushman - arXiv preprint arXiv:1901.08079, 2019"],"snippet":"… We use the pretrained common crawl version with 840B tokens and 300d vectors, which are not updated during training. 3.3 Logistic Regression Classifier In this feature-based approach, we use Logistic Regression to …","url":["https://arxiv.org/pdf/1901.08079"]}
{"year":"2019","title":"A Recurrent Deep Neural Network Model to measure Sentence Complexity for the Italian","authors":["D Schicchi"],"snippet":"… The authors have used FastText [3], a library for efficient learning of word representations and sentence classification, trained on Common Crawl [21] and Wikipedia to create a pre-trained word vector representation for …","url":["http://ceur-ws.org/Vol-2418/paper10.pdf"]}
{"year":"2019","title":"A Robust Abstractive System for Cross-Lingual Summarization","authors":["J Ouyang, B Song, K McKeown"],"snippet":"… about 23k sentences for Somali and Swahili and 51k for Tagalog); noisy, web-crawled parallel data (So- mali only, about 354k sentences); and synthetic, backtranslated parallel data created from monolingual sources including …","url":["http://www.cs.columbia.edu/~ouyangj/OuyangSongMcKeown2019.pdf"]}
{"year":"2019","title":"A Study of Neural Networks Models applied to Natural Language Inference","authors":["VG Noronha, JCP da Silva"],"snippet":"… word vectors of different size, in order to check whether the word space dimension plays an important role on the final results: – A 100d version trained on the Wikipedia 2014 + Gigaword 5 corpus, with 6B tokens; – The 300d …","url":["https://www.researchgate.net/profile/Joao_Silva45/publication/320711838_A_Study_of_Neural_Networks_Models_applied_to_Natural_Language_Inference/links/5b23a863458515270fcff1e1/A-Study-of-Neural-Networks-Models-applied-to-Natural-Language-Inference.pdf"]}
{"year":"2019","title":"A Survey of URL-based Phishing Detection","authors":["ES Aung, CT Zan, H YAMANA"],"snippet":"… 2017 [40] 98.76 98.60 98.93 98.76 99.91 Common Crawl PhishTank 1M 1M Balanced Path length, URL entropy, length ratio, '@' and '-' count, punctuation count, TLDs count, IP address, suspicious words count …","url":["https://db-event.jpn.org/deim2019/post/papers/201.pdf"]}
{"year":"2019","title":"A Survey on Document-level Machine Translation: Methods and Evaluation","authors":["S Maruf, F Saleh, G Haffari - arXiv preprint arXiv:1912.08494, 2019"],"snippet":"Page 1. A Survey on Document-level Machine Translation: Methods and Evaluation Sameen Maruf, Fahimeh Saleh, and Gholamreza Haffari Faculty of Information Technology, Monash University, Clayton VIC, Australia {firstname.lastname}@monash.edu …","url":["https://arxiv.org/pdf/1912.08494"]}
{"year":"2019","title":"A System to Monitor Cyberbullying based on Message Classification and Social Network Analysis","authors":["S Menini, G Moretti, M Corazza, E Cabrio, S Tonelli… - Proceedings of the Third …, 2019"],"snippet":"… timestep. We use English Fasttext embeddings1 trained on Common Crawl with a size of 300. Concerning hy- perparameters, our model uses no dropout and no batch normalization on the outputs of the hidden layer. Instead …","url":["https://www.aclweb.org/anthology/W19-3511"]}
{"year":"2019","title":"A Systematic Comparison Between SMT and NMT on Translating User-Generated Content","authors":["P Lohar, M Popovic, H Afli, A Way"],"snippet":"… Morever, as the Europarl corpus is a fix-domain and did not work well for our experiments, we plan to utilise other types of mix-domain parallel resource such as common crawl corpus9 … WASSA '12 (2012) 52–60 9 http://www.statmt …","url":["http://www.computing.dcu.ie/~away/PUBS/2019/A_Systematic_Comparison_Between_SMT_and_NMT_on_Translating_User_Generated_Content.pdf"]}
{"year":"2019","title":"A thesis submitted in conformity with the requirements for the degree of Doctor of Philosophy Graduate Department of Computer Science","authors":["N Naderi - 2019"],"snippet":"Page 1. COMPUTATIONAL ANALYSIS OF ARGUMENTS AND PERSUASIVE STRATEGIES IN POLITICAL DISCOURSE by Nona Naderi A thesis submitted in conformity with the requirements for the degree of Doctor of Philosophy …","url":["ftp://ftp.db.toronto.edu/public_html/public_html/pub/gh/Naderi-PhD-thesis-2019.pdf"]}
{"year":"2019","title":"A Vector Worth a Thousand Counts","authors":["DS Hain, R Jurowetzkiφ, T Buchmannψ, P Wolfψ"],"snippet":"… task convolutional neural network model trained on OntoNotes, with GloVe vectors (685k unique vectors with 300 dimensions) trained on Common Crawl. Given a patent abstract, spaCy predicts the meaning of each term in the document …","url":["https://pdfs.semanticscholar.org/69ba/b264607119c7928a9a25ea82823d9c346350.pdf"]}
{"year":"2019","title":"Abstract Text Summarization: A Low Resource Challenge","authors":["S Parida, P Motlicek - 2019"],"snippet":"… We propose an iterative data augmentation approach which uses synthetic data along with the real summarization data for the German language. To generate synthetic data, the Common Crawl (German) dataset is exploited, which covers different domains …","url":["https://infoscience.epfl.ch/record/270135"]}
{"year":"2019","title":"AC-Net: Assessing the Consistency of Description and Permission in Android Apps","authors":["Y Feng, L Chen, A Zheng, C Gao, Z Zheng - IEEE Access, 2019"],"snippet":"… Compared to the somewhat popular embeddings such as GloVe [22] (400 thousand word vectors trained on Wikipedia) and crawl [23] (2 million word vectors trained on Common Crawl), ours fully retains domain-specific characteristics of statements …","url":["https://ieeexplore.ieee.org/iel7/6287639/6514899/08694776.pdf"]}
{"year":"2019","title":"Acquiring Knowledge from Pre-trained Model to Neural Machine Translation","authors":["R Weng, H Yu, S Huang, S Cheng, W Luo - arXiv preprint arXiv:1912.01774, 2019"],"snippet":"… Following Song et al. (2019) , on the English and German, we use the monolingual data from WMT News Crawl. We select 50M sentence from year 2007 to 2017 for English and German respectively. Then, we choose 50M sentence from Common Crawl for Chinese …","url":["https://arxiv.org/pdf/1912.01774"]}
{"year":"2019","title":"Adapting Transformer-XL Techniques to QANet Architecture for SQuAD 2.0 Challenge","authors":["L Zhang"],"snippet":"… set data. • glove.840B.300dglove.840B.300d.txt: Pretrained GloVevectors. Theseare300dimensional embeddings trained on the CommonCrawl 840B corpus. • {word,char}_emb.json: Word and character embeddings. Only the …","url":["https://pdfs.semanticscholar.org/0760/248f62e5a313fb088cce37495ce79c9ba8a1.pdf"]}
{"year":"2019","title":"Adaptive Cross-Modal Few-Shot Learning","authors":["C Xing, N Rostamzadeh, BN Oreshkin, PO Pinheiro - arXiv preprint arXiv:1902.07104, 2019"],"snippet":"… category labels. GloVe is an unsupervised approach based on wordword co-occurrence statistics from large text corpora. We use the Common Crawl version trained on 840B tokens. The embeddings are of dimension 300. When …","url":["https://arxiv.org/pdf/1902.07104"]}
{"year":"2019","title":"Advanced Deep learning Methods and Applications in Open-domain Question Answering","authors":["MT Nguyễn - 2019"],"snippet":"Page 1. VIETNAM NATIONAL UNIVERSITY, HANOI UNIVERSITY OF ENGINEERING AND TECHNOLOGY Nguyen Minh Trang ADVANCED DEEP LEARNING METHODS AND APPLICATIONS IN OPEN-DOMAIN QUESTION ANSWERING MASTER THESIS …","url":["http://lib.uet.vnu.edu.vn/bitstream/123456789/1021/1/2.ToanVanLuanVan.pdf"]}
{"year":"2019","title":"Adversarial NLI: A New Benchmark for Natural Language Understanding","authors":["Y Nie, A Williams, E Dinan, M Bansal, J Weston… - arXiv preprint arXiv …, 2019"],"snippet":"… transfer. In addition to contexts from Wikipedia for Round 3, we also included contexts from the following domains: News (ex- tracted from Common Crawl), fiction (extracted from Mostafazadeh et al. 2016, Story Cloze, and Hill et al …","url":["https://arxiv.org/pdf/1910.14599"]}
{"year":"2019","title":"Adverse drug event detection from electronic health records using hierarchical recurrent neural networks with dual-level embedding","authors":["S Wunnava, X Qin, T Kakar, C Sen, EA Rundensteiner… - Drug Safety, 2019"],"snippet":"… compared the results from DLADE, which uses domainand task-specific MADE1.0 word embedding trained using wiki, and Pittsburgh EHR and PubMed articles (1,352,550 word vectors) [10, 21], with two systems that use …","url":["https://link.springer.com/article/10.1007/s40264-018-0765-9"]}
{"year":"2019","title":"AELA-DLSTMs: Attention-Enabled and Location-Aware Double LSTMs for Aspect-level Sentiment Classification","authors":["K Shuang, X Ren, Q Yang, R Li, J Loo - Neurocomputing, 2018"],"snippet":"Skip to main content …","url":["https://www.sciencedirect.com/science/article/pii/S0925231218315054"]}
{"year":"2019","title":"Aiding Intra-Text Representations with Visual Context for Multimodal Named Entity Recognition","authors":["O Arshad, I Gallo, S Nawaz, A Calefati - arXiv preprint arXiv:1904.01356, 2019"],"snippet":"… Page 5. B. Word embeddings We used 300D fasttext crawl embeddings. It contains 2 million word vectors trained with subword information on Common Crawl (600B tokens). However, we do not apply fine-tuning on these embeddings during the training stage …","url":["https://arxiv.org/pdf/1904.01356"]}
{"year":"2019","title":"Algorithmic Bias and the Biases of the Bias Catchers","authors":["D Rozado - arXiv preprint arXiv:1905.11985, 2019"],"snippet":"… This work systematically analyzed 3 popular word embedding methods: Word2vec (Skipgram) (4), Glove (9) and FastText (10), externally pretrained on a wide array of corpora such as Google News, Wikipedia, Twitter and Common Crawl …","url":["https://arxiv.org/pdf/1905.11985"]}
{"year":"2019","title":"All-in-One: Emotion, Sentiment and Intensity Prediction using a Multi-task Ensemble Framework","authors":["S Akhtar, D Ghosal, A Ekbal, P Bhattacharyya… - IEEE Transactions on …, 2019"],"snippet":"… IEEE TRANSACTIONS ON AFFECTIVE COMPUTING. 4 A. Deep Learning Models We employ the architecture of Figure 1a to train and tune all the deep learning models using pre-trained GloVe (common crawl 840 billion) word embeddings [32] …","url":["https://ieeexplore.ieee.org/abstract/document/8756111/"]}
{"year":"2019","title":"AMR-to-Text Generation with Cache Transition Systems","authors":["L Jin, D Gildea - arXiv preprint arXiv:1912.01682, 2019"],"snippet":"… 6.2 Setup The word embeddings are initialized with 300-dimensional GloVe embeddings (Penningtonetal., 2014) from the Common Crawl and are fixed during training. This embedding vocabulary consists of both English tokens and AMR concept labels …","url":["https://arxiv.org/pdf/1912.01682"]}
{"year":"2019","title":"An effective approach to candidate retrieval for cross-language plagiarism detection: A fusion of conceptual and keyword-based schemes","authors":["M Roostaee, MH Sadreddini, SM Fakhrahmad - Information Processing & …, 2020"],"snippet":"Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0306457318310148"]}
{"year":"2019","title":"An Efficient Framework for Processing and Analyzing Unstructured Text to Discover Delivery Delay and Optimization of Route Planning in Realtime","authors":["M Alshaer - 2019"],"snippet":"Page 1. HAL Id: tel-02310852 https://tel.archives-ouvertes.fr/tel-02310852 Submitted on 10 Oct 2019 HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not …","url":["https://tel.archives-ouvertes.fr/tel-02310852/document"]}
{"year":"2019","title":"An Empirical Evaluation of Text Representation Schemes on Multilingual Social Web to Filter the Textual Aggression","authors":["S Modha, P Majumder - arXiv preprint arXiv:1904.08770, 2019"],"snippet":"… Glove pre-trained model available with different embed size and trained on common crawl, Twitter. We have use Glove pre-trained model with vocabulary size 2.2 million and trained on common crawl. fastText pretrained models are available in 157 language …","url":["https://arxiv.org/pdf/1904.08770"]}
{"year":"2019","title":"An Empirical study on Pre-trained Embeddings and Language Models for Bot Detection","authors":["A Garcia-Silva, C Berrio, JM Gómez-Pérez - Proceedings of the 4th Workshop on …, 2019"],"snippet":"… scratch. We use pre-trained embeddings learned from Twitter it- self, urban dictionary definitions to accommodate the informal vocabulary often used in the social network, and common crawl as a general source of information …","url":["https://www.aclweb.org/anthology/W19-4317"]}
{"year":"2019","title":"An Ensemble Method for Producing Word Representations for the Greek Language","authors":["M Lioudakis, S Outsios, M Vazirgiannis - arXiv preprint arXiv:1912.04965, 2019"],"snippet":"… In addition, we show that CBOS outperforms the CBOW and Skip-gram models when they are trained on the same data. The future work of this research could include training of our newly proposed model with the Common Crawl dataset for the Greek language …","url":["https://arxiv.org/pdf/1912.04965"]}
{"year":"2019","title":"An Exploration of Sarcasm Detection Using Deep Learning","authors":["E SAVINI - 2019"],"snippet":"… not appear in the training data (”out-of-vocabulary” words). It also supports 157 different languages. In our research we use 300-dimensional word vectors pre-trained on Common Crawl3 (600B tokens). 4.3 ELMo ELMo (Embeddings …","url":["https://webthesis.biblio.polito.it/12440/1/tesi.pdf"]}
{"year":"2019","title":"An Extended CLEF eHealth Test Collection for Cross-Lingual Information Retrieval in the Medical Domain","authors":["S Saleh, P Pecina - European Conference on Information Retrieval, 2019"],"snippet":"… However, an additional assessment was performed [15]. CLEF eHealth 2018 Consumer Health Search Task released a document collection created using CommonCrawl platform [8] containing more than five million documents from more than thousand websites …","url":["https://link.springer.com/chapter/10.1007/978-3-030-15719-7_24"]}
{"year":"2019","title":"An integrated neural decoder of linguistic and experiential meaning","authors":["AJ Anderson, JR Binder, L Fernandino, CJ Humphries… - Journal of Neuroscience, 2019"],"snippet":"… occurrence 210 matrix (vocabulary size is 2.2million words and co-occurrences were measured across 840 billion 211 tokens from Common Crawl https://commoncrawl.org). GloVe in particular was used because it yielded 212 state-of …","url":["https://www.jneurosci.org/content/early/2019/09/27/JNEUROSCI.2575-18.2019.abstract"]}
{"year":"2019","title":"An Open-Domain System for Retrieval and Visualization of Comparative Arguments from Text","authors":["M Schildwächter"],"snippet":"… parts of the Page 23. 3.1. Retrieval of Sentences 19 CommonCrawl: Full text search index Retrieval of Sentences: Elasticsearch API Sentence Classification: Keyword or ML approach Sentence Ranking: Ordering of sentences","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2018-ma-schildwaechter-cam.pdf"]}
{"year":"2019","title":"Analysing Coreference in Transformer Outputs","authors":["ELKC Espana-Bonet, J van Genabith","ELKC Espana-Bonet, J van Genabith - DiscoMT 2019, 2019"],"snippet":"… lines S1, S3 S2 Common Crawl 2,394,878 x1 x4 Europarl 1,775,445 x1 x4 News Commentary 328,059 x4 x16 Rapid 1,105,651 x1 x4 ParaCrawl Filtered 12,424,790 x0 x1 … S1 is trained with the concatenation of Common …","url":["https://www.aclweb.org/anthology/D19-65.pdf#page=11","https://www.cs.upc.edu/~cristinae/CV/docs/coref_DiscoMT.pdf"]}
{"year":"2019","title":"Analysing Representations of Memory Impairment in a Clinical Notes Classification Model","authors":["M Ormerod, J Martínez-del-Rincón, N Robertson… - Proceedings of the 18th …, 2019"],"snippet":"… In this study we use 300-dimensional fastText word embeddings (Bojanowski et al., 2017) which were pretrained on the Common Crawl dataset using the skipgram schema (Mikolov et al., 2013), which in- volves predicting a target word based on nearby words …","url":["https://www.aclweb.org/anthology/W19-5005"]}
{"year":"2019","title":"Analysis of Joint Multilingual Sentence Representations and Semantic K-Nearest Neighbor Graphs","authors":["H Schwenk, D Kiela, M Douze - Proceedings of the AAAI Conference on Artificial …, 2019"],"snippet":"… All the statistics in this section are calculated on ten million sentences of Common Crawl data. Please remember that we apply romanization for the languages which use a Cyrillic script (Greek, Bosnian, Bulgarian, Macedonian, Serbian and Russian) …","url":["https://www.aaai.org/ojs/index.php/AAAI/article/download/4677/4555"]}
{"year":"2019","title":"Analysis of Positional Encodings for Neural Machine Translation","authors":["J Rosendahl, VAK Tran, W Wang, H Ney"],"snippet":"… We train our models on the data from the De→En and the Zh→En news translation task of WMT 2019. For the De→En task we train on CommonCrawl, Europarl, NewsCommentary and Rapid summing up …","url":["https://zenodo.eu/record/3525024/files/IWSLT2019_paper_21.pdf"]}
{"year":"2019","title":"Annotating and Recognising Visually Descriptive Language","authors":["T Alrashid, J Wang, R Gaizauskas - … on Interoperable Semantic Annotation (ISA-15), 2019"],"snippet":"… Stop words were only removed for the word embedding representation. We did not remove stop words for tf-idf because the approach 5https://spacy. io/. We use the model en vectors web lg. 6http://commoncrawl. org/the-data/ 7http://www. scikit-learn. org/ 27 Page 34 …","url":["https://sigsem.uvt.nl/isa15/ISA-15_proceedings.pdf#page=28"]}
{"year":"2019","title":"Answering Comparative Questions: Better than Ten-Blue-Links?","authors":["M Schildwächter, A Bondarenko, J Zenker, M Hagen… - arXiv preprint arXiv …, 2019"],"snippet":"… CommonCrawl: Full text search index … Clicking on a result sentence reveals its Common Crawl context—by default the ±3 sentences around it, with the … underlying corpus of our CAM system and the keyword-based search …","url":["https://arxiv.org/pdf/1901.05041"]}
{"year":"2019","title":"Architecture for semantic search over encrypted data in the cloud","authors":["J Woodworth, MA Salehi - US Patent App. 16/168,919, 2019"],"snippet":"… The dataset has a total size of 357 MB and is made up of 6,942 text files. To evaluate S3C under large scale datasets, a second dataset, the Common Crawl Corpus from AWS (a web crawl composed of over five billion web pages) was used …","url":["https://patentimages.storage.googleapis.com/f7/5f/a4/c986d736bd81ab/US20190121873A1.pdf"]}
{"year":"2019","title":"Are we consistently biased? Multidimensional analysis of biases in distributional word vectors","authors":["A Lauscher, G Glavaš - arXiv preprint arXiv:1904.11783, 2019"],"snippet":"… 4 we compare the biases of em- beddings trained with the same model (GLOVE) but on different corpora: Common Crawl (ie, noisy … Table 4: WEAT bias effects for GLOVE embeddings trained on different corpora: Wikipedia …","url":["https://arxiv.org/pdf/1904.11783"]}
{"year":"2019","title":"Are We Safe Yet? The Limitations of Distributional Features for Fake News Detection","authors":["T Schuster, R Schuster, DJ Shah, R Barzilay - arXiv preprint arXiv:1908.09805, 2019"],"snippet":"… 2019). The generator is trained with a LM objective on a large news corpus from Common Crawl dumps. The fake news detector is a simple linear classifier on top of the last hidden state of Grover's LM on the examined article …","url":["https://arxiv.org/pdf/1908.09805"]}
{"year":"2019","title":"Argument Generation with Retrieval, Planning, and Realization","authors":["X Hua, Z Hu, L Wang - arXiv preprint arXiv:1906.03717, 2019"],"snippet":"… Wachsmuth et al., 2017b, 2018b). Recent work by Stab et al. (2018) in- dexes all web documents collected in Common Crawl, which inevitably incorporates noisy, lowquality content. Besides, existing work treats individual …","url":["https://arxiv.org/pdf/1906.03717"]}
{"year":"2019","title":"Argument Search: Assessing Argument Relevance","authors":["M Potthast, L Gienapp, F Euchner, N Heilenkötter… - 2019"],"snippet":"… online debating portals. Thereafter, ArgumenText [19], which retrieves argumentative sentences from the Common Crawl, and “multi-perspective answers” in the US version of Bing3 have been published. Another loosely related …","url":["https://webis.de/downloads/publications/papers/stein_2019j.pdf"]}
{"year":"2019","title":"Articles Classification in Myanmar Language","authors":["MS Phyu, KT Nwet - 2019 International Conference on Advanced …, 2019"],"snippet":"… Fortunately, Grave et al. [3] recently released pretrained vectors for 246 languages trained on Wikipedia and common crawl … Data source Method Number of word vectors Dimension Wikipedia fastText (skip-gram) 91,497 …","url":["https://ieeexplore.ieee.org/abstract/document/8920927/"]}
{"year":"2019","title":"Artificial Intelligence: An Overview","authors":["P Grogono"],"snippet":"Page 1. Chapter 1 Artificial Intelligence: An Overview Peter Grogono Department of Computer Science and Software Engineering Concordia University Montréal, Québec The smart devices that we have become so familiar with …","url":["https://www.worldscientific.com/doi/abs/10.1142/9789811203527_0001"]}
{"year":"2019","title":"Aspect Detection using Word and Char Embeddings with (Bi) LSTM and CRF","authors":["Ł Augustyniak, T Kajdanowicz, P Kazienko - 2019 IEEE Second International …, 2019"],"snippet":"… Glove 840B - Global Vectors for Word Representation proposed by Stanford NLP Group, trained based on Common Crawl. fastText - Distributed Word Representation proposed by Facebook, trained on Common Crawl as well …","url":["https://ieeexplore.ieee.org/abstract/document/8791735/"]}
{"year":"2019","title":"Aspect-Based Sentiment Analysis Using Deep Neural Networks and Transfer Learning","authors":["S Dugar"],"snippet":"Page 1. DEPARTMENT OF INFORMATICS TECHNISCHE UNIVERSITÄT MÜNCHEN Master's Thesis in Informatics Aspect-Based Sentiment Analysis Using Deep Neural Networks and Transfer Learning Sumit Dugar Page 2. DEPARTMENT OF INFORMATICS …","url":["https://www.social.in.tum.de/fileadmin/w00bwc/www/Gerhard_Hagerer/thesis-sumit-dugar.pdf"]}
{"year":"2019","title":"Assessing Social and Intersectional Biases in Contextualized Word Representations","authors":["YC Tan, LE Celis - arXiv preprint arXiv:1911.01485, 2019"],"snippet":"Page 1. Assessing Social and Intersectional Biases in Contextualized Word Representations Yi Chern Tan, L. Elisa Celis Yale University {yichern.tan, elisa.celis}@yale.edu Abstract Social bias in machine learning has drawn …","url":["https://arxiv.org/pdf/1911.01485"]}
{"year":"2019","title":"Assessing the Impact of Contextual Embeddings for Portuguese Named Entity Recognition","authors":["J Santos, B Consoli, C dos Santos, J Terra, S Collonini… - 2019 8th Brazilian …, 2019"],"snippet":"… 2019. [19] C. Buck, K. Heafield, and B. van Ooyen, “N-gram counts and language models from the common crawl,” in Proceedings of the Language Resources and Evaluation Conference, Reykjavik, Iceland, May 2014. [20] N …","url":["https://ieeexplore.ieee.org/abstract/document/8923652/"]}
{"year":"2019","title":"Assessing the Lexico-Semantic Relational Knowledge Captured by Word and Concept Embeddings","authors":["R Denaux, JM Gomez-Perez - arXiv preprint arXiv:1909.11042, 2019"],"snippet":"… three different corpora, which we chose to study whether relation prediction capacity varies depending on the corpus size: the English United Nations corpus[Ziemski et al., 2016] (517M tokens), the En- glish Wikipedia (just under …","url":["https://arxiv.org/pdf/1909.11042"]}
{"year":"2019","title":"Assessment of text coherence using an ontology‐based relatedness measurement method","authors":["G Giray, MO Ünalır - Expert Systems"],"snippet":"Abstract This paper proposes a novel method for assessing text coherence. Central to this approach is an ontology‐based representation of text, which captures the level of relatedness between conse...","url":["https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12505"]}
{"year":"2019","title":"AStylometric INVESTIGATION OF CHARACTER VOICES IN LITERARY FICTION","authors":["K Vishnubhotla - 2019"],"snippet":"Page 1. ASTYLOMETRIC INVESTIGATION OF CHARACTER VOICES IN LITERARY FICTION by Krishnapriya Vishnubhotla A thesis submitted in conformity with the requirements for the degree of Master of Science …","url":["ftp://ftp.db.toronto.edu/public_html/cs/ftp/dist/gh/Vishnubhotla-MSc-thesis-2019.pdf"]}
{"year":"2019","title":"Asymmetry Sensitive Architecture for Neural Text Matching","authors":["T Belkacem, JG Moreno, T Dkaki, M Boughanem - European Conference on …, 2019"],"snippet":"… We adopted a cross-validation with \\(80\\%\\) to train, \\(10\\%\\) to test and \\(10\\%\\) to validate the different models. We used a public pre-trained 300-dimensional word vectors of GloVe 3 , which are trained in a Common crawl dataset …","url":["https://link.springer.com/chapter/10.1007/978-3-030-15719-7_8"]}
{"year":"2019","title":"Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures","authors":["PJ Ortiz Suárez, B Sagot, L Romary - 2019"],"snippet":"… 7http://commoncrawl.org/about/ 8http://microformats.org/wiki/ rel-nofollow 9https://www.robotstxt.org … In order to download, extract, filter, clean and classify Common Crawl we base ourselves on … Each of these processes first …","url":["https://ids-pub.bsz-bw.de/files/9021/Suarez_Sagot_Romary_Asynchronous_Pipeline_for_Processing_Huge_Corpora_2019.pdf"]}
{"year":"2019","title":"At the Lower End of Language—Exploring the Vulgar and Obscene Side of German","authors":["E Eder, U Krieg-Holz, U Hahn - Proceedings of the Third Workshop on Abusive …, 2019"],"snippet":"… 123 FASTTEXT (Grave et al., 2018) word embeddings, the latter being based on COMMON CRAWL and WIKIPEDIA … et al., 2017) trained on German tweets (TWITTER) and, finally, FASTTEXT word embeddings (Grave et al., 2018) …","url":["https://www.aclweb.org/anthology/W19-3513"]}
{"year":"2019","title":"Attending the Emotions to Detect Online Abusive Language","authors":["NS Samghabadi, A Hatami, M Shafaei, S Kar, T Solorio - arXiv preprint arXiv …, 2019"],"snippet":"… Then, we extract 5For ask.fm data, we use 300-dimensional Common Crawl Glove pre-trained embeddings, since it works better than the Twitter embedding. the DeepMoji vector for each sentence and calculate the average vector per post …","url":["https://arxiv.org/pdf/1909.03100"]}
{"year":"2019","title":"Attention Guided Graph Convolutional Networks for Relation Extraction","authors":["Z Guo, Y Zhang, W Lu - arXiv preprint arXiv:1906.07510, 2019"],"snippet":"… 5https://nlp.stanford.edu/projects/ tacred/ 6We use the 300-dimensional Glove word vectors trained on the Common Crawl corpus https://nlp. stanford.edu/projects/glove/ 7The results are produced by the open implementation of Zhang et al. (2018). Page 6. Model …","url":["https://arxiv.org/pdf/1906.07510"]}
{"year":"2019","title":"Attenuating Bias in Word Vectors","authors":["S Dev, J Phillips - arXiv preprint arXiv:1901.07656, 2019"],"snippet":"Page 1. Attenuating Bias in Word Vectors Sunipa Dev Jeff Phillips University of Utah University of Utah Abstract Word vector representations are well developed tools for various NLP and Machine Learning tasks and are known …","url":["https://arxiv.org/pdf/1901.07656"]}
{"year":"2019","title":"Attribute Sentiment Scoring With Online Text Reviews: Accounting for Language Structure and Attribute Self-Selection","authors":["I Chakraborty, M Kim, K Sudhir - 2019"],"snippet":"Page 1. Attribute Sentiment Scoring with Online Text Reviews: Accounting for Language Structure and Attribute Self-Selection Ishita Chakraborty, Minkyung Kim, K. Sudhir Yale School of Management March 2019 We thank the …","url":["http://sics.haas.berkeley.edu/pdf_2019/paper_cks.pdf"]}
{"year":"2019","title":"Augmenting Neural Machine Translation through Round-Trip Training Approach","authors":["B Ahmadnia, BJ Dorr"],"snippet":"… For the high-resource scenario (En- Es) we utilize the English-Spanish bilingual corpora from WMT'18² [29] which contains 10M sentence pairs extracting from Europarl, News-Commentary, UN and Common Crawl collections …","url":["https://www.researchgate.net/profile/Benyamin_Ahmadnia/publication/336485784_Augmenting_Neural_Machine_Translation_through_Round-Trip_Training_Approach/links/5da2b06d92851c6b4bd100ab/Augmenting-Neural-Machine-Translation-through-Round-Trip-Training-Approach.pdf"]}
{"year":"2019","title":"AutoEncoder Guided Bootstrapping of Semantic Lexicon","authors":["C Hu, M Nakano, M Okumura - Pacific Rim International Conference on Artificial …, 2019"],"snippet":"… The top-5 best candidate instances were then added to the expanded seed list for the next iteration. For the inputs of the AutoEncoder model, Glove embeddings (300 dimensions) trained on Common Crawl were used (Pennington et al …","url":["https://link.springer.com/chapter/10.1007/978-3-030-29894-4_17"]}
{"year":"2019","title":"Automated Dictionary Creation for Analyzing Text: An Illustration from Stereotype Content","authors":["G Nicolas, X Bai, ST Fiske - 2019"],"snippet":"… The Glove word embeddings used here were trained using around 840 billion words from the common crawl (a very large database of web text), and it has word-vectors with 300 dimensions for 2.2 million words (available …","url":["https://psyarxiv.com/afm8k/download?format=pdf"]}
{"year":"2019","title":"Automated extraction of attributes from natural language attribute-based access control (ABAC) Policies","authors":["M Alohaly, H Takabi, E Blanco - Cybersecurity, 2019"],"snippet":"The National Institute of Standards and Technology (NIST) has identified natural language policies as the preferred expression of policy and implicitly called for an automated translation of ABAC...","url":["https://link.springer.com/article/10.1186/s42400-018-0019-2"]}
{"year":"2019","title":"Automated Grading of Short Text Answers: Preliminary Results in a Course of Health Informatics","authors":["G De Gasperis, S Menini, S Tonelli, P Vittorini - International Conference on Web …, 2019"],"snippet":"… vectors representing both words and sub-words. To generate these embeddings we start from the pre-computed Italian language model 3 , trained on Common Crawl and Wikipedia. The latter, in particular, is suitable for our …","url":["https://link.springer.com/chapter/10.1007/978-3-030-35758-0_18"]}
{"year":"2019","title":"Automated lifelog moment retrieval based on image segmentation and similarity scores","authors":["S Taubert, S Kahl, D Kowerko, M Eibl - CLEF2019 Working Notes. CEUR Workshop …, 2019"],"snippet":"… Page 8. 4 Resources We only used resources which were open source. Our word vectors were pretrained GloVe vectors from Common Crawl which had 300 dimensions and a vocabulary of 2.2 million tokens [27]. Furthermore …","url":["http://ceur-ws.org/Vol-2380/paper_83.pdf"]}
{"year":"2019","title":"Automated organ-level classification of free-text pathology reports to support a radiology follow-up tracking engine","authors":["JM Steinkamp, CM Chambers, D Lalevic, HM Zafar… - Radiology: Artificial …, 2019"],"snippet":"… GloVe word embeddings pretrained on the Common Crawl dataset of web pages were used as input to both networks; no performance benefit was observed from continuing to train word embeddings during the experiments …","url":["https://pubs.rsna.org/doi/abs/10.1148/ryai.2019180052"]}
{"year":"2019","title":"Automatic Knowledge Extraction to build Semantic Web of Things Applications","authors":["M Noura, A Gyrard, S Heil, M Gaedke - IEEE Internet of Things Journal, 2019"],"snippet":"… The naming process used additional hypernym information derived from WebIsALOD12 the Linked Open Data version of the WebIsA Database, a database containing 11.7 million hypernymy re- lations extracted from the CommonCrawl web corpus …","url":["http://knoesis.wright.edu/sites/default/files/IEEE_IoT_Journal_2019_Concept_Extraction_Paper_Extended.pdf"]}
{"year":"2019","title":"Automatic stance detection on political discourse in Twitter","authors":["E Zotova - 2019"],"snippet":"Page 1. Automatic Stance Detection on Political Discourse in Twitter Author: Elena Zotova Advisors: Rodrigo Agerri and German Rigau Hizkuntzaren Azterketa eta Prozesamendua Language Analysis and Processing Master's Thesis …","url":["https://addi.ehu.es/bitstream/handle/10810/36184/MAL-Elena_Zotova.pdf?sequence=1&isAllowed=y"]}
{"year":"2019","title":"Automatic Text Difficulty Estimation Using Embeddings and Neural Networks","authors":["A Filighera, T Steuer, C Rensing - European Conference on Technology Enhanced …, 2019"],"snippet":"… Next, the resulting tokens are embedded. The following pre-trained embedding models were used in our experiments: the word2vec [14], the uncased Common Crawl GloVe [15], the original ELMo [16], the uncased …","url":["https://link.springer.com/chapter/10.1007/978-3-030-29736-7_25"]}
{"year":"2019","title":"Automatic Text Summarization of News Articles in Serbian Language","authors":["D Kosmajac, V Kešelj - 2019 18th International Symposium INFOTEH …, 2019"],"snippet":"… 1). A. Auxiliary word2vec generated from Bosnian Wikidumps For input representation we used pre-trained glove word embeddings for Bosnian language2. They were trained on Common Crawl and Wikipedia using fastText [20] …","url":["https://ieeexplore.ieee.org/abstract/document/8717655/"]}
{"year":"2019","title":"Automating Analysis and Feedback to Improve Mathematics' Teachers' Classroom Discourse","authors":["A Suresh, T Sumner, J Jacobs, B Foland, W Ward - Paper submitted to the ninth …, 2019"],"snippet":"… GloVe or Global vectors for word representation is an unsupervised learning algorithm trained on aggregated word-word co-occurrence statistics from a corpus. In our model, we use the vectors trained on Common Crawl with 840 billion tokens and 300 dimensions …","url":["https://www.researchgate.net/profile/Jennifer_Jacobs8/publication/332233671_Automating_Analysis_and_Feedback_to_Improve_Mathematics%27_Teachers%27_Classroom_Discourse/links/5ca7c7394585157bd32535fc/Automating-Analysis-and-Feedback-to-Improve-Mathematics-Teachers-Classroom-Discourse.pdf"]}
{"year":"2019","title":"Automating the Fact-Checking Task: Challenges and Directions","authors":["DNE da Silva - 2019"],"snippet":"Page 1. Automating the Fact-Checking Task: Challenges and Directions Dissertation zur Erlangung des Doktorgrades (Dr. rer. nat.) der Mathematisch-Naturwissenschaftlichen Fakultät der Rheinischen Friedrich-Wilhelms-Universität Bonn …","url":["http://hss.ulb.uni-bonn.de/2019/5500/5500.pdf"]}
{"year":"2019","title":"Backlink Analyser using Apache Spark","authors":["M Zeeshan, S Asim, A Nadeem Anwar - 2019"],"snippet":"… Google Page Rank is assigned by Google based on different website factors (Design, Visitors, and Quality of content). Common Crawl [1] is an open repository for web crawl data … We will use subset of Common Crawl dataset good enough to demonstrate our system …","url":["http://dspace.cuilahore.edu.pk/xmlui/bitstream/handle/123456789/1438/SE29_Backlink%20Analyzer%20using%20Apache%20Spark.pdf?sequence=1&isAllowed=y"]}
{"year":"2019","title":"BEING PROFILED: COGITAS ERGO SUM: 10 Years of Profiling the European Citizen","authors":["I Baraliuc, E Bayamlioglu, M Hildebrandt, L Janssens - 2019"],"snippet":""}
{"year":"2019","title":"Beyond Bag-of-Concepts: Vectors of Locally Aggregated Concepts","authors":["M Grootendorst, J Vanschoren"],"snippet":"… Word2Vec pre-trained embeddings were trained on the Google News data set and contain vectors for 3 million English words.1 GloVe pre-trained embeddings were trained on the Common Crawl data set and contain vectors for 1.9 million English words.2 Pre-trained …","url":["https://ecmlpkdd2019.org/downloads/paper/489.pdf"]}
{"year":"2019","title":"Bidirectional Text Compression in External Memory","authors":["P Dinklage, J Ellert, J Fischer, D Köppl, M Penschuck - arXiv preprint arXiv …, 2019"],"snippet":"Page 1. Bidirectional Text Compression in External Memory Patrick Dinklage Technische Universität Dortmund, Department of Computer Science patrick.dinklage@ tu-dortmund.de Jonas Ellert Technische Universität Dortmund …","url":["https://arxiv.org/pdf/1907.03235"]}
{"year":"2019","title":"Big Bidirectional Insertion Representations for Documents","authors":["L Li, W Chan - arXiv preprint arXiv:1910.13034, 2019"],"snippet":"… Eu- Page 3. Figure 1: Big Bidirectional Insertion Representations for Documents roparl, Rapid, News-Commentary) and parallel sentence-level data (WikiTitles, Common Crawl, Paracrawl). The test set is newstest2019. The …","url":["https://arxiv.org/pdf/1910.13034"]}
{"year":"2019","title":"Big BiRD: A Large, Fine-Grained, Bigram Relatedness Dataset for Examining Semantic Composition","authors":["S Asaadi, SM Mohammad, S Kiritchenko"],"snippet":"… Word representations: We use GloVe word embeddings pre-trained on 840B-token CommonCrawl corpus16 and fastText word embeddings pre-trained on Common Crawl and Wikipedia using CBOW.17 For the …","url":["http://saifmohammad.com/WebDocs/BiRD-NAACL2019.pdf"]}
{"year":"2019","title":"Big Data Competence Center ScaDS Dresden/Leipzig: Overview and selected research activities","authors":["E Rahm, WE Nagel, E Peukert, R Jäkel, F Gärtner… - Datenbank-Spektrum"],"snippet":"… devise multiple research activities. The most promising direction resulted in the publication of the “Dresden WebTable Corpus” (DWTC) 2 [9] based on the freely available web crawl “CommonCrawl”. The DWTC corpus consists …","url":["https://link.springer.com/article/10.1007/s13222-018-00303-6"]}
{"year":"2019","title":"Boosting Implicit Discourse Relation Recognition with Connective-based Word Embeddings","authors":["C Wu, J Su, Y Chen, X Shi - Neurocomputing, 2019"],"snippet":"Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0925231219312196"]}
{"year":"2019","title":"Bornholmsk Natural Language Processing: Resources and Tools","authors":["L Derczynski, ITU Copenhagen, AS Kjeldsen - Proceedings of the Nordic Conference …, 2019"],"snippet":"… of compensating for the high data sparsity. Embeddings are induced with 300 dimensions, in order to be compatible with the public Common Crawl-based FastText embeddings. Having induced these embeddings for Bornholmsk …","url":["http://www.derczynski.com/papers/bornholmsk.pdf"]}
{"year":"2019","title":"BTC-2019: The 2019 Billion Triple Challenge Dataset","authors":["JM Herrera, A Hogan, T Käfer - International Semantic Web Conference, 2019"],"snippet":"… Meusel et al. [43] have published the WebDataCommons, extracting RDFa, Microdata and Microformats from the massive Common Crawl dataset; the result is a collection of 17,241,313,916 RDF triples, which, to the best of …","url":["https://link.springer.com/chapter/10.1007/978-3-030-30796-7_11"]}
{"year":"2019","title":"Building and using parallel text for translation","authors":["M Simard - The Routledge Handbook of Translation and …, 2019"]}
{"year":"2019","title":"Building Knowledge Base through Deep Learning Relation Extraction and Wikidata","authors":["P Subasic, H Yin, X Lin"],"snippet":"… Very large high-quality training data set is then generated automatically by matching Common Crawl data with relation keywords extracted from knowledge database … We solve this problem by matching Common Crawl …","url":["http://ceur-ws.org/Vol-2350/paper5.pdf"]}
{"year":"2019","title":"BUILDING TYPE CLASSIFICATION FROM SOCIAL MEDIA TEXTS VIA GEO-SPATIAL TEXT MINING","authors":["M Häberle, M Werner, XX Zhu"],"snippet":"… We applied the ReLU activation function [18] after each hidden layer and the softmax function after the output layer. The neural network has been trained for 100 epochs and a batch size of 64 samples. 2http …","url":["https://elib.dlr.de/127637/1/preprint.pdf"]}
{"year":"2019","title":"Building Unbiased Comment Toxicity Classification Model with Natural Language Processing","authors":["LQ Huang, MJ Yu"],"snippet":"… For this project, we investigated word embeddings GloVe300D (11) and fastText300D (12), where both are pretrained on Common Crawl. We also implemented character level embedding layer introduced in the paper (13) …","url":["http://cs229.stanford.edu/proj2019spr/report/79.pdf"]}
{"year":"2019","title":"Building Unbiased Comment Toxicity Classification Model","authors":["LQ Huang, MJ Yu"],"snippet":"… combined them together to mitigate the potential biases in any embeddings. We experimented with GloVe 300D trained on Common Crawl [3] and fastText 300D trained on Common Crawl [2]. We also implemented …","url":["http://cs229.stanford.edu/proj2019spr/poster/79.pdf"]}
{"year":"2019","title":"CamemBERT: a Tasty French Language Model","authors":["L Martin, B Muller, PJO Suárez, Y Dupont, L Romary… - arXiv preprint arXiv …, 2019"],"snippet":"… Later (Grave et al., 2018) trained fastText word embeddings for 157 languages using Common Crawl and showed that using crawled data significantly increased the performance of the embeddings relatively to those trained only on Wikipedia …","url":["https://arxiv.org/pdf/1911.03894"]}
{"year":"2019","title":"Can Character Embeddings Improve Cause-of-Death Classification for Verbal Autopsy Narratives?","authors":["Z Yan, S Jeblee, G Hirst"],"snippet":"… Page 3. Figure 1: Embedding concatenation model architecture. d1 is the dimensionality of the word embedding (100), and d2 is the dimensionality of the character em- bedding (24). rived from GloVe vectors (Pennington et al., 2014) trained on Common Crawl …","url":["ftp://ftp.db.toronto.edu/public_html/cs/ftp/public_html/pub/gh/Yan-etal-2019.pdf"]}
{"year":"2019","title":"Capturing and measuring thematic relatedness","authors":["M Kacmajor, JD Kelleher - Language Resources and Evaluation, 2019"],"snippet":"Page 1. ORIGINAL PAPER Capturing and measuring thematic relatedness Magdalena Kacmajor1 • John D. Kelleher2 © The Author(s) 2019 Abstract In this paper we explain the difference between two aspects of semantic …","url":["https://link.springer.com/article/10.1007/s10579-019-09452-w"]}
{"year":"2019","title":"Capturing Discriminative Attributes Using Convolution Neural Network Over ConceptNet Numberbatch Embedding","authors":["V Vinayan, MA Kumar, KP Soman - … Research in Electronics, Computer Science and …, 2019"],"snippet":"… network. There the model is represented over the various dimensions of GloVe embedding, of those the 300 dimensions (trained over a common crawl corpus of size 840B) embedding performed the best for the task. Building …","url":["https://link.springer.com/chapter/10.1007/978-981-13-5802-9_69"]}
{"year":"2019","title":"Cardiff University at SemEval-2019 Task 4: Linguistic Features for Hyperpartisan News Detection","authors":["C Pérez-Almendros, LE Anke, S Schockaert - … of the 13th International Workshop on …, 2019"],"snippet":"… GloVe vectors (Pennington et al., 2014) for all the words occurring in them. To this end, we used the un- cased Common Crawl pretrained GloVe embeddings, with 300 dimensions and a vocabulary of 1.9 million words. The …","url":["https://www.aclweb.org/anthology/S19-2158"]}
{"year":"2019","title":"CatchPhish: detection of phishing websites by inspecting URLs","authors":["RS Rao, T Vaishnavi, AR Pais - Journal of Ambient Intelligence and Humanized …, 2019"],"snippet":"… 4.1 Dataset We have collected the dataset from three different sources. Legitimate sites are collected from common-crawl and Alexa database whereas phishing sites are collected from PhishTank … D2: Legitimate sites from common-crawl and phishing sites from PhishTank …","url":["https://link.springer.com/article/10.1007/s12652-019-01311-4"]}
{"year":"2019","title":"Categorising AWS Common Crawl Dataset using MapReduce","authors":["A Chiniah, A Chummun, Z Burkutally - 2019 Conference on Next Generation …, 2019"],"snippet":"Keeping track of websites connected to the Web is an impossible task given the amplitude and fluctuation of new sites being created and those going offline. In this paper we took the task to create a directory by categorising the websites using …","url":["https://ieeexplore.ieee.org/abstract/document/8883665/"]}
{"year":"2019","title":"Categorizing Comparative Sentences","authors":["A Panchenko, A Bondarenko, M Franzek, M Hagen…"],"snippet":"… containing both items from a web-scale corpus. Our sentence source is the publicly available in- dex of the DepCC (Panchenko et al., 2018), an index of more then 14 billion dependency-parsed English sentences from the Common Crawl filtered for duplicates …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/publications/2019-panchenkoetal-argminingws-compsent.pdf"]}
{"year":"2019","title":"Categorizing Emails Using Machine Learning with Textual Features","authors":["F Rudzicz, K Malikov - Advances in Artificial Intelligence: 32nd Canadian …","H Zhang, J Rangrej, S Rais, M Hillmer, F Rudzicz… - Canadian Conference on …, 2019"],"snippet":"… Lastly, domain-specific email inboxes such as DDM-Support will contain highly specific subject-matter terminology such as organization acronyms or hospital names, which are generally not present in large text corpora such as common crawl …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=7GiZDwAAQBAJ&oi=fnd&pg=PA3&dq=commoncrawl&ots=pxG3RFx1gg&sig=XpsK2sgWrDKsYko4OCiNHIo5Meg","https://link.springer.com/chapter/10.1007/978-3-030-18305-9_1"]}
{"year":"2019","title":"CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB","authors":["H Schwenk, G Wenzek, S Edunov, E Grave, A Joulin - arXiv preprint arXiv …, 2019"],"snippet":"… We are using ten snapshots of a curated common crawl corpus (Wenzek et al., 2019), totaling 32.7 billion unique sentences … we use the same underlying mining approach based on LASER and scale to a much larger …","url":["https://arxiv.org/pdf/1911.04944"]}
{"year":"2019","title":"CCMT 2019 Machine Translation Evaluation Report","authors":["M Yang, X Hu, H Xiong, J Wang, Y Jiaermuhamaiti… - China Conference on …, 2019","Y Jiaermuhamaiti, Z He, W Luo, S Huang - … , China, September 27–29, 2019, Revised …, 2019"],"snippet":"… (2) English and Chinese monolingual Corpus (Europarl v7/v8, News Commentary, Common Crawl, News Crawl, News Discussions, etc.); LDC for English and Gigaword for Chinese (LDC2011T07, LDC2009T13, LDC2007T07, LDC2009T27) …","url":["https://link.springer.com/chapter/10.1007/978-981-15-1721-1_11","https://link.springer.com/content/pdf/10.1007/978-981-15-1721-1.pdf#page=117"]}
{"year":"2019","title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data","authors":["G Wenzek, MA Lachaux, A Conneau, V Chaudhary… - arXiv preprint arXiv …, 2019"],"snippet":"… (2019) used a large scale dataset based on Common Crawl to train text … Russian, Chinese and Urdu (∆ for average) for BERT-BASE models trained either on Wikipedia or CommonCrawl … We preprocess Common Crawl by …","url":["https://arxiv.org/pdf/1911.00359"]}
{"year":"2019","title":"Characterizing the impact of geometric properties of word embeddings on task performance","authors":["B Whitaker, D Newman-Griffis, A Haldar… - arXiv preprint arXiv …, 2019"],"snippet":"… 1 3M 300-d GoogleNews vectors from https:// code.google.com/archive/p/ word2vec/ 2 2M 300-d 840B Common Crawl vectors from https: //nlp.stanford.edu/projects/glove/ 3 1M 300-d WikiNews vectors with subword …","url":["https://arxiv.org/pdf/1904.04866"]}
{"year":"2019","title":"CITIZENS IN DATA LAND","authors":["AP DE VRIES"],"snippet":"… And Indie music'. 3 https://github.com/webis-de/wasp/. 4 Consider a new service provided by The Common Crawl Foundation, http://commoncrawl org/, or, alternatively, a new community service provided via public libraries …","url":["https://www.jstor.org/stable/pdf/j.ctvhrd092.19.pdf"]}
{"year":"2019","title":"CLaC at clpsych 2019: Fusion of neural features and predicted class probabilities for suicide risk assessment based on online posts","authors":["E Mohammadi, H Amini, L Kosseim - Proceedings of the Sixth Workshop on …, 2019"],"snippet":"… 2.1 Word Embeddings As shown in Figure 1, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) have been used as pretrained word embeddings. The 300d GloVe word embedder has been pretrained on 840B tokens of web data from Common Crawl …","url":["https://www.aclweb.org/anthology/W19-3004"]}
{"year":"2019","title":"Classification Approaches to Identify Informative Tweets","authors":["P Aggarwal - Proceedings of the Student Research Workshop …, 2019"],"snippet":"… After these preprocessing steps, we represent each posting by a dense embedding, created by the mean of the individual words embeddings. We use the pretrained embeddings provided by (Mikolov et al., 2018) …","url":["https://www.researchgate.net/profile/Piush_Aggarwal/publication/335243720_Classification_approaches_to_identify_Informative_Tweets/links/5d5af768a6fdcc55e8198141/Classification-approaches-to-identify-Informative-Tweets.pdf"]}
{"year":"2019","title":"Classification of Anti-phishing Solutions","authors":["S Chanti, T Chithralekha - SN Computer Science, 2020"],"snippet":"… WestPac. PhishTank. PIRT report. Legitimate. –. Google whitelist. Manual. Alexa, Yahoo. Web crawler. World Wide Web. WestPac. Common crawl Google search. Data set size. Phishing. 203 Archives. 200 websites. 600 …","url":["https://link.springer.com/article/10.1007/s42979-019-0011-2"]}
{"year":"2019","title":"Classification of the Answers of the OMT","authors":["F Meyer, C Biemann"],"snippet":"Page 1. Universität Hamburg Fachbereich Informatik Fakultät für Mathematik, Informatik und Naturwissenschaften Classi cation of the Answers of the OMT Bachelor-Thesis Mensch-Computer-Interaktion Arbeitsbereich …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2019-ba-meyer-omt.pdf"]}
{"year":"2019","title":"Classification of virtual patent marking web-pages using machine learning techniques","authors":["A Calvo Ibañez - 2018"],"snippet":"… Having found the best model, this was then used to predict Common Crawl instances (nonseen observations). Figure 6.1 shows a schema of the first approach. Figure 6.1 – First Approach Framework Adapted from https://www.analyticsvidhya.com …","url":["https://upcommons.upc.edu/bitstream/handle/2117/127682/133741.pdf"]}
{"year":"2019","title":"Classification of Web History Tools Through Web Analysis","authors":["JRG Evangelista, DD de Oliveira Gatto, RJ Sassi - International Conference on …, 2019"],"snippet":"… 02. Archive.fo. http://archive.fo. 03. CashedPages. http://www.cachedpages. com. 04. CachedView. http://cachedview.com. 05. Common Crawl http://commoncrawl.org. 06. Screenshots.com. http://www.screenshots.com …","url":["https://link.springer.com/chapter/10.1007/978-3-030-22351-9_18"]}
{"year":"2019","title":"Classifying Pastebin Content Through the Generation of PasteCC Labeled Dataset","authors":["A Riesco, E Fidalgo, MW Al-Nabki, F Jáñez-Martino… - International Conference on …, 2019"],"snippet":"… Panchenko et al. [17] took English text from Common Crawl and constructed a large web-scale corpus using text classification … Panchenko, A., Ruppert, E., Faralli, S., Ponzetto, SP, Biemann, C.: Building a web-scale dependency-parsed corpus from commoncrawl …","url":["https://link.springer.com/chapter/10.1007/978-3-030-29859-3_39"]}
{"year":"2019","title":"Classifying Websites Using Word Vectors and Other Techniques: An Application of Zipf's Law","authors":["A Robles - 2019"],"snippet":"Page 1. CLASSIFYING WEBSITES USING WORD VECTORS AND OTHER TECHNIQUES: AN APPLICATION OF ZIPF'S LAW A THESIS Presented to the Department of Mathematics and Statistics California State University, Long Beach In Partial Fulfillment …","url":["http://search.proquest.com/openview/7a6b97a2b1aa841198182ea77c38efb6/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2019","title":"Clinical Data Extraction and Normalization of Cyrillic Electronic Health Records Via Deep-Learning Natural Language Processing","authors":["B Zhao - JCO Clinical Cancer Informatics, 2019"],"snippet":"… The medical text in the hematic was modified from the actual patient record for the purposes of illustration. (B) Word embeddings used were fastText embeddings pretrained on Wikipedia and Common Crawl data for English (EN) and Bulgarian (BG) …","url":["https://ascopubs.org/doi/pdfdirect/10.1200/CCI.19.00057"]}
{"year":"2019","title":"Cloze-driven Pretraining of Self-attention Networks","authors":["A Baevski, S Edunov, Y Liu, L Zettlemoyer, M Auli - arXiv preprint arXiv:1903.07785, 2019"],"snippet":"… We pretrain on individual examples as they oc- cur in the training corpora (§5.1). For News Crawl this is individual sentences while on Wikipedia, Bookcorpus, and Common Crawl examples are paragraph length … Common Crawl …","url":["https://arxiv.org/pdf/1903.07785"]}
{"year":"2019","title":"ClustCrypt: Privacy-Preserving Clustering of Unstructured Big Data in the Cloud","authors":["SM Zobaed, S Ahmad, R Gottumukkala, MA Salehi"],"snippet":"Page 1. ClustCrypt: Privacy-Preserving Clustering of Unstructured Big Data in the Cloud SM Zobaed∗, Sahan Ahmad∗, Raju Gottumukkala†, and Mohsen Amini Salehi∗ ∗ School of Computing & Informatics †Informatics Research …","url":["https://www.researchgate.net/profile/Mohsen_Salehi2/publication/333561272_ClustCrypt_Privacy-Preserving_Clustering_of_Unstructured_Big_Data_in_the_Cloud/links/5cfaa9f0299bf13a38457fe9/ClustCrypt-Privacy-Preserving-Clustering-of-Unstructured-Big-Data-in-the-Cloud.pdf"]}
{"year":"2019","title":"CluWords: Exploiting Semantic Word Clustering Representation for Enhanced Topic Modeling","authors":["F Viegas, S Canuto, C Gomes, W Luiz, T Rosa, S Ribas… - Proceedings of the Twelfth …, 2019"],"snippet":"… In this section we compare the proposed CluWords with three pre-trained word em- beddings spaces: (i) Word2Vec trained with GoogleNews [21]; (ii) FastText trained with WikiNews [22] and (iii) Fasttext trained on Common Crawl [22] …","url":["https://dl.acm.org/citation.cfm?id=3291032"]}
{"year":"2019","title":"CoFiF: A Corpus of Financial Reports in French Language","authors":["T Daudert, S Ahmadi - The First Workshop on Financial Technology and …, 2019"],"snippet":"… com/CoFiF/Corpus [Merity et al., 2016], or CommonCrawl 2. Considering the domain of business and economics, especially for English, corpora such as the Wall Street Journal (WSJ) Corpus [Paul and Baker, 1992], the …","url":["https://www.aclweb.org/anthology/W19-55#page=31"]}
{"year":"2019","title":"Cognition and the Structure of Bias","authors":["GM Johnson - 2019"],"snippet":"Page 1. UCLA UCLA Electronic Theses and Dissertations Title Cognition and the Structure of Bias Permalink https://escholarship.org/uc/item/7hf582vz Author Johnson, Gabbrielle Michelle Publication Date 2019 Peer reviewed|Thesis/dissertation eScholarship.org …","url":["https://cloudfront.escholarship.org/dist/prd/content/qt7hf582vz/qt7hf582vz.pdf"]}
{"year":"2019","title":"CogniVal: A Framework for Cognitive Word Embedding Evaluation","authors":["N Hollenstein, A de la Torre, N Langer, C Zhang - arXiv preprint arXiv:1909.09001, 2019"],"snippet":"… et al., 2018). We evaluate the embeddings with and without subword information trained on 16 billion to- kens of Wikipedia sentences as well as the ones trained on 600 billion tokens of Common Crawl. • ELMo models both …","url":["https://arxiv.org/pdf/1909.09001"]}
{"year":"2019","title":"Collaborative Attention Network with Word and N-Gram Sequences Modeling for Sentiment Classification","authors":["J Bao, L Zhang, B Han - International Conference on Artificial Neural Networks, 2019"],"snippet":"… words in text. In our experiments, we adopt global vectors for word representation (GloVe) [17] in the embedding layer of our model, which consists of 840 billion 300-dimension tokens trained on Common Crawl. We don't use …","url":["https://link.springer.com/chapter/10.1007/978-3-030-30490-4_8"]}
{"year":"2019","title":"Combating Fake News with Adversarial Domain Adaptation and Neural Models","authors":["B Xu - 2019"],"snippet":"Page 1. Combating Fake News with Adversarial Domain Adaptation and Neural Models by Brian Xu BS, Massachusetts Institute of Technology (2018) Submitted to the Department of Electrical Engineering and Computer Science …","url":["https://groups.csail.mit.edu/sls/publications/2019/BrianXu_MEng-Thesis.pdf"]}
{"year":"2019","title":"Combining and learning word embedding with WordNet for semantic relatedness and similarity measurement","authors":["YY Lee, H Ke, TY Yen, HH Huang, HH Chen - Journal of the Association for …, 2019"],"snippet":"… ahttps://nlp.stanford.edu/projects/glove/. bhttp://dumps.wikimedia.org/enwiki/20140102/. chttps://catalog.ldc.upenn.edu/LDC2011T07. dThe Common Crawl corpus contains raw web page data, extracted metadata and text extractions. http://commoncrawl.org …","url":["https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24289"]}
{"year":"2019","title":"Comparison of Machine Learning Approaches for Industry Classification Based on Textual Descriptions of Companies","authors":["A Tagarev, N Tulechki, S Boytcheva"],"snippet":"… GloVe vector embeddings are used. Specifically the 300dimensional GloVe vectors trained on the large Common Crawl corpus of 840 billion tokens with a vocabulary of 2.2 million words. While there is no additional training …","url":["https://acl-bg.org/proceedings/2019/RANLP%202019/pdf/RANLP134.pdf"]}
{"year":"2019","title":"Complex Security Policy? A Longitudinal Analysis of Deployed Content Security Policies","authors":["S Roth, T Barron, S Calzavara, N Nikiforakis, B Stock"],"snippet":"… server response. To determine this IA-specific influence, we chose a second archive service to corroborate the IA's data. In particular, Common Crawl (CC) [10] has been collecting snapshots of popular sites since 2013. For each …","url":["https://swag.cispa.saarland/papers/roth2020csp.pdf"]}
{"year":"2019","title":"Comprehensive Analysis of Aspect Term Extraction Methods using Various Text Embeddings","authors":["Ł Augustyniak, T Kajdanowicz, P Kazienko - arXiv preprint arXiv:1909.04917, 2019"],"snippet":"… 1. word2vec - protoplast model of any neural word embedding trained on Google News. 2. glove.840B - Global Vectors for Word Representation proposed by Stanford NLP Group, trained based on Common Crawl with …","url":["https://arxiv.org/pdf/1909.04917"]}
{"year":"2019","title":"Comprehensive trait attributions show that face impressions are organized in four dimensions","authors":["C Lin, U Keles, R Adolphs - 2019"],"snippet":"… we represented each of them with a vector of 300 computationally extracted semantic features (describing word embeddings and text classification) using a state-of-the-art neural network provided within the FastText library (61) …","url":["https://psyarxiv.com/87nex/download?format=pdf"]}
{"year":"2019","title":"Compressing Inverted Indexes with Recursive Graph Bisection: A Reproducibility Study","authors":["J Mackenzie, A Mallia, M Petri, JS Culpepper, T Suel"],"snippet":"… tool8, – Gov2 is a crawl of .gov domains from 2004, – ClueWeb09 and ClueWeb12 both correspond to the 'B' portion of the 2009 and 2012 ClueWeb crawls of the world wide web, respectively, and – CC-News contains English …","url":["https://jmmackenzie.io/pdf/mm+19-ecir.pdf"]}
{"year":"2019","title":"Computational Argumentation Synthesis as a Language Modeling Task","authors":["R El Baff, H Wachsmuth, K Al-Khatib, M Stede, B Stein"],"snippet":"Page 1. Computational Argumentation Synthesis as a Language Modeling Task Roxanne El Baff 1 Henning Wachsmuth 2 Khalid Al-Khatib 1 Manfred Stede 3 Benno Stein 1 1 Bauhaus-Universität Weimar, Weimar, Germany …","url":["https://webis.de/downloads/publications/papers/stein_2019y.pdf"]}
{"year":"2019","title":"Conceptor Debiasing of Word Representations Evaluated on WEAT","authors":["S Karve, L Ungar, J Sedoc - arXiv preprint arXiv:1906.05993, 2019"],"snippet":"… For context-independent embeddings, we used off-the-shelf Fasttext subword embeddings6, which were trained with subword information on the Common Crawl (600B tokens), the GloVe embeddings 7 trained on Wikipedia and …","url":["https://arxiv.org/pdf/1906.05993"]}
{"year":"2019","title":"Constructing the Wavelet Tree and Wavelet Matrix in Distributed Memory","authors":["P Dinklage, J Fischer, F Kurpicz - 2020 Proceedings of the Twenty-Second Workshop on …"],"snippet":"Page 1. Constructing the Wavelet Tree and Wavelet Matrix in Distributed Memory ∗ Patrick Dinklage † Johannes Fischer† Florian Kurpicz† Abstract The wavelet tree (Grossi et al. [SODA,2003]) is a compact index for texts …","url":["https://epubs.siam.org/doi/abs/10.1137/1.9781611976007.17"]}
{"year":"2019","title":"Content Similarity Analysis of Written Comments under Posts in Social Media","authors":["M Mozafari, R Farahbakhsh, N Crespi"],"snippet":"… The GloVe vectors were trained from 840 billion tokens of Common Crawl web data and have 300 dimensions [23]. This feature is extracted similar to the Google-word2vec similarity by using equation 6 for each post and comment pair …","url":["http://servicearchitecture.wp.imtbs-tsp.eu/files/2019/09/RC_SNAMS2019_37.pdf"]}
{"year":"2019","title":"Context Matters: Recovering Human Semantic Structure from Machine Learning Analysis of Large-Scale Text Corpora","authors":["MC Iordan, T Giallanza, CT Ellis, N Beckage, JD Cohen - arXiv preprint arXiv …, 2019"],"snippet":"… We also compared performance of the four Word2Vec embedding spaces to another commonly used embedding space known as GloVe28 for two main reasons; first, the GloVe embeddings are learned from the Common …","url":["https://arxiv.org/pdf/1910.06954"]}
{"year":"2019","title":"Context-Aware Crosslingual Mapping","authors":["H Aldarmaki, M Diab - arXiv preprint arXiv:1903.03243, 2019"],"snippet":"… data. We trained monolingual ELMo and FastText with de- fault parameters. We used the WMT'13 commoncrawl data for cross-lingual mapping, and the WMT'13 test sets for evaluating sentence translation retrieval. For all …","url":["https://arxiv.org/pdf/1903.03243"]}
{"year":"2019","title":"Continual Learning for Sentence Representations Using Conceptors","authors":["T Liu, L Ungar, J Sedoc - arXiv preprint arXiv:1904.09187, 2019"],"snippet":"… zero-shot CA. Best results are in boldface and the second best results are underscored. dimensional GloVe vectors (trained on the 840 billion token Common Crawl) (Pennington et al., 2014). Additional experiments with Word2Vec …","url":["https://arxiv.org/pdf/1904.09187"]}
{"year":"2019","title":"CONTRIBUTIONS TO CLINICAL INFORMATION EXTRACTION IN PORTUGUESE: CORPORA, NAMED ENTITY RECOGNITION, WORD EMBEDDINGS","authors":["FA da Costa Lopes - 2019"],"snippet":"Page 1. Fábio André da Costa Lopes CONTRIBUTIONS TO CLINICAL INFORMATION EXTRACTION IN PORTUGUESE:CORPORA,NAMED ENTITY RECOGNITION,WORD EMBEDDINGS Thesis submitted to the Faculty of Science …","url":["https://www.researchgate.net/profile/Fabio_Lopes17/publication/335639414_Contributions_to_Clinical_Information_Extraction_in_Portuguese_Corpora_Named_Entity_Recognition_Word_Embeddings/links/5d717af2a6fdcc9961b1facd/Contributions-to-Clinical-Information-Extraction-in-Portuguese-Corpora-Named-Entity-Recognition-Word-Embeddings.pdf"]}
{"year":"2019","title":"Controlling Grammatical Error Correction Using Word Edit Rate","authors":["K Hotate, M Kaneko, S Katsumata, M Komachi - … of the 57th Conference of the …, 2019"],"snippet":"… the lowest WER). In the ranking experiment, we used a 5-gram KenLM (Heafield, 2011) with Kneser-Ney smoothing trained on the web-scale Common Crawl corpus (Junczys-Dowmunt and Grundkiewicz, 2016). As an evaluation …","url":["https://www.aclweb.org/anthology/P19-2020"]}
{"year":"2019","title":"Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading","authors":["L Qin, M Galley, C Brockett, X Liu, X Gao, B Dolan… - arXiv preprint arXiv …, 2019"],"snippet":"… informative response. To enable reproducibility of our experiments, we crawled web pages using Common Crawl (http://commoncrawl.org), a service that crawls web pages and makes its historical crawls available to the public. We …","url":["https://arxiv.org/pdf/1906.02738"]}
{"year":"2019","title":"Correlation Coefficients and Semantic Textual Similarity","authors":["V Zhelezniak, A Savkov, A Shen, NY Hammerla - arXiv preprint arXiv:1905.07790, 2019"],"snippet":"… In all experiments we rely on the following publicly available word embeddings: GloVe (Pennington et al., 2014) trained on Common Crawl (840B tokens), fastText (Bojanowski et al., 2017) trained on Common Crawl …","url":["https://arxiv.org/pdf/1905.07790"]}
{"year":"2019","title":"Correlations between Word Vector Sets","authors":["V Zhelezniak, A Shen, D Busbridge, A Savkov…"],"snippet":"… For methods involving pretrained word embeddings, we use fastText (Bo- janowski et al., 2017) trained on Common Crawl (600B tokens), as previous evaluations have in- dicated that fastText vectors have uniformly the best …","url":["https://www.april.sh/assets/files/emnlp2019.pdf"]}
{"year":"2019","title":"Creation of Sentence Embeddings Based on Topical Word Representations","authors":["P Wenig"],"snippet":"Page 1. Topical Sentence Embeddings Creation of Sentence Embeddings Based on Topical Word Representations Phillip Wenig 160361 Master's thesis to obtain the degree of Master of Science in Information Systems University of Liechtenstein …","url":["https://www.researchgate.net/profile/Phillip_Wenig/publication/330761695_Creation_of_Sentence_Embeddings_Based_on_Topical_Word_Representations/links/5c531d44458515a4c74d4719/Creation-of-Sentence-Embeddings-Based-on-Topical-Word-Representations.pdf"]}
{"year":"2019","title":"Cross-collection Multi-aspect Sentiment Analysis","authors":["H Kaporo - Computer Science On-line Conference, 2019"],"snippet":"… 3.1, we set \\(n=10\\), \\(m=5\\) and run the algorithm over topic-words returned by CPTM and word vectors from glove pre-trained embeddings 2 . These embeddings are trained over common crawl (Google data) and contain 2.2 Million words …","url":["https://link.springer.com/chapter/10.1007/978-3-030-19810-7_11"]}
{"year":"2019","title":"Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models","authors":["B Myagmar, J Li, S Kimura - IEEE Access, 2019"],"snippet":"… For pre-training data, in addition to the BookCorpus and English Wikipedia datasets, cased XLNet-Large model, refered to simply as XLNet henceforth, uses Giga5 (16GB text) [35], ClueWeb 2012-B [36] and Common Crawl [37] as part of its pre-training data …","url":["https://ieeexplore.ieee.org/iel7/6287639/8600701/08894409.pdf"]}
{"year":"2019","title":"Cross-Layer Optimization of Big Data Transfer Throughput and Energy Consumption","authors":["L Di Tacchio, MDSQZ Nine, T Kosar, MF Bulut… - 2019 IEEE 12th …, 2019"],"snippet":"… The algorithms have been compared using four different datasets: i) a small files dataset, including 20,000 HTML files form the Common Crawl project [1]; ii) a medium files dataset, consisting of 5,000 image files from Flickr …","url":["https://ieeexplore.ieee.org/abstract/document/8814571/"]}
{"year":"2019","title":"Cross-Lingual Alignment of Word & Sentence Embeddings","authors":["H Aldarmaki - 2019"],"snippet":"Page 1. Cross-Lingual Alignment of Word & Sentence Embeddings by Hanan Aldarmaki B.Sc. in Computer Engineering, May 2008, The American University of Sharjah M.Phil. in Computer Speech, Text, and Internet Technology, May 2009, University of Cambridge …","url":["http://search.proquest.com/openview/97f58b5d99e2ed81065054a170f2dcda/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2019","title":"Cross-lingual Data Transformation and Combination for Text Classification","authors":["J Jiang, S Pang, X Zhao, L Wang, A Wen, H Liu… - arXiv preprint arXiv …, 2019"],"snippet":"… There are word vectors for 157 languages1, trained on Common Crawl and Wikipedia, and these models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window size of 5 and 10 negatives …","url":["https://arxiv.org/pdf/1906.09543"]}
{"year":"2019","title":"Cross-lingual Parsing with Polyglot Training and Multi-treebank Learning: A Faroese Case Study","authors":["J Barry, J Wagner, J Foster - arXiv preprint arXiv:1910.07938, 2019"],"snippet":"… the source languages. We use the precomputed Word2Vec embeddings11 released as part of the 2017 CoNLL shared task on UD parsing (Zeman et al., 2017) which were trained on CommonCrawl and Wikipedia. In order to …","url":["https://arxiv.org/pdf/1910.07938"]}
{"year":"2019","title":"Cross-Platform Evaluation for Italian Hate Speech Detection","authors":["M Corazza, S Menini, E Cabrio, S Tonelli, S Villata…"],"snippet":"… Generic embeddings: we use embedding spaces obtained directly from the Fasttext website4 for Italian. In particular, we use the Italian embeddings trained on Common Crawl and Wikipedia (Grave et al., 2018) with size 300 …","url":["http://ceur-ws.org/Vol-2481/paper22.pdf"]}
{"year":"2019","title":"CrossLang: the system of cross-lingual plagiarism detection","authors":["O Bakhteev, A Ogaltsov, A Khazov, K Safin… - 2019"],"snippet":"… They were obtained from open-source parallel OPUS [46] corpora, but also we mine parallel sentences from Common Crawl.4 Algo … the machine translation stage generates texts that differ too much from 3https://tensorflow …","url":["http://ml4ed.cc/attachments/Bakhteev.pdf"]}
{"year":"2019","title":"CUNI Submission for Low-Resource Languages in WMT News 2019","authors":["T Kocmi, O Bojar - Proceedings of the Fourth Conference on Machine …, 2019"],"snippet":"… Words in English Commoncrawl Russian-English 878k 17.4M 18.8M … Words News crawl 2018 EN 15.4M 344.3M Common Crawl KK 12.5M 189.2M News commentary KK 13.0k 218.7k News crawl Kk 772.9k 10.3M Common Crawl …","url":["https://www.aclweb.org/anthology/W19-5322"]}
{"year":"2019","title":"CUNI System for the WMT19 Robustness Task","authors":["J Helcl, J Libovický, M Popel - arXiv preprint arXiv:1906.09246, 2019"],"snippet":"… Corpus # Sentences P arallel 109 English-French Corpus 22,520k Europarl 2,007k News Commentary 200k UN Corpus 12,886k Common Crawl 3,224k M onoFrench News Crawl ('08–'14) 37,320k English News Crawl ('11–'17) 127,554k …","url":["https://arxiv.org/pdf/1906.09246"]}
{"year":"2019","title":"Curriculum Learning for Domain Adaptation in Neural Machine Translation","authors":["X Zhang, P Shapiro, G Kumar, P McNamee, M Carpuat… - arXiv preprint arXiv …, 2019"],"snippet":"… and WMT 2017 (Bojar et al., 2017), which contains data from several domains, eg parliamentary proceedings (Europarl, UN Parallel Corpus), political/economic news (news commentary, Rapid corpus), and …","url":["https://arxiv.org/pdf/1905.05816"]}
{"year":"2019","title":"Customizing Neural Machine Translation for Subtitling","authors":["E Matusov, P Wilken, Y Georgakopoulou - Proceedings of the Fourth Conference on …, 2019"],"snippet":"… These data included all other publicly available training data, including ParaCrawl, CommonCrawl, EUbookshop, JRCAcquis, EMEA, and other corpora from the OPUS collection … This was done to avoid oversampling …","url":["https://www.aclweb.org/anthology/W19-5209"]}
{"year":"2019","title":"D-NET: A Simple Framework for Improving the Generalization of Machine Reading Comprehension","authors":["H Li, X Zhang, Y Liu, Y Zhang, Q Wang, X Zhou, J Liu…"],"snippet":"… by introducing two-stream self attention. Besides BooksCorpus and Wikipedia, on which the BERT is trained, XLNET uses more corpus in its pretraining, including Giga5, ClueWeb and Common Crawl. In our system, we use …","url":["https://mrqa.github.io/assets/papers/64_Paper.pdf"]}
{"year":"2019","title":"Détection automatique de la thématique et adaptation des modèles de langage","authors":["S ZHANG"],"snippet":"Page 1. Si ZHANG SIGMA 2018/2019 AUTHÔT 52Av. Pierre Sémard 94200 Ivry-sur-Seine Détection automatique de la thématique et adaptation des modèles de langage from 01/03/2019 to 16/08/2019 Confidentiality : yes …","url":["ftp://ftp.irit.fr/IRIT/SAMOVA/INTERNSHIPS/zhang_si_2019.pdf"]}
{"year":"2019","title":"Danish Stance Classification and Rumour Resolution","authors":["AE Lillie, ER Middelboe - arXiv preprint arXiv:1907.01304, 2019"],"snippet":"Page 1. IT University of Copenhagen MSc in Software Development Thesis project KISPECI1SE Danish Stance Classification and Rumour Resolution Authors: Anders E. Lillie aedl@itu.dk Emil R. Middelboe erem@itu.dk …","url":["https://arxiv.org/pdf/1907.01304"]}
{"year":"2019","title":"Data Augmentation in Deep Learning for Hate Speech Detection in Lower Resource Settings","authors":["M Benk"],"snippet":"Page 1. Masterarbeit zur Erlangung des akademischen Grades Master of Arts der Philosophischen Fakultät der Universität Zürich Data Augmentation in Deep Learning for Hate Speech Detection in Lower Resource Settings …","url":["https://www.cl.uzh.ch/dam/jcr:57406b34-02c8-496d-9b95-9968cee3a134/benk_ma_data_augmentation.pdf"]}
{"year":"2019","title":"Data4UrbanMobility: Towards Holistic Data Analytics for Mobility Applications in Urban Regions","authors":["N Tempelmeier, Y Rietz, I Lishchuk, T Kruegel… - arXiv preprint arXiv …, 2019"],"snippet":"… Web Event-centric Web markup Annotated Web pages, eg us- ing schema.org. Web Data Commons event subset: 263 × 106 facts until November 2017 Common Crawl ToU RDFa, MicroData Focused crawls Event-centric crawls, news11 …","url":["https://arxiv.org/pdf/1903.12064"]}
{"year":"2019","title":"Debiasing Embeddings for Reduced Gender Bias in Text Classification","authors":["F Prost, N Thain, T Bolukbasi - Proceedings of the First Workshop on Gender Bias in …, 2019"],"snippet":"… 2 Classification Task This work utilizes the BiosBias dataset introduced in (De-Arteaga et al., 2019). This dataset consists of biographies identified within the Common Crawl 397,340 biographies were extracted from sixteen crawls from 2014 to 2018 …","url":["https://www.aclweb.org/anthology/W19-3810"]}
{"year":"2019","title":"DebiasingWord Embeddings Improves Multimodal Machine Translation","authors":["T Hirasawa, M Komachi - arXiv preprint arXiv:1905.10464, 2019"],"snippet":"Page 1. Debiasing Word Embeddings Improves Multimodal Machine Translation Tosho Hirasawa Tokyo Metropolitan University hirasawa-tosho@ed.tmu.ac.jp Mamoru Komachi Tokyo Metropolitan University komachi@tmu.ac.jp Abstract …","url":["https://arxiv.org/pdf/1905.10464"]}
{"year":"2019","title":"Deca: A Garbage Collection Optimizer for In-Memory Data Processing","authors":["X Shi, Z Ke, Y Zhou, H Jin, L Lu, X Zhang, L He, Z Hu… - ACM Transactions on …, 2019"],"snippet":"Page 1. 3 Deca: A Garbage Collection Optimizer for In-Memory Data Processing XUANHUA SHI and ZHIXIANG KE, Huazhong University of Science and Technology, China YONGLUAN ZHOU, University of Copenhagen, Denmark …","url":["https://dl.acm.org/ft_gateway.cfm?id=3310361&type=pdf"]}
{"year":"2019","title":"DECO: A Dataset of Annotated Spreadsheets for Layout and Table Recognition","authors":["E Koci, M Thiele, J Rehak, O Romero, W Lehner - the 15th IAPR International …, 2019"],"snippet":"… 50 files). The performance is manually assessed per file. 1http://info.nuix.com/Enron. html 2http://commoncrawl.org/ 3http://lemurproject.org/clueweb09.php/ Koci et al. [15] use a dataset of 216 annotated spreadsheets. Unlike …","url":["https://wwwdb.inf.tu-dresden.de/wp-content/uploads/deco_paper.pdf"]}
{"year":"2019","title":"Deep Contextualized Word Embeddings in Transition-Based and Graph-Based Dependency Parsing--A Tale of Two Parsers Revisited","authors":["A Kulmizev, M de Lhoneux, J Gontrum, E Fano, J Nivre - arXiv preprint arXiv …, 2019"],"snippet":"… 2018), who train ELMo on 20 million words randomly sampled from raw WikiDump and Common Crawl datasets for … In other words, while the standalone ELMo models were trained on the tokenized WikiDump and …","url":["https://arxiv.org/pdf/1908.07397"]}
{"year":"2019","title":"Deep Learning for NLP and Speech Recognition","authors":["U Kamath, J Liu, J Whitaker"],"snippet":"Page 1. Uday Kamath · John Liu · James Whitaker Deep Learning for NLP and Speech Recognition Page 2. Deep Learning for NLP and Speech Recognition Page 3. Uday Kamath • John Liu • James Whitaker Deep …","url":["https://link.springer.com/content/pdf/10.1007/978-3-030-14596-5.pdf"]}
{"year":"2019","title":"Deep learning for pollen allergy surveillance from twitter in Australia","authors":["J Rong, S Michalska, S Subramani, J Du, H Wang - BMC Medical Informatics and …, 2019"],"snippet":"… embeddings - as alternative. The pre-trained Common Crawl 840B tokens GloVe embeddings were downloaded from the website 2 . Both 50 dimensions (min) and 300 dimensions (max) options were tested. The HF embeddings …","url":["https://link.springer.com/article/10.1186/s12911-019-0921-x"]}
{"year":"2019","title":"Deep learning models for speech recognition","authors":["A Hannun, C Case, J Casper, B Catanzaro, G DIAMOS… - US Patent App. 16/542,243, 2019"],"snippet":"US20190371298A1 - Deep learning models for speech recognition - Google Patents. Deep learning models for speech recognition. Download PDF Info. Publication number US20190371298A1. US20190371298A1 US16/542,243 …","url":["https://patents.google.com/patent/US20190371298A1/en"]}
{"year":"2019","title":"Deep Learning vs. Classic Models on a New Uzbek Sentiment Analysis Dataset","authors":["E Kuriyozov, S Matlatipov, MA Alonso…"],"snippet":"… We use as input the FastText pre-trained word embeddings of size 300 (Grave et al., 2018) for Uzbek language, that were created from Wiki pages and CommonCrawl, 9 which, to our knowledge, are the only available pre-trained …","url":["http://www.grupolys.org/biblioteca/KurMatAloGom2019a.pdf"]}
{"year":"2019","title":"Deep Learning-based Categorical and Dimensional Emotion Recognition for Written and Spoken Text","authors":["BT Atmaja, K Shirai, M Akagi - INA-Rxiv. June, 2019"],"snippet":"… meaning. Glove captured the global corpus statistics from the corpus, for example, a Wikipedia document or a common crawl document. In GloVe model, the cost function is given by V ∑ i,j=1 f(Xi,j)(uT i,jvj + bi + cj − log Xi,j)2 (2) …","url":["https://osf.io/fhu29/download/?format=pdf"]}
{"year":"2019","title":"Deep Structured Semantic Model for Recommendations in E-commerce","authors":["A Larionova, P Kazakova, N Nikitinsky - International Conference on Hybrid Artificial …, 2019"],"snippet":"… We generated a vector representation for each text by inferring FastText embeddings [4] from their tokens and averaging them (FastText model is pretrained on the Russian language subset of the Common Crawl corpus [10]) …","url":["https://link.springer.com/chapter/10.1007/978-3-030-29859-3_8"]}
{"year":"2019","title":"Deepening Hidden Representations from Pre-trained Language Models for Natural Language Understanding","authors":["J Yang, H Zhao - arXiv preprint arXiv:1911.01940, 2019","JYH Zhao"],"snippet":"… the other hand. In addition to BooksCorpus and English Wikipedia, it also uses Giga5, ClueWeb 2012-B and Common Crawl for pre-training. Trained with dynamic masking, large mini-batches and a larger bytelevel BPE, full …","url":["https://arxiv.org/pdf/1911.01940","https://deeplearn.org/arxiv/101390/deepening-hidden-representations-from-pre-trained-language-models-for-natural-language-understanding"]}
{"year":"2019","title":"Defending Against Neural Fake News","authors":["R Zellers, A Holtzman, H Rashkin, Y Bisk, A Farhadi… - arXiv preprint arXiv …, 2019"],"snippet":"… Dataset. We present RealNews, a large corpus of news articles from Common Crawl … Thus, we construct one by scraping dumps from Common Crawl, limiting ourselves to the 5000 news domains indexed by Google News …","url":["https://arxiv.org/pdf/1905.12616"]}
{"year":"2019","title":"Deliverable 4.2: Data Integration (v. 1)","authors":["A Haller, JD Fernández, A Polleres, MR Kamdar - Work, 2019"],"snippet":"Page 1. Cyber-Physical Social Systems for City-wide Infrastructures Deliverable 4.2: Data Integration (v.1) Authors : Armin Haller, Javier D. Fernández, Axel Polleres, Maulik R. Kamdar Dissemination Level : Public Due date …","url":["http://cityspin.net/wp-content/uploads/2017/10/D4.2-Data-Integration.pdf"]}
{"year":"2019","title":"Design and implementation of an open source Greek POS Tagger and Entity Recognizer using spaCy","authors":["E Partalidou, E Spyromitros-Xioufis, S Doropoulos… - IEEE/WIC/ACM International …, 2019"],"snippet":"… 3.4 Evaluation and comparison of results In the first experiment the model was trained using pretrained vectors extracted from two different sources, Common Crawl and Wikipedia and can be found at the official FastText …","url":["https://dl.acm.org/citation.cfm?id=3352543"]}
{"year":"2019","title":"Detecting Aggression and Toxicity using a Multi Dimension Capsule Network","authors":["S Srivastava, P Khurana - Proceedings of the Third Workshop on Abusive …, 2019"],"snippet":"… The code for tokenization was taken from (Devlin et al., 2018) which seems to properly separate the word tokens and special characters. For training all our classification models, we have used fastText embeddings of dimension 300 trained on a common crawl …","url":["https://www.aclweb.org/anthology/W19-3517"]}
{"year":"2019","title":"Detecting associations between dietary supplement intake and sentiments within mental disorder tweets","authors":["Y Wang, Y Zhao, J Zhang, J Bian, R Zhang - Health Informatics Journal, 2019"],"snippet":"Many patients with mental disorders take dietary supplement, but their use patterns remain unclear. In this study, we developed a method to detect signals of associations between dietary supplement...","url":["https://journals.sagepub.com/doi/full/10.1177/1460458219867231"]}
{"year":"2019","title":"Detecting Clitics Related Orthographic Errors in Turkish","authors":["U Arıkan, O Güngör, S Uskudarli"],"snippet":"… For this task, GloVe was used with the dimension size of 300 and window size of 15. The pretrained word vectors for Turkish were obtained from the model trained on Common Crawl and Wikipedia using fastText (Grave et al., 2018) …","url":["https://www.researchgate.net/profile/Onur_Guengoer2/publication/337425054_Detecting_Clitics_Related_Orthographic_Errors_in_Turkish/links/5dd7e92792851c1feda68471/Detecting-Clitics-Related-Orthographic-Errors-in-Turkish.pdf"]}
{"year":"2019","title":"Detecting Hacker Threats: Performance of Word and Sentence Embedding Models in Identifying Hacker Communications","authors":["AL Queiroz, S Mckeever, B Keegan"],"snippet":"… model Source Dim. size MDL-1 SVM WEMB Word2vec Google News 300 MDL-2 SVM WEMB Glove Common Crawl 300 … MDL-4 SVM SEMB InferSent Wikipedia 4096 MDL-5 SVM SEMB SentEncoder Wiki, Web News, SNLI …","url":["http://aics2019.datascienceinstitute.ie/papers/aics_13.pdf"]}
{"year":"2019","title":"Detecting Incivility and Impoliteness in Online Discussions. Classification Approaches for German User Comments.","authors":["A Stoll, M Ziegele, O Quiring - 2019"],"snippet":"Page 1. DETECTING INCIVILITY AND IMPOLITENESS Preprint on SSRN, 22.11.2019 Anke Stoll HHU Düsseldorf anke.stoll@hhu.de Marc Ziegele HHU Düsseldorf marc.ziegele@hhu.de Oliver Quiring JGU Main quiring@uni-mainz.de ABSTRACT …","url":["https://osf.io/preprints/socarxiv/a47ch/download"]}
{"year":"2019","title":"Detecting offensive language using transfer learning","authors":["A de Bruijn, V Muhonen, T Albinonistraat, W Fokkink… - 2019"],"snippet":"Page 1. Detecting offensive language using transfer learning Alissa de Bruijn September 2019 Page 2. Master Thesis Business Analytics Detecting offensive language using transfer learning Author: Alissa de Bruijn …","url":["https://beta.vu.nl/nl/Images/stageverslag-bruijn_tcm235-926516.pdf"]}
{"year":"2019","title":"Detecting Relational States in Online Social Networks","authors":["J Zhang, L Tan, X Tao, T Pham, X Zhu, H Li, L Chang - 2018 5th International …, 2019"],"snippet":"… Since the social network we obtain from the repositories of common crawl contains missing links and partial information, stochastic estimations are used to measure the accuracy and reliability of our experimental MVVA results [19] …","url":["https://ieeexplore.ieee.org/abstract/document/8697237/"]}
{"year":"2019","title":"Detecting Topic-Oriented Speaker Stance in Conversational Speech}}","authors":["C Lai, B Alex, JD Moore, L Tian, T Hori, G Francesca - Proc. Interspeech 2019, 2019"],"snippet":"… 3.3. Lexical Features We build lexical representations over turns and topic segments using 300 dimensional GloVe word embeddings (Common Crawl, 840B tokens) [26]. We perform basic tokenization to map between the CallHome transcripts and word embeddings …","url":["https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2632.pdf"]}
{"year":"2019","title":"Detection of contradictions in pairs of texts in Kazakh","authors":["Y Yamalutdinova - 2019"],"snippet":"Page 1. BACHELOR THESIS Yuliya Yamalutdinova Detection of contradictions in pairs of texts in Kazakh Institute of Formal and Applied Linguistics Supervisor of the bachelor thesis: Mgr. Rudolf Rosa, Ph.D. Study programme: Computer Science …","url":["https://dspace.cuni.cz/bitstream/handle/20.500.11956/109076/130266752.pdf?sequence=1"]}
{"year":"2019","title":"Determining How Citations Are Used in Citation Contexts","authors":["M Färber, A Sampath - International Conference on Theory and Practice of …, 2019"],"snippet":"… 2. See https://fasttext.cc/. The pretrained vectors were trained on Common Crawl and Wikipedia using the CBOW model of fastText. fastText operates at the character level, and therefore can generate vectors for words not seen in the training corpus …","url":["https://link.springer.com/chapter/10.1007/978-3-030-30760-8_38"]}
{"year":"2019","title":"Development of a Song Lyric Corpus for the English Language","authors":["MAG Rodrigues, A de Paiva Oliveira, A Moreira - International Conference on …, 2019"],"snippet":"… languages. According to the authors, the texts that compose the corpus were extracted from CommonCrawl (commoncrawl.org), the largest publicly available general Web crawl to date with about 2 billion crawled URLs. The …","url":["https://link.springer.com/chapter/10.1007/978-3-030-23281-8_33"]}
{"year":"2019","title":"Development of an End-to-End Deep Learning Pipeline","authors":["M Nitsche, S Halbritter"],"snippet":"Page 1. Development of an End-to-End Deep Learning Pipeline Matthias Nitsche, Stephan Halbritter {matthias.nitsche, stephan.halbritter}@hawhamburg.de Hamburg University of Applied Sciences, Department of …","url":["https://users.informatik.haw-hamburg.de/~ubicomp/projekte/master2019-proj/nitsche-halbritter.pdf"]}
{"year":"2019","title":"Digital audio track suggestions for moods identified using analysis of objects in images from video content","authors":["N Brochu - US Patent App. 15/392,705, 2019"],"snippet":"… the response from the natural language model 240. Exemplary training corpuses of words can include, for example, Common Crawl (eg, 840B tokens, 2.2M vocabulary terms). The natural language model 240 can be improved …","url":["http://www.freepatentsonline.com/10276189.html"]}
{"year":"2019","title":"Diversicon: Pluggable Lexical Domain Knowledge","authors":["G Bella, F McNeill, D Leoni, FJQ Real, F Giunchiglia - Journal on Data Semantics, 2019"],"snippet":"Page 1. Journal on Data Semantics https://doi.org/10.1007/s13740-019-00107-1 ORIGINAL ARTICLE Diversicon: Pluggable Lexical Domain Knowledge Gábor Bella1 · Fiona McNeill2 · David Leoni1 · Francisco José Quesada Real3 · Fausto Giunchiglia1 …","url":["https://link.springer.com/article/10.1007/s13740-019-00107-1"]}
{"year":"2019","title":"Do It Like a Syntactician: Using Binary Gramaticality Judgements to Train Sentence Encoders and Assess Their Sensitivity to Syntactic Structure","authors":["P Gonzalez Martinez - 2019"],"snippet":"Page 1. City University of New York (CUNY) CUNY Academic Works Dissertations, Theses, and Capstone Projects Graduate Center 9-2019 Do It Like a Syntactician: Using Binary Gramaticality Judgements to Train …","url":["https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=4521&context=gc_etds"]}
{"year":"2019","title":"Do It Like a Syntactician: Using Binary Grammaticality Judgements to Train Sentence Encoders and Assess Their Sensitivity to Syntactic Structure","authors":["PG Martinez - 2019"],"snippet":"Page 1. Do it like a syntactician: using binary grammaticality judgments to train sentence encoders and assess their sensitivity to syntactic structure by Pablo González Mart´ınez A dissertation submitted to the Graduate Faculty in Linguistics in partial fulfillment of the …","url":["http://search.proquest.com/openview/f9bad35a6cc78921f32a9f8f6e0efde3/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2019","title":"Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?","authors":["I Vulić, G Glavaš, R Reichart, A Korhonen - arXiv preprint arXiv:1909.01638, 2019"],"snippet":"… Monolingual Embeddings. We use the 300-dim vectors of Grave et al. (2018) for all 15 languages, pretrained on Common Crawl and Wikipedia with fastText (Bojanowski et al., 2017).7 We trim all 5While BLI is an intrinsic task, as discussed by Glavaš et al …","url":["https://arxiv.org/pdf/1909.01638"]}
{"year":"2019","title":"Document Embedding Models on Environmental Legal Documents","authors":["S Kralj, Ž Urbancic, E Novak, K Kenda"],"snippet":"… Instead of having aa large vocabulary of pre-computed word embeddings trained on Wikipedia and Common Crawl, this newly trained model is trained on documents from a more specific domain - resulting in a vocabulary …","url":["http://ailab.ijs.si/dunja/SiKDD2019/Papers/Kralj_Urbancic_Final.pdf"]}
{"year":"2019","title":"Document Summarization Using Sentence-Level Semantic Based on Word Embeddings","authors":["K Al-Sabahi, Z Zuping - International Journal of Software Engineering and …, 2019"],"snippet":"… Training is performed on aggregated global word-word co-occurrence statistics from a corpus. In this work, we use the one trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B. 300d.zip …","url":["https://www.worldscientific.com/doi/abs/10.1142/S0218194019500086"]}
{"year":"2019","title":"Domain adaptation for part-of-speech tagging of noisy user-generated text","authors":["L März, D Trautmann, B Roth - arXiv preprint arXiv:1905.08920, 2019"],"snippet":"… The pretrained vectors for German are based on Wikipedia articles and data from Common Crawl3. We obtain 97.988 different embeddings for the tokens in TIGER and the Twitter corpus of which 75.819 were already contained …","url":["https://arxiv.org/pdf/1905.08920"]}
{"year":"2019","title":"Domain-specific word embeddings for patent classification","authors":["J Risch, R Krestel - Data Technologies and Applications, 2019"],"snippet":"… used to train word embeddings. It contains more than twice the number of tokens of the English Wikipedia (16bn) and is only exceeded by the Common Crawl data set, which consists of 600bn tokens. We assume that the embeddings …","url":["https://www.emeraldinsight.com/doi/abs/10.1108/DTA-01-2019-0002"]}
{"year":"2019","title":"Dual Monolingual Cross-Entropy Delta Filtering of Noisy Parallel Data","authors":["A Axelrod, A Kumar, S Sloto - Proceedings of the Fourth Conference on Machine …, 2019"],"snippet":"… gual English corpus comparable in size and content to the Sinhala one by randomly selecting 150k lines from Wikipedia and 6M lines from Common Crawl … Each SentencePiece model was trained on 1M lines of monolin …","url":["https://www.aclweb.org/anthology/W19-5433"]}
{"year":"2019","title":"Dynamic Packed Compact Tries Revisited","authors":["K Tsuruta, D Köppl, S Kanda, Y Nakashima, S Inenaga… - arXiv preprint arXiv …, 2019"],"snippet":"… name column. • commoncrawl is a web crawl containing the ASCII-encoded content (without HTML tags) of random web pages extracted from Common Crawl. • vital is the main text extracted from the most vital Wikipedia articles …","url":["https://arxiv.org/pdf/1904.07467"]}
{"year":"2019","title":"Dynamically Route Hierarchical Structure Representation to Attentive Capsule for Text Classification","authors":["W Zheng, Z Zheng, H Wan, C Chen"],"snippet":"Page 1. Dynamically Route Hierarchical Structure Representation to Attentive Capsule for Text Classification Wanshan Zheng1,2 , Zibin Zheng1,2 , Hai Wan1 , Chuan Chen1,2 1School of Data and Computer Science, Sun Yat …","url":["https://www.ijcai.org/proceedings/2019/0759.pdf"]}
{"year":"2019","title":"EasyChair Preprint","authors":["NS Resolution - 2019"],"snippet":"… Fancellu et al. (2016) show in their work that Page 4. Crawl data set with 840B tokens. Additionally we will try 300-dimensional pre-trained fastText2 that were also trained on Common Crawl but on a subset of 600B tokens. This differs from Fancellu et al …","url":["https://easychair.org/publications/preprint_download/QHml"]}
{"year":"2019","title":"EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks","authors":["JW Wei, K Zou - arXiv preprint arXiv:1901.11196, 2019"],"snippet":"… We suspect that EDA will work with any thesaurus. Word embeddings. We use 300-dimensional Common-Crawl word embeddings trained using GloVe (Pennington et al., 2014). We suspect that EDA will work with any pre-trained word embeddings. CNN …","url":["https://arxiv.org/pdf/1901.11196"]}
{"year":"2019","title":"Edge Computing for User-Centric Secure Search on Cloud-Based Encrypted Big Data","authors":["S Ahmad, SM Zobaed, R Gottumukkala, MA Salehi - arXiv preprint arXiv:1908.03668, 2019"],"snippet":"… We used two datasets, namely Amazon Common Crawl Corpus (ACCC) [35] and Request For Comments (RFC) [36], that have distinct characteristics and volumes. ACCC is ≈ 150 terabytes, contains web contents, and is not domain-specific …","url":["https://arxiv.org/pdf/1908.03668"]}
{"year":"2019","title":"EDUCE: Explaining model Decisions through Unsupervised Concepts Extraction","authors":["D Bouchacourt, L Denoyer - arXiv preprint arXiv:1905.11852, 2019"],"snippet":"… We test on the full test dataset composed of 7, 600 samples. We use pre-trained word vectors trained on Common Crawl [8], and keep them fixed … We use pre-trained word vectors trained on Common Crawl [8], and keep them fixed …","url":["https://arxiv.org/pdf/1905.11852"]}
{"year":"2019","title":"Efficient Classification and Unsupervised Keyphrase Extraction for Web Pages","authors":["T Haarman - 2019"],"snippet":"Page 1. MASTER'S THESIS Efficient Classification and Unsupervised Keyphrase Extraction for Web Pages Tim Haarman s2404184 Department of Artificial Intelligence University of Groningen, The Netherlands Primary …","url":["https://www.ai.rug.nl/~mwiering/Thesis_Tim_Haarman.pdf"]}
{"year":"2019","title":"Efficient Contextual Representation Learning With Continuous Outputs","authors":["LH Li, PH Chen, CJ Hsieh, KW Chang - Transactions of the Association for …, 2019"],"snippet":"Create a new account. Email. Returning user. Can't sign in? Forgot your password? Enter your email address below and we will send you the reset instructions. Email. Cancel. If the address matches an existing account you will …","url":["https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00289"]}
{"year":"2019","title":"Efficient Contextual Representation Learning Without Softmax Layer","authors":["LH Li, PH Chen, CJ Hsieh, KW Chang - arXiv preprint arXiv:1902.11269, 2019"],"snippet":"… The output layer is a sampled softmax with 8192 negative samples per batch. This model is provided in AllenNLP by Peters et al. (2018a). • ELMO-S: The input layer is the FastText embedding trained on Common Crawl (Mikolovetal …","url":["https://arxiv.org/pdf/1902.11269"]}
{"year":"2019","title":"Efficient Sentence Embedding using Discrete Cosine Transform","authors":["N Almarwani, H Aldarmaki, M Diab - arXiv preprint arXiv:1909.03104, 2019"],"snippet":"… CoordInv Coordination Inversion Table 1: Probing Tasks 3.2 Experimental setup For the word embeddings, we use pre-trained FastText embeddings of size 300 (Mikolov et al., 2018) trained on Common-Crawl. We generate DCT …","url":["https://arxiv.org/pdf/1909.03104"]}
{"year":"2019","title":"Embedding Imputation with Grounded Language Information","authors":["Z Yang, C Zhu, V Sachidananda, E Darve - arXiv preprint arXiv:1906.03753, 2019"],"snippet":"… KG2Vec 0.02% 7% 0.04% 12% 58.6 56.9 60.1 54.3 GloVe Common Crawl 1% 29% 2% 44% 44.0 33.0 45.1 27.3 … We test on two types of pre-trained word vectors GloVe (Common crawl, cased 300d) and ConceptNet Numberbatch (300d) …","url":["https://arxiv.org/pdf/1906.03753"]}
{"year":"2019","title":"EmbNum+: Effective, Efficient, and Robust Semantic Labeling for Numerical Values","authors":["P Nguyen, K Nguyen, R Ichise, H Takeda - New Generation Computing, 2019"],"snippet":"… Data portals. For example, 233 million tables were extracted from the July 2015 version of the Common Crawl [10].1 Additionally, 200,000 tables from 232 Open Data portals were analyzed by Mitlohner et al. [12]. These resources …","url":["https://link.springer.com/article/10.1007/s00354-019-00076-w"]}
{"year":"2019","title":"Emerging Cross-lingual Structure in Pretrained Language Models","authors":["A Conneau, S Wu, H Li, L Zettlemoyer, V Stoyanov - … of the 58th Annual Meeting of …, 2020","S Wu, A Conneau, H Li, L Zettlemoyer, V Stoyanov - arXiv preprint arXiv:1911.01464, 2019"],"snippet":"… but this effect is not well established for masked language models. We consider domain difference by training on Wikipedia for English and a random subset of Common Crawl of the same size for the other languages (Wiki-CC) …","url":["https://arxiv.org/pdf/1911.01464","https://www.aclweb.org/anthology/2020.acl-main.536.pdf"]}
{"year":"2019","title":"Emoji Powered Capsule Network to Detect Type and Target of Offensive Posts in Social Media","authors":["H Hettiarachchi, T Ranasinghe"],"snippet":"… Also character embeddings handle in- frequent words better than word2vec embedding as later one suffers from lack of enough training opportunity for those rare words. We used fasttext embeddings pre trained on Common Crawl (Mikolov et al., 2018) …","url":["https://www.researchgate.net/profile/Tharindu_Ranasinghe2/publication/336775156_Emoji_Powered_Capsule_Network_to_Detect_Type_and_Target_of_Offensive_Posts_in_Social_Media/links/5db1a79992851c577eba8219/Emoji-Powered-Capsule-Network-to-Detect-Type-and-Target-of-Offensive-Posts-in-Social-Media.pdf"]}
{"year":"2019","title":"EmoLabel: Semi-Automatic Methodology for Emotion Annotation of Social Media Text","authors":["L Canales, W Daelemans, E Boldrini, P Martínez-Barco - IEEE Transactions on …, 2019"],"snippet":"Page 1. 1949-3045 (c) 2019 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/8758380/"]}
{"year":"2019","title":"EMOMINER at SemEval-2019 Task 3: A Stacked BiLSTM Architecture for Contextual Emotion Detection in Text","authors":["N Chakravartula, V Indurthi - Proceedings of the 13th International Workshop on …, 2019"],"snippet":"… them with GloVe vectors. As a result, Glove vectors will have syntactic information of words (Rezaeinia et al., 2017). 3.2 Feature Extraction • Word Embeddings: Glove840B - common crawl (Pennington et al., 2014) pre-trained …","url":["https://www.aclweb.org/anthology/S19-2033"]}
{"year":"2019","title":"Emotional Embeddings: Refining Word Embeddings to Capture Emotional Content of Words","authors":["A Seyeditabari, N Tabari, S Gholizadeh, W Zadrozny - arXiv preprint arXiv …, 2019"],"snippet":"… vector spaces used here are: • Word2Vec trained full English Wikipedia dump • GloVe from their own website • fastText trained with subword information on Common Crawl • ConceptNet Numberbatch It is clear that each emotionally …","url":["https://arxiv.org/pdf/1906.00112"]}
{"year":"2019","title":"Encoder-Decoder Network with Cross-Match Mechanism for Answer Selection","authors":["Z Xie, X Yuan, J Wang, S Ju - China National Conference on Chinese Computational …, 2019"],"snippet":"… 4.2 Implementation Details. We initialized word embedding with 300d-GloVe vectors pre-trained from the 840B Common Crawl corpus [8], while the word embeddings for the out-of-vocabulary words were initialized randomly …","url":["https://link.springer.com/chapter/10.1007/978-3-030-32381-3_6"]}
{"year":"2019","title":"End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots","authors":["Y Yoon, WR Ko, M Jang, J Lee, J Kim"],"snippet":"… We used the pretrained word embedding model GloVe, trained on the Common Crawl corpus [5]. The dimension of word embedding is 300, and a zero vector is used for unknown words. A gesture is represented as a sequence of human poses …","url":["http://robotics.auckland.ac.nz/wp-content/uploads/2018/06/Final_ETRI_YoungwooYoon.pdf"]}
{"year":"2019","title":"End-to-end Neural Information Retrieval","authors":["W Yang - 2019"],"snippet":"Page 1. End-to-end Neural Information Retrieval by Wei Yang A thesis presented to the University of Waterloo in fulfillment of the thesis requirement for the degree of Master in Computer Science Waterloo, Ontario, Canada, 2019 c Wei Yang 2019 Page 2 …","url":["https://uwspace.uwaterloo.ca/bitstream/handle/10012/14597/Yang_Wei.pdf?sequence=4&isAllowed=y"]}
{"year":"2019","title":"End-to-End Speech Recognition","authors":["U Kamath, J Liu, J Whitaker - Deep Learning for NLP and Speech Recognition, 2019"],"snippet":"… of certain words. Therefore, an n-gram language model was trained using the KenLM [Hea+ 13] toolkit on the Common Crawl Repository, 1 using the 400,000 most frequent words from 250 million lines of text. The decoding …","url":["https://link.springer.com/chapter/10.1007/978-3-030-14596-5_12"]}
{"year":"2019","title":"English-Czech Systems in WMT19: Document-Level Transformer","authors":["M Popel, D Macháček, M Auersperger, O Bojar… - arXiv preprint arXiv …, 2019"],"snippet":"… brevity. sentence words (k) data set pairs (k) EN CS CzEng 1.7 57 065 618 424 543 184 Europarl v7 647 15 625 13 000 News Commentary v12 211 4 544 4 057 CommonCrawl 162 3 349 2 927 WikiTitles 361 896 840 EN NewsCrawl …","url":["https://arxiv.org/pdf/1907.12750"]}
{"year":"2019","title":"Enhancing AMR-to-Text Generation with Dual Graph Representations","authors":["LFR Ribeiro, C Gardent, I Gurevych - arXiv preprint arXiv:1909.00352, 2019"],"snippet":"… 5 Experiments and Discussion Implementation Details We extract vocabularies (size of 20,000) from the training sets and initialize the node embeddings from GloVe word em- beddings (Pennington et al., 2014) on Common Crawl …","url":["https://arxiv.org/pdf/1909.00352"]}
{"year":"2019","title":"Enhancing Semantic Word Representations by Embedding Deeper Word Relationships","authors":["A Nugaliyadde, KW Wong, F Sohel, H Xie - arXiv preprint arXiv:1901.07176, 2019"],"snippet":"… 2 https://commoncrawl.org/ differentiating similarity from association and relatedness which is reflected in table 1. The context to create the word embedding in order to test on SimLex-999 is created based on Common …","url":["https://arxiv.org/pdf/1901.07176"]}
{"year":"2019","title":"Environmental hazards, rigid institutions, and transformative change: How drought affects the consideration of water and climate impacts in infrastructure management","authors":["N Ulibarri, TA Scott - Global Environmental Change, 2019"],"snippet":"Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0959378019302213"]}
{"year":"2019","title":"eTranslation's Submissions to the WMT 2019 News Translation Task","authors":["C Oravecz, K Bontcheva, A Lardilleux, L Tihanyi… - Proceedings of the Fourth …, 2019"],"snippet":"… En→De the reduction in ParaCrawl was from 31M to 18M segments and in CommonCrawl from 2.3M to 1.4M segments with a drop of 0.2 BLEU points compared to us- ing the full sets3. No additional cleaning was ap- plied to …","url":["https://www.aclweb.org/anthology/W19-5334"]}
{"year":"2019","title":"Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates","authors":["J Iranzo-Sánchez, JA Silvestre-Cerdà, J Jorge… - arXiv preprint arXiv …, 2019"],"snippet":"… De↔Fr eubookshop, JRC-Acquis, 14.3 TildeModel En↔Es commoncrawl, eubookshop, 21.1 EU-TT2, UN, Wikipedia En↔Fr commoncrawl, giga, 38.2 undoc, news-commentary Es↔Fr DGT, eubookshop, 37.2 JRC-Acquis, UNPC …","url":["https://arxiv.org/pdf/1911.03167"]}
{"year":"2019","title":"Evaluating Commonsense in Pre-trained Language Models","authors":["X Zhou, Y Zhang, L Cui, D Huang - arXiv preprint arXiv:1911.11931, 2019"],"snippet":"… Note that XLNet-base is trained with the same data as BERT, while XLNet-large is trained with a larger dataset that consists of 32.98B subword pieces coming from Wiki, BookCorpus, Giga5, ClueWeb, and Common Crawl. RoBERTa (Liu et al …","url":["https://arxiv.org/pdf/1911.11931"]}
{"year":"2019","title":"Evaluating KGR10 Polish word embeddings in the recognition of temporal expressions using BiLSTM-CRF.","authors":["J Kocoń, M Gawor - arXiv preprint arXiv:1904.04055, 2019"],"snippet":"… The second one, called FASTTEXT4, is original FastText word embeddings set, created for 157 languages (including Polish). Authors used Wikipedia and Common Crawl5 as the linguistic data source … C2 Common Crawl …","url":["https://arxiv.org/pdf/1904.04055"]}
{"year":"2019","title":"Evaluating the Supervised and Zero-shot Performance of Multi-lingual Translation Models","authors":["C Hokamp, J Glover, D Gholipour - arXiv preprint arXiv:1906.09675, 2019"],"snippet":"… We use all available parallel data from the WMT19 news-translation task for training, with the exception of commoncrawl, which we found to be very noisy after manually checking a sample of the data, and paracrawl, which …","url":["https://arxiv.org/pdf/1906.09675"]}
{"year":"2019","title":"Evaluation of basic modules for isolated spelling error correction in Polish texts","authors":["S Rutkowski - arXiv preprint arXiv:1905.10810, 2019"],"snippet":"… How this representation is constructed is informed by the whole corpus on which the embedder was trained. The pretrained ELMo model that we used (Che et al., 2018) was trained on Wikipedia and Common Crawl corpora of Polish …","url":["https://arxiv.org/pdf/1905.10810"]}
{"year":"2019","title":"Evaluation of Czech Distributional Thesauri","authors":["P Rychlý - RASLAN 2019 Recent Advances in Slavonic Natural …, 2019"],"snippet":"… The results are summarized in Table 2. The czTenTen12 corpus was evaluated with Sketch Engine thesaurus and also with word vectors compiled by FastText. We have also included prebuild model from Common Crawl. Table …","url":["http://raslan2019.nlp-consulting.net/proceedings/raslan19.pdf#page=145","https://nlp.fi.muni.cz/raslan/raslan19.pdf#page=145"]}
{"year":"2019","title":"Evaluation of State Of Art Open-source ASR Engines with Local Inferencing","authors":["B Rizk"],"snippet":"Page 1. Institute Of Information Systems (iisys) Hof University in exchange for Media Engineering and Technology Faculty German University in Cairo Evaluation of State Of Art Open-source ASR Engines with Local …","url":["https://www.researchgate.net/profile/Basem_Rizk/publication/335524542_Evaluation_of_State_Of_Art_Open-source_ASR_Engines_with_Local_Inferencing/links/5d6aa4ae299bf1808d5c87dd/Evaluation-of-State-Of-Art-Open-source-ASR-Engines-with-Local-Inferencing.pdf"]}
{"year":"2019","title":"Evaluation of vector embedding models in clustering of text documents","authors":["T Walkowiak, M Gniewkowski"],"snippet":"… The second group of sources of word2vec models for Polish are web pages of word embedding tools like fastText, ELMo and BERT. They were trained on Polish Common Crawl and Wikipedia. However, the BERT …","url":["https://acl-bg.org/proceedings/2019/RANLP%202019/pdf/RANLP149.pdf"]}
{"year":"2019","title":"Event-Argument Linking in Hindi for Information Extraction in Disaster Domain","authors":["SK Sahoo, S Saha, A Ekbal, P Bhattacharyya…"],"snippet":"… 3.3 Word Embedding For word embedding (WE) of each word, we use pre-trained fastText [5] word vectors. These embeddings were trained on Hindi Common Crawl and Wikipedia dataset. The size of the word embedding used in our experiments is 300 …","url":["https://www.iitp.ac.in/~ai-nlp-ml/papers/Sovan_CICLing_2019.pdf"]}
{"year":"2019","title":"Evolution of the PAN Lab on Digital Text Forensics","authors":["P Rosso, M Potthast, B Stein, E Stamatatos, F Rangel… - … Retrieval Evaluation in a …, 2019"],"snippet":"… The static web search environment is comprised of the web search engine ChatNoir, which indexes the ClueWeb 2009, the ClueWeb 2012, and (as of 2017) the CommonCrawl, delivering search results in milliseconds while …","url":["https://link.springer.com/chapter/10.1007/978-3-030-22948-1_19"]}
{"year":"2019","title":"Example-Driven Question Answering","authors":["D Wang - 2019"],"snippet":"Page 1. August 19, 2019 DRAFT Example-Driven Question Answering Di Wang August 2019 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 Thesis Committee: Eric Nyberg (chair) (Carnegie Mellon …","url":["http://www.cs.cmu.edu/~diw1/thesis.pdf"]}
{"year":"2019","title":"Explicit Discourse Argument Extraction for German","authors":["P Bourgonje, M Stede - International Conference on Text, Speech, and …, 2019"],"snippet":"… The word embeddings are trained on Common Crawl and Wikipedia [6]. We generated the part-of-speech embeddings from the TIGER corpus [4]. We use a CNN with four fully connected layers. Training this on all classes from Table 1 results in an accuracy of 94.52 …","url":["https://link.springer.com/chapter/10.1007/978-3-030-27947-9_3"]}
{"year":"2019","title":"Exploitation vs. exploration—computational temporal and semantic analysis explains semantic verbal fluency impairment in Alzheimer's disease","authors":["J Tröger, N Linz, A König, P Robert, J Alexandersson… - Neuropsychologia, 2019"],"snippet":"… For deriving semantic metrics, the semantic distance between produced words was calculated based on a fastText (Joulin et al., 2016) neural word embedding, pre-trained on the French Common Crawl and Wikipedia corpora (Grave et al., 2018; Linz et al., 2017) …","url":["https://www.sciencedirect.com/science/article/pii/S0028393218305116"]}
{"year":"2019","title":"Exploiting EuroVoc's Hierarchical Structure for Classifying Legal Documents","authors":["E Filtz, S Kirrane, A Polleres, G Wohlgenannt - … \" On the Move to Meaningful Internet …, 2019","G Wohlgenannt - On the Move to Meaningful Internet Systems: OTM …"],"snippet":"… First, we tested large-scale pre-trained language models trained with generalpurpose text corpora such as GoogleNews and the CommonCrawl, but as expected both performed badly on the legal dataset, for example the Common …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=hm21DwAAQBAJ&oi=fnd&pg=PA164&dq=commoncrawl&ots=pdUzWNZpkR&sig=nBv58MNJuj5jkkROfHpNIYLbyTs","https://link.springer.com/chapter/10.1007/978-3-030-33246-4_10"]}
{"year":"2019","title":"Exploiting knowledge graphs for entity-centric prediction","authors":["S Jiang - 2018"],"snippet":"Page 1. © 2018 Shan Jiang Page 2. EXPLOITING KNOWLEDGE GRAPHS FOR ENTITY-CENTRIC PREDICTION BY SHAN JIANG DISSERTATION Submitted in partial fulfillment of the requirements for the degree of Doctor of …","url":["https://www.ideals.illinois.edu/bitstream/handle/2142/102463/JIANG-DISSERTATION-2018.pdf?sequence=1"]}
{"year":"2019","title":"Exploiting Temporal Relationships in Video Moment Localization with Natural Language","authors":["S Zhang, J Su, J Luo - arXiv preprint arXiv:1908.03846, 2019"],"snippet":"… extracted from VGG [24] fc7 layer, optical flow features are extracted from the penultimate layer [27] and the 300-d Glove feature [21] pretrained on Common Crawl (42 billion tokens) are used as the word embedding. The segment …","url":["https://arxiv.org/pdf/1908.03846"]}
{"year":"2019","title":"Exploiting the Hierarchical Structure of a Thesaurus for Document Classification","authors":["E Filtz, S Kirrane, A Polleres, G Wohlgenannt"],"snippet":"… First, we tested large-scale pre-trained language models trained with generalpropose text corpora such as GoogleNews and the CommonCrawl, but as ex- pected both performed badly on the legal dataset, for example the …","url":["https://aic.ai.wu.ac.at/~polleres/publications/filt-etal-2019COOPIS.pdf"]}
{"year":"2019","title":"Explore FREDDY","authors":["M Günther, M Thiele, W Lehner, Z Yanakiev - BTW 2019, 2019"],"snippet":"… The configuration of the search function is defined in the sidebar (Figure 3b) just as in the query view. 4 Screencast on our FREDDY website https://wwwdb.inf.tu-dresden.de/research-projects/freddy/ 5 https://dblp.uni …","url":["https://dl.gi.de/bitstream/handle/20.500.12116/21558/E08-1.pdf?sequence=1&isAllowed=y"]}
{"year":"2019","title":"Explore FREDDY: Fast Word Embeddings in Database Systems","authors":["M Günther, Z Yanakiev, M Thiele, W Lehner"],"snippet":"… The configuration of the search function is defined in the sidebar (Figure 3b) just as in the query view. 4 Screencast on our FREDDY website https://wwwdb.inf.tu-dresden.de/research-projects/freddy/ 5 https://dblp.uni …","url":["https://btw.informatik.uni-rostock.de/download/tagungsband/E08-1.pdf"]}
{"year":"2019","title":"Exploring Numeracy in Word Embeddings","authors":["A Naik, A Ravichander, C Rose, E Hovy - Proceedings of the 57th Conference of the …, 2019"],"snippet":"… FastText (Bojanowski et al., 2017): Extended Skipgram model representing words as character n-grams to incorporate sub-word information. We evaluate Wikipedia and Common Crawl variants. 3.1 Retrained Word Vectors …","url":["https://www.aclweb.org/anthology/P19-1329"]}
{"year":"2019","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","authors":["C Raffel, N Shazeer, A Roberts, K Lee, S Narang… - arXiv preprint arXiv …, 2019"],"snippet":"… unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet – for example, the Common Crawl project2 produces about 20TB of text data extracted from web pages each month …","url":["https://arxiv.org/pdf/1910.10683"]}
{"year":"2019","title":"Extending Cross-Domain Knowledge Bases with Long Tail Entities using Web Table Data","authors":["Y Oulabi, C Bizer - genre, 2019"],"snippet":"… In a second experiment, we apply the system to a large corpus of web tables extracted from the Common Crawl. This experiment allows us to get an overall im- pression of the potential of web tables for augmenting knowledge bases with long tail entities …","url":["https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_Research/Web-based_Systems/pub/OulabiBizer-LongTailEntities-EDBT2019.pdf"]}
{"year":"2019","title":"Extracting and Analyzing Context Information in User-Support Conversations on Twitter","authors":["D Martens, W Maalej - arXiv preprint arXiv:1907.13395, 2019"],"snippet":"… As the list of marketing names also includes common words (eg, 'five', 'go', or 'plus'), we used the natural language processing library spaCy [33] to remove words that appear in the vocabulary of the included …","url":["https://arxiv.org/pdf/1907.13395"]}
{"year":"2019","title":"Extracting Novel Facts from Tables for Knowledge Graph Completion (Extended version)","authors":["B Kruit, P Boncz, J Urbani - arXiv preprint arXiv:1907.00083, 2019"],"snippet":"… The first one is the T2D dataset [23], which contains a subset of the WDC Web Tables Corpus – a set of tables extracted from the CommonCrawl web scrape6. We use the latest available version of this dataset (v2, released 2017/02). In …","url":["https://arxiv.org/pdf/1907.00083"]}
{"year":"2019","title":"Extracting Novel Facts from Tables for Knowledge Graph Completion","authors":["B Kruit, P Boncz, J Urbani - International Semantic Web Conference, 2019"],"snippet":"… The first one is the T2D dataset [25], which contains a subset of the WDC Web Tables Corpus – a set of tables extracted from the CommonCrawl web scrape 2 . We use the latest available version of this dataset (v2, released 2017/02) …","url":["https://link.springer.com/chapter/10.1007/978-3-030-30793-6_21"]}
{"year":"2019","title":"Facebook AI's WAT19 Myanmar-English Translation Task Submission","authors":["PJ Chen, J Shen, M Le, V Chaudhary, A El-Kishky… - arXiv preprint arXiv …, 2019"],"snippet":"… For Myanmar language, we take five snapshots of the Commoncrawl dataset and combine them with the raw data from Buck et al. (2014) … The Myanmar monolingual data we collect from Commoncrawl contains text in both Unicode and Zawgyi encodings …","url":["https://arxiv.org/pdf/1910.06848"]}
{"year":"2019","title":"Facebook FAIR's WMT19 News Translation Task Submission","authors":["N Ng, K Yee, A Baevski, M Ott, M Auli, S Edunov - arXiv preprint arXiv:1907.06616, 2019"],"snippet":"… We train two language models LI and LN on Newscrawl and Commoncrawl respectively, then score every sentence s in Commoncrawl by HI(s)−HN (s). We select a cu- toff of 0.01, and use all sentences that score higher than …","url":["https://arxiv.org/pdf/1907.06616"]}
{"year":"2019","title":"Facilitating access to health web pages with different language complexity levels","authors":["M Alfano, B Lenzitti, D Taibi, M Helfert - 2019"],"snippet":"… The Web Data Commons (WDC) (Meusel, 2014) contains all Microformat, Microdata and RDFa data extracted from the open repository of web crawl data named Common Crawl (CC)16 … 15 http://webdatacommons.org/ 16 http://commoncrawl.org …","url":["http://doras.dcu.ie/23104/1/ICT4AWE_2019_30_CR.pdf"]}
{"year":"2019","title":"Fast and Accurate Network Embeddings via Very Sparse Random Projection","authors":["H Chen, SF Sultan, Y Tian, M Chen, S Skiena - arXiv preprint arXiv:1908.11512, 2019"],"snippet":"… WWW-200K and WWW-10K [11]: these graphs are derived from the Web graph provided by Common Crawl, where the nodes are hostnames and the edges are the hyperlinks between these websites. For simplicity, we treat this graph as an undirected graph …","url":["https://arxiv.org/pdf/1908.11512"]}
{"year":"2019","title":"Faster Neural Network Training with Data Echoing","authors":["D Choi, A Passos, CJ Shallue, GE Dahl - arXiv preprint arXiv:1907.05550, 2019"],"snippet":"… 2http://commoncrawl.org/2017/07/june-2017-crawl-archive-now-available/ 3Each time a training example is read from disk, it counts as a fresh example. 420k steps for LM1B, 60k for Common Crawl, 110k for ImageNet, 150k for CIFAR-10, and 30k for COCO …","url":["https://arxiv.org/pdf/1907.05550"]}
{"year":"2019","title":"FastSV: A Distributed-Memory Connected Component Algorithm with Fast Convergence","authors":["Y Zhang, A Azad, Z Hu - arXiv preprint arXiv:1910.05971, 2019"],"snippet":"Page 1. FastSV: A Distributed-Memory Connected Component Algorithm with Fast Convergence Yongzhe Zhang ∗ Ariful Azad † Zhenjiang Hu ‡ Abstract This paper presents a new distributed-memory algorithm called FastSV …","url":["https://arxiv.org/pdf/1910.05971"]}
{"year":"2019","title":"FastText-Based Intent Detection for Inflected Languages","authors":["K Balodis, D Deksne - Information, 2019"],"snippet":"… For the word embeddings released by Facebook, we used the ones trained on Wikipedia (https: //fasttext.cc/docs/en/pretrained-vectors.html) because the ones trained on Common Crawl (https: //fasttext.cc/docs/en/crawl-vectors.html) showed inferior results in our tests …","url":["https://www.mdpi.com/2078-2489/10/5/161/pdf"]}
{"year":"2019","title":"Feature Engineering for Text Representation","authors":["D Sarkar - Text Analytics with Python, 2019"],"snippet":"In the previous chapters, we saw how to understand, process, and wrangle text data. However, all machine learning or deep learning models are limited because they cannot understand text data directly...","url":["https://link.springer.com/chapter/10.1007/978-1-4842-4354-1_4"]}
{"year":"2019","title":"Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels","authors":["L Lange, MA Hedderich, D Klakow - arXiv preprint arXiv:1910.06061, 2019"],"snippet":"… clustering. While the Brown clustering was trained on the relatively small Europarl corpus, k- Means clustering seems to benefit from the word embeddings trained on documents from the much larger common crawl. 7 Analysis …","url":["https://arxiv.org/pdf/1910.06061"]}
{"year":"2019","title":"Feature2Vec: Distributional semantic modelling of human property knowledge","authors":["S Derby, P Miller, B Devereux - arXiv preprint arXiv:1908.11439, 2019"],"snippet":"… For our experiments, we make use of the pretrained GloVe embeddings (Pennington et al., 2014) provided in the Spacy1 package trained on the Common Crawl2. The GloVe model includes 685,000 tokens … 1https://spacy …","url":["https://arxiv.org/pdf/1908.11439"]}
{"year":"2019","title":"Feeling Anxious? Perceiving Anxiety in Tweets using Machine Learning","authors":["D Gruda, S Hasan - Computers in Human Behavior, 2019"],"snippet":"… tweets. Words-to-vectors mapping is based on the deep neural network learning GloVe (Pennington, Socher, & Manning, 2014) embedding space built from the Common Crawl Web Data (42 Billion tokens, 1.9M vocab). The …","url":["https://www.sciencedirect.com/science/article/pii/S0747563219301608"]}
{"year":"2019","title":"FIESTA: Fast IdEntification of State-of-The-Art models using adaptive bandit algorithms","authors":["HB Moss, A Moore, DS Leslie, P Rayson - arXiv preprint arXiv:1906.12230, 2019"],"snippet":"… optimiser settings and the same regularisation. All words are lower cased and we use the same Glove common crawl 840B token 300 dimension word embedding (Pennington et al., 2014). We use variational (Gal and Ghahramani …","url":["https://arxiv.org/pdf/1906.12230"]}
{"year":"2019","title":"Figurative Usage Detection of Symptom Words to Improve Personal Health Mention Detection","authors":["A Iyer, A Joshi, S Karimi, R Sparks, C Paris - arXiv preprint arXiv:1906.05466, 2019"],"snippet":"… The first four are a random initialisation, and three pre-trained embeddings. The pretrained embeddings are: (a) word2vec (Mikolov et al., 2013); (b) GloVe (trained on Common Crawl) (Pennington et al., 2014); and, (c) Numberbatch (Speer et al., 2017) …","url":["https://arxiv.org/pdf/1906.05466"]}
{"year":"2019","title":"Finding Generalizable Evidence by Learning to Convince Q&A Models","authors":["E Perez, S Karamcheti, R Fergus, J Weston, D Kiela… - arXiv preprint arXiv …, 2019"],"snippet":"… fastText We define a function BoWFT that computes the average bag-of-words representation of some text using fastText embeddings (Joulin et al., 2017). We use 300-dimensional fastText word vectors pretrained on Common Crawl …","url":["https://arxiv.org/pdf/1909.05863"]}
{"year":"2019","title":"Findings of the First Shared Task on Machine Translation Robustness","authors":["X Li, P Michel, A Anastasopoulos, Y Belinkov… - arXiv preprint arXiv …, 2019"],"snippet":"… To explore effective approaches to leverage abundant out-of-domain parallel data. • To explore novel approaches to leverage abundant monolingual data on the Web (eg, tweets, Reddit comments, commoncrawl, etc.). • To …","url":["https://arxiv.org/pdf/1906.11943"]}
{"year":"2019","title":"Findings of the WMT 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions","authors":["P Koehn, F Guzmán, V Chaudhary, J Pino - Proceedings of the Fourth Conference on …, 2019"],"snippet":"… Corpus Sentences Words Wikipedia Sinhala 155,946 4,695,602 Nepali 92,296 2,804,439 English 67,796,935 1,985,175,324 CommonCrawl Sinhala 5,178,491 110,270,445 Nepali 3,562,373 102,988,609 English 380,409,891 8,894,266,960 …","url":["https://www.aclweb.org/anthology/W19-5404"]}
{"year":"2019","title":"FlauBERT: Unsupervised Language Model Pre-training for French","authors":["H Le, L Vial, J Frej, V Segonne, M Coavoux… - arXiv preprint arXiv …, 2019"],"snippet":"… Common Crawl).11 The data were collected from three main sources: (1) monolingual data for French provided in WMT19 shared tasks (Li et al., 2019, 4 sub-corpora); (2) French text corpora offered in the OPUS collection …","url":["https://arxiv.org/pdf/1912.05372"]}
{"year":"2019","title":"Frame Augmented Alternating Attention Network for Video Question Answering","authors":["W Zhang, S Tang, Y Cao, S Pu, F Wu, Y Zhuang - IEEE Transactions on Multimedia, 2019"],"snippet":"Page 1. 1520-9210 (c) 2019 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/8811730/"]}
{"year":"2019","title":"Frequency, acceptability, and selection: A case study of clause-embedding","authors":["AS White, K Rawlins"],"snippet":"Page 1. Frequency, acceptability, and selection: A case study of clause-embedding Aaron Steven White University of Rochester aaron.white@rochester.edu Kyle Rawlins Johns Hopkins University kgr@jhu.edu Abstract We investigate …","url":["https://ling.auf.net/lingbuzz/004596/current.pdf"]}
{"year":"2019","title":"From Legal to Technical Concept: Towards an Automated Classification of German Political Twitter Postings as Criminal Offenses","authors":["F Zufall, T Horsmann, T Zesch"],"snippet":"… We use a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) for classification.30 We use the 300-dimensional German pre-trained word embeddings provided by Grave et al. (2018), which are trained on the German common crawl …","url":["https://www.researchgate.net/profile/Frederike_Zufall/publication/331475806_From_Legal_to_Technical_Concept_Towards_an_Automated_Classification_of_German_Political_Twitter_Postings_as_Criminal_Offenses/links/5ccbe9b0a6fdcc4719838905/From-Legal-to-Technical-Concept-Towards-an-Automated-Classification-of-German-Political-Twitter-Postings-as-Criminal-Offenses.pdf"]}
{"year":"2019","title":"Frontiersinpatternrecognitionandartificialintelligence","authors":["B Marleah, N Nicola, SC Yee - 2019"],"snippet":""}
{"year":"2019","title":"Frowning Frodo, Wincing Leia, and a Seriously Great Friendship: Learning to Classify Emotional Relationships of Fictional Characters","authors":["E Kim, R Klinger - arXiv preprint arXiv:1903.12453, 2019"],"snippet":"… We obtain word vectors for the embedding layer from GloVe (pre-trained on Common Crawl, d = 300, Pennington et al., 2014) and initialize out- of-vocabulary terms with zeros (including the po- sition indicators). 4 Experiments Experimental Setting …","url":["https://arxiv.org/pdf/1903.12453"]}
{"year":"2019","title":"Fusing Vector Space Models for Domain-Specific Applications","authors":["L Rettig, J Audiffren, P Cudré-Mauroux - arXiv preprint arXiv:1909.02307, 2019"],"snippet":"… Despite the convenience they bring, using such readilyavailable, pre-trained models is often suboptimal in vertical applications [2], [3]; as these models are pre-trained on large, non-specific sources (eg, Wikipedia and the Common …","url":["https://arxiv.org/pdf/1909.02307"]}
{"year":"2019","title":"Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study","authors":["JA Balazs, Y Matsuo - arXiv preprint arXiv:1904.05584, 2019"],"snippet":"Page 1. Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study Jorge A. Balazs and Yutaka Matsuo Graduate School of Engineering The University of Tokyo {jorge, matsuo}@weblab.tu-tokyo.ac.jp Abstract …","url":["https://arxiv.org/pdf/1904.05584"]}
{"year":"2019","title":"General Purpose Vector Representation for Swedish Documents: An application of Neural Language Models","authors":["S Hedström - 2019"],"snippet":"Page 1. General Purpose Vector Representation for Swedish Documents An application of Neural Language Models Simon Hedström Master's Thesis in Engineering Physics, Department of Physics, Umeå University, 2019 Page …","url":["https://umu.diva-portal.org/smash/get/diva2:1323994/FULLTEXT01.pdf"]}
{"year":"2019","title":"Generalizable prediction of academic performance from short texts on social media","authors":["I Smirnov - arXiv preprint arXiv:1912.00463, 2019"],"snippet":"… We obtained significantly better results with a model that used word-embeddings (see Methods). We also find that embeddings trained on the VK corpus outperform models trained on the Wikipedia and Common Crawl corpora (Table 1). 3 Page 4 …","url":["https://arxiv.org/pdf/1912.00463"]}
{"year":"2019","title":"Generalizing Question Answering System with Pre-trained Language Model Fine-tuning","authors":["D Su, Y Xu, GI Winata, P Xu, H Kim, Z Liu, P Fung - … of the 2nd Workshop on Machine …, 2019"],"snippet":"… (2009)) and Common Crawl (Buck et al., 1https://github.com/mrqa/MRQA-SharedTask-2019 2014) for pre-training … 2014. N-gram counts and language models from the common crawl. In LREC, volume 2, page 4. Citeseer …","url":["https://mrqa.github.io/assets/papers/63_Paper.pdf"]}
{"year":"2019","title":"Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model","authors":["P Vijayaraghavan, D Roy - arXiv preprint arXiv:1909.07873, 2019"],"snippet":"… These paraphrase datasets together contains text from various sources: Common Crawl, CzEng1.6, Europarl, News Commentary, Quora questions, and Twitter trending topic tweets. We do not use all the data for our pretraining …","url":["https://arxiv.org/pdf/1909.07873"]}
{"year":"2019","title":"Generating composite SQL queries from natural language","authors":["M De Groote - 2018"],"snippet":"… of the questions. We decided to use the Common Crawl embedding that is trained on 42 billion tokens, consists of a vocabulary of 1.9 million tokens and embeds these tokens in the 300-dimensional vector space5. All the words …","url":["https://lib.ugent.be/fulltxt/RUG01/002/494/903/RUG01-002494903_2018_0001_AC.pdf"]}
{"year":"2019","title":"Generating Language-Independent Neural Sentence Embeddings for Natural Language Classification Tasks","authors":["S Erhardt"],"snippet":"… [Rud17] At the time this thesis was written, there are Word Embeddings for more than 150 languages, trained on Common Crawl1 and Wikipedia, available. [Rud17] 1An open repository of web crawl data that can be …","url":["https://www.social.in.tum.de/fileadmin/w00bwc/www/Gerhard_Hagerer/thesis.pdf"]}
{"year":"2019","title":"Generic Web Content Extraction with Open-Source Software","authors":["A Barbaresi"],"snippet":"… Because of the vastly increasing variety of corpora, text types and use cases, it becomes more and more difficult to assess the usefulness and appropriateness of certain web texts 1https://commoncrawl.org for given research objectives …","url":["https://corpora.linguistik.uni-erlangen.de/data/konvens/proceedings/papers/kaleidoskop/camera_ready_barbaresi.pdf"]}
{"year":"2019","title":"Geo-spatial text-mining from Twitter–a feature space analysis with a view toward building classification in urban regions","authors":["M Häberle, M Werner, XX Zhu - European Journal of Remote Sensing, 2019"],"snippet":"Skip to Main Content …","url":["https://www.tandfonline.com/doi/full/10.1080/22797254.2019.1586451"]}
{"year":"2019","title":"Ghmerti at SemEval-2019 Task 6: A Deep Word-and Character-based Approach to Offensive Language Identification","authors":["E Doostmohammadi, H Sameti, A Saffar - … of the 13th International Workshop on …, 2019"],"snippet":"… The indices include 256 of the most common characters, plus 0 for padding and 1 for un- known characters. 2. xw which is the embeddings of the words in the input tweet based on FastText's 600Btoken common crawl model (Mikolov et al., 2018) …","url":["https://www.aclweb.org/anthology/S19-2110"]}
{"year":"2019","title":"GLOSS: Generative Latent Optimization of Sentence Representations","authors":["SP Singh, A Fan, M Auli - arXiv preprint arXiv:1907.06385, 2019"],"snippet":"… representations. This could be as simple as using a bag-of-words averaging of Glove (Pennington et al., 2014) word embeddings trained on a corpus such as CommonCrawl, which we re- fer to as Glove-BoW. Methods such …","url":["https://arxiv.org/pdf/1907.06385"]}
{"year":"2019","title":"GraphVite: A High-Performance CPU-GPU Hybrid System for Node Embedding","authors":["Z Zhu, S Xu, M Qu, J Tang - arXiv preprint arXiv:1903.00757, 2019"],"snippet":"Page 1. GraphVite: A High-Performance CPU-GPU Hybrid System for Node Embedding Zhaocheng Zhu Mila - Québec AI Institute Université de Montréal zhaocheng.zhu@ umontreal.ca Shizhen Xu Tsinghua University xsz12@mails.tsinghua.edu.cn …","url":["https://arxiv.org/pdf/1903.00757"]}
{"year":"2019","title":"Green AI","authors":["R Schwartz, J Dodge, NA Smith, O Etzioni - arXiv preprint arXiv:1907.10597, 2019"],"snippet":"… For example, the June 2019 Common Crawl contains 242 TB of uncompressed data,12 so even simple filtering to extract usable text is difficult … 11https://opensource.google.com/ projects/open-images-dataset 12http://commoncrawl.org/2019/07 …","url":["https://arxiv.org/pdf/1907.10597"]}
{"year":"2019","title":"Grounded Response Generation Task at DSTC7","authors":["M Galley, C Brockett, X Gao, J Gao, B Dolan"],"snippet":"… Turn 4 still pretty incredible , but quite a bit different that 10,000 meters . Table 1: Sample of the DSTC7 Sentence Generation data, which combines Reddit data (Turns 1-4) along with documents (extracted from Common Crawl) discussed in the conversations …","url":["http://workshop.colips.org/dstc7/papers/DSTC7_Task_2_overview_paper.pdf"]}
{"year":"2019","title":"Happy Together: Learning and Understanding Appraisal From Natural Language","authors":["A Rajendran, C Zhang, M Abdul-Mageed"],"snippet":"… language models (ULMFiT). Exploiting Simple GloVe Embeddings For the embedding layer, we obtain the 300-dimensional embedding vector for tokens using GloVe's Common Crawl pre-trained model [13]. GloVe embeddings …","url":["https://mageed.sites.olt.ubc.ca/files/2019/01/AffCon_aaai2019_happyDB.pdf"]}
{"year":"2019","title":"HARE: a Flexible Highlighting Annotator for Ranking and Exploration","authors":["D Newman-Griffis, E Fosler-Lussier - arXiv preprint arXiv:1908.11302, 2019"],"snippet":"… ated three commonly used benchmark embedding sets: word2vec skipgram (Mikolov et al., 2013) using GoogleNews,6 FastText skipgram with subword information on WikiNews,7 and GloVe (Pennington et al., 2014) on 840 …","url":["https://arxiv.org/pdf/1908.11302"]}
{"year":"2019","title":"HATEMINER at SemEval-2019 Task 5: Hate speech detection against Immigrants and Women in Twitter using a Multinomial Naive Bayes Classifier","authors":["N Chakravartula - Proceedings of the 13th International Workshop on …, 2019"],"snippet":"… Word Embeddings: Glove840B - common crawl, GloveTwitter27B - twitter crawl (Pennington et al., 2014) and fasttext - common crawl (Mikolov et al., 2018) pre-trained word embeddings are used to analyze their impact on the classification …","url":["https://www.aclweb.org/anthology/S19-2071"]}
{"year":"2019","title":"HealthSuggestions: moving beyond the beta version","authors":["PMP dos Santos - 2019"],"snippet":"Page 1. FACULDADE DE ENGENHARIA DA UNIVERSIDADE DO PORTO Health Suggestions: Moving Beyond the Beta Version Paulo Miguel Pereira dos Santos Master in Informatics and Computing Engineering Supervisor …","url":["https://repositorio-aberto.up.pt/bitstream/10216/121948/2/347008.2.pdf"]}
{"year":"2019","title":"Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition","authors":["GI Winata, Z Lin, J Shin, Z Liu, P Fung - arXiv preprint arXiv:1909.08504, 2019"],"snippet":"… We use FastText word embeddings trained from Common Crawl and Wikipedia (Grave et al., 2018) for English (es), Spanish (es), including four Romance languages: Catalan (ca), Portuguese (pt), French (fr), Italian …","url":["https://arxiv.org/pdf/1909.08504"]}
{"year":"2019","title":"High Quality ELMo Embeddings for Seven Less-Resourced Languages","authors":["M Ulčar, M Robnik-Šikonja - arXiv preprint arXiv:1911.10049, 2019"],"snippet":"… They used 20-million-words data randomly sampled from the raw text released by the CoNLL 2017 Shared Task - Automatically Annotated Raw Texts and Word Embeddings (Ginter et al., 2017), which is a combination of Wikipedia dump and common crawl …","url":["https://arxiv.org/pdf/1911.10049"]}
{"year":"2019","title":"Hitachi at MRP 2019: Unified Encoder-to-Biaffine Network for Cross-Framework Meaning Representation Parsing","authors":["Y Koreeda, G Morio, T Morishita, H Ozaki, K Yanai - arXiv preprint arXiv:1910.01299, 2019"],"snippet":"… Named entity label Named entity (NE) recognition is applied to the input text (see Section 7.1). GloVe We use 300-dimensional GloVe (Pennington et al., 2014) pretrained on Common Crawl2 which are kept fixed during the training …","url":["https://arxiv.org/pdf/1910.01299"]}
{"year":"2019","title":"HMM, Is This Ethical? Predicting the Ethics of Reddit Life Protips","authors":["M Coots, P Lu, L Wang"],"snippet":"… large corpus of text. GloVe representations have been trained on several large datasets that are publicly available, including corpuses from Wikipedia, Gigaword, Twitter, and Common Crawl [4]. 3. Task Definition Our problem is …","url":["https://madisoncoots.com/files/ethics.pdf"]}
{"year":"2019","title":"How Decoding Strategies Affect the Verifiability of Generated Text","authors":["L Massarelli, F Petroni, A Piktus, M Ott, T Rocktäschel… - arXiv preprint arXiv …, 2019"],"snippet":"… consisting of roughly 3 Billion Words; (iv) CC- NEWS, a de-duplicated subset of the English portion of the CommonCrawl news dataset (Nagel, 2016; Bakhtin et al., 2019; Liu et al., 2019a), which totals around 16 Billion words …","url":["https://arxiv.org/pdf/1911.03587"]}
{"year":"2019","title":"How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questions","authors":["Z Chu, M Chen, J Chen, M Wang, K Gimpel, M Faruqui… - arXiv preprint arXiv …, 2019"],"snippet":"… and En↔Fr. The English-German translation models are trained on WMT datasets, including News Commentary 13, Europarl v7, and Common Crawl, and evaluated on newstest2013 for early stopping. On the newstest2013 …","url":["https://arxiv.org/pdf/1911.09247"]}
{"year":"2019","title":"How Well Do Embedding Models Capture Non-compositionality? A View from Multiword Expressions","authors":["N Nandakumar, T Baldwin, B Salehi - Proceedings of the 3rd Workshop on Evaluating …, 2019"],"snippet":"… It tokenises text at the character level. fastText We used the 300-dimensional fastText model pre-trained on Common Crawl and Wikipedia using CBOW (fastTextpre), as well as one trained over the same Wikipedia corpus4 us- ing skip-gram (fastText) …","url":["https://www.aclweb.org/anthology/W19-2004"]}
{"year":"2019","title":"Hybrid Rule-Based Model for Phishing URLs Detection","authors":["KS Adewole, AG Akintola, SA Salihu, N Faruk… - International Conference for …, 2019","N Faruk, RG Jimoh - … International Conference, iCETiC 2019, London, UK …, 2019"],"snippet":"… 1. From this figure, data collected from different servers such as Yahoo, Alexa, Common Crawl, PhishTank and OpenPhish are preprocessed in order to extract meaningful features that can be used for categorizing phishing websites from legitimate ones …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=QF6mDwAAQBAJ&oi=fnd&pg=PA119&dq=commoncrawl&ots=T7vreYeKah&sig=sO3M90XucnzXO7OeF6horBncwb4","https://link.springer.com/chapter/10.1007/978-3-030-23943-5_9"]}
{"year":"2019","title":"Hybrid Words Representation for Airlines Sentiment Analysis","authors":["U Naseem, SK Khan, I Razzak, IA Hameed"],"snippet":"… GloVe uses ratios of co-occurrence probabilities. It is favourable to concatenate ELMo embeddings with traditional word embeddings. In this work, we have used pre-trained GloVe embedding (trained on 840 billion token from common crawl) of 300 dimensions …","url":["https://www.researchgate.net/profile/Ibrahim_Hameed/publication/336579383_Hybrid_Words_Representation_for_Airlines_Sentiment_Analysis/links/5da6e53892851caa1ba6f8c6/Hybrid-Words-Representation-for-Airlines-Sentiment-Analysis.pdf"]}
{"year":"2019","title":"Hyper: Distributed Cloud Processing for Large-Scale Deep Learning Tasks","authors":["D Buniatyan - arXiv preprint arXiv:1910.07172, 2019"],"snippet":"… [4] MinIO high performance object storage server compatible with Amazon S3 API. https://github.com/minio/minio, 2018. [Online; accessed 31- May-2019]. [5] Common Crawl Dataset. https://commoncrawl.org, 2019. [Online; accessed 31-May-2019] …","url":["https://arxiv.org/pdf/1910.07172"]}
{"year":"2019","title":"Hyperparameter Tuning for Deep Learning in Natural Language Processing","authors":["A Aghaebrahimian, M Cieliebak - 2019"],"snippet":"… on the Common Crawl, one on 42 and the other on 840 billion tokens), FastText (Bojanowski et al., 2016), dependency based (Levy and Goldberg, 2014), and ELMo (Peters et al., 2018). As shown in Ta- ble 1, the Glove …","url":["http://ceur-ws.org/Vol-2458/paper5.pdf"]}
{"year":"2019","title":"Identification Of Bot Accounts In Twitter Using 2D CNNs On User-generated Contents","authors":["M Polignano, MG de Pinto, P Lops, G Semeraro - 2019"],"snippet":"… FastTextEmb)8: 300 dimensionality vectors, composed by a vocabulary of 2 million words and n-grams of the words, case sensitive and obtained from 600 billion of tokens trained on data crawled from generic Internet web pages by Common Crawl nonprofit organization; …","url":["https://www.researchgate.net/profile/Marco_Polignano/publication/334636395_Identification_Of_Bot_Accounts_In_Twitter_Using_2D_CNNs_On_User-generated_Contents/links/5d373c10a6fdcc370a59e892/Identification-Of-Bot-Accounts-In-Twitter-Using-2D-CNNs-On-User-generated-Contents.pdf"]}
{"year":"2019","title":"Identification of Good and Bad News on Twitter","authors":["P Aggarwal, A Aker"],"snippet":"… We use tf-idf representation for each vocabulary term. 5.2.3 Embeddings Finally, we also use fasttext based embedding (Mikolov et al., 2018) vectors which are trained on common crawl having 600 billion tokens. 5.3 Classifiers …","url":["https://www.researchgate.net/profile/Ahmet_Aker3/publication/334825190_Identification_of_Good_and_Bad_News_on_Twitter/links/5d42c34992851cd04697548a/Identification-of-Good-and-Bad-News-on-Twitter.pdf"]}
{"year":"2019","title":"Identifying and Addressing Structural Inequalities in the Representativeness of Geographic Technologies","authors":["IL Johnson - 2019"],"snippet":"… knowledge graphs (Wikipedia and Google [289]), word embeddings (Wikipedia, Twitter, and Common Crawl in GloVe embeddings [238]), object detection (Instagram hashtags and Facebook [292])—and adding …","url":["http://search.proquest.com/openview/dccae6679751f41f283b33f555947aa8/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2019","title":"Identifying transfer models for machine learning tasks","authors":["P Watson, B Bhattacharjee, NC CODELLA… - US Patent App. 15/982,622, 2019"],"snippet":"US20190354850A1 - Identifying transfer models for machine learning tasks - Google Patents. Identifying transfer models for machine learning tasks. Download PDF Info. Publication number US20190354850A1. US20190354850A1 …","url":["https://patents.google.com/patent/US20190354850A1/en"]}
{"year":"2019","title":"Idiap Abstract Text Summarization System for German Text Summarization Task","authors":["S Parida, P Motlicek - 2019"],"snippet":"… The experiments performed over 1http://opennmt.net/OpenNMT-py/ Summarization.html 2https://www.swisstext.org/ 3http://commoncrawl.org/ these datasets are described in the Section 4 (de- noted as S1 experimental …","url":["http://ceur-ws.org/Vol-2458/paper9.pdf"]}
{"year":"2019","title":"IIT Varanasi at HASOC 2019: Hate Speech and Offensive Content Identification in Indo-European Languages","authors":["A Mishra, S Pal - Proceedings of the 11th annual meeting of the Forum …"],"snippet":"… embedding. One of the pretrained glove embeddings is based on the common crawl which represents each word in the dimension of 300, and the other one is based on Twitter data which represents each word in the dimension of 200 …","url":["http://irlab.daiict.ac.in/~Parth/T3-22.pdf"]}
{"year":"2019","title":"IIT-BHU at CIQ 2019: Classification of Insincere Questions","authors":["A Mishra, S Pal"],"snippet":"… Different versions of glove pre-trained em- bedding exist; however, we use embedding trained of dimension 300 on common crawl using 840B tokens and 2.2M vocabulary3. We generated random embedding of dimension 300 for out of vocabulary words …","url":["http://irlab.daiict.ac.in/~Parth/T5-4.pdf"]}
{"year":"2019","title":"Impact of Debiasing Word Embeddings on Information Retrieval","authors":["E Gerritse - 2019"],"snippet":"… Bolukbasi et al. [1] show that there is a high correlation in bias in Word2Vec trained on Google News and Glove trained on the common crawl, so we still cannot infer whether the method or the dataset is more important for creating the bias …","url":["http://www.emmagerritse.com/pdfs/FDIA_2019_paper.pdf"]}
{"year":"2019","title":"Improved Quality Estimation of Machine Translation with Pre-trained Language Representation","authors":["G Miao, H Di, J Xu, Z Yang, Y Chen, K Ouchi - CCF International Conference on …, 2019"],"snippet":"… The former is mainly obtained from the open news datasets of the WMT17 and WMT18 MT evaluation tasks, including five data sets: Europarl v7, Europarl v12, Europarl v13, Common Crawl corpus, and Rapid corpus of EU press releases …","url":["https://link.springer.com/chapter/10.1007/978-3-030-32233-5_32"]}
{"year":"2019","title":"Improving Conditioning in Context-Aware Sequence to Sequence Models","authors":["X Wang, J Weston, M Auli, Y Jernite - arXiv preprint arXiv:1911.09728, 2019"],"snippet":"… 2019) for LFQA. The dataset consists of 272,000 complex questions and answer pairs, along with supporting documents created by gathering and concatenating passages from CommonCrawl web pages which are relevant to the question …","url":["https://arxiv.org/pdf/1911.09728"]}
{"year":"2019","title":"Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data","authors":["W Zhao, L Wang, K Shen, R Jia, J Liu - arXiv preprint arXiv:1903.00138, 2019"],"snippet":"… We do not use reranking when evaluating the CoNLL-2014 data sets. But we rerank the top 12 hypothesizes us- ing the language model trained on Common Crawl (Junczys-Dowmunt and Grundkiewicz, 2016) for …","url":["https://arxiv.org/pdf/1903.00138"]}
{"year":"2019","title":"Improving Implicit Stance Classification in Tweets Using Word and Sentence Embeddings","authors":["R Schaefer, M Stede - Joint German/Austrian Conference on Artificial …, 2019"],"snippet":"… combinations. 4.2 fastText Embeddings. We use pre-trained 300-dimensional fastText [11] word vectors that have been trained on Wikipedia and Common Crawl data. For training, an extension of the CBOW model has been used …","url":["https://link.springer.com/chapter/10.1007/978-3-030-30179-8_26"]}
{"year":"2019","title":"Improving Named Entity Recognition with Commonsense Knowledge Pre-training","authors":["G Dekhili, NT Le, F Sadat - Pacific Rim Knowledge Acquisition Workshop, 2019"],"snippet":"… which is the concatenation of ConceptNet PPMI embeddings with Word2Vec embeddings trained on 100 billion words of Google News using skip-grams with negative sampling [14] and GloVe 1.2 embeddings trained on 840 billion words of the Common Crawl [16] …","url":["https://link.springer.com/chapter/10.1007/978-3-030-30639-7_2"]}
{"year":"2019","title":"Improving Neural Machine Translation of Subtitles with Finetun-ing","authors":["S Reinsperger - 2019"],"snippet":"… 3 Results 53 3.1 ParallelCorpora . . . . . 54 3.1.1 Europarl. . . . . 54 3.1.2 Common Crawl . . . . . 54 3.1.3 NewsCommentary . . . . . 56 3.1.4 Subtitles …","url":["http://www.simonrsp.com/masterthesis.pdf"]}
{"year":"2019","title":"Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back Translation","authors":["Z Li, L Specia - arXiv preprint arXiv:1910.03009, 2019"],"snippet":"… 3.1 Corpora We used all parallel corpora from the WMT19 Robustness Task on Fr↔En. For out-of-domain training, we used the WMT15 Fr↔En News Translation Task data, including Europarl v7, Common Crawl, UN, News Commentary v10, and Gigaword Corpora …","url":["https://arxiv.org/pdf/1910.03009"]}
{"year":"2019","title":"Improving Neural Machine Translation with Pre-trained Representation","authors":["R Weng, H Yu, S Huang, W Luo, J Chen - arXiv preprint arXiv:1908.07688, 2019"],"snippet":"… We use newstest2015 (NST15) as our validation set, and newstest2016 (NST16) as test sets 4. We use 40 million monolingual sentences from WMT-16 Common Crawl data-set … We use 5 million monolingual sentences …","url":["https://arxiv.org/pdf/1908.07688"]}
{"year":"2019","title":"Improving orienteering-based tourist trip planning with social sensing","authors":["F Persia, G Pilato, M Ge, P Bolzoni, D D'Auria… - Future Generation …, 2019"],"snippet":"… This is a popular technique in machine learning for uncovering subsymbolic meanings, such as word analogies. We utilized a pre-trained word vector encoding for Italian provided by fastText [32], which was trained on Common Crawl and Wikipedia …","url":["https://www.sciencedirect.com/science/article/pii/S0167739X19303929"]}
{"year":"2019","title":"Improving Quality Estimation of Machine Translation by Using Pre-trained Language Representation","authors":["G Miao, H Di, J Xu, Z Yang, Y Chen, K Ouchi - China Conference on Machine …, 2019","Y Chen, K Ouchi - Machine Translation: 15th China Conference, CCMT …, 2019"],"snippet":"… Metrics We first train the bilingual expert model [9] with large-scale parallel corpus released for the WMT17/WMT18 News Machine Translation Task, which mainly consists of five data sets, including Europarl v7, Europarl v12 …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=WuK_DwAAQBAJ&oi=fnd&pg=PA11&dq=commoncrawl&ots=XTi4UL5q8i&sig=lCeqF4TBBuqQrg4EE0rN09FZeVs","https://link.springer.com/chapter/10.1007/978-981-15-1721-1_2"]}
{"year":"2019","title":"Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader","authors":["W Xiong, M Yu, S Chang, X Guo, WY Wang - arXiv preprint arXiv:1905.07098, 2019"],"snippet":"… Page 6. A Implementation Details Throughout our experiments, we use the 300-dimension GloVe embeddings trained on the Common Crawl corpus. The hidden dimension of LSTM and the dimension of entity embeddings are both 100 …","url":["https://arxiv.org/pdf/1905.07098"]}
{"year":"2019","title":"In-call virtual assistant","authors":["R Raanani, R Levy, MY Breakstone - US Patent App. 16/165,566, 2019"],"snippet":"… At the same time, natural language processing (NLP) approaches to both topic modeling and world-knowledge modeling, have become much more efficient due to the availability of large, freely accessible natural language corpora (eg, CommonCrawl), as well as freely …","url":["https://patentimages.storage.googleapis.com/b2/cd/2c/a7fa39e3002b4f/US20190057698A1.pdf"]}
{"year":"2019","title":"Incendiary News Detection","authors":["EB Coban, E Filatova - 2019"],"snippet":"… features. We run classification experiments for unigrams, and combination of uniand bi-grams. 10https://www.nltk.org/ 11http://scikit-learn.org 12https://github.com/ otuncelli/turkish-stemmer-python 13http://commoncrawl.org/ For …","url":["https://pdfs.semanticscholar.org/8c78/f9da879fc5936ef84dc7128db691d7042fef.pdf"]}
{"year":"2019","title":"Incorporating Domain Knowledge into Natural Language Inference on Clinical Texts","authors":["M Lu, Y Fang, F Yan, M Li - IEEE Access, 2019"],"snippet":"… two domain-specific corpus: • GloVe[CC]: GloVe embeddings [21], trained on Common Crawl. • fastText[BioASQ]: fastText embeddings [22], trained on PubMed abstracts from the BioASQ challenge [23]. • fastText[MIMIC-III]: fastText …","url":["https://ieeexplore.ieee.org/iel7/6287639/6514899/08701433.pdf"]}
{"year":"2019","title":"Incorporating Syntactic Knowledge in Neural Quality Estimation for Machine Translation","authors":["N Ye, Y Wang, D Cai - China Conference on Machine Translation, 2019"],"snippet":"… One is the large-scale bilingual dataset for training the feature extraction module. It comes from the parallel corpus of WMT machine translation task, including Europarl v7, Common Crawl corpus, News Commentary v11 and so on …","url":["https://link.springer.com/chapter/10.1007/978-981-15-1721-1_3"]}
{"year":"2019","title":"Inducing Relational Knowledge from BERT","authors":["Z Bouraoui, J Camacho-Collados, S Schockaert - arXiv preprint arXiv:1911.12753, 2019"],"snippet":"… As static word embeddings for the baselines, we will use the Skip-gram word vectors that were pre-trained from the 100B words Google News data set6 (SG-GN) and GloVe word vectors which were pre-trained from the …","url":["https://arxiv.org/pdf/1911.12753"]}
{"year":"2019","title":"Inducing Schema. org Markup from Natural Language Context","authors":["GK Shahi, D Nandini, S Kumari - Kalpa Publications in Computing, 2019"],"snippet":"… extension, in 2012 another data hub called Web Data Commons [5] came up with structured data extracted from the Common Crawl … 5http:// commoncrawl.org/ 6http://webdatacommons.org/ 7The WARC file format …","url":["https://easychair.org/publications/download/DXGr"]}
{"year":"2019","title":"Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings","authors":["M Le, S Roller, L Papaxanthos, D Kiela, M Nickel - arXiv preprint arXiv:1902.00913, 2019"],"snippet":"Page 1. Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings Matt Le1 and Stephen Roller1 and Laetitia Papaxanthos2 Douwe Kiela1 and Maximilian Nickel1 1Facebook AI Research, New York …","url":["https://arxiv.org/pdf/1902.00913"]}
{"year":"2019","title":"Information extraction","authors":["S Razniewski"],"snippet":"… 8 Page 9. Taxi [Panchenko et al., 2016] 1. Crawl domain-specific text corpora in addition to WP, Commoncrawl 2. Candidate hypernymy extraction 1. Via substrings • “biomedical science” isA “science” • “microbiology” isA “biology” • “toast with bacon” isA “toast” …","url":["https://www.mpi-inf.mpg.de/fileadmin/inf/d5/teaching/ws19-20_ie/5_Taxonomy_induction_coreference_disambiguation.pdf"]}
{"year":"2019","title":"InriaFBK Drawing Attention to Offensive Language at Germeval2019","authors":["M Corazza, S Menini, E Cabrio, S Tonelli, S Villata…"],"snippet":"… This is the main reason why we chose to use FastText embeddings (Bojanowski et al., 2016), pretrained on Common Crawl and Wikipedia 3. 4.3 Recurrent model We develop a simple recurrent neural network model and use it for all subtasks …","url":["https://corpora.linguistik.uni-erlangen.de/data/konvens/proceedings/papers/germeval/Germeval_Task_2_2019_paper_1.INRIA.pdf"]}
{"year":"2019","title":"Integrating Grammatical Features into CNN Model for Emotion Classification","authors":["AC Le - 2018 5th NAFOSTED Conference on Information and …, 2018"],"snippet":"… a sentence s = 11 In this study we used the vector set GloVe [16], it is pretrained word vectors for Common Crawl (glove.42B.300d) with 300 dimensions for word embeddings to use for English data. For Vietnamese emotion …","url":["https://ieeexplore.ieee.org/abstract/document/8606875/"]}
{"year":"2019","title":"Integrating UMLS for Early Detection of Sings of Anorexia","authors":["FM Plaza-del-Arco, P López-Úbeda, MC Dıaz-Galiano… - 2019"],"snippet":"… Specifically, we use Page 6. the available pre-trained statistical models for English ”en core web md” wich version is 1.2.0. It is composed of 685k keys, 20k unique vectors (300 dimensions) and it was trained on OntoNotes …","url":["http://www.dei.unipd.it/~ferro/CLEF-WN-Drafts/CLEF2019/paper_76.pdf"]}
{"year":"2019","title":"Integrating word embeddings and document topics with deep learning in a video classification framework","authors":["Z Kastrati, AS Imran, A Kurti - Pattern Recognition Letters, 2019"],"snippet":"… GloVe contains word embeddings for a vocabulary of 400K words trained on 42 billion words from Wikipedia pages and newswire, and fastText includes word embeddings for a vocabulary of 2 million words trained on 600 billion tokens from Common Crawl …","url":["https://www.sciencedirect.com/science/article/pii/S0167865519302326"]}
{"year":"2019","title":"Intelligent sentiment analysis approach using edge computing‐based deep learning technique","authors":["H Sankar, V Subramaniyaswamy, V Vijayakumar… - Software: Practice and Experience"],"snippet":"… Word2Vec, 300d, 3 Million, 100 Billion. Common Crawl, 300d, 42 Billion, 1.9 Million. Common Crawl, 300d, 840 Billion, 2.2 Million. The main drawback of unsupervised word embedding learning is that it does not hold the sentiment …","url":["https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2687"]}
{"year":"2019","title":"Interactive Language Learning by Question Answering","authors":["X Yuan, MA Cote, J Fu, Z Lin, C Pal, Y Bengio… - arXiv preprint arXiv …, 2019"],"snippet":"Page 1. Interactive Language Learning by Question Answering Xingdi Yuan♥∗ Marc-Alexandre Côté♥∗ Jie Fu♣♠ Zhouhan Lin♦♠ Christopher Pal♣♠ Yoshua Bengio♦♠ Adam Trischler♥ ♥Microsoft Research, Montréal ♣Polytechnique …","url":["https://arxiv.org/pdf/1908.10909"]}
{"year":"2019","title":"Interactive Machine Comprehension with Information Seeking Agents","authors":["X Yuan, J Fu, MA Cote, Y Tay, C Pal, A Trischler - arXiv preprint arXiv:1908.10449, 2019"],"snippet":"… Word embeddings are initialized by the 300-dimension fastText (Mikolov et al. 2018) vectors trained on Common Crawl (600B tokens), and are fixed during training. Character embeddings are initialized by 200-dimension random vectors …","url":["https://arxiv.org/pdf/1908.10449"]}
{"year":"2019","title":"Internet of Things Anomaly Detection using Multivariate Analysis","authors":["S Ezekiel, AA Alshehri, L Pearlstein, XW Wu, A Lutz - The 3rd ICICPE 2019 Conference …"],"snippet":"… Our model uses the GloVe (Pennington et al., 2014) 300-dimensional vectors trained on the Common Crawl corpus with 42B tokens as word level features, as this resulted in the best performance in preliminary experiments …","url":["http://icicpe.org/wp-content/uploads/2019/12/ICICPE-2019-vol.31.pdf#page=90"]}
{"year":"2019","title":"Iot-based call assistant device","authors":["R Raanani, R Levy, MY Breakstone - US Patent App. 16/168,663, 2019"],"snippet":"… At the same time, natural language processing (NLP) approaches to both topic modeling and world-knowledge modeling, have become much more efficient due to the availability of large, freely accessible natural language corpora (eg, CommonCrawl), as well as freely …","url":["https://patentimages.storage.googleapis.com/c3/a1/97/799532a8db7406/US20190057079A1.pdf"]}
{"year":"2019","title":"Iterative Keyword Optimization","authors":["A Elyashar, M Reuben, R Puzis"],"snippet":"… The model was trained on Common Crawl 4 and Wikipedia 5 using the fastText library 6. We used Euclidean as the distance measure … 4 http://commoncrawl.org/ 5 https://www.wikipedia.org/ 6 https://fasttext …","url":["http://sbp-brims.org/2019/proceedings/papers/working_papers/Elyashar.pdf"]}
{"year":"2019","title":"JHU 2019 Robustness Task System Description","authors":["M Post, K Duh - Proceedings of the Fourth Conference on Machine …, 2019"],"snippet":"… the best million lines each of CommonCrawl, Gigaword, and the UN corpus; and • the MTNT training data. Data sizes are indicated in Table 1. dataset segments words Europarl 2.0m 50.2m News Commentary 200k 4.4m …","url":["https://www.aclweb.org/anthology/W19-5366"]}
{"year":"2019","title":"Johns Hopkins University Submission for WMT News Translation Task","authors":["K Marchisio, YK Lal, P Koehn - Proceedings of the Fourth Conference on Machine …, 2019"],"snippet":"… sampled bitext (x2). ParaCrawl1 and Common Crawl2 are filtered similarly, and added to form the training set for the final models. We … Crawl. ParaCrawl and Common Crawl were combined into a single corpus before filtering …","url":["https://www.aclweb.org/anthology/W19-5329"]}
{"year":"2019","title":"Joint Training for Neural Machine Translation","authors":["Y Cheng"],"snippet":"Page 1. Springer Recognizing Theses Outstanding Ph.D. Research Yong Cheng Joint Neural Translation Training Machine for Page 2. Springer Theses Recognizing Outstanding Ph.D. Research Page 3. Aims and Scope The …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=KIOrDwAAQBAJ&oi=fnd&pg=PR5&dq=commoncrawl&ots=vy1Stpb4X-&sig=1d6kjXbtaE3McDjxvY7O9-JJQOk"]}
{"year":"2019","title":"Jointly Learning to Align and Translate with Transformer Models","authors":["S Garg, S Peitz, U Nallasamy, M Paulik - arXiv preprint arXiv:1909.02074, 2019","SGSPU Nallasamy, M Paulik"],"snippet":"… by Vilar et al. (2006). We use all available bilingual data (Europarl v7, Common Crawl corpus, News Commentary v13 and Rapid corpus of EU press releases) excluding the ParalCrawl corpus. We remove sentences longer …","url":["https://arxiv.org/pdf/1909.02074","https://www.researchgate.net/profile/Stephan_Peitz/publication/336996532_Jointly_Learning_to_Align_and_Translate_with_Transformer_Models/links/5ec41124458515626cb813b1/Jointly-Learning-to-Align-and-Translate-with-Transformer-Models.pdf"]}
{"year":"2019","title":"JParaCrawl: A Large Scale Web-Based English-Japanese Parallel Corpus","authors":["M Morishita, J Suzuki, M Nagata - arXiv preprint arXiv:1911.10668, 2019"],"snippet":"… To select the candidate domains, we first identified the language of all the Common Crawl text data by CLD26 and counted how much … Since the crawled data stored on Common Crawl may not contain the entire website or might …","url":["https://arxiv.org/pdf/1911.10668"]}
{"year":"2019","title":"KaWAT: A Word Analogy Task Dataset for Indonesian","authors":["K Kurniawan - arXiv preprint arXiv:1906.09912, 2019"],"snippet":"… We used fastText pretrained embeddings introduced in (Bojanowskietal.,2017) and (Grave et al., 2018), which have been trained on Indonesian Wikipedia and Indonesian Wikipedia plus Common Crawl data respectively. We …","url":["https://arxiv.org/pdf/1906.09912"]}
{"year":"2019","title":"Keyphrase Extraction from Scholarly Articles as Sequence Labeling using Contextualized Embeddings","authors":["D Sahrawat, D Mahata, M Kulkarni, H Zhang… - arXiv preprint arXiv …, 2019"],"snippet":"… and OpenAI GPT-2 (small, medium). As a baseline, we also use 300 dimensional fixed embeddings from Glove2, Word2Vec3, and FastText4 (common-crawl, wiki-news). We also compare the proposed architecture against …","url":["https://arxiv.org/pdf/1910.08840"]}
{"year":"2019","title":"KiloGrams: Very Large N-Grams for Malware Classification","authors":["E Raff, W Fleming, R Zak, H Anderson, B Finlayson… - arXiv preprint arXiv …, 2019"],"snippet":"… A ccuracy s = 1 s = ⌈n/4⌉ Figure 1: Balanced Accuracy results (y-axis) on the Public PDF dataset as we increase then-gram size (x-axis, log-scale), and alter the hashing stride s. Using a hashing-stride retains more …","url":["https://arxiv.org/pdf/1908.00200"]}
{"year":"2019","title":"KIT's Submission to the IWSLT 2019 Shared Task on Text Translation","authors":["F Schneider, A Waibel"],"snippet":"… We made use of all allowed data, which is broken down in table 1. The allowed parallel data from WMT consists of Commoncrawl, CzEng (which makes up the vast majority of the parallel training data), Europarl, news commentrary and paracrawl …","url":["https://zenodo.eu/record/3525496/files/IWSLT2019_paper_30.pdf"]}
{"year":"2019","title":"Knowledge empowered prominent aspect extraction from product reviews","authors":["Z Luo, S Huang, KQ Zhu - Information Processing & Management, 2019"],"snippet":"Skip to main content …","url":["https://www.sciencedirect.com/science/article/pii/S0306457318305193"]}
{"year":"2019","title":"Knowledge Graph-Driven Conversational Agents","authors":["J Bockhorst, D Conathan, G Fung"],"snippet":"… We use a CNN with max pooling and pretrained Glove embeddings trained on the Common Crawl 840B dataset [6] [7]. By applying our CNN classifier as a straightforward 1-of-k document classification task, we are able to achieve …","url":["https://kr2ml.github.io/2019/papers/KR2ML_2019_paper_42.pdf"]}
{"year":"2019","title":"Knowledge-based Conversational Search","authors":["S Vakulenko - arXiv preprint arXiv:1912.06859, 2019"],"snippet":"Page 1. arXiv:1912.06859v1 [cs.IR] 14 Dec 2019 Page 2. Page 3. Knowledge-based Conversational Search DISSERTATION submitted in partial fulfillment of the requirements for the degree of Doktorin der Technischen Wissenschaften by Svitlana Vakulenko, MSc …","url":["https://arxiv.org/pdf/1912.06859"]}
{"year":"2019","title":"Kyoto University participation to the WMT 2019 news shared task","authors":["F Cromieres, S Kurohashi - Proceedings of the Fourth Conference on Machine …, 2019"],"snippet":"… Page 2. 164 3 Data preprocessing 3.1 Data used For bilingual data, we used the provided corpora: europarl (≈ 1.7M sentence pairs), common crawl(≈ 620k sentence pairs) and newscommentary (≈ 255k sentence pairs). We did not use the paracrawl corpus …","url":["https://www.aclweb.org/anthology/W19-5312"]}
{"year":"2019","title":"Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation","authors":["D Loureiro, A Jorge - arXiv preprint arXiv:1906.10007, 2019"],"snippet":"… tokens in the sentence. We choose fastText (Bojanowski et al., 2017) embeddings (pretrained on CommonCrawl), which are biased towards morphology, and avoid Out-of-Vocabulary issues as explained in §2.1. We use fastText …","url":["https://arxiv.org/pdf/1906.10007"]}
{"year":"2019","title":"Language Models are Unsupervised Multitask Learners","authors":["A Radford, J Wu, R Child, D Luan, D Amodei…"],"snippet":"… A promising source of diverse and nearly unlimited text is web scrapes such as Common Crawl … Trinh & Le (2018) used Common Crawl in their work on commonsense reasoning but noted a large amount of documents “whose content are mostly unintelligible” …","url":["https://www.techbooky.com/wp-content/uploads/2019/02/Better-Language-Models-and-Their-Implications.pdf"]}
{"year":"2019","title":"Language Models with Pre-Trained (GloVe) Word Embeddings","authors":["L Rokach, B Shapira, V Makarenkov"],"snippet":"… Despite the huge size of the Common Crawl corpus, some words may not exist with the embeddings, so we set these words to random vectors, and use the same embeddings consistently if we encounter the same unseen word again in the text …","url":["https://deepai.org/publication/language-models-with-pre-trained-glove-word-embeddings"]}
{"year":"2019","title":"Large Memory Layers with Product Keys","authors":["G Lample, A Sablayrolles, MA Ranzato, L Denoyer… - arXiv preprint arXiv …, 2019","MA Ranzato, L Denoyer, H Jégou"],"snippet":"… The train perplexity is then equal to 14.8 and keeps improving while the validation perplexity deteriorates. We therefore evaluate the benefit of our approach on a corpus that is 30 times larger and extracted from the public Common Crawl …","url":["https://arxiv.org/pdf/1907.05242","https://pdfs.semanticscholar.org/3a54/100803474df3b98e54a1693010d12c9718b5.pdf"]}
{"year":"2019","title":"Large Scale Linguistic Processing of Tweets to Understand Social Interactions among Speakers of Less Resourced Languages: The Basque Case","authors":["J Fernandez de Landa, R Agerri, I Alegria - Information, 2019"],"snippet":"… resourced languages such as Basque. However, FastText provides pre-trained models for many languages, including Basque [33] by using the common crawl data (http://commoncrawl.org). The Basque model they distribute …","url":["https://www.mdpi.com/2078-2489/10/6/212/pdf"]}
{"year":"2019","title":"Last-Mile TLS Interception: Analysis and Observation of the Non-Public HTTPS Ecosystem","authors":["XC de Carnavalet - 2019"],"snippet":"Page 1. Last-Mile TLS Interception: Analysis and Observation of the Non-Public HTTPS Ecosystem Xavier de Carné de Carnavalet A thesis in The Concordia Institute for Information Systems Engineering Presented …","url":["http://users.encs.concordia.ca/~mmannan/student-resources/Thesis-PhD-Carnavalet-2019.pdf"]}
{"year":"2019","title":"Latent Question Interpretation Through Parameter Adaptation","authors":["T Parshakova, F Rameau, A Serdega, I Kweon, DS Kim - IEEE/ACM Transactions on …, 2019"],"snippet":"… A. Implementation Details For the sake of reproducibility, we provide the technical details related to the implementation of our approach. First of all, the initial word embeddings are initialized with GloVe embeddings, which …","url":["https://www.researchgate.net/profile/Francois_Rameau/publication/334633405_Latent_Question_Interpretation_Through_Parameter_Adaptation/links/5d37e05ca6fdcc370a5a3a43/Latent-Question-Interpretation-Through-Parameter-Adaptation.pdf"]}
{"year":"2019","title":"Laying the foundations for benchmarking open data automatically: a method for surveying data portals from the whole web","authors":["A Sheffer Correa, F Soares Correa Da Silva - 20th Annual International Conference …, 2019"],"snippet":"… KEYWORDS Open Data, Common Crawl, CKAN, Socrata, ArcGIS, OpenDataSoft … Common Crawl conducts crawls once a month and persists all the content in Web Archive (WARC) file format to allow multibillion web page archives with hundreds of terabytes in size …","url":["https://dl.acm.org/citation.cfm?id=3325257"]}
{"year":"2019","title":"LCEval: Learned Composite Metric for Caption Evaluation","authors":["N Sharif, L White, M Bennamoun, W Liu, SAA Shah"],"snippet":"… Table 1: The details of pre-trained embeddings used in our experiments Name Source Dimensions Corpus Corpus Size Vocabulary Size GloVE 840B 300d [40] 300 Common Crawl 8.40E+11 2.20E+06 Word2vec Google 300d [34] …","url":["https://www.researchgate.net/profile/Naeha_Sharif2/publication/334760575_LCEval_Learned_Composite_Metric_for_Caption_Evaluation/links/5d429677a6fdcc370a715269/LCEval-Learned-Composite-Metric-for-Caption-Evaluation.pdf"]}
{"year":"2019","title":"Learning as the Unsupervised Alignment of Conceptual Systems","authors":["BD Roads, BC Love - arXiv preprint arXiv:1906.09012, 2019"],"snippet":"… We found that alignment correlations positively correlated with mapping accuracy across a variety of scenarios (Figure 3A-C). The three conceptual systems were derived from a Common Crawl text corpus (Pennington et …","url":["https://arxiv.org/pdf/1906.09012"]}
{"year":"2019","title":"Learning from Personal Longitudinal Dialog Data","authors":["C Welch, V Pérez-Rosas, JK Kummerfeld, R Mihalcea…"],"snippet":"… Message Embeddings: We also obtain word vector representations for each message using the GloVe Common Crawl pre-trained model.19 We chose this word embedding over other off-theshelf options because the Common …","url":["https://sentic.net/personal-longitudinal-dialog-data.pdf"]}
{"year":"2019","title":"Learning multilingual topics through aspect extraction from monolingual texts","authors":["J Huber, M Spiliopoulou - Proceedings of the Fifth International Workshop on …, 2019"],"snippet":"… Xu et al., 2018). It was trained on the CommonCrawl corpus, a general-purpose text corpus that includes text from several billion web pages; the GloVe embeddings were trained on 840 billion tokens. The GloVe set includes …","url":["http://www.aclweb.org/anthology/W19-0313"]}
{"year":"2019","title":"Learning Outside the Box: Discourse-level Features Improve Metaphor Identification","authors":["J Mu, H Yannakoudakis, E Shutova - arXiv preprint arXiv:1904.02246, 2019"],"snippet":"… To learn representations, we use several widelyused embedding methods:4 GloVe We use 300-dimensional pre-trained GloVe embeddings (Pennington et al., 2014) trained on the Common Crawl corpus as representations of a lemma and its arguments …","url":["https://arxiv.org/pdf/1904.02246"]}
{"year":"2019","title":"Learning Relational Fractals for Deep Knowledge Graph Embedding in Online Social Networks","authors":["J Zhang, L Tan, X Tao, D Wang, JJC Ying, X Wang - International Conference on Web …, 2019"],"snippet":"… Our twitter dataset was live streamed from a twitter API account and contains a maximum of 1675882 nodes and 160799842 links. The Google dataset was obtained from the repositories of common crawl and was sentilyzed from the stripped down WET file contents …","url":["https://link.springer.com/chapter/10.1007/978-3-030-34223-4_42"]}
{"year":"2019","title":"Learning to Generate Personalized Product Descriptions","authors":["G Elad, I Guy, K Radinsky, S Novgorodov, B Kimelfeld - 2019"],"snippet":"… For the title representation, we used fastText word embeddings2 pre-trained on Common Crawl and Wikipedia [25, 33], weighted based on each word's TF-IDF score [4].3 In addition, we included as features the participant's demo …","url":["http://www.kiraradinsky.com/files/Learning_to_Generate_Personalized_Product_Descriptions.pdf"]}
{"year":"2019","title":"Learning to Speak and Act in a Fantasy Text Adventure Game","authors":["J Urbanek, A Fan, S Karamcheti, S Jain, S Humeau… - arXiv preprint arXiv …, 2019","JUA Fan, SKSJS Humeau, EDT Rocktäschel…"],"snippet":"Page 1. Learning to Speak and Act in a Fantasy Text Adventure Game Jack Urbanek1 Angela Fan1,2 Siddharth Karamcheti1 Saachi Jain1 Samuel Humeau1 Emily Dinan1 Tim Rocktäschel1,3 Douwe Kiela1 Arthur Szlam1 Jason …","url":["https://arxiv.org/pdf/1903.03094","https://research.fb.com/wp-content/uploads/2019/11/Learning-to-Speak-and-Act-in-a-Fantasy-Text-Adventure-Game.pdf"]}
{"year":"2019","title":"Learning Word Ratings for Empathy and Distress from Document-Level User Responses","authors":["J Sedoc, S Buechel, Y Nachmany, A Buffone, L Ungar - arXiv preprint arXiv …, 2019"],"snippet":"… (2013) using 10-fold crossvalidation. For word embeddings we used off-the-shelf Fasttext subword embeddings (Mikolov et al., 2018).4 The embeddings are trained with subword information on Common Crawl (600B tokens) …","url":["https://arxiv.org/pdf/1912.01079"]}
{"year":"2019","title":"Leveraging Distributional and Relational Semantics for Knowledge Extraction from Textual Corpora","authors":["G ROSSIELLO, G SEMERARO, M DI CIANO - 2019"],"snippet":"Page 1. Page 2 …","url":["https://www.researchgate.net/profile/Gaetano_Rossiello/publication/333448156_Leveraging_Distributional_and_Relational_Semantics_for_Knowledge_Extraction_from_Textual_Corpora/links/5cee4fcca6fdcc18c8e9913b/Leveraging-Distributional-and-Relational-Semantics-for-Knowledge-Extraction-from-Textual-Corpora.pdf"]}
{"year":"2019","title":"Leveraging End-to-End Speech Recognition with Neural Architecture Search","authors":["A Baruwa, M Abisiga, I Gbadegesin, A Fakunle - arXiv preprint arXiv:1912.05946, 2019"],"snippet":"… We train a 3-gram, 5-gram and a 7-gram language model on common crawl 1. The relative performances are summarised in tables 1 and 2. Decoding is done by beam-searching for the output y that maximizes φ(c) given by …","url":["https://arxiv.org/pdf/1912.05946"]}
{"year":"2019","title":"Leveraging Hierarchical Representations for Preserving Privacy and Utility in Text","authors":["O Feyisetan, T Diethe, T Drake - arXiv preprint arXiv:1910.08917, 2019"],"snippet":"Page 1. Leveraging Hierarchical Representations for Preserving Privacy and Utility in Text Oluwaseyi Feyisetan Amazon sey@amazon.com Tom Diethe Amazon tdiethe@amazon.co.uk Thomas Drake Amazon draket@amazon.com …","url":["https://arxiv.org/pdf/1910.08917"]}
{"year":"2019","title":"Leveraging Pretrained Image Classifiers for Language-Based Segmentation","authors":["D Golub, R Martín-Martín, A El-Kishky, S Savarese - arXiv preprint arXiv:1911.00830, 2019"],"snippet":"… With Word2Vec we first embed the target labels l and the labels in the set of possible proxy labels in a shared vector space using 300-dimensional GloVe embeddings [29] trained on the Common Crawl 840B word corpus. For labels that contains multiple words …","url":["https://arxiv.org/pdf/1911.00830"]}
{"year":"2019","title":"Leveraging Unpaired Out-of-Domain Data for Image Captioning","authors":["X Chen, M Zhang, Z Wang, L Zuo, B Li, Y Yang - Pattern Recognition Letters, 2018"],"snippet":"Skip to main content …","url":["https://www.sciencedirect.com/science/article/pii/S0167865518309358"]}
{"year":"2019","title":"Leveraging Web Semantic Knowledge in Word Representation Learning","authors":["H Liu, L Fang, JG Lou, Z Li - 2019"],"snippet":"… We extract a large collection of semantic lists from the Common Crawl data7 using the patterns defined in Table 1 and filter out entries that do not exist in the vocabulary of the training data … 6http://dumps.wikimedia.org/enwiki/ 7http://commoncrawl.org/ Page 5 …","url":["https://www.aaai.org/Papers/AAAI/2019/AAAI-LiuHaoyan.142.pdf"]}
{"year":"2019","title":"Limsi-multisem at the ijcai semdeep-5 wic challenge: Context representations for word usage similarity estimation","authors":["AG Soler, M Apidianaki, A Allauzen - Proceedings of the 5th Workshop on Semantic …, 2019"],"snippet":"… Di- mensionality reduction is applied to a weighted average of the vectors of words in a sentence. Weighting is based on word frequency in Common Crawl. We use SIF in combination with 300- d GloVe vectors trained …","url":["https://www.aclweb.org/anthology/W19-5802"]}
{"year":"2019","title":"Lingua Custodia at WMT'19: Attempts to Control Terminology","authors":["F Burlot - arXiv preprint arXiv:1907.04618, 2019"],"snippet":"… to the decoder. Page 2. 2 Baseline The training parallel data provided for the task consisted of nearly 10M sentences, including Europarl (Koehn, 2005), Common-crawl, Newscommentary and Bicleaner07. The former was …","url":["https://arxiv.org/pdf/1907.04618"]}
{"year":"2019","title":"Linked Open Data Validity--A Technical Report from ISWS 2018","authors":["TA Ghor, E Agrawal, M Alam, O Alqawasmeh… - arXiv preprint arXiv …, 2019"],"snippet":"Page 1. Linked Open Data Validity A Technical Report from ISWS 2018 April 1, 2019 Bertinoro, Italy arXiv:1903.12554v1 [cs.DB] 26 Mar 2019 Page 2. Authors Main Editors Mehwish Alam, Semantic Technology Lab, ISTC-CNR …","url":["https://arxiv.org/pdf/1903.12554"]}
{"year":"2019","title":"Linking artificial and human neural representations of language","authors":["J Gauthier, R Levy - arXiv preprint arXiv:1910.01244, 2019"],"snippet":"… contrasts between the 384 sentences tested. 9We use publicly available GloVe vectors computed on Common Crawl, available in the spaCy toolkit as en vectors web lg. Page 6. 3 Results We first present the performance of …","url":["https://arxiv.org/pdf/1910.01244"]}
{"year":"2019","title":"LINSPECTOR: Multilingual Probing Tasks for Word Representations","authors":["GG Şahin, C Vania, I Kuznetsov, I Gurevych - arXiv preprint arXiv:1903.09442, 2019"],"snippet":"Page 1. LINSPECTOR Multilingual Probing Tasks for Word Representations Gözde Gül Sahin∗ UKP Lab / TU Darmstadt Clara Vania∗∗ ILCC / University of Edinburgh Ilia Kuznetsov UKP Lab / TU Darmstadt Iryna Gurevych UKP Lab / TU Darmstadt …","url":["https://arxiv.org/pdf/1903.09442"]}
{"year":"2019","title":"LIUM's Contributions to the WMT2019 News Translation Task: Data and Systems for German-French Language Pairs","authors":["F Bougares, J Wottawa, A Baillot, L Barrault, A Bardet - … 2: Shared Task Papers, Day 1 …, 2019"],"snippet":"… As it can be seen from tables 1 and 2, the effect of the cleaning step is more pronounced for the noisy parallel corpora (ie ParaCrawl and Common Crawl) … Page 3. 131 #lines #token FR #token DE europarl-v7 1.7M 45.9M 40.9 …","url":["https://www.aclweb.org/anthology/W19-5307"]}
{"year":"2019","title":"Local bow-tie structure of the web","authors":["Y Fujita, Y Kichikawa, Y Fujiwara, W Souma, H Iyetomi - Applied Network Science, 2019"],"snippet":"… This fact means that the absence of self-similarity between page level and host/domain levels. Meusel et al. (2014, 2015) investigated the publicly accessible crawl of the web gathered by the Common Crawl Foundation in 2012 (CC12) (Meusel et al. 2014; 2015) …","url":["https://link.springer.com/article/10.1007/s41109-019-0127-2"]}
{"year":"2019","title":"Logical Layout Analysis using Deep Learning","authors":["A Zulfiqar, A Ul-Hasan, F Shafait"],"snippet":"… of the text zones. GloVE provides 300 dimensional vectors, one vector for each word. We have used the one trained on common crawl having 840 billion tokens and vectors for a total of 2.2 million words. Since we also want …","url":["https://tukl.seecs.nust.edu.pk/members/projects/conference/Logical-Layout-Analysis-using-Deep-Learning.pdf"]}
{"year":"2019","title":"Longitudinal Analysis of Misuse of Bitcoin⋆","authors":["K Eldefrawy, A Gehani, A Matton"],"snippet":"… its labels). Seed data was used from previously published onion data sets, references to onions in a large collection of DNS resolver logs, and an open repository of (non-onion) web crawl data, called the Common Crawl. The …","url":["http://www.csl.sri.com/users/gehani/papers/ACNS-2019.Bitcoin_Study.pdf"]}
{"year":"2019","title":"Look Who's Talking: Inferring Speaker Attributes from Personal Longitudinal Dialog","authors":["C Welch, V Pérez-Rosas, JK Kummerfeld, R Mihalcea - arXiv preprint arXiv …, 2019"],"snippet":"… The word embedding inputs to the context encoder are 300 dimensional. 8 Features Word Embeddings: We obtain word vector representations for each message using the GloVe Common Crawl pre-trained model [12]. We …","url":["https://arxiv.org/pdf/1904.11610"]}
{"year":"2019","title":"Low Resource Sequence Tagging with Weak Labels","authors":["E Simpson, J Pfeiffer, I Gurevych"],"snippet":"… For FAMULUS, we use 300-dimensional German fastText embeddings (Grave et al. 2018), and for NER and PICO we use 300-dimensional English GloVe 3 embeddings trained on 840 billion tokens from Common Crawl. To …","url":["https://public.ukp.informatik.tu-darmstadt.de/UKP_Webpage/publications/2020/2020_AAAI_SE_LowResourceSequence.pdf"]}
{"year":"2019","title":"Low Supervision, Low Corpus size, Low Similarity! Challenges in cross-lingual alignment of word embeddings: An exploration of the limitations of cross-lingual word …","authors":["A Dyer - 2019"],"snippet":"Page 1. Low Supervision, Low Corpus size, Low Similarity! Challenges in cross-lingual alignment of word embeddings An exploration of the limitations of cross-lingual word embedding alignment in truly low resource scenarios Andrew Dyer …","url":["http://www.diva-portal.org/smash/get/diva2:1365879/FULLTEXT01.pdf"]}
{"year":"2019","title":"LSTM for Dialogue Breakdown Detection: Exploration of Different Model Types and Word Embeddings","authors":["M Hendriksen, A Leeuwenberg, MF Moens"],"snippet":"… The words are uncased. GloVe Common Crawl … The results presented in the Table 2, allow to conclude that GloVe Common Crawl demonstrate the best performance, the GloVe Twitter being the second best, the word2vec Google News is the worst. Page 9 …","url":["http://workshop.colips.org/wochat/@iwsds2019/documents/dbdc4-mariya-hendriksen-etal.pdf"]}
{"year":"2019","title":"LTL-UDE at SemEval-2019 Task 6: BERT and Two-Vote Classification for Categorizing Offensiveness","authors":["P Aggarwal, T Horsmann, M Wojatzki, T Zesch - … of the 13th International Workshop on …, 2019"],"snippet":"… word representations. The resulting posting vector is re-scaled into the range zero to one. We use the pre-trained embeddings provided by Mikolov et al. (2018), which are trained on the common crawl corpus. Classifiers We …","url":["https://www.aclweb.org/anthology/S19-2121"]}
{"year":"2019","title":"ltl. uni-due at SemEval-2019 Task 5: Simple but Effective Lexico-Semantic Features for Detecting Hate Speech in Twitter","authors":["H Zhang, M Wojatzki, T Horsmann, T Zesch - … of the 13th International Workshop on …, 2019"],"snippet":"… of LSTMs and CNNs (LSTM + CNN). We initialize all setups with the 300-dimensional word embeddings provided by Mikolov et al. (2018), which were trained on the common crawl corpus. Furthermore, in all setups, we use …","url":["https://www.aclweb.org/anthology/S19-2078"]}
{"year":"2019","title":"Machine Reading of Clinical Notes for Automated ICD Coding","authors":["M Morisio, S Malacrino"],"snippet":"Page 1. Master degree course in Computer Engineering Master Degree Thesis Machine Reading of Clinical Notes for Automated ICD Coding Supervisor Prof. Maurizio Morisio Candidate Stefano Malacrin`o Internship tutors …","url":["https://webthesis.biblio.polito.it/10958/1/tesi.pdf"]}
{"year":"2019","title":"Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness","authors":["A Bérard, I Calapodescu, M Dymetman, C Roux… - arXiv preprint arXiv …, 2019"],"snippet":"… data, we built a new training corpus named UGC (User Generated Content), closer to our domain, by combining: Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl11 and Gourmet12 (See Table 3) …","url":["https://arxiv.org/pdf/1910.14589"]}
{"year":"2019","title":"Mapping languages and demographics with georeferenced corpora","authors":["J Dunn, B Adams - 2019"],"snippet":"… To answer this question, we collect and analyze two large global-scale datasets: web-crawled data from the Common Crawl (16.65 billion words) and social media data from Twitter (4.14 billion words). This paper evaluates demographic-type informa …","url":["https://ir.canterbury.ac.nz/bitstream/handle/10092/17132/GeoComputation_19.pdf?sequence=2"]}
{"year":"2019","title":"Massive vs. Curated Word Embeddings for Low-Resourced Languages. The Case of Yor\\ub\\'a and Twi","authors":["JO Alabi, K Amponsah-Kaakyire, DI Adelani… - arXiv preprint arXiv …, 2019"],"snippet":"… The resource par excellence is Wikipedia2, an online encyclopedia currently available in 307 languages3. Other initiatives such as Common Crawl4 or the Jehovahs Witnesses site5 are also repositories for multilingual …","url":["https://arxiv.org/pdf/1912.02481"]}
{"year":"2019","title":"Massively multilingual transfer for NER","authors":["A Rahimi, Y Li, T Cohn - Proceedings of the 57th Conference of the Association …, 2019"],"snippet":"Page 1. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164 Florence, Italy, July 28 - August 2, 2019. c 2019 Association for Computational Linguistics 151 Massively Multilingual Transfer for NER …","url":["https://www.aclweb.org/anthology/P19-1015"]}
{"year":"2019","title":"MASTER UNIVERSITARIO EN INGENIERÍA DE TELECOMUNICACION","authors":["DB SANCHEZ - 2019"],"snippet":"Page 1. M´ASTER UNIVERSITARIO EN INGENIERÍA DE TELECOMUNICACI´ON TRABAJO FIN DE M´ASTER DESING AND DEVELOPMENT OF A HATE SPEECH DETECTOR IN SOCIAL NETWORKS BASED ON DEEP LEARNING TECHNOLOGIES …","url":["http://oa.upm.es/55618/1/TESIS_MASTER_DIEGO_BENITO_SANCHEZ_2019.pdf"]}
{"year":"2019","title":"Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories","authors":["K Chaloner, A Maldonado - Proceedings of the First Workshop on Gender Bias in …, 2019"],"snippet":"… WEAT's authors applied these tests to the publicly-available GloVe embeddings trained on the English-language “Common Crawl” corpus (Pennington et al., 2014) as well as the Skip-Gram (word2vec) embeddings …","url":["https://www.aclweb.org/anthology/W19-3804"]}
{"year":"2019","title":"Medical Word Embeddings for Spanish: Development and Evaluation","authors":["F Soares, M Villegas, A Gonzalez-Agirre, M Krallinger… - Proceedings of the 2nd …, 2019"],"snippet":"… makes available Word2Vec models pre-trained on about 100 billion words from Google News corpus in English1. Regarding other languages, on FastText website2 one can download pre-trained embeddings for 157 lan …","url":["https://www.aclweb.org/anthology/W19-1916"]}
{"year":"2019","title":"Meemi: Finding the Middle Ground in Cross-lingual Word Embeddings","authors":["Y Doval, J Camacho-Collados, L Espinosa-Anke… - arXiv preprint arXiv …, 2019"],"snippet":"… 10 Page 11. the WaCky project [23], containing 2 and 0.8 billion words, respectively.6 For Finnish and Russian, we use their corresponding Common Crawl monolingual corpora from the Machine Translation of News Shared Task 20167, composed of …","url":["https://arxiv.org/pdf/1910.07221"]}
{"year":"2019","title":"Membership Inference Attacks on Sequence-to-Sequence Models","authors":["S Hisamoto, M Post, K Duh - arXiv preprint arXiv:1904.05506, 2019"],"snippet":"… For example, e (d) i with d = l1 and i = 1 might refer to the first sentence in the Europarl subcorpus, while e (d) i with d = l2 and i = 1 might refer to the first sentence in the CommonCrawl subcorpus … CommonCrawl 5,000 5,000 2,389,123 2,379,123 N/A …","url":["https://arxiv.org/pdf/1904.05506"]}
{"year":"2019","title":"Metaphor Interpretation Using Word Embeddings","authors":["K Bar, N Dershowitz, L Dankin"],"snippet":"… relatively large corpus. Specifically, we use DepCC,1 a dependency-parsed “web-scale corpus” based on CommonCrawl.2 There are 365 million documents in the corpus, comprising about 252B tokens. Among other preprocessing …","url":["https://pdfs.semanticscholar.org/2033/a3f7b8b53ea277a811ac450139422793b08b.pdf"]}
{"year":"2019","title":"Methods and apparatus for detection of malicious documents using machine learning","authors":["JD Saxe, R HARANG - US Patent App. 16/257,749, 2019"],"snippet":"… decision tree, etc.). The memory 120 includes one or more datasets 112 (eg, a VirusTotal dataset and/or a Common Crawl dataset, as described in further detail below) and one or more training models 124. The malware detection …","url":["https://patentimages.storage.googleapis.com/fa/f8/d7/5843fb31e01d95/US20190236273A1.pdf"]}
{"year":"2019","title":"Microsoft Research Asia's Systems for WMT19","authors":["Y Xia, X Tan, F Tian, F Gao, W Chen, Y Fan, L Gong…"],"snippet":"… Dataset We concatenate “Europarl v9”, “News Commentary v14”, “Common Crawl corpus” and “Document-split Rapid corpus” as the ba- sic bilingual … We merge the “commoncrawl”, “europarl-v7” and part of “de-fr.bicleaner07” …","url":["http://www.statmt.org/wmt19/pdf/WMT0048.pdf"]}
{"year":"2019","title":"MIDAS: A Dialog Act Annotation Scheme for Open Domain Human Machine Spoken Conversations","authors":["D Yu, Z Yu - arXiv preprint arXiv:1908.10023, 2019"],"snippet":"… An example can be seen in the last USER2 utterance in Table 1. Word em- beddings are pre-trained with fastText (Mikolov et al., 2018) using Common Crawl. We evaluate the segmentation model on human labeled 2K human utterances of collected data …","url":["https://arxiv.org/pdf/1908.10023"]}
{"year":"2019","title":"Mining Discourse Markers for Unsupervised Sentence Representation Learning","authors":["D Sileo, T Van-De-Cruys, C Pradel, P Muller - arXiv preprint arXiv:1903.11850, 2019"],"snippet":"… We use sentences from the Depcc corpus (Panchenko et al., 2017), which consists of En- glish texts harvested from commoncrawl web data … Word embeddings are fixed GloVe embeddings with 300 dimensions, trained …","url":["https://arxiv.org/pdf/1903.11850"]}
{"year":"2019","title":"Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models","authors":["T He, J Liu, K Cho, M Ott, B Liu, J Glass, F Peng - arXiv preprint arXiv:1910.07117, 2019"],"snippet":"… For pre-training, we use the large-scale CCNEWS data (Bakhtin et al., 2019) which is a de- duplicated subset of the English portion of the CommonCrawl news data-set1. The dataset contains news articles published worldwide …","url":["https://arxiv.org/pdf/1910.07117"]}
{"year":"2019","title":"MLT-DFKI at CLEF eHealth 2019: Multi-label Classification of ICD-10 Codes with BERT","authors":["S Amin, G Neumann, K Dunfield, A Vechkaeva… - CLEF (Working Notes), 2019"],"snippet":"… have stronger linguistic signals to classify the classes where German models make mistakes [1]. The baseline proved to be a strong one, with the highest precision of all and outperforming HAN and CNN models, for both German …","url":["https://www.researchgate.net/profile/Saadullah_Amin2/publication/335681972_MLT-DFKI_at_CLEF_eHealth_2019_Multi-label_Classification_of_ICD-10_Codes_with_BERT/links/5d742a00299bf1cb809043cd/MLT-DFKI-at-CLEF-eHealth-2019-Multi-label-Classification-of-ICD-10-Codes-with-BERT.pdf"]}
{"year":"2019","title":"Mono-and Cross-lingual Semantic Word Similarity for Urdu Language","authors":["G Fatima - 2019"],"snippet":"Page 1. I Monoand Cross-lingual Semantic Word Similarity for Urdu Language By Ghazeefa Fatima CIIT/FA17-RCS-016/LHR MS Thesis In Computer Science COMSATS University Islamabad Lahore Campus Page …","url":["http://dspace.cuilahore.edu.pk/xmlui/bitstream/handle/123456789/1571/Thesis.pdf?sequence=1"]}
{"year":"2019","title":"MoRTy: Unsupervised Learning of Task-specialized Word Embeddings by Autoencoding","authors":["N Rethmeier, B Plank - Proceedings of the 4th Workshop on Representation …, 2019"],"snippet":"… Hence, we demonstrate the method's application for single-task, multi-task, small, medium and web-scale (common crawl) corpus-size settings (Section 4). Learning to scale-up by pretraining on more (un-)labeled data is both: (a) not always possible in low-resource …","url":["https://www.aclweb.org/anthology/W19-4307"]}
{"year":"2019","title":"Multi-class Document Classification Using Improved Word Embeddings","authors":["BA Rabut, AC Fajardo, RP Medina - Proceedings of the 2nd International Conference …, 2019"],"snippet":"… ACM ISBN 978-1-4503-7290-9/19/10…$15.00 https://doi.org/10.1145/3366650.3366661 42 Page 2. Common crawl)[7]. The pre-trained word embedding vectors serve as input in the classification algorithm for evaluation and prediction …","url":["https://dl.acm.org/citation.cfm?id=3366661"]}
{"year":"2019","title":"Multi-domain Dialogue State Tracking as Dynamic Knowledge Graph Enhanced Question Answering","authors":["L Zhou, K Small - arXiv preprint arXiv:1911.06192, 2019"],"snippet":"… For experiments with GloVe embeddings, we use GloVe embeddings pre-trained on Common Crawl dataset.3 The dimension of GloVe embeddings is 300, and the dimension of character-level embeddings is 100, such that Dw = 400 …","url":["https://arxiv.org/pdf/1911.06192"]}
{"year":"2019","title":"Multi-Granular Text Encoding for Self-Explaining Categorization","authors":["Z Wang, Y Zhang, M Yu, W Zhang, L Pan, L Song, K Xu… - arXiv preprint arXiv …, 2019"],"snippet":"… for each set. Hyperparameters We use the 300-dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus (Pennington et al., 2014), and set the hidden size as 100 for node embeddings. We apply dropout …","url":["https://arxiv.org/pdf/1907.08532"]}
{"year":"2019","title":"Multi-Hop Paragraph Retrieval for Open-Domain Question Answering","authors":["Y Feldman, R El-Yaniv - arXiv preprint arXiv:1906.06606, 2019"],"snippet":"Page 1. Multi-Hop Paragraph Retrieval for Open-Domain Question Answering Yair Feldman and Ran El-Yaniv Department of Computer Science Technion – Israel Institute of Technology Haifa, Israel {yairf11, rani}@cs.technion.ac.il Abstract …","url":["https://arxiv.org/pdf/1906.06606"]}
{"year":"2019","title":"Multi-Resolution Models for Learning Multilevel Abstract Representation with Application to Information Retrieval","authors":["T Cakaloglu - 2019"],"snippet":"Page 1. MULTI-RESOLUTION MODELS FOR LEARNING MULTILEVEL ABSTRACT REPRESENTATION WITH APPLICATION TO INFORMATION RETRIEVAL A Dissertation Submitted to the Graduate School University of Arkansas at Little Rock …","url":["http://search.proquest.com/openview/4bce4201a6d742c4c771e08b17dec0cb/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2019","title":"Multi-Team: A Multi-attention, Multi-decoder Approach to Morphological Analysis.","authors":["A Ustün, R van der Goot, G Bouma, G van Noord"],"snippet":"… 2018). For FastText, two sets of pre-trained embeddings are available: one is trained only on Wikipedia (Bojanowski et al., 2017), whereas the newer versions are also trained on CommonCrawl (Grave et al., 2018). Whenever …","url":["http://www.robvandergoot.com/doc/sigmorphon2019.pdf"]}
{"year":"2019","title":"Multilingual Culture-Independent Word Analogy Datasets","authors":["M Ulčar, M Robnik-Šikonja - arXiv preprint arXiv:1911.10038, 2019"],"snippet":"… language is shown in the Table 6. Table 6: Percentage of constructed analogy pairs covered by the first 200,000 word vectors from common crawl fastText embeddings. Language Coverage (%) Croatian 81.67 English 97.05 …","url":["https://arxiv.org/pdf/1911.10038"]}
{"year":"2019","title":"Multilingual Fake News Detection with Satire","authors":["G Guibon, L Ermakova, H Seffih, A Firsov…"],"snippet":"… Detection of Deception. Non-verbal communication (2014), https://nvc.uvt.nl/pdf/7.pdf 6. Bevendorff, J., Stein, B., Hagen, M., Potthast, M.: Elastic chatnoir: Search engine for the clueweb and the common crawl. In: Pasi, G., Piwowarski …","url":["https://www.researchgate.net/profile/Guillaume_Le_Noe-Bienvenu/publication/332803834_Multilingual_Fake_News_Detection_with_Satire_on_Vaccination_Topic/links/5d24917a458515c11c1f8724/Multilingual-Fake-News-Detection-with-Satire-on-Vaccination-Topic.pdf"]}
{"year":"2019","title":"Multilingual is not enough: BERT for Finnish","authors":["A Virtanen, J Kanerva, R Ilo, J Luoma, J Luotolahti… - arXiv preprint arXiv …, 2019"],"snippet":"… Second, we selected texts from the Common Crawl project6 by running aa map-reduce language detection job on the plain text material from Common Crawl. These sources were supplemented with plain text extracted …","url":["https://arxiv.org/pdf/1912.07076"]}
{"year":"2019","title":"Multilingual Sentence-Level Bias Detection in Wikipedia","authors":["D Aleksandrova, F Lareau, PA Ménard"],"snippet":"… Same BOW n-gram size and BOW size and value type as SGD. 5Available for 157 languages, pretrained on Common Crawl and Wikipedia (Grave et al., 2018) https:// fasttext.cc/docs/en/crawl-vectors.html 6Version 0.21.2 of the sklearn toolkit …","url":["https://www.researchgate.net/profile/Desislava_Aleksandrova/publication/334612399_Multilingual_Sentence-Level_Bias_Detection_in_Wikipedia/links/5d5bd0c392851c37636bfdf2/Multilingual-Sentence-Level-Bias-Detection-in-Wikipedia.pdf"]}
{"year":"2019","title":"Multimodal deep networks for text and image-based document classification","authors":["N Audebert, C Herold, K Slimani, C Vidal - APIA"],"snippet":"… For both methods, we use the SpaCy small English model [33] to perform the tokenization and punctuation removal. Individual word embeddings are then inferred using FastText [29] pretrained on the Common Crawl dataset …","url":["https://www.irit.fr/pfia2019/wp-content/uploads/2019/07/Actes_CH_PFIA2019.pdf#page=14"]}
{"year":"2019","title":"Multimodal Machine Translation with Embedding Prediction","authors":["T Hirasawa, H Yamagishi, Y Matsumura, M Komachi - arXiv preprint arXiv …, 2019"],"snippet":"… model. “+ pretrained” models are initialized with pretrained embeddings. 2018). These word embeddings are trained on Wikipedia and Common Crawl using the CBOW algorithm, and the dimension is 300. The embedding …","url":["https://arxiv.org/pdf/1904.00639"]}
{"year":"2019","title":"Multimodal Sentiment Analysis Using Deep Learning","authors":["R Sharma, N Le Tan, F Sadat - 2018 17th IEEE International Conference on Machine …, 2018"],"snippet":"… For the CNN model we used pre-trained word embeddings (GloVe 840B.300d). This is a 300-dimensional word embedding trained on 840 billion tokens from the common crawl dataset. The maximum sequence length is 200 …","url":["https://ieeexplore.ieee.org/abstract/document/8614265/"]}
{"year":"2019","title":"Named entity recognition for Polish","authors":["M Marcińczuk, A Wawer - Poznan Studies in Contemporary Linguistics, 2019"],"snippet":"AbstractIn this article we discuss the current state-of-the-art for named entity recognition for Polish. We present publicly available resources and open-source tools for named entity recognition. The overview includes various …","url":["https://www.degruyter.com/view/j/psicl.2019.55.issue-2/psicl-2019-0010/psicl-2019-0010.xml"]}
{"year":"2019","title":"Named Entity Recognition for Social Media Text","authors":["Y Zhang - 2019"],"snippet":"… We use two different pre-trained word embeddings based on Common Crawl data, which contains 840 billion tokens and 2.2 million vocabulary and Twitter data which contains 2 billion tweets, 27 billion tokens, and 1.2 million vocabulary …","url":["https://uu.diva-portal.org/smash/get/diva2:1366031/FULLTEXT01.pdf"]}
{"year":"2019","title":"Named Entity Recognition Using Gazetteer of Hierarchical Entities","authors":["M Štravs, J Zupančič - … Conference on Industrial, Engineering and Other …, 2019"],"snippet":"… To summarize, the proposed entity recognition method was tested using two languages (Slovenian and English), six different distance measures, and two different vector embeddings from Wikipedia (Wiki WV) and Common Crawl (CC WV) …","url":["https://link.springer.com/chapter/10.1007/978-3-030-22999-3_65"]}
{"year":"2019","title":"Named-entity recognition in Czech historical texts: Using a CNN-BiLSTM neural network model","authors":["H Hubková - 2019"],"snippet":"… We also tried to work with published pretrained word embeddings of contemporary Czech words provided by fastText6. These were trained on more than 178 millions of tokens from Wikipedia and 13 billions tokens based on common crawl (Grave et al., 2018) …","url":["http://www.diva-portal.org/smash/get/diva2:1325355/FULLTEXT01.pdf"]}
{"year":"2019","title":"Natural Language Processing for Book Recommender Systems","authors":["H Alharthi - 2019"],"snippet":"Page 1. Natural Language Processing for Book Recommender Systems by Haifa Alharthi Thesis submitted in partial fulfillment of the requirements for the PhD degree in Computer Science School of Electrical Engineering and Computer Science Faculty of Engineering …","url":["https://www.ruor.uottawa.ca/bitstream/10393/39134/1/Alharthi_Haifa_2019_thesis.pdf"]}
{"year":"2019","title":"Natural language processing using context-specific word vectors","authors":["B McCann, C Xiong, R Socher - US Patent App. 15/982,841, 2018"],"snippet":"… in the second language. In some examples, training of an MT-LSTM of the encoder 310 uses fixed 300-dimensional word vectors, such as the CommonCrawl-840B GloVe model for English word vectors. These word vectors …","url":["https://patentimages.storage.googleapis.com/49/87/1a/0d4e316e8e4194/US20180373682A1.pdf"]}
{"year":"2019","title":"Naver Labs Europe's Systems for the WMT19 Machine Translation Robustness Task","authors":["A Bérard, I Calapodescu, C Roux - arXiv preprint arXiv:1907.06488, 2019"],"snippet":"… 3.1 Pre-processing CommonCrawl filtering We first spent efforts on filtering and cleaning the WMT data (in particular CommonCrawl) … We filtered CommonCrawl as follows: we trained a baseline FR→EN model on WMT without …","url":["https://arxiv.org/pdf/1907.06488"]}
{"year":"2019","title":"Nested Variational Autoencoder for Topic Modeling on Microtexts with Word Vectors","authors":["T Trinh, T Quan, T Mai - arXiv preprint arXiv:1905.00195, 2019"],"snippet":"Page 1. Noname manuscript No. (will be inserted by the editor) Nested Variational Autoencoder for Topic Modeling on Microtexts with Word Vectors Trung Trinh · Tho Quan · Trung Mai Received: date / Accepted: date Abstract …","url":["https://arxiv.org/pdf/1905.00195"]}
{"year":"2019","title":"NeuMorph: Neural Morphological Tagging for Low-Resource Languages—An Experimental Study for Indic Languages","authors":["A Chakrabarty, A Chaturvedi, U Garain - ACM Transactions on Asian and Low …, 2019"],"snippet":"Page 1. 16 NeuMorph: Neural Morphological Tagging for Low-Resource Languages— An Experimental Study for Indic Languages ABHISEK CHAKRABARTY, AKSHAY CHATURVEDI, and UTPAL GARAIN, Indian Statistical Institute, India …","url":["https://dl.acm.org/citation.cfm?id=3342354"]}
{"year":"2019","title":"Neural Conversation Recommendation with Online Interaction Modeling","authors":["X Zeng, J Li, L Wang, KF Wong"],"snippet":"Page 1. Neural Conversation Recommendation with Online Interaction Modeling Xingshan Zeng1,2, Jing Li3∗, Lu Wang4, Kam-Fai Wong1,2 1The Chinese University of Hong Kong, Hong Kong, China 2MoE Key Laboratory …","url":["https://www.ccs.neu.edu/home/luwang/papers/EMNLP2019_zeng_li_wang_wong.pdf"]}
{"year":"2019","title":"Neural Facet Detection on Medical Resources","authors":["T Steffek - 2019"],"snippet":"Page 1. Neural Facet Detection on Medical Resources Thomas Steffek April 2, 2019 Page 2. Page 3. Beuth Hochschule für Technik Fachbereich VI - Informatik und Medien Database Systems and Text-based Information Systems (DATEXIS) Bachelor's thesis …","url":["https://prof.beuth-hochschule.de/fileadmin/prof/aloeser/Bachelorarbeit_Thomas-Steffek_with-title-page-1.1.pdf"]}
{"year":"2019","title":"Neural Feature Extraction for Contextual Emotion Detection","authors":["E Mohammadi, H Amini, L Kosseim"],"snippet":"… pretrained word embeddings. As the first word embedder, we chose GloVe (Pennington et al., 2014), which is pretrained on 840B tokens of web data from Common Crawl, and provides 300d vectors as word embeddings. As our sec …","url":["https://www.researchgate.net/profile/Hessam_Amini/publication/335704122_Neural_Feature_Extraction_for_Contextual_Emotion_Detection/links/5d76d6764585151ee4ab0908/Neural-Feature-Extraction-for-Contextual-Emotion-Detection.pdf"]}
{"year":"2019","title":"Neural Grammatical Error Correction by Simulating the Human Learner and the Human Proofreader","authors":["F Gaim, JW Chung, JC Park - 한국정보과학회 학술발표논문집, 2018"],"snippet":"… For this and the contrastive learning, we use a large 5-gram language model trained on the Common Crawl data [8]. Training and Decoding: To effectively handle out-of- vocabulary words, we use sub-word level tokenization and …","url":["http://www.dbpia.co.kr/Journal/ArticleDetail/NODE07613671"]}
{"year":"2019","title":"Neural Machine Translation for English–Kazakh with Morphological Segmentation and Synthetic Data","authors":["A Toral, L Edman, G Yeshmagambetova, J Spenader - … 2: Shared Task Papers, Day 1 …, 2019"],"snippet":"… 7.5 0.19 0.16 Wikititles 117.0 0.23 0.19 Table 1: Preprocessed EN–KK parallel training data. Words (M) Corpus Sentences (k) EN RU Common crawl 871.8 20.82 19.97 News-comm … Corpus Threshold Pairs left (k) CommonCrawl 0.7323 568.50 News Comm …","url":["https://www.aclweb.org/anthology/W19-5343"]}
{"year":"2019","title":"Neural network learning engine","authors":["CM Ormerod - US Patent App. 16/286,566, 2019"],"snippet":"… skill and not to limit the invention to any one embodiment, commercial word embedding tools can include Google News word embedding, which has been trained on an extensive corpus of news items, and/or GloVe word …","url":["https://patentimages.storage.googleapis.com/94/fd/43/d4a3cbb7706fec/US20190266234A1.pdf"]}
{"year":"2019","title":"Neural network-based approaches for biomedical relation classification: A review","authors":["Y Zhang, H Lin, Z Yang, J Wang, Y Sun, B Xu, Z Zhao - Journal of Biomedical …, 2019"],"snippet":"… Word2vec, Google news, https://code.google.com/archive/p/word2vec. GloVe, Wikipedia, Gigaword, Common Crawl, Twitter, https://nlp.stanford. edu/projects/glove. fastText, Wikipedia, UMBC corpus, news corpus …","url":["https://www.sciencedirect.com/science/article/pii/S1532046419302138"]}
{"year":"2019","title":"Neural NLP models under low-supervision scenarios","authors":["Y Zhang - 2019"],"snippet":"Page 1. Copyright by Ye Zhang 2019 Page 2. The Dissertation Committee for Ye Zhang certifies that this is the approved version of the following dissertation: Neural NLP Models Under Low-supervision Scenarios Committee: Matthew A Lease, Supervisor …","url":["https://repositories.lib.utexas.edu/bitstream/handle/2152/75032/ZHANG-DISSERTATION-2019.pdf?sequence=1"]}
{"year":"2019","title":"Neural Text Style Transfer via Denoising and Reranking","authors":["J Lee, Z Xie, C Wang, M Drach, D Jurafsky, AY Ng - … of the Workshop on Methods for …, 2019"],"snippet":"… 3. Fluency The post-transfer sentence should remain grammatical and fluent. We use the average log probability of the sentence posttransfer with respect to a language model trained on CommonCrawl as our measure of fluency …","url":["https://www.aclweb.org/anthology/W19-2309"]}
{"year":"2019","title":"NLNDE: The Neither-Language-Nor-Domain-Experts' Way of Spanish Medical Document De-Identification","authors":["L Lange, H Adel, J Strötgen - 2019"],"snippet":"… S2 (FLAIR+fastText): In contrast to all other runs, the second run uses only domain-independent embeddings, ie, embeddings that have been trained on standard narrative and news data from Common Crawl and Wikipedia …","url":["http://ceur-ws.org/Vol-2421/MEDDOCAN_paper_5.pdf"]}
{"year":"2019","title":"NLP@ UIOWA at SemEval-2019 Task 6: Classifying the Crass using Multi-windowed CNNs","authors":["J Rusert, P Srinivasan - Proceedings of the 13th International Workshop on …, 2019"],"snippet":"… Word embeddings for Non-Out of Vocabulary (OOV) words are obtained from Glove (Pennington et al., 2014) which has been trained on Twitter data3. Experiments were also conducted with Glove common crawl data, but no visible improvement was found …","url":["https://www.aclweb.org/anthology/S19-2125"]}
{"year":"2019","title":"Noisy Parallel Corpus Filtering through Projected Word Embeddings","authors":["M Kurfalı, R Östling - Proceedings of the Fourth Conference on Machine …, 2019"],"snippet":"… Larger monolingual corpora based on Wikipedia and common crawl data were also provided.2 To train our model, we use all the parallel data available for the English-Sinhala and EnglishNepali pairs (summarized …","url":["https://www.aclweb.org/anthology/W19-5438"]}
{"year":"2019","title":"NRC Parallel Corpus Filtering System for WMT 2019","authors":["G Bernier-Colborne, C Lo - Proceedings of the Fourth Conference on Machine …, 2019"],"snippet":"… embedding models. Common Crawl data was not used to train the bilingual word embeddings. 2.2 … representation layer. We used XLM to train a model using almost all the available data, except for the monolingual English Common Crawl data. This …","url":["https://www.aclweb.org/anthology/W19-5434"]}
{"year":"2019","title":"Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral Codes","authors":["J Cao, M Tanana, ZE Imel, E Poitras, DC Atkins…"],"snippet":"Page 1. Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral Codes Jie Cao†, Michael Tanana‡, Zac E. Imel‡, Eric Poitras‡, David C. Atkins♦, Vivek Srikumar† †School of Computing, University of Utah …","url":["https://svivek.com/research/publications/cao2019observing.pdf"]}
{"year":"2019","title":"Observing LOD Using Equivalent Set Graphs: It Is Mostly Flat and Sparsely Linked","authors":["L Asprino, W Beek, P Ciancarini, F van Harmelen… - International Semantic Web …, 2019"],"snippet":"… The two largest available crawls of LOD available today are WebDataCommons and LOD-a-lot. WebDataCommons 2 [12] consists of \\(\\sim \\)31B triples that have been extracted from the CommonCrawl datasets (November 2018 version) …","url":["https://link.springer.com/chapter/10.1007/978-3-030-30793-6_4"]}
{"year":"2019","title":"Observing the LOD Cloud using Equivalent Set Graphs: the LOD Cloud is mostly flat and sparsely linked","authors":["L Asprino, W Beek, P Ciancarini, F van Harmelen…"],"snippet":"… The two largest available crawls of LOD available today are WebDataCommons and LOD-a-lot. WebDataCommons5 [12] consists of ∼31B triples that have been extracted from the CommonCrawl datasets (November 2018 version) …","url":["https://www.cs.vu.nl/~frankh/postscript/ISWC2019-LODanalytics.pdf"]}
{"year":"2019","title":"OECD Analytical Database on Individual Multinationals and their Affiliates (ADIMA)","authors":["G Pilgrim, N Ahmad, D Doyle - 2019"],"snippet":"… Secondly, information from MNE webpages is used from an open source 'copy of the internet' generated via web crawling from the Common Crawl 4 . This process develops a graph of the links between companies, from …","url":["https://www.gtap.agecon.purdue.edu/resources/download/9310.docx"]}
{"year":"2019","title":"Offensive Language and Hate Speech Detection for Danish","authors":["GI Sigurbergsson, L Derczynski - arXiv preprint arXiv:1908.04531, 2019"],"snippet":"… sample of text. Pre-trained Embeddings. The pre-trained FastText [24] embeddings are trained on data from the Common Crawl project and Wikipedia, in 157 languages (including English and Danish). FastText also provides …","url":["https://arxiv.org/pdf/1908.04531"]}
{"year":"2019","title":"On extracting data from tables that are encoded using HTML","authors":["JC Roldán, P Jiménez, R Corchuelo - Knowledge-Based Systems, 2019"],"snippet":"Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S095070511930509X"]}
{"year":"2019","title":"On Implementing the Binary Interpolative Coding Algorithm","authors":["GE PIBIRI - 2019"],"snippet":"… Table 4. Decoding time measured in average nanoseconds spent per decoded integer, for the run-aware implementation (ra) and for the not run-aware implementation. • CCNews is an English subset of the freely available news from CommonCrawl 3, consisting of …","url":["http://pages.di.unipi.it/pibiri/papers/BIC.pdf"]}
{"year":"2019","title":"On Measuring and Mitigating Biased Inferences of Word Embeddings","authors":["S Dev, T Li, J Phillips, V Srikumar - arXiv preprint arXiv:1908.09369, 2019"],"snippet":"Page 1. arXiv:1908.09369v1 [cs.CL] 25 Aug 2019 On Measuring and Mitigating Biased Inferences of Word Embeddings Sunipa Dev, Tao Li, Jeff Phillips, Vivek Srikumar School of Computing University of Utah Abstract Word …","url":["https://arxiv.org/pdf/1908.09369"]}
{"year":"2019","title":"On Measuring Social Biases in Sentence Encoders","authors":["C May, A Wang, S Bordia, SR Bowman, R Rudinger - arXiv preprint arXiv:1903.10561, 2019"],"snippet":"Page 1. On Measuring Social Biases in Sentence Encoders Chandler May1 Alex Wang2 Shikha Bordia2 Samuel R. Bowman2 Rachel Rudinger1 1Johns Hopkins University 2New York University {cjmay,rudinger}@jhu.edu {alexwang,sb6416,bowman}@nyu.edu Abstract …","url":["https://arxiv.org/pdf/1903.10561"]}
{"year":"2019","title":"On Optimally Partitioning Variable-Byte Codes","authors":["GE Pibiri, R Venturini - IEEE Transactions on Knowledge and Data …, 2019"],"snippet":"Page 1. 1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/8691421/"]}
{"year":"2019","title":"On relevance of enriching word embeddings in solving Natural Language Inference problem","authors":["T Wesołowski"],"snippet":"Page 1. Jagiellonian University Faculty of Mathematics and Computer Science Theoretical Computer Science Stationary Studies Index number: 1079621 Tomasz Wesołowski On relevance of enriching word embeddings in solving Natural Language Inference problem …","url":["http://algo.edu.pl/OnRelevanceOfWordEmbeddings.pdf"]}
{"year":"2019","title":"On Slicing Sorted Integer Sequences","authors":["GE Pibiri - arXiv preprint arXiv:1907.01032, 2019"],"snippet":"… 2009. • CCNews is a dataset of news freely available from CommonCrawl: http://commoncrawl.org/ 2016/10/news-dataset-available. Precisely, the datasets consists of the news appeared from 09/01/16 to 30/03/18. Identifiers …","url":["https://arxiv.org/pdf/1907.01032"]}
{"year":"2019","title":"On the Effect of Low-Frequency Terms on Neural-IR Models","authors":["S Hofstätter, N Rekabsaz, C Eickhoff, A Hanbury - arXiv preprint arXiv:1904.12683, 2019"],"snippet":"… collection. The details of the resulting 1Provided in the form of evaluation tuples: top1000.dev.tsv 242B lower-cased (CommonCrawl) from: https://nlp.stanford.edu/ projects/glove/ Table 1: Left: Details of the vocabularies. Right …","url":["https://arxiv.org/pdf/1904.12683"]}
{"year":"2019","title":"On the Robustness of Unsupervised and Semi-supervised Cross-lingual Word Embedding Learning","authors":["Y Doval, J Camacho-Collados, L Espinosa-Anke… - arXiv preprint arXiv …, 2019"],"snippet":"… google.com/site/rmyeid/projects/polyglot 2The sources of the web-corpora are: UMBC (Han et al., 2013), 1-billion (Cardellino, 2016), itWaC and sdeWaC (Ba- roni et al., 2009), Hamshahri (AleAhmad et al., 2009), and Common Crawl downloaded from http://www …","url":["https://arxiv.org/pdf/1908.07742"]}
{"year":"2019","title":"On Using Machine Learning to Identify Knowledge in API Reference Documentation","authors":["D Fucci, A Mollaalizadehbahnemiri, W Maalej - arXiv preprint arXiv:1907.09807, 2019"],"snippet":"… For the deep learning classifiers in our benchmark, we train GloVe [19] embeddings based on four large corpora, summarized in Table 3. The Common Crawl (CC) is a pre-trained embedding downloaded in …","url":["https://arxiv.org/pdf/1907.09807"]}
{"year":"2019","title":"On Using SpecAugment for End-to-End Speech Translation","authors":["P Bahar, A Zeyer, R Schlüter, H Ney"],"snippet":"… For MT training, we use the TED, and the OpenSubtitles2018 corpora, as well as the data provided by the WMT 2018 evaluation (Europarl, ParaCrawl, CommonCrawl, News Commentary, and Rapid), a total of 65M lines of parallel sentences …","url":["https://www-i6.informatik.rwth-aachen.de/publications/download/1122/Bahar-IWSLT-2019.pdf"]}
{"year":"2019","title":"One Epoch Is All You Need","authors":["A Komatsuzaki - arXiv preprint arXiv:1906.06669, 2019"],"snippet":"… Trinh & Le (2018) pointed out that CommonCrawl contains a large portion of corrupt samples, which makes it unsuitable for the training. The proportion of the corrupt samples in CommonCrawl is substantially higher than 50 …","url":["https://arxiv.org/pdf/1906.06669"]}
{"year":"2019","title":"Online Parallel Data Extraction with Neural Machine Translation","authors":["D Ruiter - 2019"],"snippet":"Page 1. Universität des Saarlandes Master's Thesis Online Parallel Data Extraction with Neural Machine Translation submitted in fulfillment of the degree requirements of the MSc in Language Science and Technology at Saarland University …","url":["https://www.clubs-project.eu/assets/publications/other/MSc_Thesis_Ruiter.pdf"]}
{"year":"2019","title":"Ontological Traceability using Natural Language Processing","authors":["R Benitez - 2019"],"snippet":"Page 1. Ontological Traceability using Natural Language Processing A master thesis presented by Edder de la Rosa Benitez Submitted to the Department of Organization and Information in partial fulfillment of the …","url":["https://dspace.library.uu.nl/bitstream/handle/1874/383214/Master_Thesis_E_De_la_Rosa.pdf?sequence=2"]}
{"year":"2019","title":"OpenCeres: When Open Information Extraction Meets the Semi-Structured Web","authors":["C Lockard, P Shiralkar, XL Dong"],"snippet":"… 5.1 Experimental Setup Datasets: Our primary dataset is the augmented SWDE corpus described in Section 4. In addition, we used the set of 315 movie websites (comprising 433,000 webpages) found in Common …","url":["http://lunadong.com/publication/openCeres_naacl.pdf"]}
{"year":"2019","title":"OPTIMIZE THE LEARNING RATE OF NEURAL ARCHITECTURE IN MYANMAR STEMMER","authors":["Y Oo, KM Soe"],"snippet":"… Word vector pre-trained on large text corpora have been released on [10] \"Learning Word Vectors for 157 Languages \" that trained on 3 billion words from Wikipedia and Common Crawl using Continuous bag-of-words (CBOW) 300-dimension …","url":["https://www.academia.edu/download/61248451/120191117-8847-1ko3nhm.pdf"]}
{"year":"2019","title":"Optimizer Comparison with Dropout for Neural Sequence Labeling in Myanmar Stemmer","authors":["O Yadanar, KM Soe - 2019 IEEE International Conference on Industry 4.0 …, 2019"],"snippet":"… Parameter initialization: It has used Learning Word Vectors for 157 Languages that trained on 3 billion words from Wikipedia and Common Crawl using CBOW 300-dimension (E. Grave, P. Bojanowski, P. Gupta, A. Joulin, T. Mikolov,2018) for both word and character …","url":["https://ieeexplore.ieee.org/abstract/document/8784850/"]}
{"year":"2019","title":"Optimizing Social Media Data Using Genetic Algorithm","authors":["S Das, AK Kolya, D Das - Metaheuristic Approaches to Portfolio Optimization, 2019"],"snippet":"Page 1. 126 Copyright © 2019, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. Chapter 6 DOI: 10.4018/978-1-5225-8103-1.ch006 ABSTRACT Twitter-based …","url":["https://www.igi-global.com/chapter/optimizing-social-media-data-using-genetic-algorithm/233176"]}
{"year":"2019","title":"Overview of the CLEF eHealth Evaluation Lab 2019","authors":["E Kanoulas, D Li, L Azzopardi, R Spijker, G Zuccon… - Experimental IR Meets …"],"snippet":"… More specifically, for the Abstract and Title Screening subtask the PubMed Document Identifiers (PMIDs) of potentially relevant 4http://commoncrawl. org/(last accessed on 28 May 2019) … It consists of web pages acquired from the CommonCrawl …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=LqGsDwAAQBAJ&oi=fnd&pg=PA322&dq=commoncrawl&ots=8duC39Wv1R&sig=Tt9HYmgrR17eWjcTJPZvsij9B5g"]}
{"year":"2019","title":"P-SIF: Document Embeddings Using Partition Averaging","authors":["V Gupta, A Saw, P Nokhiz, P Netrapalli, P Rai…"],"snippet":"… Page 5. evaluation. We use the PARAGRAM-SL999 (PSL) as word embeddings, obtained by training on the PPDB dataset. 7 We use the fixed weighting parameter a value of 10−3, and the word frequencies p(w) are estimated from the commoncrawl dataset …","url":["https://vgupta123.github.io/docs/AAAI-GuptaV.3656.pdf"]}
{"year":"2019","title":"P2L: Predicting Transfer Learning for Images and Semantic Relations","authors":["B Bhattacharjee, N Codella, JR Kender, S Huo… - arXiv preprint arXiv …, 2019"],"snippet":"… We use the CC-DBP [12] dataset: the text of Common Crawl1 and the semantic relations schema and training data from DBpedia [1]. DBpedia is a knowledge graph extracted from the infoboxes from Wikipedia … 4.3.2 Validation on Common Crawl - DBpedia …","url":["https://arxiv.org/pdf/1908.07630"]}
{"year":"2019","title":"PaDAWaNS","authors":["TLM Brands"],"snippet":"Page 1. PaDAWaNS Proactive Domain Abuse Warning and Notification System by TLM Brands to obtain the degree of Master of Science at the Delft University of Technology, to be defended publicly on Tuesday January 15, 2019 at 11:00 AM …","url":["https://www.sidnlabs.nl/downloads/theses/thesis_brands_padawans.pdf"]}
{"year":"2019","title":"Parallel External Memory Wavelet Tree and Wavelet Matrix Construction","authors":["J Ellert, F Kurpicz - International Symposium on String Processing and …, 2019"],"snippet":"… CC \\((\\sigma =242)\\) contains websites (without HTML tags) that have been crawled by the Common Crawl corpus (http://commoncrawl.org), and. Wiki \\((\\sigma =213)\\) are recent Wikipedia dumps containing XML files that …","url":["https://link.springer.com/chapter/10.1007/978-3-030-32686-9_28"]}
{"year":"2019","title":"Paraphrase-Sense-Tagged Sentences","authors":["A Cocos, C Callison-Burch, S Chen, D Khashabi… - Transactions, 2019"],"snippet":"Skip to main content …","url":["http://callison-burch.github.io/publications.html"]}
{"year":"2019","title":"PDRCNN: Precise Phishing Detection with Recurrent Convolutional Neural Networks","authors":["W Wang, F Zhang, X Luo, S Zhang - Security and Communication Networks, 2019"],"snippet":"… This method first encodes the URL string using the one-hot encoding method, and then inputs each encoded character vector into the LSTM neurons for training and testing. The method achieved an accuracy of 0.935 on the …","url":["http://downloads.hindawi.com/journals/scn/2019/2595794.pdf"]}
{"year":"2019","title":"Peer Review and the Production of Scholarly Knowledge: Automated Textual Analysis of Manuscripts Revised for Publication in Administrative Science Quarterly","authors":["D Strang, F Dokshin - The Production of Managerial Knowledge and …, 2019"],"snippet":"… numbers, and filter out “stop words.” Stop words are the most common words in the English language (eg, “the,” “not,” “a”). 2 Next, for each word in the pre-processed sentences, we generate word vectors from a GloVe model …","url":["https://www.emeraldinsight.com/doi/abs/10.1108/S0733-558X20190000059006"]}
{"year":"2019","title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization","authors":["J Zhang, Y Zhao, M Saleh, PJ Liu - arXiv preprint arXiv:1912.08777, 2019"],"snippet":"… T5 (Raffel et al., 2019) generalized the text-to- text framework to a variety of NLP tasks and showed the advantage of scaling up model size (to 11 billion parameters) and pre-training corpus, introducing C4, a massive text corpus …","url":["https://arxiv.org/pdf/1912.08777"]}
{"year":"2019","title":"People represent mental states in terms of rationality, social impact, and valence: Validating the 3d Mind Model","authors":["MA Thornton, D Tamir"],"snippet":"Page 1. Running head: MENTAL STATE DIMENSIONS 1 People represent mental states in terms of rationality, social impact, and valence: Validating the 3d Mind Model Mark A. Thornton* and Diana I. Tamir Department of …","url":["https://psyarxiv.com/akhpq/download?format=pdf"]}
{"year":"2019","title":"PhishFry–A Proactive Approach to Classify Phishing Sites using SCIKIT Learn","authors":["D Brites, M Wei"],"snippet":"… [Online]. Available: http://5000best.com/websites/. [Accessed 2019]. [26] OpenPhish, \"OpenPhish,\" 2019. [Online]. Available: https://openphish.com/. [27] Amazon Web Services, \"Common Crawl,\" Amazon, 2019. [Online] …","url":["https://www.shsu.edu/mxw032/publication/19gc-bw.pdf"]}
{"year":"2019","title":"Phishing Detection Based on Machine Learning and Feature Selection Methods","authors":["M Almseidin, AMA Zuraiq, M Al-kasassbeh, N Alnidami - International Journal of …, 2019"],"snippet":"… Phishing webpages are collected from Phish-Tank and Open-Phish, while legitimate web-pages are collected from Alexa and Common Crawl. These web-pages are downloaded on two distinct sessions, from January to May 2015 and through May to June 2017 …","url":["https://onlinejour.journals.publicknowledgeproject.org/index.php/i-jim/article/download/11411/6259"]}
{"year":"2019","title":"Phishing URL Detection Via Capsule-Based Neural Network","authors":["Y Huang, J Qin, W Wen - 2019 IEEE 13th International Conference on Anti …, 2019"],"snippet":"… [27] VirusTotal, https://www.virustotal.com/ [28] Common Crawl, https://commoncrawl.org/ [29] J. Ma, LK Saul, S. Savage, and GM VoelNer, “Beyond blacNlists: learning to detect malicious web sites from suspicious …","url":["https://ieeexplore.ieee.org/abstract/document/8925000/"]}
{"year":"2019","title":"Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages","authors":["Y Kim, P Petrov, P Petrushkov, S Khadivi, H Ney - arXiv preprint arXiv:1909.09524, 2019"],"snippet":"Page 1. Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages Yunsu Kim1∗ Petre Petrov1,2∗ Pavel Petrushkov2 Shahram Khadivi2 Hermann Ney1 1RWTH Aachen University, Aachen …","url":["https://arxiv.org/pdf/1909.09524"]}
{"year":"2019","title":"PKUSE at SemEval-2019 Task 3: Emotion Detection with Emotion-Oriented Neural Attention Network","authors":["L Ma, L Zhang, W Ye, W Hu - Proceedings of the 13th International Workshop on …, 2019"],"snippet":"… Table 1: Datasets for Semeval-2019 Task 3. 4.2 Experiments The model is implemented using Keras 2.0 (Chollet et al., 2017). We experiment with Stanford's GloVe 300 dimensional word embeddings trained on 840 billion words from Common Crawl …","url":["https://www.aclweb.org/anthology/S19-2049"]}
{"year":"2019","title":"PLAGO: A SYSTEM FOR PLAGIARISM DETECTION AND INTERVENTION IN MASSIVE COURSES","authors":["CT Guida - 2019"],"snippet":"… Web Crawl: Used for queuing and monitoring of importing web pages from the CommonCrawl.org public dataset (described in 3.5.2). • Admin Options … pages. Common Crawl is a non-profit organization which offers a public …","url":["https://smartech.gatech.edu/bitstream/handle/1853/61787/GUIDA-THESIS-2019.pdf?sequence=1&isAllowed=y"]}
{"year":"2019","title":"Poetry: Identification, Entity Recognition, and Retrieval","authors":["IV Foley, J John - 2019"],"snippet":"Page 1. University of Massachusetts Amherst ScholarWorks@UMass Amherst Doctoral Dissertations Dissertations and Theses 2019 Poetry: Identification, Entity Recognition, and Retrieval John J. Foley IV Follow this and additional …","url":["https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=2628&context=dissertations_2"]}
{"year":"2019","title":"Populating Web Scale Knowledge Graphs using Distantly Supervised Relation Extraction and Validation","authors":["A Gliozzo, MR Glass, S Dash, M Canim - arXiv preprint arXiv:1908.08104, 2019"],"snippet":"… Also, a web-scale experiment conducted to extend DBPedia with knowledge from Common Crawl shows that our system is not only scalable but also does not require any adaptation cost, while yielding substantial accuracy gain. 1 Introduction …","url":["https://arxiv.org/pdf/1908.08104"]}
{"year":"2019","title":"Precise Detection of Content Reuse in the Web","authors":["C Ardi, J Heidemann - ACM SIGCOMM Computer Communication Review, 2019"],"snippet":"… We verify our algorithm and its choices with controlled experiments over three web datasets: Common Crawl (2009/10), GeoCities (1990s–2000s), and a phishing corpus (2014) … In the Common Crawl dataset of 40.5×109 chunks, we set the threshold to 105 …","url":["https://dl.acm.org/citation.cfm?id=3336940"]}
{"year":"2019","title":"Predicting ConceptNet Path Quality Using Crowdsourced Assessments of Naturalness","authors":["Y Zhou, S Schockaert, JA Shah - arXiv preprint arXiv:1902.07831, 2019"],"snippet":"… The number in parenthesis after each feature name indicates the dimension of that feature. Vertex embedding (300) This feature is taken directly from the 300dimensional GloVe (25) embedding, pre-trained on the Common Crawl2 dataset with 840 billion tokens …","url":["https://arxiv.org/pdf/1902.07831"]}
{"year":"2019","title":"Predicting Word Concreteness and Imagery","authors":["J Charbonnier, C Wartena - Proceedings of the 13th International Conference on …, 2019"],"snippet":"… The other two version (also available with and without subword information) with 2 million word vectors trained on the Common Crawl with 600B tokens. In our experiments we used the version trained on Common Crawl without …","url":["https://www.aclweb.org/anthology/W19-0415"]}
{"year":"2019","title":"Probing Contextualized Sentence Representations with Visual Awareness","authors":["Z Zhang, R Wang, K Chen, M Utiyama, E Sumita… - arXiv preprint arXiv …, 2019"],"snippet":"… We used newsdev2016 as the dev set and newstest2016 as the test set. 2) For the EN-DE translation task, 4.43M bilingual sentence pairs of the WMT14 dataset were used as training data, including Common Crawl, News Commentary, and Europarl v7 …","url":["https://arxiv.org/pdf/1911.02971"]}
{"year":"2019","title":"Product Classification Using Microdata Annotations","authors":["Z Zhang, M Paramita - International Semantic Web Conference, 2019"],"snippet":"… dimension of the continuous vector representation of each word. In this work, we use the GloVe word embedding vectors pre-trained on the Common Crawl corpus 3 with 300 dimensions. Since we are dealing with content from e …","url":["https://link.springer.com/chapter/10.1007/978-3-030-30793-6_41"]}
{"year":"2019","title":"Provision and Usage of Provenance Data in the WebIsALOD Knowledge Graph","authors":["S Hertling, H Paulheim - CEUR Workshop Proceedings, 2018"],"snippet":"… As described in [6], the Copyright c 2018 for this paper by its authors. Copying permitted for private and academic purposes. 1 https://commoncrawl.org 2 NP stands for noun phrase. 3 https://www.w3.org/TR/skos-reference/ Page 2. isa:concept/_Gmail …","url":["http://ceur-ws.org/Vol-2317/article-06.pdf"]}
{"year":"2019","title":"PT-CoDE: Pre-trained Context-Dependent Encoder for Utterance-level Emotion Recognition","authors":["W Jiao, MR Lyu, I King - arXiv preprint arXiv:1910.08916, 2019"],"snippet":"… Here, we utilize the 300-dimensional pre-trained GloVe word vectors1 (Pennington et al., 2014) trained over 840B Common Crawl to initialize the word embedding layer. Those words that cannot be found in the GloVe …","url":["https://arxiv.org/pdf/1910.08916"]}
{"year":"2019","title":"QE BERT: Bilingual BERT using Multi-task Learning for Neural Quality Estimation","authors":["H Kim, JH Lim, HK Kim, SH Na - Proceedings of the Fourth Conference on Machine …, 2019"],"snippet":"… We used parallel data provided for the WMT19 news machine translation task6 to pre-train QE BERT. The English-Russian parallel data set consisted of the ParaCrawl corpus, Common Crawl corpus, News Commentary corpus, and Yandex …","url":["https://www.aclweb.org/anthology/W19-5407"]}
{"year":"2019","title":"QED: A Fact Verification and Evidence Support System","authors":["J Luken - 2019"],"snippet":"… embedding layers, as described below. 4.3.2 Embedding We use GloVe word embeddings (Pennington et al., 2014) with 300 dimensions pretrained using CommonCrawl to get a vector representation of the evidence sentence. We","url":["https://etd.ohiolink.edu/!etd.send_file?accession=osu1555074124008897&disposition=inline"]}
{"year":"2019","title":"Quantifying the Semantic Core of Gender Systems","authors":["DBPH Wallach"],"snippet":"… 4The FASTTEXT word embeddings were trained using Common Crawl and Wikipedia data, using CBOW with po- sition weights, with character n-grams of length 5. For more information, see http://fasttext.cc/docs/en …","url":["https://openreview.net/pdf?id=ByxcApoPwS"]}
{"year":"2019","title":"QuAVONet: Answering Questions on the SQuAD Dataset with QANet and Answer Verifier","authors":["J Cervantes"],"snippet":"… 5.2 Implementation Details For the word embeddings, I used the starter code's 300-dimensional GloVE vectors trained on the CommonCrawl dataset [6]. These embeddings remained unchanged and were not trained for any of my models …","url":["https://pdfs.semanticscholar.org/f71e/5c6cdd9e06068625eb82b3d9647823e80503.pdf"]}
{"year":"2019","title":"Quick and (maybe not so) Easy Detection of Anorexia in Social Media Posts","authors":["E Mohammadi, H Amini, L Kosseim - 2019"],"snippet":"… As shown in Figure 1, these token vectors are then fed to the hidden layer. Two different pretrained word embeddings were experimented with. The first word embedder was the 300d version of GloVe [26] that was pretrained …","url":["https://www.researchgate.net/profile/Hessam_Amini/publication/334848955_Quick_and_maybe_not_so_Easy_Detection_of_Anorexia_in_Social_Media_Posts/links/5d434b9992851cd04699c9ce/Quick-and-maybe-not-so-Easy-Detection-of-Anorexia-in-Social-Media-Posts.pdf"]}
{"year":"2019","title":"Quotient Hash Tables-Efficiently Detecting Duplicates in Streaming Data","authors":["R Géraud, M Lombard-Platet, D Naccache - arXiv preprint arXiv:1901.04358, 2019"],"snippet":"Page 1. arXiv:1901.04358v1 [cs.DS] 14 Jan 2019 Quotient Hash Tables - Efficiently Detecting Duplicates in Streaming Data Rémi Gérauda,c, Marius Lombard-Platet∗ a,b, and David Naccachea,c aDépartement d'informatique …","url":["https://arxiv.org/pdf/1901.04358"]}
{"year":"2019","title":"Racial bias in legal language","authors":["D Rice, JH Rhodes, T Nteta - Research & Politics, 2019"],"snippet":"Although racial bias in the law is widely recognized, it remains unclear how these biases are in entrenched in the language of the law, judicial opinions. In th...","url":["https://journals.sagepub.com/doi/pdf/10.1177/2053168019848930"]}
{"year":"2019","title":"Random Projection in Deep Neural Networks","authors":["PI Wójcik - arXiv preprint arXiv:1812.09489, 2018"],"snippet":"Page 1. Akademia Górniczo-Hutnicza im. Stanisława Staszica w Krakowie Wydział Informatyki, Elektroniki i Telekomunikacji Katedra Informatyki Rozprawa doktorska Zastosowania metody rzutu przypadkowego w głębokich …","url":["https://arxiv.org/pdf/1812.09489"]}
{"year":"2019","title":"Real or Fake? Learning to Discriminate Machine from Human Generated Text","authors":["A Bakhtin, S Gross, M Ott, Y Deng, MA Ranzato… - arXiv preprint arXiv …, 2019"],"snippet":"… CCNews: We collect a de-duplicated subset of the English portion of the CommonCrawl news dataset [Nagel, 2016], which totals around 16 Billion words … Sebastian Nagel. Cc-news. http://web.archive.org/save/http …","url":["https://arxiv.org/pdf/1906.03351"]}
{"year":"2019","title":"Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks","authors":["B Adler, G Boscaini-Gilroy - arXiv preprint arXiv:1907.02030, 2019"],"snippet":"… new problem. Many unsupervised text embeddings are trained on the CommonCrawl 1 dataset of approx. 840 billion tokens. This … dataset. Supervised datasets are 1CommonCrawl found at http://commoncrawl.org/ unlikely ever …","url":["https://arxiv.org/pdf/1907.02030"]}
{"year":"2019","title":"Real-time event detection using recurrent neural network in social sensors","authors":["VQ Nguyen, TN Anh, HJ Yang - International Journal of Distributed Sensor Networks, 2019"],"snippet":"We proposed an approach for temporal event detection using deep learning and multi-embedding on a set of text data from social media. First, a convolutional neural network augmented with multiple w...","url":["https://journals.sagepub.com/doi/pdf/10.1177/1550147719856492"]}
{"year":"2019","title":"Real-world Conversational AI for Hotel Bookings","authors":["B Li, N Jiang, J Sham, H Shi, H Fazal - arXiv preprint arXiv:1908.10001, 2019"],"snippet":"… We compare the following models: 1) Averaged GloVe + feedforward: We use 100dimensional, trainable GloVe embeddings [17] trained on Common Crawl, and produce sentence embeddings for each of the two inputs by averaging across all tokens …","url":["https://arxiv.org/pdf/1908.10001"]}
{"year":"2019","title":"Recommendation System with Aspect-Based Sentiment Analysis","authors":["Q Du, D Zhu, W Duan"],"snippet":"… The word vectors model we use is the \"en_core_web_lg\" model in spaCy. The model contains English multi-task CNN trained on OntoNotes 5[3], with GloVe[8] vectors trained on Common Crawl. It provides 300dimensional …","url":["http://rafaelsilva.com/wp-content/uploads/2018/12/014-Aspect-based-sentiment-analysis.pdf"]}
{"year":"2019","title":"Refining Word Reesprentations by Manifold Learning","authors":["C Yonghe, H Lin, L Yang, Y Diao, S Zhang, F Xiaochao"],"snippet":"… judgment. This is exemplified by the WS353[Finkelstein et al., 2001]word similarity ground truth in Figure 1. Based on the Common Crawl corpus (42B), the Glove model is used to train 300-dimensional word vectors. The similarity …","url":["https://www.ijcai.org/proceedings/2019/0749.pdf"]}
{"year":"2019","title":"Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation","authors":["IJ Unanue, EZ Borzeshi, M Piccardi - arXiv preprint arXiv:1909.13466, 2019"],"snippet":"… De-En: The German-English dataset (de-en) has been taken from the WMT18 news translation shared task1. The training set contains over 5M sentence pairs collected from the Europarl, CommonCrawl and Newscommentary parallel corpora …","url":["https://arxiv.org/pdf/1909.13466"]}
{"year":"2019","title":"Rel4KC: A Reinforcement Learning Agent for Knowledge Graph Completion and Validation","authors":["X Lin, P Subasic, H Yin - 2019"],"snippet":"… The fact triples extracted from free text (Common Crawl) are then fed to the trained RL agent to determine their trustworthiness. If passing the validation, a triple is entered into target KG … The free text used in this study is Common Crawl corpus …","url":["http://www.cse.msu.edu/~zhaoxi35/DRL4KDD/1.pdf"]}
{"year":"2019","title":"Repositioning privacy concerns: Web servers controlling URL metadata","authors":["R Ferreira, RL Aguiar - Journal of Information Security and Applications, 2019"],"snippet":"… on empirical observation of web browsers and HTTP server implementations, and while some implementations allow longer URLs (eg, 100.000 octets) this value remains a reasonable assumption for practical purposes 1 . Our …","url":["https://www.sciencedirect.com/science/article/pii/S2214212618302588"]}
{"year":"2019","title":"Representation Learning for Question Classification via Topic Sparse Autoencoder and Entity Embedding","authors":["D Li, J Zhang, P Li - IEEE Big Data, 2018"],"snippet":"… WordNet 2. The embeddings of entity-related information are also trained with skip-gram. The word embeddings are initialized with the 300 dimensional pretrained vectors 3 from the Common Crawl of 840 billion tokens and 2.2 …","url":["http://research.baidu.com/Public/uploads/5c1c9ab3069f4.pdf"]}
{"year":"2019","title":"Representing Overlaps in Sequence Labeling Tasks with a Novel Tagging Scheme: bigappy-unicrossy","authors":["G Berk, B Erden, T Güngör"],"snippet":"… a language-independent system based on the bidirectional LSTM-CRF model provided by [7]. Similar to Deep-BGT system [2], we make use of the pretrained word embeddings provided by fastText [6]. The word embeddings …","url":["https://www.cmpe.boun.edu.tr/~gungort/papers/Representing%20Overlaps%20in%20Sequence%20Labeling%20Tasks%20with%20a%20Novel%20Tagging%20Scheme%20-%20bigappy-unicrossy.pdf"]}
{"year":"2019","title":"Review and Visualization of Facebook's FastText Pretrained Word Vector Model","authors":["JC Young, A Rusli - … International Conference on Engineering, Science, and …, 2019"],"snippet":"… Machine Learning (ML). Currently, FastText provides pretrained Word2Vec model for 157 language that trained on Common Crawl and Wikipedia (Bahasa Indonesia is one from the provided model) [15]. In its Word2Vec model …","url":["https://ieeexplore.ieee.org/abstract/document/8863015/"]}
{"year":"2019","title":"RIPPED: Recursive Intent Propagation using Pretrained Embedding Distances","authors":["M Ball - 2019"],"snippet":"… GloVe (Pennington et al., 2014) is a word embedding model trained on data from the Common Crawl corpus6. GloVe is a log-bilinear regression model that incorporates both local context windows and global matrix …","url":["https://cs.brown.edu/research/pubs/theses/ugrad/2019/ball.michael.pdf"]}
{"year":"2019","title":"RNN Embeddings for Identifying Difficult to Understand Medical Words","authors":["H Pylieva, A Chernodub, N Grabar, T Hamon - … of the 18th BioNLP Workshop and …, 2019"],"snippet":"… improve classification accuracy for our specific problem. We note that FastText word embeddings trained on Wikipedia and Common Crawl5 texts have an important part of words from our dataset. According to our analysis, the …","url":["https://www.aclweb.org/anthology/W19-5011"]}
{"year":"2019","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","authors":["Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy… - arXiv preprint arXiv …, 2019"],"snippet":"… (16GB). • CC-NEWS, which we collected from the En- glish portion of the CommonCrawl News dataset (Nagel, 2016) … STORIES, a dataset introduced in Trinh and Le (2018) containing a subset of CommonCrawl data filtered …","url":["https://arxiv.org/pdf/1907.11692"]}
{"year":"2019","title":"Robust Argument Unit Recognition and Classification","authors":["D Trautmann, J Daxenberger, C Stab, H Schütze… - arXiv preprint arXiv …, 2019"],"snippet":"… 2http://commoncrawl.org/2016/02/ february-2016-crawl-archive-now-available/ 3https://www.elastic.co/products/ elasticsearch the topic. Each document was checked for its corresponding WARC file at the Common Crawl In …","url":["https://arxiv.org/pdf/1904.09688"]}
{"year":"2019","title":"Robust Named Entity Recognition with Truecasing Pretraining","authors":["S Mayhew, N Gupta, D Roth - arXiv preprint arXiv:1912.07095, 2019"],"snippet":"… and Kauchak (2011) and used in Susanto, Chieu, and Lu (2016), and a specially preprocessed large dataset from English Common Crawl (CC).1 … 1commoncrawl.org 2In a naming clash, the moses script is called …","url":["https://arxiv.org/pdf/1912.07095"]}
{"year":"2019","title":"SACABench: Benchmarking Suffix Array Construction","authors":["J Bahne, N Bertram, M Böcker, J Bode, J Fischer… - International Symposium on …, 2019"],"snippet":"… We removed every character but A, C, G, and T. CommonCrawl (\\(\\sigma =242,\\mathrm {avg\\_lcp}=3,995, \\mathrm {max\\_lcp}=605,632\\)), which is a crawl of the web done by the CommonCrawl Corpus (http://commoncrawl.org) without any HTML tags …","url":["https://link.springer.com/chapter/10.1007/978-3-030-32686-9_29"]}
{"year":"2019","title":"Samsung and University of Edinburgh's System for the IWSLT 2019","authors":["J Wetesko, M Chochowski, P Przybysz, P Williams… - 2019"],"snippet":"… CommonCrawl and NewsCrawl corpora we used the approach de- scribed in [5]. Two RNN language models were constructed using Marian toolkit: in-domain trained with MUST-C corpus and out-of-domain created using …","url":["https://www.zora.uzh.ch/id/eprint/176328/1/IWSLT2019_paper_34.pdf"]}
{"year":"2019","title":"Satellite System Graph: Towards the Efficiency Up-Boundary of Graph-Based Approximate Nearest Neighbor Search","authors":["C Fu, C Wang, D Cai - arXiv preprint arXiv:1907.06146, 2019"],"snippet":"Page 1. Satellite System Graph: Towards the Efficiency Up-Boundary of Graph-Based Approximate Nearest Neighbor Search Cong Fu, Changxu Wang, Deng Cai ∗ The State Key Lab of CAD&CG, College of Computer Science …","url":["https://arxiv.org/pdf/1907.06146"]}
{"year":"2019","title":"SberQuAD--Russian Reading Comprehension Dataset: Description and Analysis","authors":["P Efimov, L Boytsov, P Braslavski - arXiv preprint arXiv:1912.09723, 2019"],"snippet":"… We tokenized text using spaCy16. To initialize the embedding layer for BiDAF, DocQA, DrQA, and R-Net we use Russian case-sensitive fastText embeddings trained on Common Crawl and Wikipedia17. This initialization is used for both questions and paragraphs …","url":["https://arxiv.org/pdf/1912.09723"]}
{"year":"2019","title":"SC-UPB at the VarDial 2019 Evaluation Campaign: Moldavian vs. Romanian Cross-Dialect Topic Identification","authors":["C Onose, DC Cercel, S Trausan-Matu - Proceedings of the Sixth Workshop on NLP …, 2019"],"snippet":"… (2018), Nordic Language Processing Laboratory (NLPL) word embedding repository (Kutuzov et al., 2017) and Common Crawl (CC) word vectors (Grave et al., 2018). The relevant details for each word vector representation model can be viewed in Table 2 …","url":["https://www.aclweb.org/anthology/W19-1418"]}
{"year":"2019","title":"Scalable Cross-Lingual Transfer of Neural Sentence Embeddings","authors":["H Aldarmaki, M Diab - arXiv preprint arXiv:1904.05542, 2019"],"snippet":"… We used WMT'12 Common Crawl data for crosslingual alignment, and WMT'12 test sets for evaluations. We used the augmented SNLI data de- scribed in (Dasgupta et al., 2018) and their translations for training the mono-lingual and joint InferSent models …","url":["https://arxiv.org/pdf/1904.05542"]}
{"year":"2019","title":"SECNLP: A Survey of Embeddings in Clinical Natural Language Processing","authors":["K KS, S Sangeetha - arXiv preprint arXiv:1903.01039, 2019","KK Subramanyam, S Sivanesan - Journal of Biomedical Informatics, 2019"],"snippet":"Skip to main content Skip to article …","url":["https://arxiv.org/pdf/1903.01039","https://www.sciencedirect.com/science/article/pii/S1532046419302436"]}
{"year":"2019","title":"Security In Plain TXT","authors":["A Portier, H Carter, C Lever"],"snippet":"… These seed domains are compiled from a combination of sources, including the Alexa top 1 million, the TLD zone files for COM, NAME, NET, ORG, and BIZ, sites captured by the Common Crawl project, multiple public domain …","url":["http://www.henrycarter.org/papers/plaintxt19.pdf"]}
{"year":"2019","title":"Security Posture Based Incident Forecasting","authors":["D Mulugeta - 2019"],"snippet":"Page 1. Page 2. Page 3. Security Posture Based Incident Forecasting A Thesis Submitted to the Faculty of Drexel University by Dagmawi Mulugeta in partial fulfillment of the requirements for the degree of Master of Science June 2019 Page 4 …","url":["http://search.proquest.com/openview/a6f070655e6045b93b595adc3b0965ae/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2019","title":"See-Through-Text Grouping for Referring Image Segmentation","authors":["DJ Chen, S Jia, YC Lo, HT Chen, TL Liu - … of the IEEE International Conference on …, 2019"],"snippet":"… The representation st is visual-attended and its goodness is linked to the predicted segmentation map Pt−1. The GloVe model in our implementation is pre-trained on Common Crawl in 840B tokens. Following …","url":["http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_See-Through-Text_Grouping_for_Referring_Image_Segmentation_ICCV_2019_paper.pdf"]}
{"year":"2019","title":"Semantic Characteristics of Schizophrenic Speech","authors":["K Bar, V Zilberstein, I Ziv, H Baram, N Dershowitz… - arXiv preprint arXiv …, 2019"],"snippet":"… Specifically, we used Hebrew pretrained vectors provided by fastText (Grave et al., 2018), which were created from Wikipedia,3 as well as from other content extracted from the web with Common Crawl.4 Overall, 97% of the words in our corpus exist in fastText …","url":["https://arxiv.org/pdf/1904.07953"]}
{"year":"2019","title":"Semantic similarity measure for Thai language","authors":["P Wongchaisuwat"],"snippet":"… In this paper, pre-trained word vectors from fastText [10] and Thai2vec [1] corpus are used to compute the similarity between given words. The facebook research distributed the word vector trained on a common crawl and Wikipedia using the fastText model …","url":["https://saki.siit.tu.ac.th/isai-nlp2018/uploads_final/5__a25c56af02784c266f98ef0378499ff1/iSAI-NLP2018_0005_final.pdf"]}
{"year":"2019","title":"Semantic Textual Similarity Measures for Case-Based Retrieval of Argument Graphs","authors":["M Lenz, S Ollinger, P Sahitaj, R Bergmann - International Conference on Case-Based …, 2019"],"snippet":"… Word2vec GoogleNews 3 vectors are trained on the Google News dataset on about 100B tokens. GloVe 4 is trained on the Common Crawl dataset on 840B tokens. fastText 5 vectors are trained on Wikipedia and Common Crawl …","url":["https://link.springer.com/chapter/10.1007/978-3-030-29249-2_15"]}
{"year":"2019","title":"Semi-supervised machine learning with word embedding for classification in price statistics","authors":["H Martindale, E Rowland, T Flower - 16th Meeting of the Ottawa Group on Price …, 2019"],"snippet":"Page 1. Office for National Statistics 1 Semi-supervised machine learning with word embedding for classification: April 2019 26/04/2019 Semi-supervised machine learning with word embedding for classification in price statistics …","url":["https://eventos.fgv.br/sites/eventos.fgv.br/files/arquivos/u161/semi-supervised_ml_for_price_stats-ottawa_group.pdf"]}
{"year":"2019","title":"Semi-supervised Neural Machine Translation via Marginal Distribution Estimation","authors":["Y Wang, Y Xia, L Zhao, J Bian, T Qin, E Chen, TY Liu - IEEE/ACM Transactions on …, 2019"],"snippet":"Page 1. 2329-9290 (c) 2019 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/8732422/"]}
{"year":"2019","title":"SENPAI: Supporting Exploratory Text Analysis through Semantic & Syntactic Pattern Inspection","authors":["M Samory, T Mitra - 2019"],"snippet":"… lemmatization, so as to remove surface form variations which do not alter the meaning of a word, eg the lemma for both “moved” and “moves” is “move.” Then, we encode lemmas with the corresponding 300-dimensional word …","url":["http://people.cs.vt.edu/tmitra/public/papers/icwsm19-SENPAI.pdf"]}
{"year":"2019","title":"Sense disambiguation for Punjabi language using supervised machine learning techniques","authors":["VP Singh, P Kumar - Sādhanā, 2019"],"snippet":"… The character n-grams of length 5 have been applied to words in window of size 5 with 10 negative samples [10]. It has been trained on the Punjabi Wikipedia and the raw web data fetched by common crawl method. 6 Working of WSD System for Punjabi language …","url":["https://link.springer.com/article/10.1007/s12046-019-1206-x"]}
{"year":"2019","title":"Sentence and Word Weighting for Neural Machine Translation Domain Adaptation","authors":["PP Chen"],"snippet":"Page 1. Sentence and Word Weighting for Neural Machine Translation Domain Adaptation Pinzhen (Patrick) Chen Undergraduate Dissertation Artificial Intelligence and Software Engineering School of Informatics The …","url":["https://project-archive.inf.ed.ac.uk/ug4/20191530/ug4_proj.pdf"]}
{"year":"2019","title":"Sentence Classification and Information Retrieval for Petroleum Engineering","authors":["TF Ferraz, GABA Ferreira, FG Cozman, I Santos"],"snippet":"… Accordingly, we used a word embedding representation in order to represent the words as vectors and then be able to define and compute distances between terms. We used a pre-trained embedding model called ”Common Crawl” [Pennington et al. 2014] …","url":["http://www.bracis2019.ufba.br/Camera_Ready/199118_1.pdf"]}
{"year":"2019","title":"Sentence Mover's Similarity: Automatic Evaluation for Multi-Sentence Texts","authors":["E Clark, A Celikyilmaz, NA Smith"],"snippet":"… We obtain GloVe embeddings, which are type-based, 300-dimensional embeddings trained on Common Crawl,9 using spaCy,10 while the ELMo em- beddings are character-based, 1,024-dimensional, contextual …","url":["https://homes.cs.washington.edu/~nasmith/papers/clark+celikyilmaz+smith.acl19.pdf"]}
{"year":"2019","title":"Sentence-Level Content Planning and Style Specification for Neural Text Generation","authors":["X Hua, L Wang - arXiv preprint arXiv:1909.00734, 2019"],"snippet":"… Statistics are shown in Table 1. Input Keyphrases and Label Construction. To obtain the input keyphrase candidates and their sentence-level selection labels, we first construct queries to retrieve passages from Wikipedia and news articles collected from commoncrawl …","url":["https://arxiv.org/pdf/1909.00734"]}
{"year":"2019","title":"Sentiment Analysis","authors":["D Sarkar - Text Analytics with Python, 2019"],"snippet":"In this chapter, we cover one of the most interesting and widely used aspects pertaining to natural language processing (NLP), text analytics, and machine learning. The problem at hand is sentiment...","url":["https://link.springer.com/chapter/10.1007/978-1-4842-4354-1_9"]}
{"year":"2019","title":"Separate Chaining Meets Compact Hashing","authors":["D Köppl - arXiv preprint arXiv:1905.00163, 2019"],"snippet":"Page 1. Separate Chaining Meets Compact Hashing Dominik Köppl Department of Informatics, Kyushu University, Japan Society for Promotion of Science Abstract While separate chaining is a common strategy for resolving …","url":["https://arxiv.org/pdf/1905.00163"]}
{"year":"2019","title":"Sequence Labeling to Detect Stuttering Events in Read Speech","authors":["S Alharbi, M Hasan, AJH Simons, S Brumfitt, P Green - Computer Speech & …, 2019"],"snippet":"… In the present study, we used a pre-trained GloVe model to generate word embeddings for each utterance. This model was trained on the Common Crawl (CC) corpus (1.9 M vocab) Pennington et al. (2014). 6. Automatic Speech Recognition System …","url":["https://www.sciencedirect.com/science/article/pii/S0885230819302967"]}
{"year":"2019","title":"Sequence Time Expression Recognition in the Spanish Clinical Narrative","authors":["A Ruiz-de-la-Cuadra, JL López-Cuadrado… - 2019 IEEE 32nd …, 2019"],"snippet":"… embedding (Table 1). Name Training Words Size Resource Glo200Ve Non-zero entries [37] 840 B 300 Common Crawl Spanish Billion Word [38] Word2Vec [39] 1.5 B 300 Sensem, Ancora Corpus, OPUS Project, etc. EVEX Word2Vec …","url":["https://ieeexplore.ieee.org/abstract/document/8787434/"]}
{"year":"2019","title":"Sequence-to-sequence Pre-training with Data Augmentation for Sentence Rewriting","authors":["Y Zhang, T Ge, F Wei, M Zhou, X Sun - arXiv preprint arXiv:1909.06002, 2019"],"snippet":"… Specifically, for a correct sentence, a back translation model trained with the public GEC data first generates 10 best outputs; then a 5-gram language model (JunczysDowmunt and Grundkiewicz, 2016) trained on Common …","url":["https://arxiv.org/pdf/1909.06002"]}
{"year":"2019","title":"Sequential Attention-based Network for Noetic End-to-End Response Selection","authors":["Q Chen, W Wang - arXiv preprint arXiv:1901.02609, 2019"],"snippet":"… Embedding Training corpus #Words glove.6B.300d Wikipedia + Gigaword 0.4M glove.840B.300d Common Crawl 2.2M glove.twitter.27B.200d Twitter 1.2M … 1.0M crawl-300d-2M.vec Common Crawl 2.0M word2vec.300d Linux manual pages 0.3M …","url":["https://arxiv.org/pdf/1901.02609"]}
{"year":"2019","title":"Sequential Matching Model for End-to-end Multi-turn Response Selection","authors":["Q Chen, W Wang - ICASSP 2019-2019 IEEE International Conference on …, 2019"],"snippet":"… Re- sults on the Ubuntu development set are shown in Table 3. We can see that word2vec embedding trained on the training dataset achieves better results than Fasttext [23] embedding trained on the unlabeled corpus …","url":["https://ieeexplore.ieee.org/abstract/document/8682538/"]}
{"year":"2019","title":"Sequential transfer learning in NLP for text summarization","authors":["P Fecht"],"snippet":"… With W and ˜W, the model generates two sets of word vectors which are supposed to perform equally if X is symmetric [64]. The GloVe model has been trained on varying sized datasets from one up to 42 billion (Common Crawl) tokens of data …","url":["https://www.inovex.de/fileadmin/files/Fachartikel_Publikationen/Theses/sequential-transfer-learning-in-nlp-for-text-summarization-pascal-fecht-2019.pdf"]}
{"year":"2019","title":"Should John Be More Likely A Physician Than Lisa: Bias-Performance Trade-Off for Gendered Pronoun Resolution","authors":["S Goel, J Li, H Zheng"],"snippet":"… the female gendered words. For our case, we are using the pre-trained Glove6 (these cotain 840B tokens and are trained on the Common Crawl corpus) embeddings to get the hard-debiased em- beddings. To obtain these …","url":["https://shivankgoel.github.io/notes/ds/Gendered_Pronoun_Resolution.pdf"]}
{"year":"2019","title":"Similarity Driven Approximation for Text Analytics","authors":["G Hu, Y Zhang, S Rigo, TD Nguyen - arXiv preprint arXiv:1910.07144, 2019"],"snippet":"… For example, the Google Books Ngram data set contains 2.2 TB of data [1], and the Common Crawl corpus contains petabytes of data [2]. Processing such large text data sets can be computationally expensive, especially if it involves sophisticated algorithms …","url":["https://arxiv.org/pdf/1910.07144"]}
{"year":"2019","title":"Situating Sentence Embedders with Nearest Neighbor Overlap","authors":["LH Lin, NA Smith - arXiv preprint arXiv:1909.10724, 2019"],"snippet":"… GloVe average 100 Wikipedia 2014 + Gigaword 5 (6B tokens, uncased) 300 Wikipedia 2014 + Gigaword 5 (6B tokens, uncased) 300 Common Crawl (840B tokens, cased) FastText average 300 Wikipedia + UMBC + statmt.org …","url":["https://arxiv.org/pdf/1909.10724"]}
{"year":"2019","title":"Six dimensions describe action understanding: the ACT-FASTaxonomy","authors":["MA Thornton, D Tamir, PS Hall - PsyArXiv. June, 2019"],"snippet":"… different algorithm. For the present purposes, we used a pre-trained version of GloVe based on the Common Crawl: a set of 840 billion tokens generated by scraping the entire web. For model comparison, we derived an …","url":["https://psyarxiv.com/gt6bw/download/?format=pdf"]}
{"year":"2019","title":"SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization","authors":["H Jiang, P He, W Chen, X Liu, J Gao, T Zhao - arXiv preprint arXiv:1911.03437, 2019"],"snippet":"… For example, the well-known “Common Crawl project” is producing text data extracted from web pages at a rate of about 20TB per month. The resulting extremely large text corpus allows us to train extremely large neural network-based general language models …","url":["https://arxiv.org/pdf/1911.03437"]}
{"year":"2019","title":"Social Relation Extraction from Chatbot Conversations: A Shortest Dependency Path Approach","authors":["M Glas - SKILL 2019-Studierendenkonferenz Informatik, 2019"],"snippet":"… The dictionary used here, 5 https://github.com/zalandoresearch/flair 6 https://spacy.io/ 7 http://commoncrawl.org/ 8 https://catalog.ldc.upenn.edu/LDC2013T19 Page 8. 8 Markus Glas Fig. 3: Example of a dependency path within a sentence containing two entities …","url":["https://dl.gi.de/bitstream/handle/20.500.12116/28989/SKILL2019-01.pdf?sequence=1"]}
{"year":"2019","title":"Social Sensing for Improving the User Experience in Orienteering","authors":["F Persia, S Helmer, S Pugacs, G Pilato - 2019 IEEE 13th International Conference on …, 2019"],"snippet":"… ing. In particular, we have used the spaCy “en core web md” language model, which is an “English multi-task Convolutional Neural Network trained on OntoNotes [34], with GloVe [35] vectors trained on Common Crawl [36]” …","url":["https://ieeexplore.ieee.org/abstract/document/8665498/"]}
{"year":"2019","title":"SOK: A Comprehensive Reexamination of Phishing Research from the Security Perspective","authors":["A Das, S Baki, AE Aassal, R Verma, A Dunbar - arXiv preprint arXiv:1911.00953, 2019"],"snippet":"Page 1. REEXAMINING PHISHING RESEARCH 1 SOK: A Comprehensive Reexamination of Phishing Research from the Security Perspective Avisha Das, Shahryar Baki, Ayman El Aassal, Rakesh Verma, and Arthur Dunbar …","url":["https://arxiv.org/pdf/1911.00953"]}
{"year":"2019","title":"Sparse Victory–A Large Scale Systematic Comparison of Count-Based and Prediction-Based Vectorizers for Text Classification","authors":["R Chakraborty, K Arora, A Elhence"],"snippet":"… Corpus (100 billion words). For greater ease of comparison both the GloVe and fastText models have a dimension of 300 and have been trained on the Common Crawl Corpus (640 billion words). The ELMo embedding has …","url":["https://acl-bg.org/proceedings/2019/RANLP%202019/pdf/RANLP022.pdf"]}
{"year":"2019","title":"ST-Sem: A Multimodal Method for Points-of-Interest Classification Using Street-Level Imagery","authors":["SS Noorian, A Psyllidis, A Bozzon - International Conference on Web Engineering, 2019"],"snippet":"… representing each word as a bag of character n-grams. We use pre-trained word vectors for 2 languages (English and German), trained on Common Crawl and Wikipedia 6 . According to the detected language l, the corresponding pre …","url":["https://link.springer.com/chapter/10.1007/978-3-030-19274-7_3"]}
{"year":"2019","title":"STAR-GCN: Stacked and Reconstructed Graph Convolutional Networks for Recommender Systems","authors":["J Zhang, X Shi, S Zhao, I King - arXiv preprint arXiv:1905.13129, 2019"],"snippet":"… For movie features, we concatenate the title name, release year, and one-hot encoded genres. We process title names by averaging the off-the-shelf 300-dimensional GloVe CommonCrawl word vector [Pennington et al., 2014] of each word …","url":["https://arxiv.org/pdf/1905.13129"]}
{"year":"2019","title":"STD: An Automatic Evaluation Metric for Machine Translation Based on Word Embeddings","authors":["P Li, C Chen, W Zheng, Y Deng, F Ye, Z Zheng - IEEE/ACM Transactions on Audio …, 2019"],"snippet":"… H and M are their means respectively. The word embedding used in our STD implementation is the freely-available fastText word embedding1 [11], which has 2 million word vectors trained on Common Crawl (600B tokens) …","url":["https://ieeexplore.ieee.org/abstract/document/8736840/"]}
{"year":"2019","title":"Streaming Infrastructure and Natural Language Modeling with Application to Streaming Big Data","authors":["Y Du - 2019"],"snippet":"… In our research, we try to find an alternative resource to study such data. Common Crawl is a massive multi-petabyte dataset hosted by Amazon. It contains archived HTML web page data from 2008 to date. Common …","url":["https://tigerprints.clemson.edu/all_dissertations/2329/"]}
{"year":"2019","title":"Structured Two-Stream Attention Network for Video Question Answering","authors":["L Gao, P Zeng, J Song, YF Li, W Liu, T Mei, HT Shen - Proceedings of the AAAI …, 2019"],"snippet":"… consisting of M words, is first converted into a sequence Q = {qm}M m=1, where qm is a one-hot vector representing the word at position m. Next, we employ the word embedding GloVe (Pennington, Socher, and Manning …","url":["https://www.aaai.org/ojs/index.php/AAAI/article/view/4602/4480"]}
{"year":"2019","title":"Study of Tibetan Text Classification based on fastText","authors":["W Ma, H Yu, J Ma - 3rd International Conference on Computer Engineering …"],"snippet":"… Every single text in all data is a line, and the \"__label__ + tag\" is added at the beginning of each line. Pre-training data set: fastText publishes word vectors in 157 languages [13], which are trained on Common Crawl and Wikipedia using fastText …","url":["https://download.atlantis-press.com/article/125913150.pdf"]}
{"year":"2019","title":"SUBMISSION OF WRITTEN WORK","authors":["O ERSITY, F CO"],"snippet":"Page 1. IT U N IV ERSITY O F CO PEN H A G EN SUBMISSION OF WRITTEN WORK Class code: Name of course: Course manager: Course e-portfolio: Thesis or project title: Supervisor: Full Name: Birthdate (dd/mm-yyyy) …","url":["http://www.derczynski.com/itu/docs/Multilingual%20hate%20speech%20detection.pdf","https://www.derczynski.com/itu/docs/Multilingual%20hate%20speech%20detection.pdf"]}
{"year":"2019","title":"Subword-based Compact Reconstruction of Word Embeddings","authors":["S Sasaki, J Suzuki, K Inui - Proceedings of the 2019 Conference of the North …, 2019"],"snippet":"… or embedding vectors), especially those trained on a vast amount of text data, such as the Common Crawl (CC) cor … word embeddings trained from GloVe.840B and fastText.600B are available: https://github.com/losyer …","url":["https://www.aclweb.org/anthology/N19-1353"]}
{"year":"2019","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems","authors":["A Wang, Y Pruksachatkun, N Nangia, A Singh…"],"snippet":"… We also include a baseline where for each task we simply predict the majority class, as well as a bag-of-words baseline where each input is represented as an average of its tokens' GloVe word vectors (300-dimensional and trained …","url":["https://w4ngatang.github.io/static/papers/superglue.pdf"]}
{"year":"2019","title":"Supervised Multimodal Bitransformers for Classifying Images and Text","authors":["D Kiela, S Bhooshan, H Firooz, D Testuggine - arXiv preprint arXiv:1909.02950, 2019"],"snippet":"… We describe each of the baselines in more detail below. • Bag of words (Bow) We sum 300-dimensional GloVe embeddings (Pennington, Socher, and Manning 2014) (trained on Common Crawl) for all words in the text …","url":["https://arxiv.org/pdf/1909.02950"]}
{"year":"2019","title":"Supplementary Material for “Multi-task Learning of Hierarchical Vision-Language Representation”","authors":["DK Nguyen, T Okatani"],"snippet":"… Questions and captions were tokenized using Python Natural Language Toolkit (nltk) [2]. We used the vocabulary provided by the CommonCrawl-840B GloVe model for English word vectors [8], and set out-of-vocabulary words to unk …","url":["https://pdfs.semanticscholar.org/83a6/fd8eadd36c22bdac861bd2b20aba87968c3d.pdf"]}
{"year":"2019","title":"Survey on Publicly Available Sinhala Natural Language Processing Tools and Research","authors":["N de Silva - arXiv preprint arXiv:1906.02358, 2019"],"snippet":"… [21] further provided two monolingual corpora for Sinhala. Those were a 155k+ sentences of filtered Sinhala Wikipedia8 and 5178k+ sentences of Sinhala common crawl9. 2.2 Data Sets Specific data sets for Sinhala, as expected. is scarce …","url":["https://arxiv.org/pdf/1906.02358"]}
{"year":"2019","title":"Synchronous Bidirectional Neural Machine Translation","authors":["L Zhou, J Zhang, C Zong - Transactions of the Association for Computational …, 2019"],"snippet":"Create a new account. Email. Returning user. Can't sign in? Forgot your password? Enter your email address below and we will send you the reset instructions. Email. Cancel. If the address matches an existing account you will …","url":["https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00256"]}
{"year":"2019","title":"Syntactic dependencies correspond to word pairs with high mutual information","authors":["R Futrell, P Qian, E Gibson, E Fedorenko, IA Blank"],"snippet":"… 2.5 Dataset We use the Common Crawl corpus (Buck et al., 2014) of English web text … Entropy, 19:275–307. Buck, C., Heafield, K., and Van Ooyen, B. (2014). N-gram counts and language models from the common crawl. In …","url":["http://socsci.uci.edu/~rfutrell/papers/futrell2019syntactic.pdf"]}
{"year":"2019","title":"Syntactically Supervised Transformers for Faster Neural Machine Translation","authors":["N Akoury, K Krishna, M Iyyer - arXiv preprint arXiv:1906.02780, 2019"],"snippet":"… For English-German, we evaluate on WMT 2014 En↔De as well as IWSLT 2016 En→De, while for English-French we train on the Europarl / Common Crawl subset of the full WMT 2014 En→Fr data and evaluate over the full dev/test sets …","url":["https://arxiv.org/pdf/1906.02780"]}
{"year":"2019","title":"Syntax-aware Multilingual Semantic Role Labeling","authors":["S He, Z Li, H Zhao - arXiv preprint arXiv:1909.00310, 2019"],"snippet":"… The pre-trained word em- bedding is 100-dimensional GloVe vectors (Pennington et al., 2014) for English, 300-dimensional fastText vectors (Grave et al., 2018) trained on Common Crawl and Wikipedia for other languages …","url":["https://arxiv.org/pdf/1909.00310"]}
{"year":"2019","title":"Syntax-Aware Sentence Matching with Graph Convolutional Networks","authors":["Y Lei, Y Hu, X Wei, L Xing, Q Liu - International Conference on Knowledge Science …, 2019"],"snippet":"… 4.2 Experiment Setting. In order to compare with the baseline, we use the same setting as BiMPM. We initialize word embeddings in the word representation layer with the 300-dimensional GloVe word vectors …","url":["https://link.springer.com/chapter/10.1007/978-3-030-29563-9_31"]}
{"year":"2019","title":"System and method for chat community question answering","authors":["N Londhe, S Kannan, N Bojja - US Patent App. 16/272,142, 2019"],"snippet":"US20190260694A1 - System and method for chat community question answering - Google Patents. System and method for chat community question answering. Download PDF Info. Publication number US20190260694A1. US20190260694A1 …","url":["https://patentimages.storage.googleapis.com/0c/f5/b6/7687c26806b141/US20190260694A1.pdf"]}
{"year":"2019","title":"System and method for concise display of query results via thumbnails with indicative images and differentiating terms","authors":["TP O'hara - US Patent 10,459,999, 2019"],"snippet":"… grams). In the case of a meta-search engine without access to the underlying indexes, one approach is to use data from the Common Crawl to derive global n-gram counts for TF-IDF and language modeling filtering. Another …","url":["http://www.freepatentsonline.com/10459999.html"]}
{"year":"2019","title":"System for creating a reasoning graph and for ranking of its nodes","authors":["B Agapiev - US Patent App. 15/793,751, 2019"],"snippet":"… View, Calif. and Common Crawl Foundation of Beverly Hills, Calif.) are processed (20) to identify statements of causal relationships (22, 24), which are then analyzed to extract causes and associated effect pairs (26). These …","url":["https://patentimages.storage.googleapis.com/ca/d2/fd/8b3a7f8fa4ec15/US20190073420A1.pdf"]}
{"year":"2019","title":"TüBa-D/DP Stylebook","authors":["D de Kok, S Pütz - 2019"],"snippet":"… Table 1: Subcorpora of the TüBa-D/DP. Subcorpus Genre Sentences Tokens Europarl Parliamentary proceedings 2.2M 55M taz (1986-2009) Newspaper 29.9M 393.7M Wikipedia (2019) Encyclopedia 42.2M …","url":["https://sfb833-a3.github.io/tueba-ddp/stylebook/stylebook-r4.pdf"]}
{"year":"2019","title":"TabbyXL: Rule-Based Spreadsheet Data Extraction and Transformation","authors":["A Shigarov, V Khristyuk, A Mikhailov, V Paramonov - International Conference on …, 2019"],"snippet":"… a spreadsheet-like format. Barik et al. [2] extracted 0.25M unique spreadsheets from Common Crawl 1 archive. Chen and Cafarella [6] reported about 0.4M spreadsheets of ClueWeb09 Crawl 2 archive. Spreadsheets can be …","url":["https://link.springer.com/chapter/10.1007/978-3-030-30275-7_6"]}
{"year":"2019","title":"Tackling Graphical NLP problems with Graph Recurrent Networks","authors":["L Song - 2019"],"snippet":"Page 1. Tackling Graphical NLP problems with Graph Recurrent Networks by Linfeng Song Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Supervised by Professor Daniel Gildea Department of Computer Science …","url":["https://www.cs.rochester.edu/~lsong10/papers/Linfeng_Song_PhD_thesis.pdf"]}
{"year":"2019","title":"TARGER: Neural Argument Mining at Your Fingertips","authors":["A Chernodub, O Oliynyk, P Heidenreich, A Bondarenko…"],"snippet":"… Our background collection for the retrieval of argumentative sentences is formed by the DepCC corpus (Panchenko et al., 2018), a linguistically pre-processed subset of the Common Crawl containing 14.3 … Building a …","url":["https://webis.de/downloads/publications/papers/bondarenko_2019b.pdf"]}
{"year":"2019","title":"Task definition, annotated dataset, and supervised natural language processing models for symptom extraction from unstructured clinical notes","authors":["JM Steinkamp, W Bala, A Sharma, JJ Kantrowitz - Journal of Biomedical Informatics, 2019"],"snippet":"… Our word embeddings consisted of 300-dimensional Global Vectors (GloVe) [35] trained on the web common crawl data set concatenated with 300-dimensional custom trained FastText [28] vectors trained on the entirety …","url":["https://www.sciencedirect.com/science/article/pii/S153204641930276X"]}
{"year":"2019","title":"Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents","authors":["A Nowak, P Kunstman - arXiv preprint arXiv:1901.02081, 2019"],"snippet":"… The model architecture is shown in Figure 3. Embeddings layer: Each token is represented by 1452 dimensional vector, consisting of: • 300-dimensional GloVe (Pennington et al., 2014) embedding (cased, trained on 840B tokens from Common Crawl) …","url":["https://arxiv.org/pdf/1901.02081"]}
{"year":"2019","title":"Techniques for Inverted Index Compression","authors":["GE Pibiri, R Venturini - arXiv preprint arXiv:1908.10598, 2019"],"snippet":"Page 1. Techniques for Inverted Index Compression GIULIO ERMANNO PIBIRI, ISTI-CNR, Italy ROSSANO VENTURINI, University of Pisa, Italy The data structure at the core of large-scale search engines is the inverted index …","url":["https://arxiv.org/pdf/1908.10598"]}
{"year":"2019","title":"Tell me you can read me","authors":["CE SUM, T THEOR"],"snippet":"Page 55. Complying with the obligation of transparency imposes indeed on the data controller the prior obligation to determine–deliberately or not, consciously or not–who are the targeted data subjects, and what are they supposed to find intelligible and easily accessible …","url":["https://pdfs.semanticscholar.org/8c2a/8c105a49e59c457c68b8390b49694c4c4c20.pdf#page=55"]}
{"year":"2019","title":"Temporal Context-Aware Representation Learning for Question Routing","authors":["X Zhang, W Cheng, B Zong, Y Chen, J Xu, D Li…"],"snippet":"… The state-of-the-art document embedding model, InferSent [3], is applied to compute the similarity between questions. We use the pre-trained 300-dimensional word vectors from fastText[19], which is trained on Common Crawl containing 600B tokens …","url":["https://xuczhang.github.io/papers/wsdm20_tcqr.pdf"]}
{"year":"2019","title":"Temporally Grounding Language Queries in Videos by Contextual Boundary-aware Prediction","authors":["J Wang, L Ma, W Jiang - arXiv preprint arXiv:1909.05010, 2019"],"snippet":"… 2015) features are adopted for all compared methods. Each word from the query is represented by GloVe (Pennington, Socher, and Manning 2014) word embedding vectors pre-trained on Common Crawl. We set hidden neuron size of LSTM to 512 …","url":["https://arxiv.org/pdf/1909.05010"]}
{"year":"2019","title":"Text Classification Using SVM Enhanced by Multithreading and CUDA","authors":["S Chatterjee, PG Jose, D Datta - International Journal of Modern Education and …, 2019"],"snippet":"Page 1. IJ Modern Education and Computer Science, 2019, 1, 11-23 Published Online January 2019 in MECS (http://www.mecs-press.org/) DOI: 10.5815/ijmecs.2019.01.02 Copyright © 2019 MECS IJ Modern Education and Computer Science, 2019, 1, 11-23 …","url":["http://search.proquest.com/openview/ab6d5a2cbbb23e2cba642a09784b043e/1?pq-origsite=gscholar&cbl=2026674"]}
{"year":"2019","title":"Text Corpus for NLP","authors":["C Room"],"snippet":"… Sep 2019. Common Crawl publishes 240 TiB of uncompressed data from 2.55 billion web pages. Of these, 1 billion URLs were not present in previous crawls. Common Crawl started in 2008. In 2013, they moved from ARC to Web ARChive (WARC) file format …","url":["https://devopedia.org/text-corpus-for-nlp"]}
{"year":"2019","title":"TEXT QUALITY EVALUATION METHODS AND PROCESSES","authors":["AA Pala, A Kagoshima, M Tober - US Patent App. 15/863,408, 2019"],"snippet":"… In one possible implementation, the reference text 2000 can be parts, or the complete version, of Wikipedia, for a given language, or one or more books, or Common Crawl, or any other corpus that consists of human-written high quality text …","url":["http://www.freepatentsonline.com/y2019/0213247.html"]}
{"year":"2019","title":"The AFRL WMT19 Systems: Old Favorites and New Tricks","authors":["J Gwinnup, G Erdmann, T Anderson - Proceedings of the Fourth Conference on …, 2019"],"snippet":"… Corpus Total Retained CommonCrawl 723,256 655,069 newscommentary 290,866 264,089 Yandex 1,000,000 901,307 ParaCrawl 12,061,155 5,173,675 UN2016 11,365,709 9,871,406 Total Lines 25,440,968 16,865,546 …","url":["https://www.aclweb.org/anthology/W19-5318"]}
{"year":"2019","title":"The BEA-2019 Shared Task on Grammatical Error Correction","authors":["C Bryant, M Felice, ØE Andersen, T Briscoe - … Workshop on Innovative Use of NLP for …, 2019"],"snippet":"Page 1. Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52–75 Florence, Italy, August 2, 2019. c 2019 Association for Computational Linguistics 52 The BEA-2019 …","url":["https://www.aclweb.org/anthology/W19-4406"]}
{"year":"2019","title":"The BLCU System in the BEA 2019 Shared Task","authors":["L Yang, C Wang - Proceedings of the Fourteenth Workshop on Innovative …, 2019"],"snippet":"… JunczysDowmunt and Grundkiewicz (2016); JunczysDowmunt et al. (2018) utilize the Common Crawl corpus to train the language model and pre-train part of the NMT model. Inspired by these studies, we also try to use a monolingual corpus for data augmentation …","url":["https://www.aclweb.org/anthology/W19-4421"]}
{"year":"2019","title":"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings","authors":["AC Kozlowski, M Taddy, JA Evans - American Sociological Review, 2019"],"snippet":"We argue word embedding models are a useful tool for the study of culture using a historical analysis of shared understandings of social class as an empirical case. Word embeddings represent semant...","url":["https://journals.sagepub.com/doi/abs/10.1177/0003122419877135"]}
{"year":"2019","title":"The impact of individual audit partners on their clients' narrative disclosures","authors":["C Mauritz, M Nienhaus, C Oehler - 2019"],"snippet":"Page 1. The impact of individual audit partners on their clients' narrative disclosures ∗ Christoph Mauritz1, Martin Nienhaus2, and Christopher Oehler2 1University of Münster 2Goethe-University Frankfurt September 5, 2019 Abstract …","url":["http://www.geaba.de/wp-content/uploads/2019/09/Mauritz-Nienhaus-Oehler_2.pdf"]}
{"year":"2019","title":"The LAIX Systems in the BEA-2019 GEC Shared Task","authors":["R Li, C Wang, Y Zha, Y Yu, S Guo, Q Wang, Y Liu… - … on Innovative Use of NLP for …, 2019"],"snippet":"… Table 1 lists the data sets used in Restricted Track and Unrestricted Track, including FCE (Yannakoudakis et al., 2011), Lang-82 (Mizumoto et al., 2012), NUCLE (Ng et al., 2014), W&I+LOCNESS (Bryant et al., 2019) and Com …","url":["https://www.aclweb.org/anthology/W19-4416"]}
{"year":"2019","title":"The LIG system for the English-Czech Text Translation Task of IWSLT 2019","authors":["L Vial, B Lecouteux, D Schwab, H Le, L Besacier - arXiv preprint arXiv:1911.02898, 2019"],"snippet":"… C is a speech translation corpus of TED talks, similar to the test data of the task, and we added the News Commentary corpus, which consists of political and economic commentaries, be- cause it was the second smallest corpus …","url":["https://arxiv.org/pdf/1911.02898"]}
{"year":"2019","title":"The Linked Open Data cloud is more abstract, flatter and less linked than you may think!","authors":["L Asprino, W Beek, P Ciancarini, F van Harmelen… - arXiv preprint arXiv …, 2019"],"snippet":"… The two largest available crawls of LOD that are available today are WebDataCommons and LOD-a-lot. WebDataCommons5 [12] consists of ∼31B triples that have been extracted from the CommonCrawl datasets (November 2018 version) …","url":["https://arxiv.org/pdf/1906.08097"]}
{"year":"2019","title":"The NiuTrans Machine Translation Systems for WMT19","authors":["B Li, Y Li, C Xu, Y Lin, J Liu, H Liu, Z Wang, Y Zhang…"],"snippet":"… For EN↔RU, we used the following resource provided by WMT, including News Commentaryv14, ParaCrawl-v3, CommonCrawl and Yandex … corpus via random samplimng from 2M monolingual data selected by Xenc in the …","url":["http://nlplab.com/members/xiaotong_files/2019-wmt.pdf"]}
{"year":"2019","title":"The Quest to Automate Fact-checking","authors":["C Li"],"snippet":"… The model contains 300-dimensional vectors for 3 million words and phrases. https://code.google.com/archive/p/word2vec/ 2: Global Vectors for Word Representation using The Common Crawl corpus which contains …","url":["https://pdfs.semanticscholar.org/13e0/ef9f40c767060b510e2aa75740a3eda60ad4.pdf"]}
{"year":"2019","title":"The relationship between implicit intergroup attitudes and beliefs","authors":["B Kurdi, TC Mann, TES Charlesworth, MR Banaji - Proceedings of the National …, 2019"],"snippet":"Skip to main content. Submit; About: Editorial Board; PNAS Staff; FAQ; Rights and Permissions; Site Map. Contact; Journal Club; Subscribe: Subscription Rates; Subscriptions FAQ; Open Access; Recommend PNAS to Your …","url":["https://www.pnas.org/content/early/2019/02/26/1820240116.short"]}
{"year":"2019","title":"The RWTH Aachen University Machine Translation Systems for WMT 2019","authors":["J Rosendahl, C Herold, Y Kim, M Graça, W Wang… - Proceedings of the Fourth …, 2019"],"snippet":"… For De→En, we use data from CommonCrawl, Europarl, NewsCommentary and Rapid … (2017)), but without tied embedding weights, on the data from CommonCrawl, Europarl, NewsCommentary and Rapid ie about 6M sentence pairs …","url":["https://www.aclweb.org/anthology/W19-5338"]}
{"year":"2019","title":"The Semantic Web: Two Decades On","authors":["A Hogan"],"snippet":"Page 1. Semantic Web 0 (0) 1 1 IOS Press 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 11 11 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 29 30 30 31 31 32 32 33 …","url":["http://www.semantic-web-journal.net/system/files/swj2303.pdf"]}
{"year":"2019","title":"The Source-Target Domain Mismatch Problem in Machine Translation","authors":["J Shen, PJ Chen, M Le, J He, J Gu, M Ott, M Auli… - arXiv preprint arXiv …, 2019"],"snippet":"… For Myanmar monolingual data, we use the language split Commoncrawl data from (Buck et al., 2014) which includes texts in various domains crawled from the web. We use the myanmar-tools2 library to classify and convert all Zawgyi text to Unicode …","url":["https://arxiv.org/pdf/1909.13151"]}
{"year":"2019","title":"The TALP-UPC Machine Translation Systems for WMT19 News Translation Task: Pivoting Techniques for Low Resource MT","authors":["N Casas, JAR Fonollosa, C Escolano, C Basta… - Proceedings of the Fourth …, 2019"],"snippet":"… 4.2 English-Russian The available parallel English-Russian corpora for the shared task included News Commentary v14, Wiki Titles v1, Common Crawl corpus, ParaCrawl v3, Yandex Corpus and the United Nations Parallel Corpus v1.0 (Ziemski et al., 2016) …","url":["https://www.aclweb.org/anthology/W19-5311"]}
{"year":"2019","title":"The Universitat d'Alacant submissions to the English-to-Kazakh news translation task at WMT 2019","authors":["VM Sánchez-Cartagena, JA Pérez-Ortiz…"],"snippet":"… 556 corpus lang. raw cleaned News Crawl kk 783k 783k Wiki dumps kk 1.7M 1.7M Common Crawl kk 10.9M 5.4M News Crawl en 200M 200M … The same filtering was applied to the monolingual Kazakh Common Crawl corpus …","url":["https://www.dlsi.ua.es/~fsanchez/pub/pdf/sanchez-cartagena19a.pdf"]}
{"year":"2019","title":"The University of Helsinki submissions to the WMT19 news translation task","authors":["A Talman, U Sulubacak, R Vázquez, Y Scherrer… - arXiv preprint arXiv …, 2019"],"snippet":"… removing all sentence pairs with a length difference ratio above a certain threshold: for CommonCrawl, ParaCrawl and Rapid we used a threshold of 3, for WikiTitles a threshold of 2, and for all other data sets a threshold of 9; …","url":["https://arxiv.org/pdf/1906.04040"]}
{"year":"2019","title":"The University of Sydney's Machine Translation System for WMT19","authors":["L Ding, D Tao - arXiv preprint arXiv:1907.00494, 2019"],"snippet":"… 3 Data Preparation We used all available parallel corpus 3 for Finnish→ English except the “Wiki Headlines” due to the large number of incomplete sentences, and for monolingual target side English data, we selected all …","url":["https://arxiv.org/pdf/1907.00494"]}
{"year":"2019","title":"The Web is missing an essential part of infrastructure: an Open Web Index","authors":["D Lewandowski - arXiv preprint arXiv:1903.03846, 2019"],"snippet":"… A search engine needs to keep its index current, meaning it needs to update at least a part of it every minute. This is an important requirement that is not being met by any of the current projects (like Common Crawl) …","url":["https://arxiv.org/pdf/1903.03846"]}
{"year":"2019","title":"TiFi: Taxonomy Induction for Fictional Domains [Extended version]","authors":["CX Chu, S Razniewski, G Weikum - arXiv preprint arXiv:1901.10263, 2019"],"snippet":"Page 1. TiFi: Taxonomy Induction for Fictional Domains [Extended version] ∗ Cuong Xuan Chu Max Planck Institute for Informatics Saarbrücken, Germany cxchu@mpi-inf. mpg.de Simon Razniewski Max Planck Institute for Informatics …","url":["https://arxiv.org/pdf/1901.10263"]}
{"year":"2019","title":"TLR at BSNLP2019: A Multilingual Named Entity Recognition System","authors":["JG Moreno, EL Pontes, M Coustaty, A Doucet - Proceedings of the 7th Workshop on …, 2019"],"snippet":"… in Figure 1. 3.1 FastText Embedding In this layer, we used pre-trained embeddings for each language trained on Common Crawl and Wikipedia using fastText (Bojanowski et al., 2017; Grave et al., 2018). These models were …","url":["https://www.aclweb.org/anthology/W19-3711"]}
{"year":"2019","title":"TMU Transformer System Using BERT for Re-ranking at BEA 2019 Grammatical Error Correction on Restricted Track","authors":["M Kaneko, K Hotate, S Katsumata, M Komachi - … Workshop on Innovative Use of NLP …, 2019"],"snippet":"… The 5-gram language model for re-ranking was trained on a subset of the Common Crawl corpus (Chollampatt and Ng, 2018a).5 We used a Python spell checker tool6 on the GEC model hy- pothesis sentences. 3.3 Evaluation …","url":["https://www.aclweb.org/anthology/W19-4422"]}
{"year":"2019","title":"Top-K Attention Mechanism for Complex Dialogue System","authors":["CU Shina, JW Chab - 2019"],"snippet":"… Then, the model submit the candidate with the highest value among the given candidates as the final correct an- swer. They randomly sampled one of the 99 negative samples to prevent bias during learning and used …","url":["http://workshop.colips.org/dstc7/papers/33.pdf"]}
{"year":"2019","title":"Toponym Identification in Epidemiology Articles--A Deep Learning Approach","authors":["MR Davari, L Kosseim, TD Bui - arXiv preprint arXiv:1904.11018, 2019"],"snippet":"… In order to measure the effect of such domain specific information, we experimented with 2 other pretrained word embedding models: Google News Word2vec [11], and a GloVe Model trained on Common Crawl [24] … Common Crawl GloVe 2.2M 300 29.84 …","url":["https://arxiv.org/pdf/1904.11018"]}
{"year":"2019","title":"Toward Automated Worldwide Monitoring of Network-Level Censorship","authors":["Z Weinberg - 2018"],"snippet":"Page 1. Toward Automated Worldwide Monitoring of Network-level Censorship Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Electrical and Computer Engineering Zachary Weinberg BA Chemistry, Columbia University …","url":["http://search.proquest.com/openview/11a5908644ea63a6b01b3f0c4d23ce4e/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2019","title":"Toward Gender-Inclusive Coreference Resolution","authors":["YT Cao, H Daumé III - arXiv preprint arXiv:1910.13913, 2019"],"snippet":"Page 1. Toward Gender-Inclusive Coreference Resolution YANG TRISTA CAO, University of Maryland HAL DAUMÉ III, Microsoft Research & University of Maryland ABSTRACT Correctly resolving textual mentions of people …","url":["https://arxiv.org/pdf/1910.13913"]}
{"year":"2019","title":"Towards a Global Perspective on Web Tracking","authors":["N Samarasinghe, M Mannan - Computers & Security, 2019"],"snippet":"… Schelter et al. Schelter and Kunegis (2016) performed a large scale analysis of third-party trackers using the Common Crawl 2012 corpus. The corpus may contain tracking information of residential as well as institutional users …","url":["https://www.sciencedirect.com/science/article/pii/S0167404818314007"]}
{"year":"2019","title":"Towards an Automated Extraction of ABAC Constraints from Natural Language Policies","authors":["M Alohaly, H Takabi, E Blanco - IFIP International Conference on ICT Systems …, 2019"],"snippet":"… model. To configure the model, we set one hyper-parameter value at a time. Our default settings: dropout = 0, decay rate = 0, number of BiLSTM cells (ie, layers) = 1, and GloVe (Common crawl) with 300 dimensions. To determine …","url":["https://link.springer.com/chapter/10.1007/978-3-030-22312-0_8"]}
{"year":"2019","title":"Towards an automated method to assess data portals in the deep web","authors":["AS Correa, RM de Souza, FSC da Silva - Government Information Quarterly, 2019"],"snippet":"Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0740624X18305185"]}
{"year":"2019","title":"Towards Content Expiry Date Determination: Predicting Validity Periods of Sentences","authors":["A Almquist, A Jatowt2r0000"],"snippet":"… For this, we use Common Crawl dataset 16 which is a web dump composed of billions of websites with plain text versions available … For each sentence found in the Common Crawl dataset we identify DATE, TIME and DURATION …","url":["http://www.dl.kuis.kyoto-u.ac.jp/~adam/ecir19a.pdf"]}
{"year":"2019","title":"Towards Content Transfer through Grounded Text Generation","authors":["RE Dataset","S Prabhumoye, C Quirk, M Galley - arXiv preprint arXiv:1905.05293, 2019"],"snippet":"… 3This list is provided in the data release of this paper. 4http://commoncrawl.org/ loaded from Common Crawl for reproducibility and consistency. The HTML derived from Common Crawl is then processed to get the plain text of the news article …","url":["https://arxiv.org/pdf/1905.05293","https://deepai.org/publication/towards-content-transfer-through-grounded-text-generation"]}
{"year":"2019","title":"Towards countering hate speech and personal attack in social media","authors":["P Charitidis, S Doropoulos, S Vologiannidis… - arXiv preprint arXiv …, 2019"],"snippet":"… each language. After conducting some preliminary experiments, the best pre-trained embedding choice for Greek and French language was using fastText embeddings [45], trained on Common Crawl and Wikipedia. For English …","url":["https://arxiv.org/pdf/1912.04106"]}
{"year":"2019","title":"Towards Functionally Similar Corpus Resources for Translation","authors":["M Kunilovskaya, S Sharoff"],"snippet":"… Secondly, we used lemmatised texts, with stop words filtered out (biLSTMlex in Table 1). For both scenarios we used pre-trained word embeddings of size 300, trained on the English Wikipedia and CommonCrawl data, using …","url":["http://corpus.leeds.ac.uk/serge/publications/2019-RANLP.pdf"]}
{"year":"2019","title":"Towards Multimodal Emotion Recognition in German Speech Events in Cars using Transfer Learning","authors":["D Cevher, S Zepf, R Klinger - arXiv preprint arXiv:1909.02764, 2019"],"snippet":"… We use a neural network with an embedding layer (frozen weights, pretrained on Common Crawl and Wikipedia (Grave et al., 2018)), a bidirectional LSTM (Schuster and Paliwal, 1997), and two dense layers followed by a soft max output layer …","url":["https://arxiv.org/pdf/1909.02764"]}
{"year":"2019","title":"Towards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper)","authors":["S Castro, D Hazarika, V Pérez-Rosas, R Zimmermann… - arXiv preprint arXiv …, 2019"],"snippet":"… 768. We also considered averaging Common Crawl pre-trained 300 dimensional GloVe word vectors (Pennington et al., 2014) for each token; however, it resulted in lower performance as compared to BERT-based features …","url":["https://arxiv.org/pdf/1906.01815"]}
{"year":"2019","title":"Towards Non-task-specific Distillation of BERT via Sentence Representation Approximation","authors":["B Wu, H Zhang, M Li, Z Wang, Q Feng, J Huang… - arXiv preprint arXiv …, 2020","HZ Bowen Wu, M Li, Z Wang, Q Feng, J Huang…"],"snippet":"… paper. 4.3 Hyperparameters For the student model in our proposed distilling method, we employ the 300-dimension GloVe (840B Common Crawl version; Pennington et al., 2014) to initialize the word embeddings. The number …","url":["https://arxiv.org/pdf/2004.03097","https://www.researchgate.net/profile/Bowen_Wu10/publication/337113946_Towards_Non-task-specific_Distillation_of_BERT_via_Sentence_Representation_Approximation/links/5dc5cffc4585151435f7df39/Towards-Non-task-specific-Distillation-of-BERT-via-Sentence-Representation-Approximation.pdf"]}
{"year":"2019","title":"Towards Robust Named Entity Recognition for Historic German","authors":["S Schweter, J Baiter - arXiv preprint arXiv:1906.07592, 2019"],"snippet":"… 69.59% Common Crawl 68.97% Wikipedia + Common Crawl 72.00% Wikipedia + Common Crawl + Character 74.50 … 69.62% Riedl and Padó (2018) (with transfer-learning) 74.33% ONB Wikipedia 75.80% CommonCrawl 78.70% Wikipedia + CommonCrawl 79.46 …","url":["https://arxiv.org/pdf/1906.07592"]}
{"year":"2019","title":"Towards semantic-rich word embeddings","authors":["G Beringer, M Jabłonski, P Januszewski, A Sobecki…"],"snippet":"… collected (III), for the our approach. We use a pretrained embedding model from spaCy - en_vectors_web_lg, which contains 300-dimensional word vectors trained on Common Crawl with GloVe2. We compare results on the …","url":["https://annals-csis.org/Volume_18/drp/pdf/120.pdf"]}
{"year":"2019","title":"Towards Unsupervised Grammatical Error Correction using Statistical Machine Translation with Synthetic Comparable Corpus","authors":["S Katsumata, M Komachi - arXiv preprint arXiv:1907.09724, 2019"],"snippet":"… makes up for the synthetic target data. To compare the fluency, the outputs of each best iter on JFLEG were evaluated with the perplexity based on the Common Crawl language model10. The perplexity of USMTforward in iter …","url":["https://arxiv.org/pdf/1907.09724"]}
{"year":"2019","title":"Tracking Naturalistic Linguistic Predictions with Deep Neural Language Models","authors":["M Heilbron, B Ehinger, P Hagoort, FP de Lange - arXiv preprint arXiv:1909.04400, 2019"],"snippet":"… Non-predictive controls We included two non-predictive and potentially confounding variables: first, frequency which we quantified as unigram surprise (−log p(w)) which was based on a word's lemma count in the CommonCrawl corpus, obtained via spaCy …","url":["https://arxiv.org/pdf/1909.04400"]}
{"year":"2019","title":"Transfer Learning across Languages from Someone Else's NMT Model","authors":["T Kocmi, O Bojar - arXiv preprint arXiv:1909.10955, 2019"],"snippet":"… WMT 2012 WMT 2018 English - French Commoncrawl, Europarl, Giga FREN, News commentary, UN corpus WMT 2013 WMT dis. 2015 … Based on our previous experiments, we ex- clude the noisiest corpus, ie web crawled ParaCrawl or Commoncrawl …","url":["https://arxiv.org/pdf/1909.10955"]}
{"year":"2019","title":"Transfer Learning from Transformers to Fake News Challenge Stance Detection (FNC-1) Task","authors":["V Slovikovskaya - arXiv preprint arXiv:1910.14353, 2019"],"snippet":"… 9XLNet is named after TransformerXL 10These corpora include (1) BOOK CORPUS [Zhu et al., 2015] plus English Wikipedia, the original data used to train BERT (16GB); (2) CC-NEWS, which authors collected from the English …","url":["https://arxiv.org/pdf/1910.14353"]}
{"year":"2019","title":"Transforma at SemEval-2019 Task 6: Offensive Language Analysis using Deep Learning Architecture","authors":["R Ong - arXiv preprint arXiv:1903.05280, 2019"],"snippet":"… This allows us to evaluate the increase in di- mensionality on the performance of our models 3. GloVe: Common Crawl (300d) - Trained on 42B tokens, 1.9M vocabulary of unique words … Table 7: T - GloVe Twitter, CC - GloVe Common Crawl …","url":["https://arxiv.org/pdf/1903.05280"]}
{"year":"2019","title":"transformers. zip: Compressing Transformers with Pruning and Quantization","authors":["R Cheong, R Daniel - 2019"],"snippet":"… 9: return M 4 Page 5. 4 Experiments 4.1 Dataset We train and evaluate on the WMT English - German translation task. Specifically, we train on all of Europarl, Common Crawl, and News Commentary, validate on the …","url":["https://pdfs.semanticscholar.org/fe82/735fe8ae2163a37aa2787eee0db8efc745b6.pdf"]}
{"year":"2019","title":"Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation","authors":["N Pourdamghani, N Aldarrab, M Ghazvininejad…"],"snippet":"… For Arabic we use MultiUN (Tiedemann, 2012). For French we use CommonCrawl For German we use a mix of CommonCrawl (1.7M), and NewsCommentary (300K) … For Spanish we use CommonCrawl (1.8M), and Europarl (200K) …","url":["https://www.isi.edu/~jonmay/pubs/acl19.pdf"]}
{"year":"2019","title":"Tree Edit Distance Learning via Adaptive Symbol Embeddings","authors":["BPCGA Micheli, B Hammer"],"snippet":"Deep Learning Monitor. Paper Detail. Close This Page. Tree Edit Distance Learning via Adaptive Symbol Embeddings. 2018-06-18 13:54:45; Benjamin Paaßen, Claudio Gallicchio, Alessio Micheli, Barbara Hammer; 0. Abstract …","url":["https://deeplearn.org/arxiv/38595/tree-edit-distance-learning-via-adaptive-symbol-embeddings"]}
{"year":"2019","title":"TU Wien@ TREC Deep Learning'19--Simple Contextualization for Re-ranking","authors":["S Hofstätter, M Zlabinger, A Hanbury - arXiv preprint arXiv:1912.01385, 2019"],"snippet":"… For the full task we generated initial rankings with Anserini using BM25 and utilized the validation sets to tune the re-ranking 1https://github.com/microsoft/BlingFire 242B CommonCrawl lower-cased: https://nlp.stanford.edu/projects/glove …","url":["https://arxiv.org/pdf/1912.01385"]}
{"year":"2019","title":"Twitter Sentiment on Affordable Care Act using Score Embedding","authors":["M Farhadloo - arXiv preprint arXiv:1908.07061, 2019"],"snippet":"… The embeddings pre-trained on Common Crawl data were only available in dimension 300 and were trained on 840 billion tokens with vocabulary … of available unlabeled training data had an impact on the performance …","url":["https://arxiv.org/pdf/1908.07061"]}
{"year":"2019","title":"Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English","authors":["F Guzmán, PJ Chen, M Ott, J Pino, G Lample, P Koehn… - arXiv preprint arXiv …, 2019"],"snippet":"… M monolingual Wikipedia (en) 67.8M 2.0B Common Crawl (ne) 3.6M 103.0M Wikipedia (ne) 92.3K 2.8M Sinhala–English … 5M monolingual Wikipedia (en) 67.8M 2.0B Common Crawl (si) 5.2M 110.3M Wikipedia (si) 155.9K 4.7M …","url":["https://arxiv.org/pdf/1902.01382"]}
{"year":"2019","title":"Type: Report Dissemination level: Public Due Date (in months): 24 (August 2019)","authors":["CSGOER Network"],"snippet":"Page 1. X Modal X Cultural X Lingual X Domain X Site Global OER Network Grant Agreement Number: 761758 Project Acronym: X5GON Project title: Cross Modal, Cross Cultural, Cross Lingual, Cross Domain, and Cross Site …","url":["https://www.x5gon.org/wp-content/uploads/2019/10/D5.2_afterJSTrev_26Aug19.pdf"]}
{"year":"2019","title":"UdS-DFKI Participation at WMT 2019: Low-Resource (en-gu) and Coreference-Aware (en-de) Systems","authors":["C España-Bonet, D Ruiter - Proceedings of the Fourth Conference on Machine …, 2019"],"snippet":"… proportions. Our base system uses CommonCrawl … x1 Parallel CommonCrawl 2,394,878 x1 x4 Europarl 1,775,445 x1 x4 NewsCommentary 328,059 x4 x16 Rapid 1,105,651 x1 x4 ParaCrawlFiltered 12,424,790 x0 x1 Table …","url":["https://www.aclweb.org/anthology/W19-5315"]}
{"year":"2019","title":"Understanding and Mitigating the Security Risks of Content Inclusion in Web Browsers","authors":["S Arshad - 2019"],"snippet":"… 47 5.1 Sample URL grouping. . . . . 73 5.2 Narrowing down the Common Crawl to the candidate set used in our analysis (from left to right) . . . . 79 5.3 Vulnerable pages and sites in the candidate set …","url":["http://search.proquest.com/openview/5a3bdc0060c7ad7004f26c77dae937c2/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2019","title":"Uni-and Multimodal and Structured Representations for Modeling Frame Semantics","authors":["T Botschen - 2019"],"snippet":"Page 1. Uniand Multimodal and Structured Representations for Modeling Frame Semantics Vom Fachbereich Informatik der Technischen Universität Darmstadt genehmigte Dissertation zur Erlangung des akademischen Grades …","url":["http://tuprints.ulb.tu-darmstadt.de/8484/1/Dissertation_TeresaBotschen.pdf"]}
{"year":"2019","title":"Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations","authors":["H Wu, J Mao, Y Zhang, Y Jiang, L Li, W Sun, WY Ma - arXiv preprint arXiv:1904.05521, 2019"],"snippet":"Page 1. Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations Hao Wu1,3,4,6,∗,†, Jiayuan Mao5,6,∗,†, Yufeng Zhang2,6,†, Yuning Jiang6, Lei Li6, Weiwei Sun1,3,4, Wei-Ying Ma6 …","url":["https://arxiv.org/pdf/1904.05521"]}
{"year":"2019","title":"Unraveling the Search Space of Abusive Language in Wikipedia with Dynamic Lexicon Acquisition","authors":["WF Chen, K Al-Khatib, M Hagen, H Wachsmuth…"],"snippet":"… The hidden state is employed to predict the probability of 'not-attack' using a linear regression layer. We use 300-dimensional word embeddings (Pennington et al., 2014) pre-trained on the Common Crawl with 840 …","url":["https://webis.de/downloads/publications/papers/stein_2019z.pdf"]}
{"year":"2019","title":"Unsupervised Cross-lingual Representation Learning at Scale","authors":["A Conneau, K Khandelwal, N Goyal, V Chaudhary… - arXiv preprint arXiv …, 2019"],"snippet":"… As shown in Figure 1, the CommonCrawl Corpus that we collected has significantly more monolingual data than the previously used Wikipedia corpora. Figure 3 shows that for the same BERTBase architecture, all models …","url":["https://arxiv.org/pdf/1911.02116"]}
{"year":"2019","title":"Unsupervised Extraction of Partial Translations for Neural Machine Translation","authors":["B Marie, A Fujita - Proceedings of the 2019 Conference of the North …, 2019"],"snippet":"… We extracted monolingual data ourselves from the Common Crawl project8 for Bengali (5.3M lines) and Malay (4.6M lines … 8http://commoncrawl org/ 9https://fasttext.cc/ 10The extraction of 100k partial translations from …","url":["https://www.aclweb.org/anthology/N19-1384"]}
{"year":"2019","title":"Unsupervised Joint Training of Bilingual Word Embeddings","authors":["B Marie, A Fujita - Proceedings of the 57th Conference of the Association …, 2019"],"snippet":"… For en- id, we used English (100M lines) and Indonesian (77M lines) Common Crawl corpora.5 We then mapped the word embeddings into a BWE space using VECMAP,6 one of the best and most robust methods for unsupervised mapping (Glavas et al., 2019) …","url":["https://www.aclweb.org/anthology/P19-1312"]}
{"year":"2019","title":"Unsupervised Lemmatization as Embeddings-Based Word Clustering","authors":["R Rosa, Z Žabokrtský - arXiv preprint arXiv:1908.08528, 2019"],"snippet":"… For the experiments reported in this paper, we use the pretrained word embedding dictionaries available from the FastText website.78 The word embeddings had been trained on Wikipedia9 and Common Crawl10 texts …","url":["https://arxiv.org/pdf/1908.08528"]}
{"year":"2019","title":"Unsupervised Question Answering by Cloze Translation","authors":["P Lewis, L Denoyer, S Riedel - arXiv preprint arXiv:1906.04980, 2019"],"snippet":"… Question Corpus We mine questions from En- glish pages from a recent dump of common crawl using simple selection criteria:3 We select sen … 3http:// commoncrawl.org/ 4We also experimented with language model pretraining …","url":["https://arxiv.org/pdf/1906.04980"]}
{"year":"2019","title":"Updating Pre-trained Word Vectors and Text Classifiers using Monolingual Alignment","authors":["P Bojanowski, O Celebi, T Mikolov, E Grave, A Joulin - arXiv preprint arXiv …, 2019"],"snippet":"… Indeed, despite their size, large web data such as Common Crawl lack coverage for highly technical expert fields such as medicine or law … Training data. We take two subsets of the May 2017 dump of the Common Crawl …","url":["https://arxiv.org/pdf/1910.06241"]}
{"year":"2019","title":"Updating verbal fluency analysis for the 21st century: Applications for psychiatry","authors":["TB Holmlund, J Cheng, PW Foltz, AS Cohen, B Elvevåg - Psychiatry Research, 2019"],"snippet":"… To base the analysis on a corpus with a wide variety of animal-word sources, we used a set of pre-trained word vectors calculated from approximately 42 billion tokens from the entire internet, courtesy of the Common Crawl project (Pennington et al., 2014) …","url":["https://www.sciencedirect.com/science/article/pii/S0165178118324181"]}
{"year":"2019","title":"Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs","authors":["A Fan, C Gardent, C Braud, A Bordes - 2019"],"snippet":"… WikiSum Second, we experiment on the WikiSum CommonCrawl (Liu et al., 2018b) summarization dataset4 with 1.5 million examples … denotes results from (Liu et al., 2018b) that use data scraped from unrestricted web search, not the static CommonCrawl version …","url":["https://hal.archives-ouvertes.fr/hal-02277063/document"]}
{"year":"2019","title":"Using logical form encodings for unsupervised linguistic transformation: Theory and applications","authors":["T Gröndahl, N Asokan - arXiv preprint arXiv:1902.09381, 2019"],"snippet":"Page 1. arXiv:1902.09381v1 [cs.CL] 25 Feb 2019 Using logical form encodings for unsupervised linguistic transformation: Theory and applications Tommi Gröndahl N. Asokan Abstract We present a novel method to architect …","url":["https://arxiv.org/pdf/1902.09381"]}
{"year":"2019","title":"Using the Semantic Web as a source of training data","authors":["C Bizer, A Primpeli, R Peeters - Datenbank-Spektrum, 2019"],"snippet":"… The Web Data Commons (WDC) project 4 monitors the adoption of schema.org annotations on the Web by analysing the Common Crawl 5 , a series of public web corpora each containing several billion HTML pages [12]. The …","url":["https://link.springer.com/article/10.1007/s13222-019-00313-y"]}
{"year":"2019","title":"Using Whole Document Context in Neural Machine Translation","authors":["V Macé, C Servan - arXiv preprint arXiv:1910.07481, 2019"],"snippet":"… models are evaluated on the same standard corpora that have Page 3. Corpora #lines # EN # DE Common Crawl 2.2M 54M 50M Europarl V9† 1.8M 50M 48M News Comm. V14† 338K 8.2M 8.3M ParaCrawl V3 27.5M 569M …","url":["https://arxiv.org/pdf/1910.07481"]}
{"year":"2019","title":"Variational Auto-Decoder: Neural Generative Modeling from Partial Data","authors":["A Zadeh, YC Lim, PP Liang, LP Morency - arXiv preprint arXiv:1903.00840, 2019"],"snippet":"… CMU-MOSEI consists of 23,500 sentences and CMU-MOSI consists of 2199 sentences. For text modality, the datasets contain GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens from the Common Crawl dataset …","url":["https://arxiv.org/pdf/1903.00840"]}
{"year":"2019","title":"Vernon-fenwick at SemEval-2019 Task 4: Hyperpartisan News Detection using Lexical and Semantic Features","authors":["V Srivastava, A Gupta, D Prakash, SK Sahoo, RR Rohit… - Proceedings of the 13th …, 2019"],"snippet":"… semantic space. We have used 300-dimensional Glove embeddings trained on Common Crawl data of 2.2 million words and 840 billion tokens. An ar- ticle was tokenized into sentences and further into words to obtain it's article representation …","url":["https://www.aclweb.org/anthology/S19-2189"]}
{"year":"2019","title":"Video Question Answering with Spatio-Temporal Reasoning","authors":["Y Jang, Y Song, CD Kim, Y Yu, Y Kim, G Kim - International Journal of Computer …, 2019"],"snippet":"Page 1. International Journal of Computer Vision https://doi.org/10.1007/s11263-01901189-x Video Question Answering with Spatio-Temporal Reasoning Yunseok Jang1 · Yale Song2 · Chris Dongjoo Kim1 · Youngjae Yu1 · Youngjin Kim1 · Gunhee Kim1 …","url":["https://link.springer.com/article/10.1007/s11263-019-01189-x"]}
{"year":"2019","title":"Vir is to Moderatus as Mulier is to Intemperans Lemma Embeddings for Latin","authors":["R Sprugnoli, M Passarotti, G Moretti"],"snippet":"… Both Facebook and the organizers of the CoNLL shared tasks on multilingual parsing have pre-computed and released word embeddings trained on Latin texts crawled from the web: the former using the fastText model on …","url":["https://www.researchgate.net/profile/Rachele_Sprugnoli/publication/336798734_Vir_is_to_Moderatus_as_Mulier_is_to_Intemperans_Lemma_Embeddings_for_Latin/links/5db2a47e92851c577ec259b4/Vir-is-to-Moderatus-as-Mulier-is-to-Intemperans-Lemma-Embeddings-for-Latin.pdf"]}
{"year":"2019","title":"Vision-based Page Rank Estimation with Graph Networks","authors":["TI Denk, S Güner"],"snippet":"… The Open PageRank initiative provides freely available data that was built on top of Common Crawl [do/19], which provides high quality crawl data of webp ages since 2013. Open PageRank uses the number of backlinks of …","url":["https://www.researchgate.net/profile/Timo_Denk/publication/334824445_Vision-based_Page_Rank_Estimation_with_Graph_Networks/links/5d429cb692851cd04696fd56/Vision-based-Page-Rank-Estimation-with-Graph-Networks.pdf"]}
{"year":"2019","title":"VizNet: Towards A Large-Scale Visualization Learning and Benchmarking Repository","authors":["K Hu, N Gaikwad, M Bakker, M Hulsebos, E Zgraggen…"],"snippet":"… Corpora The first category of corpora includes data tables harvested from the web. In particular, we use horizontal relational tables from the WebTables 2015 corpus [6], which extracts structured tables from the Common Crawl …","url":["https://hci.stanford.edu/~cagatay/projects/viznet/VizNet-CHI19-Submission.pdf"]}
{"year":"2019","title":"Wanca in Korp: Text corpora for underresourced Uralic languages","authors":["H Jauhiainen, T Jauhiainen, K Lindén - DATA AND HUMANITIES (RDHUM) 2019 …"],"snippet":"… In addition to conducting our own crawling, we also used the pre-crawled corpus distributed by the Common Crawl Foundation … 2 In addition to conducting our own crawling, we used the pre-crawled corpus distributed by the Common Crawl Foundation …","url":["https://researchportal.helsinki.fi/files/126205806/Proceedings_RDHum2019.pdf#page=23"]}
{"year":"2019","title":"WDC Product Data Corpus and Gold Standard for Large-Scale Product Matching-Version 2.0","authors":["R Peeters, A Primpeli, C Bizer"],"snippet":"… methods. The Web Data Commons project regularly extracts schema.org annotations from the Common Crawl, a large public web corpus. November 2017 version of the WDC schema.org data set contains 365 million offers …","url":["http://webdatacommons.org/largescaleproductcorpus/v2/"]}
{"year":"2019","title":"Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual Word Embeddings","authors":["H Wang, J Henderson, P Merlo - arXiv preprint arXiv:1904.09446, 2019"],"snippet":"… (2018a)11, we use their pretrained CBOW embeddings of 300 dimensions. For English, Italian and German, the models are trained on the WacKy corpus. The Finnish model is trained from Common Crawl and the Spanish model is trained from WMT News Crawl …","url":["https://arxiv.org/pdf/1904.09446"]}
{"year":"2019","title":"Web Archive Analysis Using Hive and SparkSQL","authors":["X Wang, Z Xie - 2019 ACM/IEEE Joint Conference on Digital Libraries …, 2019"],"snippet":"… Keywords web archive, big data, distributed computation 1 Introduction Web preservation organizations such as Common Crawl or Internet Archive are common sources of web archive data … We use a data set from Common Crawl May 2018 collection …","url":["https://ieeexplore.ieee.org/abstract/document/8791112/"]}
{"year":"2019","title":"Web Engineering: 19th International Conference, ICWE 2019, Daejeon, South Korea, June 11–14, 2019, Proceedings","authors":["M Bakaev"],"snippet":"Page 1. Maxim Bakaev Flavius Frasincar In-Young Ko (Eds.) Web Engineering 19th International Conference, ICWE 2019 Daejeon, South Korea, June 11–14, 2019 Proceedings 123 Page 2. Lecture Notes in Computer Science …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=5R6VDwAAQBAJ&oi=fnd&pg=PR5&dq=commoncrawl&ots=X57GCPV1TC&sig=41aU_I70hr0H-D_h9MbSG1Ruryc"]}
{"year":"2019","title":"Web table integration and profiling for knowledge base augmentation","authors":["O Lehmberg - 2019"],"snippet":"Page 1. Web Table Integration and Profiling for Knowledge Base Augmentation Inauguraldissertation zur Erlangung des akademischen Grades eines Doktors der Naturwissenschaften der Universität Mannheim …","url":["https://madoc.bib.uni-mannheim.de/52346/1/thesis.pdf"]}
{"year":"2019","title":"Web View: Measuring & Monitoring Representative Information on Websites","authors":["A Saverimoutou, B Mathieu, S Vaton - ICIN 2019-QOE-MANAGEMENT 2019, 2019"],"snippet":"… XRay [8] and AdFisher run automated personalization detection experiments and Common Crawl 7 uses an Apache Nutch based crawler … 4http://phantomjs. org/ 5https://www.seleniumhq.org/ 6https://github.com/ghostwords/chameleon …","url":["https://hal.archives-ouvertes.fr/hal-02072471/document"]}
{"year":"2019","title":"WebIsAGraph: A Very Large Hypernymy Graph from a Web Corpus","authors":["F Stefano, I Finocchi, SP Ponzetto, V Paola - Sixth Italian Conference on …, 2019","S Faralli, I Finocchi, SP Ponzetto, P Velardi - 2019"],"snippet":"… In this paper, we present WebIsAGraph, a very large hypernymy graph compiled from a dataset of is-a relationships extracted from the CommonCrawl. We provide the resource together with a Neo4j plugin to …","url":["https://iris.luiss.it/handle/11385/192535","https://www.researchgate.net/profile/Stefano_Faralli2/publication/336899588_WebIsAGraph_A_Very_Large_Hypernymy_Graph_from_a_Web_Corpus/links/5db9a6c24585151435d5b691/WebIsAGraph-A-Very-Large-Hypernymy-Graph-from-a-Web-Corpus.pdf"]}
{"year":"2019","title":"What a neural language model tells us about spatial relations","authors":["M Ghanimifard, S Dobnik - Proceedings of the Combined Workshop on Spatial …, 2019"],"snippet":"… Finally, we also use pre-trained GloVe embeddings on the Common Crawl (CC) dataset with 42B tokens4 … On multi-word test suite the P-vectors perform slightly better. On both test suites, GloVe trained on Common Crawl performs …","url":["https://www.aclweb.org/anthology/W19-1608"]}
{"year":"2019","title":"What are Links in Linked Open Data? A Characterization and Evaluation of Links between Knowledge Graphs on the Web","authors":["A Haller, JD Fernández, MR Kamdar, A Polleres - Working Papers on Information …, 2019"],"snippet":"Page 1. What are Links in Linked Open Data? A Characterization and Evaluation of Links between Knowledge Graphs on the Web Armin Haller, Javier D. Fernández, Maulik R. Kamdar, Axel Polleres Arbeitspapiere zum Tätigkeitsfeld …","url":["http://epub.wu.ac.at/7193/1/20191002ePub_LOD_link_analysis.pdf"]}
{"year":"2019","title":"What does Neural Bring? Analysing Improvements in Morphosyntactic Annotation and Lemmatisation of Slovenian, Croatian and Serbian","authors":["N Ljubešić, K Dobrovoljc - Proceedings of the 7th Workshop on Balto-Slavic …, 2019"],"snippet":"… neural morphosyntactic taggers, we also experiment with various embeddings, mostly (1) the original CoNLL 2017 word2vec (w2v) embeddings for Slovenian and Croatian (Ginter et al., 2017) (there are none available for …","url":["https://www.aclweb.org/anthology/W19-3704"]}
{"year":"2019","title":"Who Needs Words? Lexicon-Free Speech Recognition","authors":["T Likhomanenko, G Synnaeve, R Collobert - arXiv preprint arXiv:1904.04479, 2019"],"snippet":"… char GCNN-20B no 6.4 2.7 3.6 1.5 4https://github.com/facebookresearch/wav2letter 5Speaker adaptation; pronunciation lexicon 612k hours AM train set and common crawl LM 7Speaker adaptation; 3k acoustic states 8Data augmentation; n-gram LM …","url":["https://arxiv.org/pdf/1904.04479"]}
{"year":"2019","title":"WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia","authors":["H Schwenk, V Chaudhary, S Sun, H Gong, F Guzmán - arXiv preprint arXiv …, 2019"],"snippet":"… recall. In this work, we chose the global mining op- tion. This will allow us to scale the same ap- proach to other, potentially huge, corpora for which document-level alignments are not easily available, eg Common Crawl. An …","url":["https://arxiv.org/pdf/1907.05791"]}
{"year":"2019","title":"WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale","authors":["K Sakaguchi, RL Bras, C Bhagavatula, Y Choi - arXiv preprint arXiv:1907.10641, 2019"],"snippet":"… Ensemble Neural LMs Trinh and Le (2018) is one of the first attempts to apply a neural language model which is pre-trained on a very large corpora (including LM-1-Billion, CommonCrawl, SQuAD, and Gutenberg Books). In …","url":["https://arxiv.org/pdf/1907.10641"]}
{"year":"2019","title":"Word Embedding Based Extension of Text Categorization Topic Taxonomies","authors":["T Eljasik-Swoboda, F Engel, M Kaufmann, M Hemmje"],"snippet":"… ArgumenText is a practical implementation of an AM engine (Stab et al., 2018). It employs a two-step mechanism in which a large collection of documents (http://commoncrawl.org/, in Stab et al.'s experiment with 683 …","url":["http://ceur-ws.org/Vol-2348/paper01.pdf"]}
{"year":"2019","title":"Word Embedding Models for Query Expansion in Answer Passage Retrieval","authors":["S MASTER"],"snippet":"Page 1. MASTER'S THESIS Word Embedding Models for Query Expansion in Answer Passage Retrieval NIRMAL ROY Page 2. Page 3. Word Embedding Models for Query Expansion in Answer Passage Retrieval THESIS submitted …","url":["https://pdfs.semanticscholar.org/f436/c49151fd8d00c59655a939bbbd552f1577c4.pdf"]}
{"year":"2019","title":"Word Embedding Visualization Via Dictionary Learning","authors":["J Zhang, Y Chen, B Cheung, BA Olshausen - arXiv preprint arXiv:1910.03833, 2019"],"snippet":"… similar. For simplicity, we show the results for the 300 dimensional GloVe word vectors[30] pretrained on CommonCrawl [2]. We shall discuss the difference across different embedding models at the end in this section. Once …","url":["https://arxiv.org/pdf/1910.03833"]}
{"year":"2019","title":"Word Embeddings (Also) Encode Human Personality Stereotypes","authors":["O Agarwal, F Durupınar, NI Badler, A Nenkova - … of the Eighth Joint Conference on …, 2019"],"snippet":"… or profession. We experimented with GloVe representations (Pennington et al., 2014) trained on Common crawl (6B tokens, 400K vocab, 300d) and symmetric pattern (SP) based representations (Schwartz et al., 2015). We …","url":["https://www.aclweb.org/anthology/S19-1023"]}
{"year":"2019","title":"Word Embeddings and Gender Stereotypes in Swedish and English","authors":["R Précenth - 2019"],"snippet":"Page 1. UUDM Project Report 2019:15 Examensarbete i matematik, 30 hp Handledare: David Sumpter Examinator: Denis Gaidashev Maj 2019 Department of Mathematics Uppsala University Word Embeddings and Gender Stereotypes in Swedish and English …","url":["https://uu.diva-portal.org/smash/get/diva2:1313459/FULLTEXT01.pdf"]}
{"year":"2019","title":"Word Embeddings for Fine-Grained Sentiment Analysis","authors":["D Bacon, R Dalal, MRD Kodandarama, MR Hari…"],"snippet":"… Lastly, we considered the word embedding sub-model. We used the GLoVe word vectoring [11] trained on Common Crawl [https://commoncrawl.org/] as implemented by spaCy [7]. This resulted in a vector-dimension of 300 for each word …","url":["https://divatekodand.github.io/files/word_embeddings.pdf"]}
{"year":"2019","title":"Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey","authors":["E Çano, M Morisio - arXiv preprint arXiv:1902.00753, 2019"],"snippet":"… This bundle contains data of Common Crawl (http: //commoncrawl.org/), a nonprofit organization that builds and maintains free and public text sets by crawling the Web. CommonCrawl42 is a highly reduced version easier and faster to work with …","url":["https://arxiv.org/pdf/1902.00753"]}
{"year":"2019","title":"Word Embeddings for the Armenian Language: Intrinsic and Extrinsic Evaluation","authors":["K Avetisyan, T Ghukasyan - arXiv preprint arXiv:1906.03134, 2019"],"snippet":"… A year later, Facebook released another batch of fastText embeddings, trained on Common Crawl and Wikipedia [2]. Other publicly available embeddings include 4 … these embeddings were trained on Wikipedia and Common Crawl, using CBOW architecture with …","url":["https://arxiv.org/pdf/1906.03134"]}
{"year":"2019","title":"Word Embeddings in Low Resource Gujarati Language","authors":["I Joshi, P Koringa, S Mitra - 2019 International Conference on Document Analysis …, 2019"],"snippet":"… (2014) released GloVe models trained on Wikipedia, Gigaword and Common Crawl (840B tokens). A notable effort is the work of Al-Rfou et al … Word embeddings for Gujarati language were released as a part of …","url":["https://ieeexplore.ieee.org/abstract/document/8893052/"]}
{"year":"2019","title":"Word Similarity Datasets for Thai: Construction and Evaluation","authors":["P Netisopakul, G Wohlgenannt, A Pulich - arXiv preprint arXiv:1904.04307, 2019"],"snippet":"… The models are trained on Common Crawl and Wikipedia corpora using fastText [13], regarding settings they report the us- age of the CBOW algorithm, 300 dimensions, a window size of 5 and 10 negatives. The model is large and contains 2M vectors …","url":["https://arxiv.org/pdf/1904.04307"]}
{"year":"2019","title":"Word Usage Similarity Estimation with Sentence Representations and Automatic Substitutes","authors":["AG Soler, M Apidianaki, A Allauzen - arXiv preprint arXiv:1905.08377, 2019"],"snippet":"… al., 2014). We use 300-dimensional GloVe embeddings pre-trained on Common Crawl (840B tokens).5 The representation of a sentence is obtained by averaging the GloVe embeddings of the words in the sentence. SIF (Smooth …","url":["https://arxiv.org/pdf/1905.08377"]}
{"year":"2019","title":"Word-embedding data as an alternative to questionnaires for measuring the affective meaning of concepts","authors":["A van Loon, J Freese - 2019"],"snippet":"… Here we include information from both algorithms. The GloVe embeddings we use have been trained on text obtained from Wikipedia, Twitter, and Common Crawl. The Word2vec embeddings we use are trained on the Google News Corpus …","url":["https://osf.io/preprints/socarxiv/r7ewx/download"]}
{"year":"2019","title":"Word-Embeddings and Grammar Features to Detect Language Disorders in Alzheimer's Disease Patients","authors":["JS Guerrero-Cristancho, JC Vásquez-Correa… - TecnoLógicas, 2020"],"snippet":"… occurrence in a document [13]. Said authors considered a pre-trained model with the Common Crawl dataset, whose vocabulary size exceeds the 2 million and contains 840 billion words. A logistic regression classifier and …","url":["https://revistas.itm.edu.co/index.php/tecnologicas/article/download/1387/1456"]}
{"year":"2019","title":"WTMED at MEDIQA 2019: A Hybrid Approach to Biomedical Natural Language Inference","authors":["Z Wu, Y Song, S Huang, Y Tian, F Xia - Proceedings of the 18th BioNLP Workshop …, 2019"],"snippet":"Page 1. Proceedings of the BioNLP 2019 workshop, pages 415–426 Florence, Italy, August 1, 2019. c 2019 Association for Computational Linguistics 415 WTMED at MEDIQA 2019: A Hybrid Approach to Biomedical Natural Language Inference …","url":["https://www.aclweb.org/anthology/W19-5044"]}
{"year":"2019","title":"X-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension","authors":["M Abdou, C Sas, R Aralikatte, I Augenstein, A Søgaard - arXiv preprint arXiv …, 2019"],"snippet":"… All monolingual models' word embeddings were initialised using fastText embeddings trained on each language's Wikipedia and common crawl corpora,7 except for the comparison experiments described in sub-section …","url":["https://arxiv.org/pdf/1908.05111"]}
{"year":"2019","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","authors":["Z Yang, Z Dai, Y Yang, J Carbonell, R Salakhutdinov… - arXiv preprint arXiv …, 2019"],"snippet":"Page 1. XLNet: Generalized Autoregressive Pretraining for Language Understanding Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1, Ruslan Salakhutdinov1, Quoc V. Le2 1Carnegie Mellon University, 2Google …","url":["https://arxiv.org/pdf/1906.08237"]}
{"year":"2019","title":"YNU Wb at HASOC 2019: Ordered Neurons LSTM with Attention for Identifying Hate Speech and Offensive Language","authors":["B Wang, SL Yunxia Ding, X Zhou - Proceedings of the 11th annual meeting of the …, 2019"],"snippet":"… And the pre-training word vector we used is fastText, which is provided by Mikolov et al. [7]. It is a 2 million word vector trained using subword information on Common Crawl with 600B tokens, and its dimension is 300. 4.3 Result …","url":["http://ceur-ws.org/Vol-2517/T3-2.pdf"]}
{"year":"2019","title":"YNUWB at SemEval-2019 Task 6: K-max pooling CNN with average meta-embedding for identifying offensive language","authors":["B Wang, X Zhou, X Zhang - Proceedings of the 13th International Workshop on …, 2019"],"snippet":"… FastText is provided by Mikolov et al. (Mikolov et al., 2018), it is a 2 million word vector trained using subword information on Common Crawl with 600B tokens, and its dimension is 300. Glove is provided by Jeffrey Pennington et al …","url":["https://www.aclweb.org/anthology/S19-2143"]}
{"year":"2019","title":"Zastosowania metody rzutu przypadkowego w głębokich sieciach neuronowych","authors":["PI Wójcik"],"snippet":"Page 1. Akademia Górniczo-Hutnicza im. Stanisława Staszica w Krakowie Wydział Informatyki, Elektroniki i Telekomunikacji Katedra Informatyki Rozprawa doktorska Zastosowania metody rzutu przypadkowego w głębokich …","url":["http://www.doktoraty.iet.agh.edu.pl/_media/2018:pwojcik:phd.pdf"]}
{"year":"2019","title":"Zero-Resource Cross-Lingual Named Entity Recognition","authors":["MS Bari, S Joty, P Jwalapuram - arXiv preprint arXiv:1911.09812, 2019"],"snippet":"… We use FastText embeddings (Grave et al. 2018), which are trained on Common Crawl and Wikipedia, and SGD with a gradient clipping of 5.0 to train the model. We found that the learning rate was crucial for training, and …","url":["https://arxiv.org/pdf/1911.09812"]}
{"year":"2019","title":"Zero-Resource Neural Machine Translation with Monolingual Pivot Data","authors":["A Currey, K Heafield"],"snippet":"… We use all available parallel corpora for EN↔DE (Europarl v7, Common Crawl, and News Commentary v11) and for EN↔RU (Common Crawl, News Commentary v11, Yandex Corpus, and Wiki Headlines) to train the initial …","url":["https://kheafield.com/papers/edinburgh/pivot.pdf"]}
{"year":"2019","title":"Zero-shot Learning and Knowledge Transfer in Music Classification and Tagging","authors":["J Choi, J Lee, J Park, J Nam - arXiv preprint arXiv:1906.08615, 2019"],"snippet":"… We utilized a pretrained GloVe model available online. It contains 19 million vocabularies with 300 dimensional embedding trained from documents in Common Crawl data. We then evaluated the model on MTAT and GTZAN …","url":["https://arxiv.org/pdf/1906.08615"]}
{"year":"2019","title":"Zero-Shot Question Classification Using Synthetic Samples","authors":["H Fu, C Yuan, X Wang, Z Sang, S Hu, Y Shi - 2018 5th IEEE International Conference …, 2019"],"snippet":"… The detailed data set is listed in Table 1. All experiments follow the principle of counterpart parameters. The Chinese and English word vectors are pre-trained using Glove respectively on Samsung and Common Crawl corpus. The word dimension is 300 …","url":["https://ieeexplore.ieee.org/abstract/document/8691209/"]}
{"year":"2019","title":"Zero-Shot Semantic Segmentation via Variational Mapping","authors":["N Kato, T Yamasaki, K Aizawa - Proceedings of the IEEE International Conference on …, 2019"],"snippet":"… Dataset Unseen classes PASCAL-50 aeroplane, bicycle, bird, boat, bottle PASCAL-51 bus, car, cat, chair, cow PASCAL-52 diningtable, dog, horse, motorbike, person PASCAL-53 potted plant, sheep, sofa, train, tv/monitor …","url":["http://openaccess.thecvf.com/content_ICCVW_2019/papers/MDALC/Kato_Zero-Shot_Semantic_Segmentation_via_Variational_Mapping_ICCVW_2019_paper.pdf"]}
{"year":"2020","title":"18 Evaluation of Greek Word Embeddings","authors":["S Outsios, C Karatsalos, K Skianis, M Vazirgiannis"],"snippet":"… wiki. The last model has been trained on Common Crawl and Wikipedia data using FastText based on CBOW model with position-weights (Grave et al., 2018), mentioned as cc+wiki. Category gr_def gr_neg1 0 cc.el.300 …","url":["http://www.eleto.gr/download/Conferences/12th%20Conference/Papers-and-speakers/12th_18-02-20_OutsiosStamatis-KaratsalosChristos-SkianisK-VazirgiannisMichalis_Paper1_V04.pdf"]}
{"year":"2020","title":"\" Thy algorithm shalt not bear false witness\": An Evaluation of Multiclass Debiasing Methods on Word Embeddings","authors":["T Schlender, G Spanakis - arXiv preprint arXiv:2010.16228, 2020"],"snippet":"… However, surprisingly, the WEAT score measured in ConceptNet is the worst of all three. The GloVe embeddings seem to carry the most bias concerning the RNSB and MAC metrics, which is intuitive when considering the common crawl data it was trained on …","url":["https://arxiv.org/pdf/2010.16228"]}
{"year":"2020","title":"A Benchmark of Rule-Based and Neural Coreference Resolution in Dutch Novels and News","authors":["C Poot, A van Cranenburgh - arXiv preprint arXiv:2011.01615, 2020"],"snippet":"… 5 Evaluation Before presenting our main benchmark results, we discuss the issue of coreference evaluation metrics. 6We use Fasttext common crawl embeddings, https://fasttext.cc/docs/en/crawl-vectors.html Page 6 …","url":["https://arxiv.org/pdf/2011.01615"]}
{"year":"2020","title":"A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer","authors":["V Iashin, E Rahtu - arXiv preprint arXiv:2005.08271, 2020"],"snippet":"Page 1. IASHIN, RAHTU: A BETTER USE OF AUDIO-VISUAL CUES 1 A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer Vladimir Iashin vladimir.iashin@tuni.fi Esa Rahtu esa.rahtu@tuni.fi …","url":["https://arxiv.org/pdf/2005.08271"]}
{"year":"2020","title":"A brief tour to the NLP Sesame Street","authors":["E Montoya"],"snippet":"… In addition to the strategy to verify fake news this research provided of a large corpus of news articles from Common Crawl named RealNews, as Grover needed a large corpus of news with metadata which was not available or …","url":["https://chatbotslife.com/a-brief-tour-to-the-nlp-sesame-street-7bba02d75ae3"]}
{"year":"2020","title":"A Call for More Rigor in Unsupervised Cross-lingual Learning","authors":["M Artetxe, S Ruder, D Yogatama, G Labaka, E Agirre - arXiv preprint arXiv …, 2020"],"snippet":"… However, as of November 2019, Wikipedia exists in only 307 languages3 of which nearly half have less than 10,000 articles. While one could hope to overcome this by taking the entire web as a corpus, as …","url":["https://arxiv.org/pdf/2004.14958"]}
{"year":"2020","title":"A Character-Level BiGRU-Attention for Phishing Classification","authors":["L Yuan, Z Zeng, Y Lu, X Ou, T Feng - International Conference on Information and …, 2019"],"snippet":"… In addition, Common Crawl that stored a great deal of websites is an open website for crawler learners. There are 800,000 websites provided as legitimate websites data … Phish urls. Legal urls. Data sources. Phish Tank. Common Crawl …","url":["https://link.springer.com/chapter/10.1007/978-3-030-41579-2_43"]}
{"year":"2020","title":"A Comprehensive Survey of Grammar Error Correction","authors":["Y Wang, Y Wang, J Liu, Z Liu - arXiv preprint arXiv:2005.06600, 2020"],"snippet":"… Common Crawl. The Common Crawl corpus [10] is a repository of web crawl data which is open to everyone. It completes crawls monthly since 2011. • EVP … [28] Word-Level L1 Yes None Error Selection Wikipedia, 2014 Common Crawl …","url":["https://arxiv.org/pdf/2005.06600"]}
{"year":"2020","title":"A Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation Language Models","authors":["U Naseem, I Razzak, SK Khan, M Prasad - arXiv preprint arXiv:2010.15036, 2020"],"snippet":"Page 1. A Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation Language Models USMAN NASEEM∗, School of Computer Science, The University of Sydney, Australia …","url":["https://arxiv.org/pdf/2010.15036"]}
{"year":"2020","title":"A Cross-lingual Natural Language Processing Framework for Infodemic Management","authors":["R Pal, R Pandey, V Gautam, K Bhagat, T Sethi - arXiv preprint arXiv:2010.16357, 2020"],"snippet":"… The algorithm effectively minimizes this function to learn meaningful vector representations. The version of Glove used for experimentation is the publicly available common Crawl (840B tokens, 2.2M vocab, cased, 300 dimension vectors) …","url":["https://arxiv.org/pdf/2010.16357"]}
{"year":"2020","title":"A Deep Learning Approach to Interest Analysis","authors":["T Meer - 2020"],"snippet":"Page 1. A Deep Learning Approach to Interest Analysis Thomas van der Meer A thesis submitted for the degree of Master of Business Informatics Department of Information and Computing Sciences Utrecht University The …","url":["https://dspace.library.uu.nl/bitstream/handle/1874/398939/scriptie_eindversie_tvdm.pdf?sequence=1"]}
{"year":"2020","title":"A Deep Learning-Based Approach for Identifying the Medicinal Uses of Plant-Derived Natural Compounds. Front. Pharmacol. 11: 584875. doi: 10.3389/fphar …","authors":["S Yoo, HC Yang, S Lee, J Shin, S Min, E Lee, M Song… - Frontiers in Pharmacology …, 2020"],"snippet":"… alphaisothiocyanatotoluene.” In this study, we used the pre-trained fastText model with Wikipedia and Common Crawl (Grave et al., 2018). The model additionally learned from the DrugBank indication and PubMed literature …","url":["https://pdfs.semanticscholar.org/2105/4ac827a06c594f54e1ffe9c865fcbb994980.pdf"]}
{"year":"2020","title":"A deep search method to survey data portals in the whole web: toward a machine learning classification model","authors":["AS Correa, A Melo Jr, FSC da Silva - Government Information Quarterly, 2020"],"snippet":"… Later, the same authors (AS Correa & da Silva, 2019) took advantage of the URL index of the Common Crawl project (an open repository of web crawl data) to survey potential data portals by searching the URL text strings …","url":["https://www.sciencedirect.com/science/article/pii/S0740624X20302896"]}
{"year":"2020","title":"A Deep-Learning-Based Blocking Technique for Entity Linkage","authors":["F Azzalini, M Renzi, L Tanca - International Conference on Database Systems for …, 2020"],"snippet":"… attribute value \\(t[A_{k}]\\) is transformed into a real-valued vector \\(\\mathbf{v} (w)\\). The fastText model we use is crawl-300d-2M-subword [3] where each word is represented as a 300-dimensional vector and the …","url":["https://link.springer.com/chapter/10.1007/978-3-030-59410-7_37"]}
{"year":"2020","title":"A Focused Study to Compare Arabic Pre-training Models on Newswire IE Tasks","authors":["W Lan, Y Chen, W Xu, A Ritter - arXiv preprint arXiv:2004.14519, 2020"],"snippet":"… three times; (2) add the Arabic shuffled Os- car data (Ortiz Suárez et al., 2019), a large-scale multilingual dataset obtained by language identification and filtering of the Common Crawl corpus … XLM-Rbase CommonCrawl 55.6B …","url":["https://arxiv.org/pdf/2004.14519"]}
{"year":"2020","title":"A Framework for Word Embedding Based Automatic Text Summarization and Evaluation","authors":["TT Hailu, J Yu, TG Fantaye - Information, 2020"],"snippet":"Text summarization is a process of producing a concise version of text (summary) from one or more information sources. If the generated summary preserves meaning of the original text, it will help the users to make fast and …","url":["https://www.mdpi.com/2078-2489/11/2/78/pdf"]}
{"year":"2020","title":"A German Language Voice Recognition System using DeepSpeech","authors":["J Xu, K Matta, S Islam, A Nürnberger"],"snippet":"… 1, pp. 517–520, 1992. [14] Christopher Cieri, David Miller and Kevin Walker, “The Fisher Corpus: a Resource for the Next Generations of Speech-to-Text,” LREC, 2004. [15] CommomCrawl, “English language model,” http://commoncrawl.org/, 2020 …","url":["https://www.researchgate.net/profile/Kaveen_Matta_Kumaresh/publication/342657372_German_Voice_Recognition_System_using_DeepSpeech/links/5efee6e3a6fdcc4ca447681a/German-Voice-Recognition-System-using-DeepSpeech.pdf"]}
{"year":"2020","title":"A Gradient Boosting-Seq2Seq System for Latin POS Tagging and Lemmatization","authors":["GGA Celano - LREC 2020 Workshop Language Resources and …"],"snippet":"… prefixes, infixes, or suffixes to be weighted. Some models for Latin, such as the one based on texts from Common Crawl and Wikipedia, have already been computed and are freely available. 7 However, since the data released …","url":["https://www.academia.edu/download/63734156/LT4HALAbook20200624-19244-er3k3d.pdf#page=126"]}
{"year":"2020","title":"A graph based framework for structured prediction tasks in sanskrit","authors":["A Krishna, A Gupta, P Goyal, B Santra, P Satuluri - Computational Linguistics, 2020"],"snippet":"Page 1. A Graph Based Framework for Structured Prediction Tasks in Sanskrit Amrith Krishna* University of Cambridge Bishal Santra Indian Institute of Technology Kharagpur Ashim Gupta† University of Utah Pavankumar Satuluri Chinmaya Vishwavidyapeeth …","url":["https://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00390"]}
{"year":"2020","title":"A Graph-Theoretic Approach for the Detection of Phishing Webpages","authors":["CL Tan, KL Chiew, KSC Yong, J Abdullah, Y Sebastian - Computers & Security, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S016740482030078X"]}
{"year":"2020","title":"A Hybrid Approach for Aspect-Based Sentiment Analysis Using Deep Contextual Word Embeddings and Hierarchical Attention","authors":["MM Trusca, D Wassenberg, F Frasincar, R Dekker - arXiv preprint arXiv:2004.08673, 2020","R Dekker - Web Engineering: 20th International Conference, ICWE …"],"snippet":"… The last two conditions for f are necessary to prevent overweighting of either rare or frequent co-occurrences. In this paper, we choose to use 300-dimension GloVe word embeddings trained on the Common Crawl (42 billion words) [14]. Word2vec …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=XpnqDwAAQBAJ&oi=fnd&pg=PA365&dq=commoncrawl&ots=-hereKCQai&sig=PCXnLnE9TjyMtqs05nolBDRi69g","https://arxiv.org/pdf/2004.08673"]}
{"year":"2020","title":"A Large Scale Study on Health Information Retrieval for Laypersons","authors":["Z Liu - 2020"],"snippet":"… 3.1 Description of Document Collection The consumer-oriented health search task uses a dataset called clefehealth2018 corpus, which was created by acquiring web pages from various health do- mains(websites) using the CommonCrawl platform1 …","url":["https://cs.anu.edu.au/courses/CSPROJECTS/20S1/reports/u6022937_report.pdf"]}
{"year":"2020","title":"A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal","authors":["D Gholipour Ghalandari, C Hokamp, J Glover, G Ifrim - arXiv, 2020","DG Ghalandari, C Hokamp, NT Pham, J Glover, G Ifrim - arXiv preprint arXiv …, 2020"],"snippet":"… We also automatically extend these source articles by looking for related articles in the Common Crawl archive … Table 1: Example event summary and linked source ar- ticles from the Wikipedia Current Events Portal, and …","url":["https://arxiv.org/pdf/2005.10070","https://ui.adsabs.harvard.edu/abs/2020arXiv200510070G/abstract"]}
{"year":"2020","title":"A Large-Scale Semi-Supervised Dataset for Offensive Language Identification","authors":["S Rosenthal, P Atanasova, G Karadzhov, M Zampieri… - arXiv preprint arXiv …, 2020"],"snippet":"… The first layer of the LSTM model is an embedding layer, which we initialize with a concatenation of the GloVe 300-dimensional (Pennington et al., 2014) and FastText's Common Crawl 300dimensional embeddings (Grave et al., 2018). The Page 5 …","url":["https://arxiv.org/pdf/2004.14454"]}
{"year":"2020","title":"A Longitudinal Analysis of Job Skills for Entry-Level Data Analysts","authors":["T Dong, J Triche - Journal of Information Systems Education, 2020"],"snippet":"… Therefore, we used the Common Crawl dataset to address this problem (http:// commoncrawl.org/). Common Crawl is a non-profit organization that builds and maintains an open repository of web crawl data that is, in essence, a copy of the Internet …","url":["http://jise.org/Volume31/n4/JISEv31n4p312.pdf"]}
{"year":"2020","title":"A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages","authors":["P Ortiz Suárez, L Romary, B Sagot - arXiv, 2020","PO Suárez, L Romary, B Sagot - arXiv preprint arXiv:2006.06202, 2020"],"snippet":"… Abstract. We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for several mid-resource languages …","url":["https://arxiv.org/pdf/2006.06202","https://ui.adsabs.harvard.edu/abs/2020arXiv200606202O/abstract"]}
{"year":"2020","title":"A Multilingual Evaluation for Online Hate Speech Detection","authors":["M Corazza, S Menini, E Cabrio, S Tonelli, S Villata - ACM Transactions on Internet …, 2020"],"snippet":"… In particular, we use the Italian and German embeddings trained on Common Crawl and Wikipedia [33] with size 300 … English Fasttext Crawl embeddings: English embeddings trained by Fasttext9 on Common Crawl with an embedding size of 300 …","url":["https://dl.acm.org/doi/abs/10.1145/3377323"]}
{"year":"2020","title":"A Neural-based model to Predict the Future Natural Gas Market Price through Open-domain Event Extraction","authors":["MT Chau, D Esteves, J Lehmann"],"snippet":"… Strong baseline We feed the price and sentence embedding of filtered news using spaCy small English (Context tensor trained on [39], 300-d embedding vector) and large English model (trained on both [39] and Common Crawl …","url":["http://ceur-ws.org/Vol-2611/paper2.pdf"]}
{"year":"2020","title":"A NOVEL APPROACH FOR NAMED ENTITY RECOGNITION ON HINDI LANGUAGE USING RESIDUAL BILSTM NETWORK","authors":["R Shelke, D Thakore"],"snippet":"… It provides word embeddings for Hindi (and 157 other languages) and is based on the CBOW (Continuous Bag-of-Words) model. The CBOW model learns by predicting the current word based on its context, and it was trained …","url":["http://www.academia.edu/download/63216061/120200506-26612-102sbv8.pdf"]}
{"year":"2020","title":"A novel approach to sentiment analysis in Persian using discourse and external semantic information","authors":["R Dehkharghani, H Emami - arXiv preprint arXiv:2007.09495, 2020"],"snippet":"Page 1. * Corresponding Author A novel approach to sentiment analysis in Persian using discourse and external semantic information *Rahim Dehkharghani, Faculty of Engineering, University of Bonab, Bonab, Iran rdehkharghani …","url":["https://arxiv.org/pdf/2007.09495"]}
{"year":"2020","title":"A Novel BGCapsule Network for Text Classification","authors":["AK Gangwar, V Ravi - arXiv preprint arXiv:2007.04302, 2020"],"snippet":"… GloVe. We used GloVe [21] pretrained model. The GloVe model trained on 2.2 million vocabularies, 840 billion tokens of web data from Common Crawl. This Glove embedding projected each word to a 300-dimensional vector …","url":["https://arxiv.org/pdf/2007.04302"]}
{"year":"2020","title":"A novel reasoning mechanism for multi-label text classification","authors":["R Wang, R Ridley, W Qu, X Dai - Information Processing & Management"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0306457320309341"]}
{"year":"2020","title":"A Performance Comparison among Different Amounts of Context on Deep Learning Based Intent Classification Models","authors":["M Jung, J Kim, JY Jang, H Jung, S Shin - 2020 International Conference on …, 2020"],"snippet":"… We also employ word embeddings trained on Common Crawl of fastText [13], a library for efficient text classification and representation learning. We apply a bidirectional LSTM (Bi-LSTM) network [6] to build an LSTM based intent classification model …","url":["https://ieeexplore.ieee.org/abstract/document/9289467/"]}
{"year":"2020","title":"A Practical Approach for Taking Down Avalanche Botnets Under Real-World Constraints","authors":["D Preuveneers, A Duda, W Joosen, M Korczynski"],"snippet":"Page 1. A Practical Approach for Taking Down Avalanche Botnets Under Real-World Constraints Victor Le Pochat∗, Tim Van hamme∗, Sourena Maroofi§, Tom Van Goethem∗, Davy Preuveneers∗, Andrzej Duda§, Wouter Joosen …","url":["https://lirias.kuleuven.be/retrieve/567093/"]}
{"year":"2020","title":"A Practical Guide to Hybrid Natural Language Processing: Combining Neural Models and Knowledge Graphs for NLP","authors":["JM Gomez-Perez, R Denaux, A Garcia-Silva - 2020"],"snippet":"Page 1. Jose Manuel Gomez-Perez Ronald Andres Garcia-Silva Denaux A to Practical Hybrid Natural Guide Language Processing Combining Neural Models and Knowledge Graphs for NLP Page 2. A Practical Guide …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=Ou_rDwAAQBAJ&oi=fnd&pg=PR7&dq=commoncrawl&ots=7ExXbVHzPG&sig=WOLotv9GbQ2RA9QHICwuff_hHVM"]}
{"year":"2020","title":"A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks","authors":["AS Lin, S Rao, A Celikyilmaz, E Nouri, C Brockett… - arXiv preprint arXiv …, 2020"],"snippet":"… We extract text recipes from Common Crawl,2 one of the largest web sources of text … CommonCrawl text-text recipe pairs We randomly choose 200 text-text recipes pairs (spanning 5 dishes) from the test … Table 3: Results for …","url":["https://arxiv.org/pdf/2005.09606"]}
{"year":"2020","title":"A Revised Generative Evaluation of Visual Dialogue","authors":["D Massiceti, V Kulharia, PK Dokania, N Siddharth… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. A Revised Generative Evaluation of Visual Dialogue Daniela Massiceti Viveka Kulharia Puneet K. Dokania N. Siddharth Philip HS Torr University of Oxford {daniela, viveka, puneet, nsid, phst} @robots.ox.ac.uk Abstract …","url":["https://arxiv.org/pdf/2004.09272"]}
{"year":"2020","title":"A Study on Transformer-based Machine Comprehension with Curriculum Learning","authors":["MQ BUI - 2020"],"snippet":"Page 1. Japan Advanced Institute of Science and Technology JAIST Repository https://dspace.jaist.ac.jp/ Title A Study on Transformer-based Machine Comprehension with Curriculum Learning Author(s) BUI, MINH …","url":["https://dspace.jaist.ac.jp/dspace/bitstream/10119/16864/5/paper.pdf"]}
{"year":"2020","title":"A Survey of Document Grounded Dialogue Systems (DGDS)","authors":["L Ma, WN Zhang, M Li, T Liu - arXiv preprint arXiv:2004.13818, 2020"],"snippet":"Page 1. A Survey of Document Grounded Dialogue Systems (DGDS) LONGXUAN MA, Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China WEI-NAN ZHANG, Research Center …","url":["https://arxiv.org/pdf/2004.13818"]}
{"year":"2020","title":"A Survey on Contextual Embeddings","authors":["Q Liu, MJ Kusner, P Blunsom - arXiv preprint arXiv:2003.07278, 2020"],"snippet":"… T5 introduces a new pre-training dataset, Colossal Clean Crawled Corpus by cleaning the web pages from Common Crawl … by training a Transformerbased masked language model on one hundred languages, using more …","url":["https://arxiv.org/pdf/2003.07278"]}
{"year":"2020","title":"A Systematic Study of Inner-Attention-Based Sentence Representations in Multilingual Neural Machine Translation","authors":["R Vázquez, A Raganato, M Creutz, J Tiedemann - Computational Linguistics, 2020"],"snippet":"Page 1. Computational Linguistics Just Accepted MS. https://doi.org/10.1162/ COLI_a_00377 © Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license A Systematic Study of Inner-Attention-Based …","url":["https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00377"]}
{"year":"2020","title":"A Text Augmentation Approach using Similarity Measures based on Neural Sentence Embeddings for Emotion Classification on Microblogs","authors":["YK Shyang, JLS Yan - 2020 IEEE 2nd International Conference on Artificial …, 2020"],"snippet":"… vectors. We used two InferSent models available, which were InferSent trained using GloVe on Common Crawl 840B (InferSent-GloVe) and InferSent trained using fastText on Common Crawl 600B (InferSentfastText). Four …","url":["https://ieeexplore.ieee.org/abstract/document/9257826/"]}
{"year":"2020","title":"A Transformer-based Audio Captioning Model with Keyword Estimation","authors":["Y Koizumi, R Masumura, K Nishida, M Yasuda, S Saito - arXiv preprint arXiv …, 2020"],"snippet":"… sions different. We use the bottleneck feature of VGGish [11] (Dx = 128) for audio embedding, and fastText [18] trained on the Common Crawl corpus (Dw = 300) for caption-word and keyword embedding, respectively. Since the …","url":["https://arxiv.org/pdf/2007.00222"]}
{"year":"2020","title":"A web analytics approach to map the influence and reach of CCAFS","authors":["B Carneiro, G Resce, Y Ma, G Pacillo, P Läderach - 2020"],"snippet":"Page 1. A web analytics approach to map the influence and reach of CCAFS Working Paper No. 326 CGIAR Research Program on Climate Change, Agriculture and Food Security (CCAFS) Page 2. A web analytics …","url":["https://cgspace.cgiar.org/bitstream/handle/10568/110588/Working%20Paper-326.pdf?sequence=1&isAllowed=y"]}
{"year":"2020","title":"A WRITTEN TEST FOR ARTIFICIAL GENERAL INTELLIGENCE","authors":["M Casteluccio - Strategic Finance, 2020"],"snippet":"… If you input a few words, GPT-3 will write a completed thought or sentence. The model was trained on data from Common Crawl, a nonprofit that builds and maintains an open repository of web crawl data accessible to the public for free (common crawl.org) …","url":["http://search.proquest.com/openview/e8f242b31761f187b35a3bb15ab0e724/1?pq-origsite=gscholar&cbl=48426"]}
{"year":"2020","title":"Accenture at CheckThat! 2020: If you say so: Post-hoc fact-checking of claims using transformer-based models","authors":["E Williams, P Rodrigues, V Novak - arXiv preprint arXiv:2009.02431, 2020","V Novak"],"snippet":"… BPE) instead of WordPiece. [23] The base-roberta model was pre-trained on 160GB of text extracted from BookCorpus, English Wikipedia, CC-News, OpenWebText, and Stories (a subset of CommonCrawl Data) [14]. At the time …","url":["http://ceur-ws.org/Vol-2696/paper_226.pdf","https://arxiv.org/pdf/2009.02431"]}
{"year":"2020","title":"Accurate and Fast URL Phishing Detector: A Convolutional Neural Network Approach","authors":["W Wei, Q Ke, J Nowak, M Korytkowski, R Scherer… - Computer Networks, 2020"],"snippet":"… The database downloaded during the article writing contained 10,604 records. To obtain legitimate websites, the second part of the training dataset was downloaded from the Common Crawl Foundation (http://commoncrawl.org/) …","url":["https://www.sciencedirect.com/science/article/pii/S1389128620301109"]}
{"year":"2020","title":"Active Learning for Spreadsheet Cell Classification","authors":["J Gonsior, J Rehak, M Thiele, E Koci, M Günther…"],"snippet":"… Another recent corpus is Fuse [4], which comprises 249, 376 unique spreadsheets, extracted from Common Crawl2. Each spreadsheet is accompanied by a JSON file, which includes NLP token extraction and metrics …","url":["https://wwwdb.inf.tu-dresden.de/wp-content/uploads/SEAData2.pdf"]}
{"year":"2020","title":"Adaptive GloVe and FastText Model for Hindi Word Embeddings","authors":["V Gaikwad, Y Haribhakta - Proceedings of the 7th ACM IKDD CoDS and 25th …, 2020"],"snippet":"… developed using original GloVe model, FastText model (FastTextHin), Adaptive FastText model (AFM) (trained on Hindi monolingual corpus [11] and FastText embeddings published on the website [25] (FastTextWeb) …","url":["https://dl.acm.org/doi/abs/10.1145/3371158.3371179"]}
{"year":"2020","title":"ADD: Academic Disciplines Detector Based on Wikipedia","authors":["A Gjorgjevikj, K Mishev, D Trajanov - IEEE Access, 2020"],"snippet":"… For representation of the textual data into fixed-length vector form using pre-trained text encoding models, in addition to the model files, word vectors trained with GloVe [38] on Common Crawl (840B tokens)6 and with FastText …","url":["https://ieeexplore.ieee.org/iel7/6287639/8948470/08948031.pdf"]}
{"year":"2020","title":"Advanced Semantics for Commonsense Knowledge Extraction","authors":["TP Nguyen, S Razniewski, G Weikum - arXiv preprint arXiv:2011.00905, 2020"],"snippet":"Page 1. Advanced Semantics for Commonsense Knowledge Extraction Tuan-Phong Nguyen Max Planck Institute for Informatics tuanphong@mpi-inf.mpg.de Simon Razniewski Max Planck Institute for Informatics srazniew@mpi-inf.mpg.de …","url":["https://arxiv.org/pdf/2011.00905"]}
{"year":"2020","title":"Advanced Web Crawlers","authors":["JM Patel - Getting Structured Data from the Internet, 2020"],"snippet":"… A good starting point is using common crawl's fork of Apache Nutch 1.x–based crawler. The codebase is open sourced and available on the GitHub repo (https://github.com/commoncrawl), and it's not only well …","url":["https://link.springer.com/chapter/10.1007/978-1-4842-6576-5_8"]}
{"year":"2020","title":"Advancements in Deep Learning Theory and Applications: Perspective in 2020 and beyond","authors":["MN Saadat, M Shuaib - Advances and Applications in Deep Learning, 2020"],"snippet":"The aim of this chapter is to introduce newcomers to deep learning, deep learning platforms, algorithms, applications, and open-source datasets. This chapter will give you a broad overview of the term deep learning, in context …","url":["https://www.intechopen.com/books/advances-and-applications-in-deep-learning/advancements-in-deep-learning-theory-and-applications-perspective-in-2020-and-beyond"]}
{"year":"2020","title":"Advancing Neural Language Modeling in Automatic Speech Recognition","authors":["K Irie - 2020"],"snippet":"Page 1. Advancing Neural Language Modeling in Automatic Speech Recognition Von der Fakultät für Mathematik, Informatik und Naturwissenschaften der RWTH Aachen University zur Erlangung des akademischen Grades …","url":["https://www-i6.informatik.rwth-aachen.de/publications/download/1142/IrieKazuki--AdvancingNeuralLanguageModelinginAutomaticSpeechRecognition--2020.pdf"]}
{"year":"2020","title":"Adversarial Self-Supervised Data-Free Distillation for Text Classification","authors":["X Ma, Y Shen, G Fang, C Chen, C Jia, W Lu - arXiv preprint arXiv:2010.04883, 2020"],"snippet":"… DeepFace (Taigman et al., 2014) is trained on user images under confidential policies for protecting users. Further, some datasets, like Common Crawl dataset used in GPT3 (Brown et al., 2020), contain nearly a trillion words and are difficult to transmit and store …","url":["https://arxiv.org/pdf/2010.04883"]}
{"year":"2020","title":"Adversarial Training for Large Neural Language Models","authors":["X Liu, H Cheng, P He, W Chen, Y Wang, H Poon, J Gao - arXiv preprint arXiv …, 2020"],"snippet":"… For continual pre-training of RoBERTa, we use Wikipedia (13GB), OPENWEBTEXT (public Reddit content (Gokaslan and Cohen); 38GB), STORIES (a subset of CommonCrawl (Trinh and Le, 2018); 31GB). 2https://dumps.wikimedia.org/enwiki/ Page 5 …","url":["https://arxiv.org/pdf/2004.08994"]}
{"year":"2020","title":"Affective Conditioning on Hierarchical Attention Networks applied to Depression Detection from Transcribed Clinical Interviews","authors":["D Xezonaki, G Paraskevopoulos, A Potamianos…"],"snippet":"… Next, for both corpora we tokenize the speaker turns by splitting them into words. We use 300D GloVe [31] pretrained word embeddings, trained on the Common Crawl corpus, to extract word representations. Implementation …","url":["https://indico2.conference4me.psnc.pl/event/35/contributions/3166/attachments/895/934/Thu-3-1-3.pdf"]}
{"year":"2020","title":"Affective Conditioning on Hierarchical Networks applied to Depression Detection from Transcribed Clinical Interviews","authors":["D Xezonaki, G Paraskevopoulos, A Potamianos… - arXiv preprint arXiv …, 2020"],"snippet":"… Next, for both corpora we tokenize the speaker turns by splitting them into words. We use 300D GloVe [31] pretrained word embeddings, trained on the Common Crawl corpus, to extract word representations. Implementation …","url":["https://arxiv.org/pdf/2006.08336"]}
{"year":"2020","title":"AgglutiFiT: Efficient Low-Resource Agglutinative Language Model Fine-Tuning","authors":["Z Li, X Li, J Sheng, W Slamu - IEEE Access, 2020"],"snippet":"… test set. For crosslingual pre-training language models, we use the XLM − R model loaded from the torch.Hub that It is trained on 2.5TB of CommonCrawl data, in 17 languages and uses a large vocabulary size of 95K. XLM …","url":["https://ieeexplore.ieee.org/iel7/6287639/8948470/09164940.pdf"]}
{"year":"2020","title":"AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages","authors":["A Kunchukuttan, D Kakwani, S Golla, A Bhattacharyya… - arXiv preprint arXiv …, 2020"],"snippet":"… FastText also provides embeddings trained on Wikipedia + CommonCrawl corpus … 1. We augmented our crawls with some data from other sources: Leipzig corpus (Goldhahn et al., 2012) (Tamil and Bengali), WMT NewsCrawl …","url":["https://arxiv.org/pdf/2005.00085"]}
{"year":"2020","title":"Algorithmic Bias: On the Implicit Biases of Social Technology","authors":["G Johnson - 2020"],"snippet":"… This study found that parsing software trained on a dataset called “the common crawl”—an assemblage of 840 billion words collected by crawling the internet—resulted in the program producing “human-like semantic …","url":["http://philsci-archive.pitt.edu/17169/1/Algorithmic%20Bias.pdf"]}
{"year":"2020","title":"ALOD2Vec Matcher Results for OAEI 2020","authors":["H Paulheim"],"snippet":"… like DBpedia [8] – but instead on the whole Web: The dataset consists of hypernymy relations extracted from the Common Crawl3, a … 3 see http://commoncrawl.org/ 4 see http://webisa.webdatacommons.org/concept …","url":["http://disi.unitn.it/~pavel/om2020/papers/oaei20_paper2.pdf"]}
{"year":"2020","title":"An AI-Based System for Formative and Summative Assessment in Data Science Courses","authors":["P Vittorini, S Menini, S Tonelli - International Journal of Artificial Intelligence in …, 2020"],"snippet":"Massive open online courses (MOOCs) provide hundreds of students with teaching materials, assessment tools, and collaborative instruments. The assessment a.","url":["https://link.springer.com/article/10.1007/s40593-020-00230-2"]}
{"year":"2020","title":"An Analysis of Dataset Overlap on Winograd-Style Tasks","authors":["A Emami, A Trischler, K Suleman, JCK Cheung - arXiv preprint arXiv:2011.04767, 2020"],"snippet":"… This is a form of data contamination. One of the earliest works that trained a language model on Common Crawl data identified and removed a training documents that overlapped with one of their evaluation datasets (Trinh and Le, 2018) …","url":["https://arxiv.org/pdf/2011.04767"]}
{"year":"2020","title":"An Approach to NMT Re-Ranking Using Sequence-Labeling for Grammatical Error Correction","authors":["B Wang, K Hirota, C Liu, Y Dai, Z Jia"],"snippet":"Page 1. NMT Re-Ranking Using Sequence-Labeling for GEC Paper: An Approach to NMT Re-Ranking Using Sequence-Labeling for Grammatical Error Correction Bo Wang, Kaoru Hirota, Chang Liu, Yaping Dai † , and Zhiyang Jia …","url":["https://www.jstage.jst.go.jp/article/jaciii/24/4/24_557/_pdf"]}
{"year":"2020","title":"An Effective Phishing Detection Model Based on Character Level Convolutional Neural Network from URL","authors":["A Aljofey, Q Jiang, Q Qu, M Huang, JP Niyigena - Electronics, 2020"],"snippet":"Phishing is the easiest way to use cybercrime with the aim of enticing people to give accurate information such as account IDs, bank details, and passwords. This type of cyberattack is usually triggered by emails, instant messages, or …","url":["https://www.mdpi.com/2079-9292/9/9/1514/pdf"]}
{"year":"2020","title":"An Empirical Investigation of Performances of Different Word Embedding Algorithms in Comment Clustering","authors":["E Dorani, N Duru, T Yıldız - 2019 Innovations in Intelligent Systems and …"],"snippet":"… In this study, we used the Common Crawl (1.9 million words)1 and (2.0 million word)2 pre-trained word vectors for Glove and FastText, respectively … Such that we used the Common Crawl (1.9 million words) and (2.0 million word) …","url":["https://ieeexplore.ieee.org/abstract/document/8946379/"]}
{"year":"2020","title":"An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training","authors":["K Arumae, Q Sun, P Bhatia - arXiv preprint arXiv:2010.00784, 2020"],"snippet":"… We processed publicly available bio-medical and non-bio-medical corpora for pre-training our models. For non-bio-medical data, we use BookCorpus and English Wikipedia data, CommonCrawl Stories (Trinh and Le, 2018) …","url":["https://arxiv.org/pdf/2010.00784"]}
{"year":"2020","title":"An Empirical Study of Pre-trained Transformers for Arabic Information Extraction","authors":["W Lan, Y Chen, W Xu, A Ritter - Proceedings of the 2020 Conference on Empirical …, 2020"],"snippet":"… XLM-Rlarge CommonCrawl 295B/55.6B/2.9B SentencePiece 250k/80k/ 14k yes large 550M … and the Gigaword portion three times; (2) adding the Arabic section of the Oscar corpus (Ortiz Suárez et al., 2019), a large-scale …","url":["https://www.aclweb.org/anthology/2020.emnlp-main.382.pdf"]}
{"year":"2020","title":"An Empirical Study of Transformer-Based Neural Language Model Adaptation","authors":["K Li, Z Liu, T He, H Huang, F Peng, D Povey… - ICASSP 2020-2020 IEEE …, 2020"],"snippet":"… We denote the merged corpus as “Source1” in all tables. The second corpus is an English subset (10%) of the Common Crawl News (CCNews) [27], a news corpus contains articles published worldwide between September 2016 and February 2019 …","url":["https://ieeexplore.ieee.org/abstract/document/9053399/"]}
{"year":"2020","title":"An Empirical Study on Explainable Prediction of Text Complexity: Preliminaries for Text Simplification","authors":["C Garbacea, M Guo, S Carton, Q Mei - arXiv preprint arXiv:2007.15823, 2020"],"snippet":"… In our experiments we use the 12 layer XLNeT base pre-trained model on 2https://docs.fast.ai/text.html 4 Page 5. the English Wikipedia and the Books corpus (similar to BERT), and additionally also on Giga5 ClueWeb …","url":["https://arxiv.org/pdf/2007.15823"]}
{"year":"2020","title":"An Empirical Study on Release Engineering of Artificial Intelligence Products","authors":["M Xiu - arXiv preprint arXiv:2012.01403, 2020"],"snippet":"… SPIRAL CelebA HQ 10 (Config: Network Structure) hub.Module Text Embedding Albert Wikipedia, BooksCorpus, Stories, CommonCrawl, Giga5, Clue Web 4 (Config: Model Size) (Data Pre-processing: Cased or Not) hub.Module …","url":["https://arxiv.org/pdf/2012.01403"]}
{"year":"2020","title":"An English–Swahili parallel corpus and its use for neural machine translation in the news domain","authors":["F Sánchez-Martınez, VM Sánchez-Cartagena…"],"snippet":"… 9https://commoncrawl.github.io/ cc-crawl-statistics/plots/languages 10https://github.com/ CLD2Owners/cld2 300 Page 3. of crawling and, from the remaining 3 232, only 908 ended up containing data in both languages. Document alignment …","url":["https://www.dlsi.ua.es/~fsanchez/pub/pdf/sanchez-martinez20b.pdf"]}
{"year":"2020","title":"An Enhanced Sentiment Analysis Framework Based on Pre-Trained Word Embedding","authors":["EH Mohamed, MES Moussa, MH Haggag - International Journal of Computational …, 2020"],"snippet":"Login to your account …","url":["https://www.worldscientific.com/doi/abs/10.1142/S1469026820500315"]}
{"year":"2020","title":"An Evaluation Benchmark for Testing the Word Sense Disambiguation Capabilities of Machine Translation Systems","authors":["A Raganato, Y Scherrer, J Tiedemann - … of The 12th Language Resources and …, 2020"],"snippet":"… Page 2. 3669 CS–EN DE–EN FI–EN FR–EN RU–EN Books GlobalVoices Europarl JW300 News-Comm. Tatoeba TED Talks EU Bookshop MultiUN Common Crawl Table 1: Corpora used to extract the MuCoW test suites. The …","url":["https://www.aclweb.org/anthology/2020.lrec-1.452.pdf"]}
{"year":"2020","title":"An Evaluation Model for Auto-generated Cognitive Scripts","authors":["AM ELMougi, YMK Omar, R Hodhod"],"snippet":"… Fig. 6. The Cinema Linear Cognitive Script Converted into Text. B. Computing the GloVe Similarity Ratio Threshold The proposed model uses GloVe vectors of 300 dimensions that are created by training Common Crawl (840B tokens …","url":["https://pdfs.semanticscholar.org/14fd/085addcbcebe7e531198d52f041c4e86a3d9.pdf"]}
{"year":"2020","title":"An Evaluation of Recent Neural Sequence Tagging Models in Turkish Named Entity Recognition","authors":["G Aras, D Makaroglu, S Demir, A Cakir - arXiv preprint arXiv:2005.07692, 2020"],"snippet":"Page 1. An Evaluation of Recent Neural Sequence Tagging Models in Turkish Named Entity Recognition Gizem Arasa,, Didem Makaroglua,b, Seniz Demirc and Altan Cakirb aDemiroren Teknoloji AS, Istanbul, Turkey bDepartment …","url":["https://arxiv.org/pdf/2005.07692"]}
{"year":"2020","title":"An Exploration in L2 Word Embedding Alignment","authors":["P Liao"],"snippet":"… It is also run for Wikipedia Chinese fastText embeddings to Common Crawl English fastText embeddings to test the method on a large corpus of a different domain. All fastText embeddings are pretrained and downloaded from their website16 …","url":["https://pdfs.semanticscholar.org/513e/16939e369cf09be34ac4c983d20be53a94b1.pdf"]}
{"year":"2020","title":"An Exploratory Approach to the Corpus Filtering Shared Task WMT20","authors":["A Kejriwal, P Koehn"],"snippet":"… We make the number of lines taken by the wikipedia data be between 40-50% of the number of lines taken by the CommonCrawl data … entropy as defined in Equation 5. We find that the scores we got were ex- tremely …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.108.pdf"]}
{"year":"2020","title":"An Interactive Network for End-to-End Review Helpfulness Modeling","authors":["J Du, L Zheng, J He, J Rong, H Wang, Y Zhang - Data Science and Engineering, 2020"],"snippet":"Review helpfulness prediction aims to prioritize online reviews by quality. Existing methods largely combine review texts and star ratings for helpfulness.","url":["https://link.springer.com/article/10.1007/s41019-020-00133-1"]}
{"year":"2020","title":"An Iterative Knowledge Transfer NMT System for WMT20 News Translation Task","authors":["J Kim, S Park, S Kim, Y Choi - Proceedings of the Fifth Conference on Machine …, 2020"],"snippet":"… Kyoto Free Translation Task 0.44M TED Talks 0.24M Monolingual Data (En) Europarl v10 2.29M News Commentary v15 0.6M News Crawl 23.35M News Discussions 63.51M Monolingual Data (Ja) News Crawl …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.11.pdf"]}
{"year":"2020","title":"An Open-Domain Web Search Engine for Answering Comparative Questions","authors":["T Abye, T Sager, AJ Triebel"],"snippet":"… pp. 91–96 (2017) 3. Bevendorff, J., Stein, B., Hagen, M., Potthast, M.: Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl. In: Azzopardi, L., Hanbury, A., Pasi, G., Piwowarski, B. (eds.) Advances in Information Retrieval …","url":["http://ceur-ws.org/Vol-2696/paper_130.pdf"]}
{"year":"2020","title":"Analogical frames by constraint satisfaction","authors":["L De Vine - 2020"],"snippet":"Page 1. Analogical Frames by Constraint Satisfaction A THESIS SUBMITTED IN FULFILMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY Lance De Vine BMaths …","url":["https://eprints.qut.edu.au/198036/1/Lance_De%20Vine_Thesis.pdf"]}
{"year":"2020","title":"Analysis of the communication of smart city features through social media","authors":["JP Fontanilles"],"snippet":"Page 1. Jordi Pascual Fontanilles Analysis of the communication of smart city features through social media MASTER'S THESIS Supervised by Dr. Antonio Moreno Ribas Computer Security Engineering and Artificial …","url":["https://deim.urv.cat/~itaka/itaka2/PDF/acabats/MEMORIA_TFM_JordiPascual.pdf"]}
{"year":"2020","title":"Analyzing Sustainability Reports Using Natural Language Processing","authors":["A Luccioni, E Bailor, N Duchene - arXiv preprint arXiv:2011.08073, 2020"],"snippet":"… In fact, research in financial NLP has found that using general-purpose NLP models trained on corpora such as Wikipedia and the Common Crawl fail to capture domainspecific terms and concepts which are critical for a coherent …","url":["https://arxiv.org/pdf/2011.08073"]}
{"year":"2020","title":"Analyzing the Effect of Community Norms on Gender Bias","authors":["NR Raut - 2020"],"snippet":"… We use word2vec to train embeddings on the comment data for each of the subreddit. For fasttext, we use 2 million word vectors trained with subword information on Common Crawl …","url":["http://search.proquest.com/openview/18f1238b848a27a836459d849f5795c8/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2020","title":"ANDI@ CONCRETEXT: Predicting concreteness in context for English and Italian using distributional models and behavioural norms","authors":["A Rotaru - Proceedings of the 7th evaluation campaign of Natural …, 2020"],"snippet":"… Skip-gram (Google News – 100B) 21 21 21 21 GloVe (Common Crawl – 840B) 21 21 21 21 ConceptNet NumberBatch (ConceptNet + Skip-gram + GloVe) … abs(V(w) - V(c)) Behavioural norms (frequency, etc.) 20 20 20 20 …","url":["http://ceur-ws.org/Vol-2765/paper100.pdf"]}
{"year":"2020","title":"Announcing CzEng 2.0 Parallel Corpus with over 2 Gigawords","authors":["T Kocmi, M Popel, O Bojar - arXiv preprint arXiv:2007.03006, 2020"],"snippet":"… format. New parallel data come from Europarl (v10), News commentary, Wikititles, Commoncrawl, Paracrawl2, WikiMatrix (Schwenketal., 2019), and Tilde MODEL Corpus (EESC, EMA, Rapid; Rozis and Skadinš, 2017). We …","url":["https://arxiv.org/pdf/2007.03006"]}
{"year":"2020","title":"Anomaly detection with Generative Adversarial Networks and text patches","authors":["A Drozdyuk, N Eke"],"snippet":"… We used the model containing two million word vectors trained on Common Crawl … The depressive data was similarly cleaned. After cleaning the data it was converted to word vectors using the FastText model trained on Common Crawl and Wikipedia [12] …","url":["https://norberte.github.io/assets/pdf/GAN%20Project%20Report.pdf"]}
{"year":"2020","title":"Another factor to consider is the user's perspective. Everyone uses web search. Using a search engine to look things up on the web is the most popular activity on the …","authors":["D Lewandowski"],"snippet":"… Indexes that offer complete access do, however, exist. At the top of this list is Common Crawl,NOTE 12 a nonprofit project that aims to provide a web index for anyone who's interested … Common Crawl represents an important development …","url":["https://pandoc.networkcultures.org/epub/SotQreader/ch010.xhtml"]}
{"year":"2020","title":"Answering Comparative Questions with Arguments","authors":["A Bondarenko, A Panchenko, M Beloucif, C Biemann… - Datenbank-Spektrum, 2020"],"snippet":"… analyzing Wikidata and DBpedia as additional sources of (structured) information besides the retrieval of sentences/documents from the Common Crawl … Ruppert E, Faralli S, Ponzetto SP, Biemann C (2018) Building a …","url":["https://link.springer.com/article/10.1007/s13222-020-00346-8"]}
{"year":"2020","title":"Answering Event-Related Questions over Long-term News Article Archives","authors":["J Wang, A Jatowt, M Färber, M Yoshikawa"],"snippet":"… We can see that the actual time scope (January, 1988) of the first question is reflected relatively well by its distribution of relevant documents as generally 4 We use Glove [23] embeddings trained on the Common Crawl dataset with 300 dimensions. Page 6 …","url":["http://www.aifb.kit.edu/images/1/19/QA_ECIR2020.pdf"]}
{"year":"2020","title":"Application of Machine Learning Techniques for Text Generation","authors":["S Martí Román - 2020"],"snippet":"Page 1. Escola Tècnica Superior d'Enginyeria Informàtica Universitat Politècnica de València Application of Machine Learning Techniques for Text Generation DEGREE FINAL WORK Degree in Computer Engineering Author: Salvador Martí Román …","url":["https://riunet.upv.es/bitstream/handle/10251/149583/Mart%C3%AD%20-%20Uso%20de%20t%C3%A9cnicas%20de%20aprendizaje%20autom%C3%A1tico%20para%20la%20generaci%C3%B3n%20de%20texto.pdf?sequence=1"]}
{"year":"2020","title":"Application of Machine Learning to Classify News Headlines","authors":["P Guttula, RM Aburas, S Srijan"],"snippet":""}
{"year":"2020","title":"AQuaMuSe: Automatically Generating Datasets for Query-Based Multi-Document Summarization","authors":["S Kulkarni, S Chammas, W Zhu, F Sha, E Ie - arXiv preprint arXiv:2010.12694, 2020"],"snippet":"… is a nontrivial task in itself and there are several con1https://commoncrawl org … paragraphs, we use a pre-processed and cleaned version of the Common Crawl corpus (Raffel et al … We illustrate our approach us- ing Google's Natural …","url":["https://arxiv.org/pdf/2010.12694"]}
{"year":"2020","title":"AraWEAT: Multidimensional Analysis of Biases in Arabic Word Embeddings","authors":["A Lauscher, R Takieddin, SP Ponzetto, G Glavaš - arXiv preprint arXiv:2011.01575, 2020"],"snippet":"… For FT, we investigate two models, one trained on the portions of Wikipedia and CommonCrawl corpora written in Modern Standard Arabic (MS) and the other on portions written in Egyptian Arabic.9 We evaluate the four variants …","url":["https://arxiv.org/pdf/2011.01575"]}
{"year":"2020","title":"ArchiMeDe@ DANKMEMES: A New Model Architecture for Meme Detection","authors":["J Setpal, G Sarti"],"snippet":"… We fine-tune representations over the available meme textual data and use them as components of our end-to-end system. 1umberto-commoncrawl-cased-v1 in the HuggingFace's model hub (Wolf et al., 2019) Page 4. 2.3 Visual input …","url":["http://ceur-ws.org/Vol-2765/paper138.pdf"]}
{"year":"2020","title":"Are All Good Word Vector Spaces Isomorphic?","authors":["I Vulić, S Ruder, A Søgaard - arXiv preprint arXiv:2004.04070, 2020"],"snippet":"… 3Recent initiatives replace training on Wikipedia with training on larger CommonCrawl data (Grave et al., 2018; Conneau et al., 2020), but the large differences in corpora sizes between high-resource and low-resource languages are not removed …","url":["https://arxiv.org/pdf/2004.04070"]}
{"year":"2020","title":"Are All Languages Created Equal in Multilingual BERT?","authors":["S Wu, M Dredze - arXiv preprint arXiv:2005.09093, 2020"],"snippet":"… (2019) train a multilingual masked language model (Devlin et al., 2019) on 2.5TB of CommonCrawl filtered data covering 100 languages and show it outperforms a Wikipedia-based model on low resource languages (Urdu …","url":["https://arxiv.org/pdf/2005.09093"]}
{"year":"2020","title":"Argumentative relation classification for argumentative dialogue systems","authors":["C Schindler - 2020"],"snippet":"… With this setup, args outperformed ArgumenText in the category related. 3.2.2. ArgumenText The search engine ArgumenText [35] uses the English part of CommonCrawl2 to retrieve relevant documents … 2http://commoncrawl …","url":["https://oparu.uni-ulm.de/xmlui/bitstream/handle/123456789/33973/BScThesis_SchindlerC.pdf?sequence=3&isAllowed=y"]}
{"year":"2020","title":"Argumentative Topology: Finding Loop (holes) in Logic","authors":["S Tymochko, Z New, L Bynum, E Purvine, T Doster… - arXiv preprint arXiv …, 2020"],"snippet":"… Word embeddings are performed using two pretrained models: Word2Vec trained on the Google News dataset [12] and GloVe trained on Common Crawl [13]. We compute the persistence diagrams of the topological …","url":["https://arxiv.org/pdf/2011.08952"]}
{"year":"2020","title":"ArgumenText: Argument Classification and Clustering in a Generalized Search Scenario","authors":["J Daxenberger, B Schiller, C Stahlhut, E Kaiser… - Datenbank-Spektrum, 2020"],"snippet":"… Full size image. For the public version of the ArgumenText search engine, we indexed more than 400 million English and German web pages from the CommonCrawl project and segmented all documents into sentences [21] …","url":["https://link.springer.com/article/10.1007/s13222-020-00347-7"]}
{"year":"2020","title":"Artificial Intelligence in mental health and the biases of language based models","authors":["I Straw, C Callison-Burch - PloS one, 2020"],"snippet":"… 52]. As described by Pennington et al. GloVe embeddings were trained on text copora from Wikipedia data, Gigaword and web data from Common Crawl which built a vocabulary of 400,000 frequent words [57]. Word2Vec was …","url":["https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0240376"]}
{"year":"2020","title":"ASAPPpy: a Python Framework for Portuguese STS?","authors":["J Santos, A Alves, HG Oliveira"],"snippet":"… 20 Page 8. From those, CBOW Word2vec and GloVe, both with 300-dimensioned vectors, were selected; (ii) fastText.cc embeddings [9], which provide word vectors for 157 languages, trained on Common Crawl and Wikipedia using fastText …","url":["http://ceur-ws.org/Vol-2583/2_ASAPPpy.pdf"]}
{"year":"2020","title":"Ascent of Pre-trained State-of-the-Art Language Models","authors":["K Nagda, A Mukherjee, M Shah, P Mulchandani… - Advanced Computing …, 2020"],"snippet":"… 6.2 Dataset. Similar to BERT, XLNet was pre-trained using English Wikipedia dataset (13 GB of plain text), as well as CommonCrawl, Giga5 and ClueWeb 2012-B datasets [11]. The large variant of the model has a sequence and …","url":["https://link.springer.com/chapter/10.1007/978-981-15-3242-9_26"]}
{"year":"2020","title":"Aspect-Controlled Neural Argument Generation","authors":["B Schiller, J Daxenberger, I Gurevych - Training"],"snippet":"… Consequently, the following preprocessing steps ultimately target retrieval and classification of sentences. To evaluate different data sources, we use a dump from Common-Crawl2 (CC) and Reddit comments3 …","url":["https://public.ukp.informatik.tu-darmstadt.de/UKP_Webpage/publications/2020/2020_PP_BES_aspect_controlled_argument_generation_v0.2.pdf"]}
{"year":"2020","title":"Assessing Demographic Bias in Named Entity Recognition","authors":["S Mishra, S He, L Belli"],"snippet":"… level confidence via the Constrained ForwardBackward algorithm [5]. Different versions of this model were trained on CoNLL 03 NER benchmark dataset [27] by utilizing varying embedding methods: (a) GloVe uses GloVe 840B …","url":["https://kg-bias.github.io/NER_Bias_KG_Bias.pdf"]}
{"year":"2020","title":"Assessing Suitable Word Embedding Model for Malay Language through Intrinsic Evaluation","authors":["YT Phua, KH Yew, OM Foong, MYW Teow - 2020 International Conference on …, 2020"],"snippet":"… This mode was trained on Common Crawl and Wikipedia [26], and pre-trained word vectors for 294 languages were trained on Wikipedia [22]. The results of the evaluation were 0.477 for Pearson correlation coefficient and 0.51 …","url":["https://ieeexplore.ieee.org/abstract/document/9247707/"]}
{"year":"2020","title":"ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations","authors":["F Alva-Manchego, L Martin, A Bordes, C Scarton… - arXiv preprint arXiv …, 2020"],"snippet":"… stopwords). We use the 50k most frequent words of the FastText word embeddings vo- cabulary (Bojanowski et al., 2016). This vo- cabulary was originally sorted with frequencies of words in the Common Crawl. This score …","url":["https://arxiv.org/pdf/2005.00481"]}
{"year":"2020","title":"Attention-based hierarchical recurrent neural networks for MOOC forum posts analysis","authors":["N Capuano, S Caballé, J Conesa, A Greco - Journal of Ambient Intelligence and …, 2020"],"snippet":"… WikiNER corpora (Nivre et al. 2016) for Italian as well as the OntoNotes (Pradhan and Ramshaw 2017) and the Common Crawl Footnote 2 corpora for English. Text categorization model. Independently from the specific document …","url":["https://link.springer.com/article/10.1007/s12652-020-02747-9"]}
{"year":"2020","title":"Attention-based Model for Evaluating the Complexity of Sentences in English Language","authors":["D Schicchi, G Pilato, GL Bosco - 2020 IEEE 20th Mediterranean Electrotechnical …, 2020"],"snippet":"… The output of the attention layer (context-vector) is given as input to a dense layer, which gives the probability that the sentence belongs to either hard-to-understand or easy-to- 1www.wikipedia.org 2www.commoncrawl …","url":["https://ieeexplore.ieee.org/abstract/document/9140531/"]}
{"year":"2020","title":"Attribute Sentiment Scoring with Online Text Reviews: Accounting for Language Structure and Missing Attributes","authors":["I Chakraborty, M Kim, K Sudhir - 2020"],"snippet":"… 10These embeddings have been trained on different corpus like Wikipedia dumps, Gigaword news dataset and web data from Common Crawl and have more than 5 billion unique tokens. Page 18. 17 lenging text and image classification problems (Wang et al …","url":["https://www.ishitachakra.com/JobMarketPaper_IshitaYale.pdf"]}
{"year":"2020","title":"Augmenting cross-domain knowledge bases using web tables","authors":["Y Oulabi - 2020"],"snippet":"Page 1. Augmenting Cross-Domain Knowledge Bases Using Web Tables Inauguraldissertation zur Erlangung des akademischen Grades eines Doktors der Naturwissenschaften der Universität Mannheim vorgelegt von Yaser …","url":["https://madoc.bib.uni-mannheim.de/55962/1/Oulabi2020_PhD_thesis.pdf"]}
{"year":"2020","title":"Author2Vec: A Framework for Generating User Embedding","authors":["X Wu, W Lin, Z Wang, E Rastorgueva - arXiv preprint arXiv:2003.11627, 2020"],"snippet":"… the user. We used the Facebook FastText (https:// fasttext.cc/) pre-trained Word2Vec model: crawl-300d-2M, which is a model with 2 million word vectors trained on Common Crawl (600B to- kens). The baseline implementations …","url":["https://arxiv.org/pdf/2003.11627"]}
{"year":"2020","title":"Autoencoding Improves Pre-trained Word Embeddings","authors":["M Kaneko, D Bollegala - arXiv preprint arXiv:2010.13094, 2020"],"snippet":"… 3M words learnt from the Google News corpus), GloVe2 (300-dimensional word embeddings for ca. 2.1M words learnt from the Common Crawl), and fastText3 (300dimensional embeddings for ca. 2M words learnt from the Common Crawl) …","url":["https://arxiv.org/pdf/2010.13094"]}
{"year":"2020","title":"Automated coding of implicit motives: A machine‑learning approach","authors":["JS Pang, H Ring"],"snippet":"… experiments we decided to use Facebook's FastText subword embeddings of 300 dimensions trained on Common Crawl (600 billion tokens).5 This is the set of pre-trained vectors that we used to derive word features from …","url":["https://link.springer.com/content/pdf/10.1007/s11031-020-09832-8.pdf"]}
{"year":"2020","title":"Automated Short Answer Grading: A Simple Solution for a Difficult Task","authors":["S Menini, S Tonelli, G De Gasperis, P Vittorini"],"snippet":"… combining vectors representing both words and subwords. To generate these embeddings we start from the pre-computed Italian language model3 trained on Common Crawl and Wikipedia. The latter, in particular, is suitable …","url":["https://pdfs.semanticscholar.org/9ff2/6a502dd1b3e0c136af4bc2ca9af9b901fce4.pdf"]}
{"year":"2020","title":"Automatic Detection of Machine Generated Text: A Critical Survey","authors":["G Jawahar, M Abdul-Mageed, LVS Lakshmanan - arXiv preprint arXiv:2011.01314, 2020"],"snippet":"… GPT-3 (Brown et al., 2020) fragments from CommonCrawl (570GB / 175B) three previous news articles and title of a proposed article body of the proposed article top-p fake news Table 1: Summary of the characteristics of TGMs that can act as threat models …","url":["https://arxiv.org/pdf/2011.01314"]}
{"year":"2020","title":"Automatic language identification of short texts","authors":["A Avenberg - 2020"],"snippet":"Page 1. UPTEC F 20043 Examensarbete 30 hp September 2020 Automatic language identification of short texts Anna Avenberg Page 2. Teknisknaturvetenskaplig fakultet UTH-enheten Besöksadress: Ångströmlaboratoriet …","url":["https://www.diva-portal.org/smash/get/diva2:1473718/FULLTEXT01.pdf"]}
{"year":"2020","title":"Automatic Metaphor Interpretation Using Word Embeddings","authors":["K Bar, N Dershowitz, L Dankin - arXiv preprint arXiv:2010.02665, 2020"],"snippet":"… relatively large corpus. Specifically, we use DepCC,1 a dependency-parsed “web-scale corpus” based on CommonCrawl.2 There are 365 million documents in the corpus, comprising about 252B tokens. Among other preprocessing …","url":["https://arxiv.org/pdf/2010.02665"]}
{"year":"2020","title":"Automatic Poetry Generation from Prosaic Text","authors":["T Van de Cruys - Proceedings of the 58th Annual Meeting of the …, 2020"],"snippet":"Page 1. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2471–2480 July 5 - 10, 2020. c 2020 Association for Computational Linguistics 2471 Automatic Poetry Generation from Prosaic Text …","url":["https://www.aclweb.org/anthology/2020.acl-main.223.pdf"]}
{"year":"2020","title":"Automatic Short Answer Grading using Text-to-Text Transfer Transformer Model","authors":["S Haller - 2020"],"snippet":"Page 1. Faculty of Electrical Engineering, Mathematics & Computer Science Automatic Short Answer Grading using Text-to-Text Transfer Transformer Model Stefan Haller M.Sc. Thesis in Business Information …","url":["http://essay.utwente.nl/83879/7/Haller_MA_EEMCS.pdf"]}
{"year":"2020","title":"Automatic Speech Recognition for ILSE-Interviews: Longitudinal Conversational Speech Recordings covering Aging and Cognitive Decline","authors":["A Abulimiti, J Weiner, T Schultz"],"snippet":"… In addition, we selected text data similar to ILSE from large amounts of Common Crawl Data1 based on Term Frequency–Inverse Document Frequency (tf-idf) for the training of Recurrent Neural Network (RNN) based …","url":["https://indico2.conference4me.psnc.pl/event/35/contributions/3209/attachments/742/780/Thu-SS-1-6-9.pdf"]}
{"year":"2020","title":"Automatic text segmentation based on relevant context","authors":["S Kim, WW Chang, N Lipka, F Dernoncourt, CY Park - US Patent App. 16/368,334, 2020"],"snippet":"US20200311207A1 - Automatic text segmentation based on relevant context - Google Patents. Automatic text segmentation based on relevant context. Download PDF Info. Publication number US20200311207A1. US20200311207A1 …","url":["https://patents.google.com/patent/US20200311207A1/en"]}
{"year":"2020","title":"Automating Just-In-Time Comment Updating","authors":["Z Liu, X Xia, M Yan, S Li","Z Liu, X Xia, M Yan, S Li - Automated Software Engineering, to appear, 2020"],"snippet":"Page 1. Automating Just-In-Time Comment Updating Zhongxin Liu∗† Zhejiang University China liu_zx@zju.edu.cn Xin Xia‡ Monash University Australia xin.xia@monash.edu Meng Yan Chongqing University China mengy@cqu.edu.cn …","url":["https://pdfs.semanticscholar.org/0b03/b1526e88c8780214697bbe9163b95a59cdbd.pdf","https://xin-xia.github.io/publication/ase202.pdf"]}
{"year":"2020","title":"Automating Text Naturalness Evaluation of NLG Systems","authors":["E Çano, O Bojar - arXiv preprint arXiv:2006.13268, 2020"],"snippet":"… The model is trained on RealNews, a large news collection they derived from Page 4. 4 E. Çano and O. Bojar Common Crawl1 dumps … They are typically assessed by 1 https://commoncrawl.org/ 2 http://www.statmt.org …","url":["https://arxiv.org/pdf/2006.13268"]}
{"year":"2020","title":"BanFakeNews: A Dataset for Detecting Fake News in Bangla","authors":["MZ Hossain, MA Rahman, MS Islam, S Kar - arXiv preprint arXiv:2004.08789, 2020"],"snippet":"… words in it. We experiment with the Bangla 300 dimensional word vectors pre-trained7 with Fasttext (Grave et al., 2018) on Wikipedia8 and Common Crawl9, where we have a coverage of 55.21%. Additionally, we experiment …","url":["https://arxiv.org/pdf/2004.08789"]}
{"year":"2020","title":"Bangla Text Classification using Transformers","authors":["T Alam, A Khan, F Alam - arXiv preprint arXiv:2011.04446, 2020"],"snippet":"… NSP task is removed and only MLM loss is used for pretraining. XLM-RoBERTa [18] is the multilingual variant of RoBERTa trained with a multilingual MLM. It is trained on one hundred languages, with more than two terabytes of filtered Common Crawl data …","url":["https://arxiv.org/pdf/2011.04446"]}
{"year":"2020","title":"Bankruptcy Map: A System for Searching and Analyzing US Bankruptcy Cases at Scale","authors":["E Choi, G Brassil, K Keller, J Ouyang, K Wang"],"snippet":"… layers [14]. The network was pre-trained on the large OntoNotes dataset, with GloVe vectors used for feature creation trained on Common Crawl data [3, 16]. The model recognized named entities and their types. We collected …","url":["https://cpb-us-w2.wpmucdn.com/express.northeastern.edu/dist/d/53/files/2020/02/CJ_2020_paper_57.pdf"]}
{"year":"2020","title":"BARThez: a Skilled Pretrained French Sequence-to-Sequence Model","authors":["MK Eddine, AJP Tixier, M Vazirgiannis - arXiv preprint arXiv:2010.12321, 2020"],"snippet":"… Other than that, BARTHez corpus is similar to FlauBERT's. It primarily consists in the French part of CommonCrawl, NewsCrawl, Wikipedia and other smaller corpora that are listed in Table 1. To clean the corpus from noisy examples …","url":["https://arxiv.org/pdf/2010.12321"]}
{"year":"2020","title":"Benchmarking Neural and Statistical Machine Translation on Low-Resource African Languages","authors":["K Duh, P McNamee, M Post, B Thompson"],"snippet":"… The columns CommonCrawl and Wikipedia indicate the amount of monolingual data on the web, which can be viewed as an indicator of the upper limit of how much web-crawled data we may be able to obtain. CommonCrawl …","url":["https://pdfs.semanticscholar.org/3bde/97a22dab1147b0f3209805315bbff9b82674.pdf"]}
{"year":"2020","title":"BERT-based Ensembles for Modeling Disclosure and Support in Conversational Social Media Text","authors":["K Pant, T Dadu, R Mamidi - 2020"],"snippet":"… Gokaslan, A., Cohen, V.: Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus (2019) 5. Nagel, S.: Cc-news (2016), http://web.archive.org/save/ http://commoncrawl. org/2016/10/newsdataset-available/ 6. Rajendran …","url":["http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.83b74a0e278a6d03.424552542d626173656420456e73656d626c657320666f72204d6f64656c696e6720446973636c6f73757265202620537570706f727420696e20436f6e766572736174696f6e616c20536f6369616c204d6564696120546578742e706466.pdf"]}
{"year":"2020","title":"BERT-Based Simplification of Japanese Sentence-Ending Predicates in Descriptive Text","authors":["T Kato, R Miyata, S Sato - Proceedings of the 13th International Conference on …, 2020"],"snippet":"… For any parts having more than one word, the average of the embedding is used. To obtain the embedding vectors, we used existing Japanese pre-trained word vectors that were trained on Common Crawl and Wikipedia using fastText.7 …","url":["https://www.aclweb.org/anthology/2020.inlg-1.31.pdf"]}
{"year":"2020","title":"BERTweet: A pre-trained language model for English Tweets","authors":["DQ Nguyen, T Vu, AT Nguyen - arXiv preprint arXiv:2005.10200, 2020"],"snippet":"… The pre-trained RoBERTa is a strong language model for English, learned from 160GB of texts covering books, Wikipedia, CommonCrawl news, CommonCrawl stories, and web text contents. XLM-R is a cross-lingual variant …","url":["https://arxiv.org/pdf/2005.10200"]}
{"year":"2020","title":"Better Web Corpora For Corpus Linguistics And NLP","authors":["V Suchomel"],"snippet":"Page 1. Masaryk University Faculty of Informatics Better Web Corpora For Corpus Linguistics And NLP Doctoral Thesis Vít Suchomel Brno, Spring 2020 Page 2. Masaryk University Faculty of Informatics Better Web Corpora …","url":["https://is.muni.cz/th/u4rmz/Better_Web_Corpora_For_Corpus_Linguistics_And_NLP.pdf"]}
{"year":"2020","title":"Beyond English-Centric Multilingual Machine Translation","authors":["A Fan, S Bhosale, H Schwenk, Z Ma, A El-Kishky… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Beyond English-Centric Multilingual Machine Translation Angela Fan∗, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary …","url":["https://arxiv.org/pdf/2010.11125"]}
{"year":"2020","title":"Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube","authors":["J Hessel, Z Zhu, B Pang, R Soricut - arXiv preprint arXiv:2004.14338, 2020"],"snippet":"… corresponding visual content. However, in contrast to the highly diverse corpora utilized for text-based pretraining (Wikipedia, Common Crawl, etc.), pretraining for web videos so far has been limited to instructional videos. This domain …","url":["https://arxiv.org/pdf/2004.14338"]}
{"year":"2020","title":"Biases as Values: Evaluating Algorithms in Context","authors":["M Díaz - 2020"],"snippet":"Page 1. NORTHWESTERN UNIVERSITY Biases as Values: Evaluating Algorithms in Context A DISSERTATION SUBMITTED TO THE GRADUATE SCHOOL IN PARTIAL FULFILLMENT OF THE REQUIREMENTS for the degree DOCTOR OF PHILOSOPHY …","url":["http://search.proquest.com/openview/83eed19485a394e067ee5a9b03d84ef2/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2020","title":"Biomedical Information Extraction Pipelines for Public Health in the Age of Deep Learning","authors":["AM Ranganatha - 2019"],"snippet":"Page 1. Biomedical Information Extraction Pipelines for Public Health in the Age of Deep Learning by Arjun Magge Ranganatha A Dissertation Presented in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy …","url":["http://search.proquest.com/openview/bf6cbc4695dc3135c8e78ff548e670f8/1?pq-origsite=gscholar&cbl=2026366&diss=y"]}
{"year":"2020","title":"Blocking Techniques for Entity Linkage: A Semantics-Based Approach","authors":["F Azzalini, S Jin, M Renzi, L Tanca - Data Science and Engineering, 2020"],"snippet":"… each attribute value \\(t[A_{k}]\\) is transformed into a real-valued vector \\(\\mathbf{v }(w)\\). The fastText model we use is crawl-300d-2M-subword [23] where each word is represented as a 300-dimensional vector and the …","url":["https://link.springer.com/article/10.1007/s41019-020-00146-w"]}
{"year":"2020","title":"Bottom-Up Modeling of Permissions to Reuse Residual Clinical Biospecimens and Health Data","authors":["E Umberfield - 2020"],"snippet":"Page 1. Bottom-Up Modelling of Permissions to Reuse Residual Clinical Biospecimens and Health Data by Elizabeth Umberfield A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor …","url":["https://deepblue.lib.umich.edu/bitstream/handle/2027.42/162937/eliewolf_1.pdf?sequence=1"]}
{"year":"2020","title":"BREXIT: Psychometric Profiling the Political Salubrious through Machine Learning","authors":["J Usher, P Dondio"],"snippet":"… We used the English multi-task Convoluted Neural Network trained on OntoNotes, with GloVe vectors trained on Common Crawl, which assigns word vectors, context-specific token vectors, POS tags, dependency parse …","url":["http://wims2020.sigappfr.org/wp-content/uploads/2020/06/WIMS'20/p178-Usher.pdf"]}
{"year":"2020","title":"BREXIT: Psychometric Profiling the Political Salubrious through Machine Learning: Predicting personality traits of Boris Johnson through Twitter political text","authors":["J Usher, P Dondio - Proceedings of the 10th International Conference on …, 2020"],"snippet":"… We used the English multi-task Convoluted Neural Network trained on OntoNotes, with GloVe vectors trained on Common Crawl, which assigns word vectors, context-specific token vectors, POS tags, dependency parse …","url":["https://dl.acm.org/doi/abs/10.1145/3405962.3405981"]}
{"year":"2020","title":"Building a user-generated content north-african arabizi treebank: Tackling hell","authors":["D Seddah, F Essaidi, A Fethi, M Futeral, B Muller… - Proceedings of the 58th …, 2020"],"snippet":"… used data-driven language identification models to extract NArabizi samples among the whole collection of the Common-Crawl-based OSCAR … one based on search query-based web-crawling and the other from a cleaned version …","url":["https://www.aclweb.org/anthology/2020.acl-main.107.pdf"]}
{"year":"2020","title":"Building a Wide Reach Corpus for Secure Parser Development","authors":["T Allison, W Burke, V Constantinou, E Goh…"],"snippet":"… [17] CGR Lavanya Pamulaparty and MS Rao, “A novel approach for avoiding overload in the web crawling.” Odisha, India: High Performance Computing and Applications (ICHPCA), 2014. [18] “Common Crawl,” https://commoncrawl.org …","url":["http://spw20.langsec.org/papers/corpus_LangSec2020.pdf"]}
{"year":"2020","title":"Building LARO: Language Agnostic Sentence Embeddings from finetuned RoBERTa⋆","authors":["AS Salvado"],"snippet":"… XLM-RoBERTa is the successor of RoBERTa and a large multi-lingual language model. It is a Transformer-based model, technically a Transformer en- coder, and was trained on 2.5TB of filtered CommonCrawl data in 100 …","url":["https://users.informatik.haw-hamburg.de/~ubicomp/projekte/master2020-proj/soblechero.pdf"]}
{"year":"2020","title":"Building Web Corpora for Minority Languages","authors":["H Jauhiainen, T Jauhiainen, K Lindén - Proceedings of the 12th Web as Corpus …, 2020"],"snippet":"… Common Crawl Foundation3 regularly crawls the Internet and offers the texts it finds for free download. Smith et al … Kanerva et al. (2014) used the morphological analyser OMorFi4 to find Finnish sentences in the Common Crawl corpus …","url":["https://www.aclweb.org/anthology/2020.wac-1.4.pdf"]}
{"year":"2020","title":"Caliskan Et Al-authors-full","authors":["A Caliskan, JJ Bryson, A Narayanan"],"snippet":"… We use the largest of the four corpora provided—the “Common Crawl” corpus obtained from a large-scale crawl of the web, containing 840 billion tokens (roughly, words). Tokens in this corpus are casesensitive, resulting in 2.2 million different ones …","url":["https://www.studeersnel.nl/nl/document/technische-universiteit-delft/machine-learning/werkstukessay/caliskan-et-al-authors-full/9896508/view"]}
{"year":"2020","title":"CALM: Continuous Adaptive Learning for Language Modeling","authors":["K Arumae, P Bhatia - arXiv preprint arXiv:2004.03794, 2020"],"snippet":"… We processed publicly available biomedical and non-biomedical corpora for pre-training our models. For non-biomedical data, we use BookCorpus and English Wikipedia data, CommonCrawl Stories (Trinh and Le, 2018), and OpenWebText (Gokaslan and Cohen) …","url":["https://arxiv.org/pdf/2004.03794"]}
{"year":"2020","title":"Can Embeddings Adequately Represent Medical Terminology? New Large-Scale Medical Term Similarity Datasets Have the Answer!","authors":["C Schulz, D Juric - arXiv preprint arXiv:2003.11082, 2020"],"snippet":"… 3) Non-medical: As a comparison, we also include - the GloVe word embedding (Pennington, Socher, and Manning 2014); - 2 Fasttext embeddings trained on Wikipedia and Common Crawl (plus its model (M)) (Mikolov et al. 2018) …","url":["https://arxiv.org/pdf/2003.11082"]}
{"year":"2020","title":"Can Emojis Convey Human Emotions? A Study to Understand the Association between Emojis and Emotions","authors":["AAM Shoeb, G de Melo - Proceedings of the 2020 Conference on Empirical …, 2020"],"snippet":"… Here, sim(v1,v2) denotes the cosine similarity be- tween two vectors. We first consider the widely used 300dimensional GloVe (Pennington et al., 2014) models pretrained on CommonCrawl 840B and Twitter, as these contain emojis …","url":["https://www.aclweb.org/anthology/2020.emnlp-main.720.pdf"]}
{"year":"2020","title":"Can I Take Your Subdomain? Exploring Related-Domain Attacks in the Modern Web","authors":["M Squarcina, M Tempesta, L Veronese, S Calzavara… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Can I Take Your Subdomain? Exploring Related-Domain Attacks in the Modern Web Marco Squarcina1, Mauro Tempesta1, Lorenzo Veronese1, Stefano Calzavara2, Matteo Maffei1 1TU Wien, 2Università Ca' Foscari Venezia Abstract …","url":["https://arxiv.org/pdf/2012.01946"]}
{"year":"2020","title":"Can Knowledge Rich Sentences Help Language Models to Solve Common Sense Reasoning Problems?","authors":["A Prakash - 2019"],"snippet":"… 20 Page 33. Figure 3. RoBERTa network architecture 2. CommonCrawl News was used, which contained 63 million news articles between September 2016 and February 2019 3. OPENWEBTEXT (Gokaslan and Cohen 2019) which is a text corpus containing …","url":["http://search.proquest.com/openview/08e0d3a7c85dcbbd4875abd0d3c48e17/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2020","title":"Capturing Word Order in Averaging Based Sentence Embeddings","authors":["JH Lee, JC Collados, LE Anke, S Schockaert"],"snippet":"… two million words. These word vectors were trained on Common Crawl with 600 billion tokens [19]. The sentences that we used for training, validation and testing were obtained from an English Wikipedia dump. All sentences …","url":["http://josecamachocollados.com/papers/ECAI2020_Capturing_Word_Order_in_Averaging_Based_Sentence_Embeddings.pdf"]}
{"year":"2020","title":"Caspar: Extracting and Synthesizing User Stories of Problems from App Reviews","authors":["H Guo, MP Singh"],"snippet":"Page 1. Caspar: Extracting and Synthesizing User Stories of Problems from App Reviews Hui Guo Secure Computing Institute North Carolina State University Raleigh, North Carolina hguo5@ncsu.edu Munindar P. Singh Secure …","url":["https://hguo5.github.io/Caspar/docs/Caspar_ICSE_20.pdf"]}
{"year":"2020","title":"CatchPhish: A URL and Anti-Phishing Research Platform","authors":["S Waddell"],"snippet":"Page 1. CatchPhish: A URL and Anti-Phishing Research Platform Stephen Waddell MInf Project (Part 2) Report Master of Informatics School of Informatics University of Edinburgh 2020 Page 2. Page 3. 3 Abstract In this work, I …","url":["https://groups.inf.ed.ac.uk/tulips/projects/19-20/waddell-2020.pdf"]}
{"year":"2020","title":"CauseNet: Towards a Causality Graph Extracted from the Web","authors":["S Heindorf, Y Scholten, H Wachsmuth, ACN Ngomo…"],"snippet":"… Web To extract causal relations from the web at scale, we analyze the ClueWeb12 web crawl, which comprises about 733,019,372 English web pages crawled between February and May 2012.3 We chose this crawl over …","url":["https://webis.de/downloads/publications/papers/potthast_2020a.pdf"]}
{"year":"2020","title":"CC-News-En: A Large English News Corpus","authors":["J Mackenzie, R Benham, M Petri, JR Trippas…"],"snippet":"… Temporal Growth. The Common Crawl foundation are constantly adding new documents to CC-News … 10DMOZ is now superseded by Curlie: https://www.curlie. org 11https://github.com/commoncrawl/news-crawl/issues/8 Page 4 …","url":["https://www.johannetrippas.com/papers/mackenzie2020ccnews.pdf"]}
{"year":"2020","title":"CCAligned: A Massive Collection of Cross-Lingual Web-Document Pairs","authors":["A El-Kishky, V Chaudhary, F Guzmán, P Koehn - Proc. of EMNLP, 2020"],"snippet":"… we mined over 392 million aligned documents (100M with English and 292M without English) across 68 Common Crawl snapshots. We assess the efficacy of this rule-based alignment in the next section. We select a small subset …","url":["https://www.researchgate.net/profile/Ahmed_El-Kishky/publication/337273813_A_Massive_Collection_of_Cross-Lingual_Web-Document_Pairs/links/5f992509458515b7cfa40eb4/A-Massive-Collection-of-Cross-Lingual-Web-Document-Pairs.pdf"]}
{"year":"2020","title":"Chart-based Zero-shot Constituency Parsing on Multiple Languages","authors":["T Kim, B Li, S Lee"],"snippet":"… (2019)), the XLM model trained with masked language modeling on 100 languages (XLM, Conneau and Lample (2019)), and the XLM-R and XLM-R-large models that are trained with the filtered CommonCrawl data (Wenzek et al. 2019) by Conneau et al. (2019) …","url":["https://openreview.net/pdf?id=JY-3BheD5LB"]}
{"year":"2020","title":"ChemTables: A Dataset for Semantic Classification of Tables in Chemical Patents","authors":["Z Zhai, C Druckenbrodt, C Thorne, SA Akhondi… - 2020"],"snippet":"… This model and the baselines it compares to are evaluated on a web table dataset, built by extracting tables from top 500 web pages which contain the highest numbers of tables in a subset of the April 2016 Common Crawl corpus [20] …","url":["https://www.researchsquare.com/article/rs-127219/latest.pdf"]}
{"year":"2020","title":"Circles are like Ellipses, or Ellipses are like Circles? Measuring the Degree of Asymmetry of Static and Contextual Embeddings and the Implications to Representation …","authors":["W Zhang, M Campbell, Y Yu, S Kumaravel - arXiv preprint arXiv:2012.01631, 2020"],"snippet":"Page 1. Circles are like Ellipses, or Ellipses are like Circles? Measuring the Degree of Asymmetry of Static and Contextual Word Embeddings and the Implications to Representation Learning Wei Zhang 1, Murray Campbell 1 …","url":["https://arxiv.org/pdf/2012.01631"]}
{"year":"2020","title":"CiTIUS at the TREC 2020 Health Misinformation Track","authors":["M Fernández-Pichel, DE Losada, JC Pichel…"],"snippet":"… 2 DOCUMENTS AND TOPICS In the TREC 2020 Health Misinformation Track, a news corpus from January 2020 to April 2020 was provided. The documents were obtained from CommonCrawl News, which contains news articles from all over the world …","url":["http://persoal.citius.usc.es/jcpichel/docs/2020_TREC_MFernandezPichel.pdf"]}
{"year":"2020","title":"Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network","authors":["M Karim, BR Chakravarthi, M Arcan, JP McCrae… - arXiv preprint arXiv …, 2020"],"snippet":"… The fourth one called fastText [17], which is trained on common crawl and Wikipedia using CBOW with position-weights, in di- mension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. Eventually …","url":["https://arxiv.org/pdf/2004.07807"]}
{"year":"2020","title":"Classification of cancer pathology reports with Deep Learning methods","authors":["S Martina"],"snippet":"Page 1. PHD PROGRAM IN SMART COMPUTING DIPARTIMENTO DI INGEGNERIA DELL'INFORMAZIONE (DINFO) Classification of cancer pathology reports with Deep Learning methods Stefano Martina Dissertation presented …","url":["https://flore.unifi.it/bitstream/2158/1187936/1/thesis.pdf"]}
{"year":"2020","title":"Classification of cancer pathology reports: a large-scale comparative study","authors":["S Martina, L Ventura, P Frasconi - IEEE Journal of Biomedical and Health Informatics, 2020"],"snippet":"… GloVe) [20]). It is a common practice to use pre-compiled libraries of word vectors trained on several billion tokens extracted from various sources such as Wikipedia, the English Gigaword 5, Common Crawl, or Twitter. These …","url":["https://arxiv.org/pdf/2006.16370"]}
{"year":"2020","title":"Classification of Cyberbullying Text in Arabic","authors":["BA Rachid, H Azza, HHB Ghezala - 2020 International Joint Conference on Neural …, 2020"],"snippet":"… The second set of pre-trained embeddings is the one provided in [24], in which word vectors were trained on online encyclopedia Wikipedia and the Common Crawl corpus using an extension of the fastText model (Fasttext embeddings) …","url":["https://ieeexplore.ieee.org/abstract/document/9206643/"]}
{"year":"2020","title":"Classifying Sequences of Extreme Length with Constant Memory Applied to Malware Detection","authors":["E Raff, W Fleshman, R Zak, HS Anderson, B Filar… - arXiv preprint arXiv …, 2020"],"snippet":"… This better demonstrates the gap between current deep learning and domain knowledge based approaches for classifying malware. We also use the Common Crawl to collect 676,843 benign PDF 103 104 105 106 107 108 …","url":["https://arxiv.org/pdf/2012.09390"]}
{"year":"2020","title":"CLEF eHealth Evaluation Lab 2020","authors":["M Krallinger"],"snippet":"… This collection consists of over 5 million medical webpages from selected domains acquired from the CommonCrawl [7]. Given the positive feedback received for this document collection, it will be used again in the 2020 CHS task …","url":["https://link.springer.com/content/pdf/10.1007/978-3-030-45442-5_76.pdf"]}
{"year":"2020","title":"Clinical XLNet: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation","authors":["K Huang, A Singh, S Chen, ET Moseley, C Deng… - arXiv preprint arXiv …, 2019"],"snippet":"… Pretraining Clinical XLNet. The text representation generated from large pre-training models depends on the corpus it is pre-trained on. XLNet is pre-trained on common language corpora such as BookCorpus, Wikipedia, Common Crawl and etc …","url":["https://arxiv.org/pdf/1912.11975"]}
{"year":"2020","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark","authors":["L Xu, X Zhang, L Li, H Hu, C Cao, W Liu, J Li, Y Li… - arXiv preprint arXiv …, 2020"],"snippet":"… CLUECorpus2020 (Xu et al., 2020) It contains 100 GB Chinese raw corpus, which is retrieved from Common Crawl … CLUEOSCAR6 OSCAR is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus …","url":["https://arxiv.org/pdf/2004.05986"]}
{"year":"2020","title":"CLUECorpus2020: A Large-scale Chinese Corpus for Pre-trainingLanguage Model","authors":["L Xu, X Zhang, Q Dong - arXiv preprint arXiv:2003.01355, 2020"],"snippet":"… Common Crawl is an organization that crawls the web and freely provides its archives and datasets to the public. Common Crawl usually crawls internet web content once a month. Common Crawl's web archives consist of petabytes of data collected since 2011 …","url":["https://arxiv.org/pdf/2003.01355"]}
{"year":"2020","title":"Clustering Approach to Topic Modeling in Users Dialogue","authors":["E Feldina, O Makhnytkina - Proceedings of SAI Intelligent Systems Conference, 2020"],"snippet":"… FastText using FastText weights based on the pre-trained CBOW model with a word window size of five trained on Common Crawl and Wikipedia, vector size 300. Table 2. Results of the implementation of the clustering …","url":["https://link.springer.com/chapter/10.1007/978-3-030-55187-2_44"]}
{"year":"2020","title":"CO-EVOLUTION OF CULTURE AND MEANING REVEALED THROUGH LARGE-SCALE SEMANTIC ALIGNMENT","authors":["B THOMPSON, S ROBERTS, G LUPYAN"],"snippet":"… We also replicated on word embeddings derived from the OpenSubtitles database (Li- son & Tiedemann, 2016) and a combination of Wikipedia and the Common Crawl dataset (Grave, Bojanowski, Gupta, Joulin, & Mikolov, 2018)) …","url":["https://brussels.evolang.org/proceedings/papers/EvoLang13_paper_62.pdf"]}
{"year":"2020","title":"COD3S: Diverse Generation with Discrete Semantic Signatures","authors":["N Weir, J Sedoc, B Van Durme - arXiv preprint arXiv:2010.02882, 2020"],"snippet":"… The model is trained on the co-released corpus CausalBank, which comprises causal statements harvested from English Common Crawl (Buck et al., 2014) … 2014. N-gram counts and language models from the common crawl …","url":["https://arxiv.org/pdf/2010.02882"]}
{"year":"2020","title":"Combination of Neural Machine Translation Systems at WMT20","authors":["B Marie, R Rubino, A Fujita - Proceedings of the Fifth Conference on Machine …, 2020"],"snippet":"… As En- glish monolingual data, we used all the provided data, but sampled only 200M lines from the “Common Crawl” corpora, except the “News Discussions” and “Wiki Dumps” corpora … corpora but also sampled only …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.23.pdf"]}
{"year":"2020","title":"Combinatorial feature embedding based on CNN and LSTM for biomedical named entity recognition","authors":["M Cho, C Park, J Ha, S Park - Journal of Biomedical Informatics, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S1532046420300083"]}
{"year":"2020","title":"Combining Character and Word Embeddings for Affect in Arabic Informal Social Media Microblogs","authors":["AI Alharbi, M Lee - International Conference on Applications of Natural …, 2020"],"snippet":"… Here, the researchers derived the training data from three separate sources: Wikipedia, Twitter and Common Crawl webpages crawl data; they employed two word-level models to learn word representations for general NLP tasks …","url":["https://link.springer.com/chapter/10.1007/978-3-030-51310-8_20"]}
{"year":"2020","title":"Combining Different Parsers and Datasets for CAPITEL UD Parsing","authors":["F Sánchez-León - Proceedings of the Iberian Languages Evaluation …, 2020"],"snippet":"… Besides, we have used fastText word embeddings trained on Common Crawl and Wikipedia corpora.5 Table 1 shows results on development set of a model built with the training material using different word embeddings.6 Increasing …","url":["http://ceur-ws.org/Vol-2664/capitel_paper1.pdf"]}
{"year":"2020","title":"Combining Visual and Textual Features for Semantic Segmentation of Historical Newspapers","authors":["R Barman, M Ehrmann, S Clematide, SA Oliveira… - arXiv preprint arXiv …, 2020"],"snippet":"… First, four pre-trained embeddings of the Flair library14 are used with their default implementation settings, as follows: - fastText-fr, ie the French fastText embeddings of size 300 pre-trained on Common Crawl and Wikipedia; …","url":["https://arxiv.org/pdf/2002.06144"]}
{"year":"2020","title":"Commonsense Aesthetics","authors":["AK Roek - 2020"],"snippet":"Page 1. COMMONSENSE AESTHETICS Aaron Kurosu Roek A DISSERTATION PRESENTED TO THE FACULTY OF PRINCETON UNIVERSITY IN CANDIDACY FOR THE DEGREE OF DOCTOR OF …","url":["http://search.proquest.com/openview/ec2e04cd24fc776ff0cb09566fcf7621/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2020","title":"Commonsense Learning: An Indispensable Path towards Human-centric Multimedia","authors":["B Huang, S Tang, G Shen, G Li, X Wang, W Zhu - … of the 1st International Workshop on …, 2020"],"snippet":"… BERT uses Bookcorpus [84] and English Wikipedia with a size of about 13GB, while XLNet uses more than 100GB of corpus. T5 uses Colossal Clean Crawled Corpus captured from the Common Crawl website with a size of 750GB …","url":["https://dl.acm.org/doi/abs/10.1145/3422852.3423484"]}
{"year":"2020","title":"Communication-Efficient String Sorting","authors":["T Bingmann, P Sanders, M Schimek - arXiv preprint arXiv:2001.08516, 2020"],"snippet":"Page 1. arXiv:2001.08516v1 [cs.DC] 23 Jan 2020 Communication-Efficient String Sorting Timo Bingmann, Peter Sanders, Matthias Schimek Karlsruhe Institute of Technology, Karlsruhe, Germany {bingmann,sanders}@kit.edu, matthias schimek@gmx.de …","url":["https://arxiv.org/pdf/2001.08516"]}
{"year":"2020","title":"Comparative Analysis of Deep Learning Models for Myanmar Text Classification","authors":["MS Phyu, KT Nwet - Asian Conference on Intelligent Information and …, 2020"],"snippet":"… Grave et al. [3] published pre-trained word vectors for two hundred forty-six languages trained on common crawl and Wikipedia. They proposed bag-of-character n-grams based on skip-gram that could capture sub-word information to enrich word vectors …","url":["https://link.springer.com/chapter/10.1007/978-3-030-41964-6_7"]}
{"year":"2020","title":"Comparative Analysis of Machine Learning Algorithms for Computer-Assisted Reporting Based on Fully Automated Cross-Lingual RadLex® Mappings","authors":["ME Maros, CG Cho, AG Junge, B Kämpgen, V Saase… - 2020"],"snippet":"… However, pre-trained word vector models for 157 languages, which were pre-trained on Common Crawl and Wikipedia by the fastText package authors are available for direct download (https://fasttext.cc/docs/en/crawl-vectors.html) [58] …","url":["https://www.preprints.org/manuscript/202004.0354/download/final_file"]}
{"year":"2020","title":"COMPARATIVE ANALYSIS OF SUBDOMAIN ENUMERATION TOOLS AND STATIC CODE ANALYSIS","authors":["GJ Kathrine, RT Baby, V Ebenzer"],"snippet":"… Certificates= censys,certspotter, Google CT APIs: AlienVault, BinaryEdge, BufferOver, CIRCL, CommonCrawl, DNSDB, GitHub, HackerTarget, NetworksDB, PassiveTotal, Pastebin.. Web Archives: ArchiveIt, ArchiveToday, Arquivo, Wayback and others …","url":["https://www.researchgate.net/profile/Ronnie_Joseph2/publication/342501456_COMPARATIVE_ANALYSIS_OF_SUBDOMAIN_ENUMERATION_TOOLS_AND_STATIC_CODE_ANALYSIS/links/5ef76e2d299bf18816eae517/COMPARATIVE-ANALYSIS-OF-SUBDOMAIN-ENUMERATION-TOOLS-AND-STATIC-CODE-ANALYSIS.pdf"]}
{"year":"2020","title":"Comparative Analysis of Word Embeddings for Capturing Word Similarities","authors":["M Toshevska, F Stojanovska, J Kalajdjieski - arXiv preprint arXiv:2005.03812, 2020"],"snippet":"… architectures [23]. In our experiments, we have used pre-trained models both trained with subword information on Wikipedia 2017 (16B tokens) and trained with subword information on Common Crawl (600B tokens)4. 2 https …","url":["https://arxiv.org/pdf/2005.03812"]}
{"year":"2020","title":"Comparing Different Methods for Named Entity Recognition in Portuguese Neurology Text","authors":["F Lopes, C Teixeira, HG Oliveira - Journal of Medical Systems, 2020"],"snippet":"… In order to check which was preferable, two different WE models were used: A pre-trained general Portuguese FastText model, Footnote 3 based on billions of tokens from Wikipedia and Common Crawl [41], with a 5-character window (general language) …","url":["https://link.springer.com/article/10.1007/s10916-020-1542-8"]}
{"year":"2020","title":"Comparing High Dimensional Word Embeddings Trained on Medical Text to Bag-of-Words for Predicting Medical Codes","authors":["V Yogarajan, H Gouk, T Smith, M Mayo, B Pfahringer - Asian Conference on …, 2020"],"snippet":"… Our embeddings are trained to the exact same specifications as the Wikipedia and common crawl fastText models in [10] … For 300-dimensional embeddings, W300 are word embeddings that are trained by fastText on Wikipedia and other common crawl text …","url":["https://link.springer.com/chapter/10.1007/978-3-030-41964-6_9"]}
{"year":"2020","title":"Comparing Neural Network Parsers for a Less-resourced and Morphologically-rich Language: Amharic Dependency Parser","authors":["BE Seyoum, Y Miyao, BY Mekonnen - Proceedings of the first workshop on …, 2020"],"snippet":"… For this purpose, we used the trained model for Amharic using fasttext7. The data for training the model is from Wikipedia and Common Crawl8. The models were trained using continuous bag of words (CBOW) …","url":["https://www.aclweb.org/anthology/2020.rail-1.5.pdf"]}
{"year":"2020","title":"Comparing pre-trained language models for Spanish hate speech detection","authors":["FM Plaza-del-Arco, MD Molina-González… - Expert Systems with …, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S095741742030868X"]}
{"year":"2020","title":"Comparing Probabilistic, Distributional and Transformer-Based Models on Logical Metonymy Interpretation","authors":["G Rambelli, E Chersoni, A Lenci, P Blache, CR Huang - … of the 1st Conference of the …, 2020"],"snippet":"… in random order. XLNet's training corpora were the same as BERT plus Giga5, ClueWeb 2012-B and Common Crawl, for a total of 32.89B subword piece. Also in this case, we used the large pre-trained model. GPT-2 (Radford …","url":["https://www.aclweb.org/anthology/2020.aacl-main.26.pdf"]}
{"year":"2020","title":"Comparing supervised learning algorithms for Spatial Nominal Entity recognition","authors":["A Medad, M Gaio, L Moncla, S Mustière, YL Nir - AGILE: GIScience Series, 2020"],"snippet":"… We have made the hypothesis that as FastText is a model of pretrained vectors (300 dimensions) on Wikipedia and Common Crawl, it provides a generic representation of words … CC BY 4.0 License. Page 12. Common Crawl and Wikipedia using the CBOW method …","url":["https://agile-giss.copernicus.org/articles/1/15/2020/agile-giss-1-15-2020.pdf"]}
{"year":"2020","title":"Comparison between machine learning and human learning from examples generated with machine teaching","authors":["GE Jaimovitch López - 2020"],"snippet":"… For instance, progress in areas like NLP (Natural Language Processing) has led to the development of outstanding deep neural networks such as GPT-3, a task-agnostic model trained using huge data repositories like …","url":["https://riunet.upv.es/bitstream/handle/10251/152771/Jaimovitch%20-%20Comparaci%C3%B3n%20entre%20el%20aprendizaje%20de%20machine%20learning%20y%20humanos%20desde%20ejemplos%20genera....pdf?sequence=1"]}
{"year":"2020","title":"Comparison of Named Entity Recognition Tools Applied to News Articles","authors":["S Vychegzhanin, E Kotelnikov - 2019 Ivannikov Ispras Open Conference (ISPRAS), 2019"],"snippet":"… spaCy Python MIT Bloom embeddings and a residual convolutional neural network en_core_web_sm OntoNotes en_core_web_md OntoNotes, Common Crawl en_core_web_lg OntoNotes, Common Crawl xx_ent_wiki_sm WikiNER ru2 …","url":["https://ieeexplore.ieee.org/abstract/document/8991165/"]}
{"year":"2020","title":"Comprehensive Stereotype Content Dictionaries Using a Semi‐Automated Method","authors":["G Nicolas, X Bai, ST Fiske - European Journal of Social Psychology"],"snippet":"… word embeddings used here are Word2Vec's model pretrained on Google News (Mikolov, Chen, Corrado, & Dean, 2013) and Glove' model pretrained on the Common Crawl (Pennington, Socher, & Manning, 2014; presented in Supplement) …","url":["https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2724"]}
{"year":"2020","title":"Computational Approaches for Identifying Sensational Soft News","authors":["V Indurthi - 2020"],"snippet":"Page 1. Computational Approaches for Identifying Sensational Soft News Thesis submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science and Engineering by Research by Vijayasaradhi Indurthi 201450803 …","url":["http://web2py.iiit.ac.in/research_centres/publications/download/mastersthesis.pdf.a54f3e371a9ec80e.46696e616c5f5468657369735f4d535f42795f52657365617263685f56696a617961736172616468695f496e6475727468692e706466.pdf"]}
{"year":"2020","title":"Computational explorations of semantic cognition","authors":["AS Rotaru - 2020"],"snippet":"Page 1. 1 UNIVERSITY COLLEGE LONDON (UCL) Computational explorations of semantic cognition PHD THESIS Armand Stefan Rotaru Supervisors: Primary: Prof. Gabriella Vigliocco Secondary: Prof. Lewis Griffin Page 2. 2 …","url":["https://discovery.ucl.ac.uk/id/eprint/10106344/13/Rotaru_10106344_thesis.pdf"]}
{"year":"2020","title":"Computational Mechanisms of Language Understanding and Use in the Brain and Behaviour","authors":["I Kajic - 2020"],"snippet":"Page 1. Computational Mechanisms of Language Understanding and Use in the Brain and Behaviour by Ivana Kajić A thesis presented to the University of Waterloo in fulfillment of the thesis requirement for the degree of Doctor of Philosophy in Computer Science …","url":["https://uwspace.uwaterloo.ca/bitstream/handle/10012/16439/Kajic_Ivana.pdf?sequence=1"]}
{"year":"2020","title":"Connections and selections: Comparing multivariate predictions and parameter associations from latent variable models of picture naming","authors":["GM Walker, J Fridriksson, G Hickok - Cognitive Neuropsychology, 2020"],"snippet":"Connectionist simulation models and processing tree mathematical models of picture naming have complementary advantages and disadvantages. These model types were compared in terms of their predicti...","url":["https://www.tandfonline.com/doi/abs/10.1080/02643294.2020.1837092"]}
{"year":"2020","title":"Constraining the Transformer NMT Model with Heuristic Grid Beam Search","authors":["G Xie, A Way, J Du, L Wang"],"snippet":"… the training corpus consists of 4.4 Million segments from Europarl (Koehn, 2005) and CommonCrawl (Smith et al., 2013); … Smith, JR, Saint-Amand, H., Plamada, M., Koehn, P., Callison-Burch, C., and Lopez, A. (2013). Dirt cheap …","url":["http://www.computing.dcu.ie/~away/PUBS/2020/TransformerGridSearch.pdf"]}
{"year":"2020","title":"Contemporary Polish Language Model (Version 2) Using Big Data and Sub-Word Approach","authors":["K Wołk"],"snippet":"… In this paper, we present a set of 6-gram language models based on a big-data training of the contemporary Polish language, using the Common Crawl corpus (a compilation of over 3.25 billion webpages) and other resources …","url":["https://indico2.conference4me.psnc.pl/event/35/contributions/3915/attachments/957/996/Thu-3-8-7.pdf"]}
{"year":"2020","title":"Context-aware Feature Generation for Zero-shot Semantic Segmentation","authors":["Z Gu, S Zhou, L Niu, Z Zhao, L Zhang - arXiv preprint arXiv:2008.06893, 2020"],"snippet":"… in the supplementary. Following SPNet [43], we concatenate two different types of word embeddings (d = 600, 300 for each), ie, word2vec [30] trained on Google News and fast-Text [15] trained on Common Crawl. The word …","url":["https://arxiv.org/pdf/2008.06893"]}
{"year":"2020","title":"Contextual Question Answering with Improved Embedding Models","authors":["G He"],"snippet":"… In the GloVe word embedding based BiDAF++ model, we utilize pretrained GloVe(Pennington et al., 2014) embeddings with 300 output dimensions (840B.300d). These embeddings have been prertrained on a common crawl of 840 billion to- kens …","url":["https://georgehe.me/coqa.pdf"]}
{"year":"2020","title":"Contextualized Embeddings in Named-Entity Recognition: An Empirical Study on Generalization","authors":["B Taillé, V Guigue, P Gallinari - arXiv preprint arXiv:2001.08053, 2020"],"snippet":"… 3 Word Representations Word embeddings map each word to a single vector which results in a lexical representation. We take GloVe 840B embeddings [13] trained on Common Crawl as the pretrained word embeddings baseline …","url":["https://arxiv.org/pdf/2001.08053"]}
{"year":"2020","title":"Contextualized Emotion Recognition in Conversation as Sequence Tagging","authors":["Y Wang, J Zhang, J Ma, S Wang, J Xiao - Proceedings of the 21th Annual Meeting of …, 2020"],"snippet":"… GloVe vectors trained on Common Crawl 840B with 300 dimensions are used as fixed word em- beddings. We use a 12-layers 4-heads Transformer encoder of which the inner-layer dimensionality is 2048 and the hidden size is 100 …","url":["https://www.aclweb.org/anthology/2020.sigdial-1.23.pdf"]}
{"year":"2020","title":"Controllable Text Generation","authors":["S Prabhumoye - 2020"],"snippet":"Page 1. CARNEGIE MELLON UNIVERSITY Controllable Text Generation Should machines re ect the way humans interact in society? esis Proposal by Shrimai Prabhumoye esis proposal submi ed in partial ful llment for the degree of Doctor of Philosophy esis committee …","url":["https://www.cs.cmu.edu/~sprabhum/docs/proposal.pdf"]}
{"year":"2020","title":"ConvBERT: Improving BERT with Span-based Dynamic Convolution","authors":["Z Jiang, W Yu, D Zhou, Y Chen, J Feng, S Yan - arXiv preprint arXiv:2008.02496, 2020"],"snippet":"Page 1. ConvBERT: Improving BERT with Span-based Dynamic Convolution Zihang Jiang1∗, Weihao Yu1∗, Daquan Zhou1, Yunpeng Chen2, Jiashi Feng1, Shuicheng Yan2 1National University of Singapore, 2Yitu Technology …","url":["https://res.arxiv.org/pdf/2008.02496"]}
{"year":"2020","title":"Correcting the Autocorrect: Context-Aware Typographical Error Correction via Training Data Augmentation","authors":["K Shah, G de Melo - arXiv preprint arXiv:2005.01158, 2020"],"snippet":"… from 2We do not rely on embeddings trained on CommonCrawl, as Web data contains substantially more misspelling forms. 3Specifically, those with a character length three standard deviations above or below mean. Hence …","url":["https://arxiv.org/pdf/2005.01158"]}
{"year":"2020","title":"Crawling the German Health Web: Exploratory Study and Graph Analysis","authors":["R Zowalla, T Wetter, D Pfeifer - Journal of Medical Internet Research, 2020"],"snippet":"Journal of Medical Internet Research - International Scientific Journal for Medical Research, Information and Communication on the Internet.","url":["https://www.jmir.org/2020/7/e17853/"]}
{"year":"2020","title":"Creating semantic representations","authors":["FÅ Nielsen, LK Hansen - Statistical Semantics, 2020"],"snippet":"… Evaluations in 2017 with fastText trained for 3 days on either the very large Common Crawl data set or a combination of the English Wikipedia and news datasets set a new state-of-the-art on 88.5% for the accuracy on a …","url":["https://link.springer.com/chapter/10.1007/978-3-030-37250-7_2"]}
{"year":"2020","title":"Creation of a database based on artificial intelligence in order to understand the role played by biofilms on outbreaks","authors":["APP de Melo - 2020"],"snippet":"Page 1. FACULDADE DE ENGENHARIA DA UNIVERSIDADE DO PORTO Creation of a database based on artificial intelligence in order to understand the role played by biofilms on outbreaks Ana Patrícia Pinheiro de Melo Integrated Master in Bioengineering …","url":["https://repositorio-aberto.up.pt/bitstream/10216/130013/2/428683.pdf"]}
{"year":"2020","title":"Creative Natural Language Generation: Humor and Beyond","authors":["NT Hossain - 2020"],"snippet":"Page 1. Creative Natural Language Generation: Humor and Beyond by Nabil Tarique Hossain Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Supervised by Professor Henry Kautz and Dr …","url":["https://urresearch.rochester.edu/fileDownloadForInstitutionalItem.action?itemId=36525&itemFileId=189977"]}
{"year":"2020","title":"Credibility Assessment of User Generated health information of the Bengali language in microblogging sites employing NLP techniques","authors":["A Benazir, S Sharmin"],"snippet":"… word. We use the pre-trained word vectors of Fasttext 5 over Word2Vec 6 and GloVe 7 since it has the best multi lingual word vectors amongst the three, supporting 157 languages, trained on Common Crawl and Wikipedia. We …","url":["https://www.researchgate.net/profile/Afsara_Benazir2/publication/347524581_Credibility_Assessment_of_User_Generated_health_information_of_the_Bengali_language_in_microblogging_sites_employing_NLP_techniques/links/5fe0f9eaa6fdccdcb8ef603e/Credibility-Assessment-of-User-Generated-health-information-of-the-Bengali-language-in-microblogging-sites-employing-NLP-techniques.pdf"]}
{"year":"2020","title":"Cross-Cultural Polarity and Emotion Detection Using Sentiment Analysis and Deep Learning--a Case Study on COVID-19","authors":["AS Imran, SM Doudpota, Z Kastrati, R Bhatra - arXiv preprint arXiv:2008.10031, 2020"],"snippet":"… corpora, a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and …","url":["https://arxiv.org/pdf/2008.10031"]}
{"year":"2020","title":"Cross-lingual Inductive Transfer to Detect Offensive Language","authors":["K Pant, T Dadu - arXiv preprint arXiv:2007.03771, 2020"],"snippet":"… We further finetune XLM-R, pretrained on 2.5 TB Common Crawl corpus spanning 100 languages … XLM-R is a transformer-based cross-lingual model pretrained using a multilingual masked language model objective on 2.5 …","url":["https://arxiv.org/pdf/2007.03771"]}
{"year":"2020","title":"Cross-Lingual Information Retrieval in the Medical Domain","authors":["S Saleh - 2020"],"snippet":"Page 1. DOCTORAL THESIS Shadi Saleh Cross-lingual Information Retrieval in the Medical Domain Institute of Formal and Applied Linguistics Supervisor of the doctoral thesis: doc. RNDr. Pavel Pecina, PhD. Study programme …","url":["https://dspace.cuni.cz/bitstream/handle/20.500.11956/123570/140087429.pdf?sequence=1"]}
{"year":"2020","title":"Cross-Lingual Relation Extraction with Transformers","authors":["J Ni, T Moon, P Awasthy, R Florian - arXiv preprint arXiv:2010.08652, 2020"],"snippet":"… Conneau et al., 2020). mBERT was pre-trained with Wikipedia text of 104 languages with the largest sizes, and XLM-R were pre-trained with Wikipedia text and CommonCrawl Corpus of 100 languages. Both models use no …","url":["https://arxiv.org/pdf/2010.08652"]}
{"year":"2020","title":"Cross-lingual Retrieval for Iterative Self-Supervised Training","authors":["C Tran, Y Tang, X Li, J Gu - arXiv preprint arXiv:2006.09526, 2020"],"snippet":"… 5 Experiment Evaluation We pretrained an mBART model with Common Crawl dataset constrained to the 25 languages as in [19] for which we have evaluation data … We subsample the resulting common crawl data to 100 million sentences in each language …","url":["https://arxiv.org/pdf/2006.09526"]}
{"year":"2020","title":"Cross-lingual Transfer Learning for Semantic Role Labeling in Russian","authors":["I Alimova, E Tutubalina, A Kirillovich"],"snippet":"… The model is also based on Transformer architecture (Vaswani et al., 2017). We applied the XLM-R Masked Language Model, which is pretrained on 2.5 TB of CommonCrawl data, in 100 languages, with 8 heads, 6 layers, 1024 hidden units per layer …","url":["https://www.researchgate.net/profile/Alexander_Kirillovich/publication/342734555_Cross-lingual_Transfer_Learning_for_Semantic_Role_Labeling_in_Russian/links/5f0410d0299bf1881607dae8/Cross-lingual-Transfer-Learning-for-Semantic-Role-Labeling-in-Russian.pdf"]}
{"year":"2020","title":"Cross-Lingual Word Embeddings for Turkic Languages","authors":["E Kuriyozov, Y Doval, C Gómez-Rodríguez - arXiv preprint arXiv:2005.08340, 2020"],"snippet":"… Cross-lingual embeddings used for both experiments were trained under the following conditions: • Monolingual word embeddings were obtained from available pre-trained word vectors (Grave et al., 2018) trained on …","url":["https://arxiv.org/pdf/2005.08340"]}
{"year":"2020","title":"Cross-Modal Transfer Learning for Multilingual Speech-to-Text Translation","authors":["C Tran, C Wang, Y Tang, Y Tang, J Pino, X Li - arXiv preprint arXiv:2010.12829, 2020"],"snippet":"… This model is pretrained using two types of noise in g — random span masking and order permutation — as described in [3]. We re-use the finetuned mBART50 models from [13] which are pretrained on …","url":["https://arxiv.org/pdf/2010.12829"]}
{"year":"2020","title":"CS-NLP team at SemEval-2020 Task 4: Evaluation of State-of-the-artNLP Deep Learning Architectures on Commonsense Reasoning Task","authors":["S Saeedi, A Panahi, S Saeedi, AC Fong - arXiv preprint arXiv:2006.01205, 2020"],"snippet":"… The architecture of RoBERTalarge is comprised of of 24-layer, 1024-hidden dimension, 16-self attention heads, 355M parameters and pretrained on book corpus plus English Wikipedia, English CommonCrawl News, and WebText corpus …","url":["https://arxiv.org/pdf/2006.01205"]}
{"year":"2020","title":"Culprit Analytics from Detective Novels","authors":["A Motwani - 2020"],"snippet":"Page 1. Culprit Analytics from Detective Novels Thesis submitted in partial fulfillment of the requirements for the degree of Masters of Science in Computer Science and Engineering by Research by Aditya Motwani …","url":["http://web2py.iiit.ac.in/research_centres/publications/download/mastersthesis.pdf.8981260fae86b1a4.4d535f5468657369735f46696e616c202833292e706466.pdf"]}
{"year":"2020","title":"Cultural Cartography with Word Embeddings","authors":["DS Stoltz, MA Taylor - arXiv preprint arXiv:2007.04508, 2020"],"snippet":"… fastText embeddings are trained on Wikipedia data dumps and the 25 billion web pages of the Common Crawl), and thus are not trained on the researcher's own corpus. Corpus-trained embeddings, by contrast, are word vectors trained exclusively on the …","url":["https://arxiv.org/pdf/2007.04508"]}
{"year":"2020","title":"Cultural Differences in Bias? Origin and Gender Bias in Pre-Trained German and French Word Embeddings","authors":["M Kurpicz-Briki"],"snippet":"… The validation experiments in English were executed on the same pre-trained word embeddings as in the original experiments (Caliskan et al., 2017): • GloVe pre-trained word embeddings using the ”Common …","url":["http://ceur-ws.org/Vol-2624/paper6.pdf"]}
{"year":"2020","title":"Cultural influences on word meanings revealed through large-scale semantic alignment","authors":["B Thompson, SG Roberts, G Lupyan - Nature Human Behaviour, 2020"],"snippet":"If the structure of language vocabularies mirrors the structure of natural divisions that are universally perceived, then the meanings of words in different languages should closely align. By contrast, if shared word meanings are …","url":["https://www.nature.com/articles/s41562-020-0924-8"]}
{"year":"2020","title":"Current Limitations of Language Models: What You Need is Retrieval","authors":["A Komatsuzaki - arXiv preprint arXiv:2009.06857, 2020"],"snippet":"… Most naturally available samples as well as the reasonable output of most tasks have rather limited length, though others (eg books) do not. For example, the average sample length of WebText is only about 1000 tokens …","url":["https://arxiv.org/pdf/2009.06857"]}
{"year":"2020","title":"Curriculum Pre-training for End-to-End Speech Translation","authors":["C Wang, Y Wu, S Liu, M Zhou, Z Yang - arXiv preprint arXiv:2004.10093, 2020"],"snippet":"… (2017) 6LibriSpeech En-Fr, IWSLT En-De and Fisher-CallHome Es-En 7https://wit3.fbk.eu/mt.php?release= 2017-01-trnted 8Europarl v7, Common Crawl, News Comentary v13 and Rapid corpus of EU press releases. using …","url":["https://arxiv.org/pdf/2004.10093"]}
{"year":"2020","title":"CX DB8: A queryable extractive summarizer and semantic search engine","authors":["A Roush - arXiv preprint arXiv:2012.03942, 2020"],"snippet":"… unsupervised models. Since unsupervised models are usually trained on massive corpuses, like Wikipedia or Common Crawl (Penninglon et al., 2014), they do not overfit as much to any particular topic or domain. Furthermore …","url":["https://arxiv.org/pdf/2012.03942"]}
{"year":"2020","title":"Dávid Márk Nemeskey Natural Language Processing Methods for Language Modeling","authors":["CV Erzsébet, Z Horváth, A Benczúr, A Kornai"],"snippet":"… 83 4.2.2 Common Crawl . . . . . 84 … Chapter 4 details our work of compiling Webcorpus 2.0, a new Hungarian gigaword corpus, from the Common Crawl and the Hungarian Wikipedia. Its main purpose being a …","url":["https://hlt.bme.hu/media/pdf/thesis.pdf"]}
{"year":"2020","title":"DAN+: Danish Nested Named Entities and Lexical Normalization","authors":["B Plank, KN Jensen, R van der Goot"],"snippet":"… Twitter data. Bert variants For Danish BERT we use the model trained by Botxo (https://github.com/ botxo/nordic_bert), which is pre-trained on Wikipedia, Common Crawl, Danish debate forums and Danish open subtitles. For …","url":["http://www.robvandergoot.com/doc/danP.pdf"]}
{"year":"2020","title":"Danish Clinical Event Extraction Developing a clinical event extraction system for electronic health records using deep learning and active learning","authors":["F WONSILD, MG MØLLER - 2020"],"snippet":"Page 1. Danish Clinical Event Extraction Developing a clinical event extraction system for electronic health records using deep learning and active learning FREDERIK WONSILD MATHIAS GIOVANNI MØLLER Master's thesis …","url":["https://www.derczynski.com/itu/docs/clin-events_frwo_mgmo.pdf"]}
{"year":"2020","title":"Data augmentation techniques for the Video Question Answering task","authors":["A Falcon, O Lanz, G Serra - arXiv preprint arXiv:2008.09849, 2020"],"snippet":"… To compute the word embeddings for the question and the answers, we consider GloVe [23], pretrained on the Common Crawl dataset3, which outputs a vector of size E = 300 for 3 The Common Crawl dataset is available …","url":["https://arxiv.org/pdf/2008.09849"]}
{"year":"2020","title":"Data selection for unsupervised translation of German–Upper Sorbian","authors":["L Edman, A Toral, G van Noord - Proceedings of the Fifth Conference on Machine …, 2020"],"snippet":"… gz. For German, we use monolingual data from News Crawl and Common Crawl … 19.57 Table 3: BLEU scores of models trained using 5 million sentences from News Crawl and various amounts of sentences from Common Crawl …","url":["https://www.aclweb.org/anthology/2020.wmt-1.130.pdf"]}
{"year":"2020","title":"Data-driven Crosslinguistic Modeling of Constituent Ordering Preferences","authors":["ZY Liu - 2020"],"snippet":"Page 1. Data-driven Crosslinguistic Modeling of Constituent Ordering Preferences By Zoey (Ying) Liu Dissertation Submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Linguistics in the Office of Graduate Studies of the …","url":["https://www.researchgate.net/profile/Zoey_Liu2/publication/343836788_Data-driven_Crosslinguistic_Modeling_of_Constituent_Ordering_Preferences/links/5f4408cb92851cd3022569fe/Data-driven-Crosslinguistic-Modeling-of-Constituent-Ordering-Preferences.pdf"]}
{"year":"2020","title":"Data-driven models and computational tools for neurolinguistics: a language technology perspective","authors":["E Artemova, A Bakarov, A Artemov, E Burnaev… - arXiv preprint arXiv …, 2020"],"snippet":"… The corpus size can be estimated by the number of tokens8: so, the size of English Wikipedia is 1M tokens, the size of Google news corpus is 1B tokens, and the size of CommonCrawl corpus is 600B tokens. Structured expert-based …","url":["https://arxiv.org/pdf/2003.10540"]}
{"year":"2020","title":"Dataless Short Text Classification Based on Biterm Topic Model and Word Embeddings","authors":["Y Yang, H Wang, J Zhu, Y Wu, K Jiang, W Guo, W Shi"],"snippet":"… We set the number of iterations to 50 as our models achieve competitive performance since then. For word embeddings, we employ the widely used GloVe Common Crawl as mentioned before. It contains 840B to- kens, 2.2M vocab and 300d vectors …","url":["https://www.ijcai.org/Proceedings/2020/0549.pdf"]}
{"year":"2020","title":"Dataset for Automatic Summarization of Russian News","authors":["I Gusev - arXiv preprint arXiv:2006.11063, 2020"],"snippet":"… 3.3 Abstractive methods All of the tested models are based on a sequence-to-sequence framework. Pointergenerator and CopyNet were trained only on our train dataset, and mBART was pretrained on texts of 25 languages extracted from the Common Crawl …","url":["https://arxiv.org/pdf/2006.11063"]}
{"year":"2020","title":"Dataset for Automatic Summarization of Russian","authors":["I Gusev - arXiv preprint arXiv:2006.11063, 2020"],"snippet":"… sequence-to-sequence framework. Pointer-generator and CopyNet were trained only on our training dataset, and mBART was pretrained on texts of 25 languages extracted from the Common Crawl. We performed no additional …","url":["https://www.researchgate.net/profile/Ilya_Gusev2/publication/342352344_Dataset_for_Automatic_Summarization_of_Russian_News/links/5f16164a92851c1eff22059b/Dataset-for-Automatic-Summarization-of-Russian-News.pdf"]}
{"year":"2020","title":"Datasets and Performance Metrics for Greek Named Entity Recognition","authors":["N Bartziokas, T Mavropoulos, C Kotropoulos - 11th Hellenic Conference on Artificial …, 2020"],"snippet":"… performance. Thus, most works omit such supplementary features. Established word embeddings, usually pre-trained on large corpora, such as Common Crawl's or Wikipedia's collections, are fine-tuned to a great extent. Google …","url":["https://dl.acm.org/doi/abs/10.1145/3411408.3411437"]}
{"year":"2020","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention","authors":["P He, X Liu, J Gao, W Chen - arXiv preprint arXiv:2006.03654, 2020"],"snippet":"… the setting of BERT [4], except that we use the BPE vocabulary as [2, 5]. For training data, we use Wikipedia (English Wikipedia dump3; 12GB), BookCorpus [26] (6GB), OPENWEBTEXT (public Reddit content [27]; 38GB) …","url":["https://arxiv.org/pdf/2006.03654"]}
{"year":"2020","title":"DECAB-LSTM: Deep Contextualized Attentional Bidirectional LSTM for cancer hallmark classification","authors":["L Jiang, X Sun, F Mercaldo, A Santone - Knowledge-Based Systems, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0950705120306158"]}
{"year":"2020","title":"Decoding individual identity from brain activity elicited in imagining common experiences","authors":["AJ Anderson, K McDermott, B Rooks, KL Heffner… - Nature Communications, 2020"],"snippet":"Everyone experiences common events differently. This leads to personal memories that presumably provide neural signatures of individual identity when events are reimagined. We present initial evidence that these signatures …","url":["https://www.nature.com/articles/s41467-020-19630-y"]}
{"year":"2020","title":"Deep Exogenous and Endogenous Influence Combination for Social Chatter Intensity Prediction","authors":["S Dutta, S Masud, S Chakrabarti, T Chakraborty - arXiv preprint arXiv:2006.07812, 2020"],"snippet":"… news on discussions. We report on extensive experiments using a two-month-long discussion corpus of Reddit, and a contemporaneous corpus of online news articles from the Common Crawl. ChatterNet shows considerable …","url":["https://arxiv.org/pdf/2006.07812"]}
{"year":"2020","title":"Deep Intelligent Contextual Embedding for Twitter Sentiment Analysis","authors":["K Musial-Gabrys, U Naseem - International Conference on Document Analysis and …, 2019"],"snippet":"… and GloVe. In our model we have used pre-trained GloVe embedding of 300 dimensions which are trained on 840 billion token from common crawl because it gives better results as compared to Word2Vec in our case. GloVe …","url":["https://opus.lib.uts.edu.au/bitstream/10453/137711/1/ICDAR_2019_paper_279.pdf"]}
{"year":"2020","title":"Deep Learning Based Multi-Label Text Classification of UNGA Resolutions","authors":["F Sovrano, M Palmirani, F Vitali - arXiv preprint arXiv:2004.03455, 2020"],"snippet":"… The pre-trained models we are going to use are: a GloVe model from Spacy [19] and pre-trained on data from Common Crawl [20], and the Universal Sentence Encoder (USE) model for document embedding coming from …","url":["https://arxiv.org/pdf/2004.03455"]}
{"year":"2020","title":"Deep Learning for Twitter Sentiment Analysis: The Effect of Pre-trained Word Embedding","authors":["A Krouska, C Troussas, M Virvou - Machine Learning Paradigms, 2020"],"snippet":"… The model contains 300-dimensional vectors for 3 million words and phrases. Crawl GloVe was trained on a Common Crawl dataset of 42 billion tokens (words), providing a vocabulary of 2 million words with an embedding vector …","url":["https://link.springer.com/chapter/10.1007/978-3-030-49724-8_5"]}
{"year":"2020","title":"Deep learning model for end-to-end approximation of COSMIC functional size based on use-case names","authors":["M Ochodek, S Kopczyńska, M Staron - Information and Software Technology, 2020"],"snippet":"… we investigate different pre-trained word embeddings to learn that using the embeddings trained on Wikipedia+Gigaworld (300d), Common Crawl 840B/42B (300d), and Stack Overflow (200d) give the best prediction accuracy. This paper is structured as follows …","url":["https://www.sciencedirect.com/science/article/pii/S0950584920300628"]}
{"year":"2020","title":"Deep N-ary Error Correcting Output Codes","authors":["H Zhang, JT Zhou, T Wang, IW Tsang, RSM Goh - arXiv preprint arXiv:2009.10465, 2020"],"snippet":"… For the Bi-LSTMs model of TREC and SST text datasets, we use the 300-dimensional publicly available pre-trained word embeddings as the word-level feature representation, which is trained by fastText4 package …","url":["https://arxiv.org/pdf/2009.10465"]}
{"year":"2020","title":"Deep Neural Attention-Based Model for the Evaluation of Italian Sentences Complexity","authors":["D Schicchi, G Pilato, GL Bosco - 2020 IEEE 14th International Conference on …, 2020"],"snippet":"… based algorithms. To the best of our knowledge, the most prominent sentence-based corpus for the Italian language is the PACCSS-IT corpus [19]. It has 1www.wikipedia. org 2www.commoncrawl.org 254 Page 3. been created …","url":["https://ieeexplore.ieee.org/abstract/document/9031472/"]}
{"year":"2020","title":"Deep Neural Networks Ensemble with Word Vector Representation Models to Resolve Coreference Resolution in Russian","authors":["A Sboev, R Rybka, A Gryaznov - Advanced Technologies in Robotics and Intelligent …, 2020"],"snippet":"… Among the context-insensitive vectorization models, the following were compared: Word2vec model, 3 trained on corpus of articles from the Russian part of Wikipedia (further referred to as RuWiki) and data from CommonCrawl 4 ; …","url":["https://link.springer.com/chapter/10.1007/978-3-030-33491-8_4"]}
{"year":"2020","title":"Deep Neural Networks for Sentiment Analysis in Tweets with Emoticons","authors":["M Narayanaperumal - 2020"],"snippet":"Page 1. DEEP NEURAL NETWORKS FOR SENTIMENT ANALYSIS IN TWEETS WITH EMOTICONS by Mutharasu Narayanaperumal A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Information Systems …","url":["http://search.proquest.com/openview/ce5f7af40a2bea968b30a3ab132f22bb/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2020","title":"Deep Question Answering: A New Teacher For DistilBERT","authors":["F Tamburini, P Cimiano, S Preite"],"snippet":"Page 1. Alma Mater Studiorum · Universit`a di Bologna SCUOLA DI SCIENZE Corso di Laurea Magistrale in Informatica Deep Question Answering: A New Teacher For DistilBERT Relatore: Chiar.mo Prof. Fabio Tamburini Correlatore: Chiar.mo Prof. Philipp Cimiano …","url":["https://amslaurea.unibo.it/20384/1/MasterThesisBologna.pdf"]}
{"year":"2020","title":"DeepPhish: Automated Phishing Detection Using Recurrent Neural Network","authors":["M Arivukarasi, A Antonidoss - Advances in Smart System Technologies"],"snippet":"… At that point, we prepared an irregular timberland classifier with 100 choice trees. 6 Conclusions. To assess the methodologies, we utilized a database that included one million real URLs from the common crawl database and …","url":["https://link.springer.com/chapter/10.1007/978-981-15-5029-4_18"]}
{"year":"2020","title":"DeepSinger: Singing Voice Synthesis with Data Mined From the Web","authors":["Y Ren, X Tan, T Qin, J Luan, Z Zhao, TY Liu - arXiv preprint arXiv:2007.04590, 2020"],"snippet":"… A variety of tasks collect training data from the Web, such as the large-scale web-crawled text dataset ClueWeb [3] and Common Crawl3 for language modeling [40], LETOR [31] for search ranking [4], and WebVision [22] for image classification …","url":["https://arxiv.org/pdf/2007.04590"]}
{"year":"2020","title":"Definition of Phishing Sites Based on the Team Model of Fuzzy Neural Networks","authors":["II Ismagilov, AA Murtazin, DV Kataseva, AS Katasev… - Helix, 2020"],"snippet":"… To obtain a set of data on legitimate sites, two sources were used: Alexa Internet and Common Crawl … Common Crawl is a non-profit organization; it crawls monthly the Internet and makes its archives and datasets available …","url":["https://helixscientific.pub/index.php/home/article/download/237/190"]}
{"year":"2020","title":"DeL-haTE: A Deep Learning Tunable Ensemble for Hate Speech Detection","authors":["J Melton, A Bagavathi, S Krishnan - arXiv preprint arXiv:2011.01861, 2020"],"snippet":"… We compare the following five word embedding methods: Word2Vec vectors trained on Google News corpus [17], GloVe vectors trained on CommonCrawl (GLoVe-CC) and Twitter (GLoVe-Twitter) corpora [18], and FastText vectors …","url":["https://arxiv.org/pdf/2011.01861"]}
{"year":"2020","title":"Delay Mitigation for Backchannel Prediction in Spoken Dialog System","authors":["AI Adiba, T Homma, D Bertero, T Sumiyoshi… - Conversational Dialogue Systems …"],"snippet":"… individual word. The word embedding is then used as input for our model architecture. We found that our dataset in the fastText model trained with the Common Crawl dataset 2 had the smallest number of unknown words. Thus, the …","url":["https://link.springer.com/chapter/10.1007/978-981-15-8395-7_10"]}
{"year":"2020","title":"DeLFT and entity-fishing: Tools for CLEF HIPE 2020 Shared Task","authors":["T Kristanti, L Romary - CLEF 2020-Conference and Labs of the Evaluation …, 2020"],"snippet":"… Word Embeddings We use various static word embeddings: Global Vectors for Word Representation (GloVe) [14], English fastText Common Crawl [1,11], and French Wikipedia fastText.5 We also use ELMo [16] contextualized …","url":["https://hal.inria.fr/hal-02974946/document"]}
{"year":"2020","title":"Depthwise Separable Convolutional Neural Network for Confidential Information Analysis","authors":["Y Lu, J Jiang, M Yu, C Liu, C Liu, W Huang, Z Lv - International Conference on …, 2020"],"snippet":"… Word2Vec. The Word2VecModified-Wikipedia are trained on Wikipedia through modified Word2vec. The GloVe-Crawl840B are trained on Common Crawl through GloVe. The GloVe-Wikipedia are trained on Wikipedia through GloVe …","url":["https://link.springer.com/chapter/10.1007/978-3-030-55393-7_40"]}
{"year":"2020","title":"Design2Struct: Generating website structures from design images using neural networks","authors":["MM Velzel - 2020"],"snippet":"… The second contribution is the release of a large CommonCrawl2 based dataset, filtered and transformed to be used in the field of GUI to structure conversion. The dataset is 1https://github.com/mvelzel/Design2Struct 2https://commoncrawl.org …","url":["http://essay.utwente.nl/81988/1/VELZEL_BA_EEMCS.pdf"]}
{"year":"2020","title":"Detecting Alzheimer's Disease by Exploiting Linguistic Information from Nepali Transcript","authors":["S Thapa, S Adhikari, U Naseem, P Singh, G Bharathy… - International Conference on …, 2020"],"snippet":"… For pre-trained Nepali Word2Vec model, the model created by Lamsal [15] is used in the study. Similarly, for the pre-trained fastText embeddings, the pre-trained word vectors trained on Common Crawl and Wikipedia using fastText were used [14] …","url":["https://link.springer.com/chapter/10.1007/978-3-030-63820-7_20"]}
{"year":"2020","title":"Detecting and Visualizing Hate Speech in Social Media: A Cyber Watchdog for Surveillance","authors":["S Modha, P Majumder, T Mandl, C Mandalia - Expert Systems with Applications, 2020"],"snippet":"… The word vectors were trained on 600 billion tokens of the Common Crawl corpus (Simonite, 2013). The Common Crawl is a nonprofit organization that crawls the web and freely provides its archives and datasets to the public …","url":["https://www.sciencedirect.com/science/article/pii/S0957417420305492"]}
{"year":"2020","title":"Detecting Deceptive Language in Crime Interrogation","authors":["YY Kao, PH Chen, CC Tzeng, ZY Chen, B Shmueli… - International Conference on …, 2020"],"snippet":"… fastText is a lightweight library for text representation. Its pre-trained model, trained on Common Crawl and Wikipedia corpus, has the ability to capture hidden information about a language such as word analogies or semantic …","url":["https://link.springer.com/chapter/10.1007/978-3-030-50341-3_7"]}
{"year":"2020","title":"Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases","authors":["W Guo, A Caliskan - arXiv preprint arXiv:2006.03955, 2020"],"snippet":"… intersectional group members. Caliskan et al. [1] have shown that social biases are embedded in linguistic regularities learned by GloVe. These embeddings are trained on the word co-occurrence statistics of the Common Crawl corpus …","url":["https://arxiv.org/pdf/2006.03955"]}
{"year":"2020","title":"Detecting Entailment in Code-Mixed Hindi-English Conversations","authors":["S Chakravarthy, A Umapathy, AW Black - Proceedings of the Sixth Workshop on …, 2020"],"snippet":"… XLM-RoBERTa (XLM-R) (Conneau et al., 2020) is trained on the CommonCrawl corpus, which in- cludes Romanized Hindi text, making this model the closest one to being pre-trained on Hinglish. 3 Task Definition Khanuja et al …","url":["https://www.aclweb.org/anthology/2020.wnut-1.22.pdf"]}
{"year":"2020","title":"Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank","authors":["E Briakou, M Carpuat - arXiv preprint arXiv:2010.03662, 2020"],"snippet":"… (2018) work on subtitles and Common Crawl corpora where sentence alignment errors abound, and Pham et al … di- vergence English-French parallel sentences drawn from OpenSubtitles and CommonCrawl corpora by prior work (Vyas et al., 2018) …","url":["https://arxiv.org/pdf/2010.03662"]}
{"year":"2020","title":"Detecting Hallucinated Content in Conditional Neural Sequence Generation","authors":["C Zhou, J Gu, M Diab, P Guzman, L Zettlemoyer… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. DETECTING HALLUCINATED CONTENT IN CONDITIONAL NEURAL SEQUENCE GENERATION Chunting Zhou1∗, Jiatao Gu2, Mona Diab2, Paco Guzman2, Luke Zettlemoyer2, Marjan Ghazvininejad2 Language …","url":["https://arxiv.org/pdf/2011.02593"]}
{"year":"2020","title":"Detecting Incivility and Impoliteness in Online Discussions.","authors":["AK Stoll, M Ziegele, O Quiring - Computational Communication Research, 2020"],"snippet":"Page 1. VOL. 2, NO. 1, 2020 109 Detecting Impoliteness and Incivility in Online Discussions Classification Approaches for German User Comments Anke Stoll, Marc Ziegele, Oliver Quiring CCR 2 (1): 109–134 DOI: 10.5117/CCR2020.1.005.KATH …","url":["https://computationalcommunication.org/ccr/article/download/19/10"]}
{"year":"2020","title":"Detecting misogyny in Spanish Tweets. An approach based on linguistics features and word embeddings","authors":["JA García-Díaz, M Cánovas-García… - Future Generation …, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0167739X20301928"]}
{"year":"2020","title":"DETECTING OUT-OF-DISTRIBUTION TRANSLATIONS WITH VARIATIONAL TRANSFORMERS","authors":["WAT ZEI"],"snippet":"… The following datasets were used in our experiments: (1) WMT EN ↔ DE: The training set for translation tasks between English (EN) and German (DE) composed of news-commentary-v13 with 284k sentences pairs …","url":["https://openreview.net/pdf/e2667f2c5169fcbdca8e1d0596e67792da06d3a0.pdf"]}
{"year":"2020","title":"Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks","authors":["D Emelin, I Titov, R Sennrich - arXiv preprint arXiv:2011.01846, 2020"],"snippet":"Page 1. Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks Denis Emelin1, Ivan Titov1, 2, and Rico Sennrich3, 1 1University of Edinburgh, Scotland 2University of Amsterdam …","url":["https://arxiv.org/pdf/2011.01846"]}
{"year":"2020","title":"Detecting, Classifying, and Mapping Retail Storefronts Using Street-level Imagery","authors":["S Sharifi Noorian, S Qiu, A Psyllidis, A Bozzon… - Proceedings of the 2020 …, 2020","SS Noorian, S Qiu, A Psyllidis, A Bozzon, GJ Houben"],"snippet":"… a bag of character n-grams. As our evaluation will focus on the Manhattan Borough of New York City, the pre-trained (on Common Crawl and Wikipedia 3) word vectors for English are used. According to the desired language …","url":["https://dl.acm.org/doi/pdf/10.1145/3372278.3390706","https://qiusihang.github.io/files/publications/icmr2020detecting.pdf"]}
{"year":"2020","title":"Detection of Emerging Words in Portuguese Tweets","authors":["A Pinto, H Moniz, F Batista - 9th Symposium on Languages, Applications and …, 2020"],"snippet":"… We have used two pre-trained word vectors for Portuguese4, the first one trained on the Common Crawl5, and the second trained on the … www.nilc.icmc. usp.br/nilc/projects/unitex-pb/web/dicionarios.html 4 https://fasttext.cc/docs …","url":["https://drops.dagstuhl.de/opus/volltexte/2020/13016/pdf/OASIcs-SLATE-2020-3.pdf"]}
{"year":"2020","title":"Detection of Harassment on Twitter with Deep Learning Techniques","authors":["I Espinoza, F Weiss - Machine Learning and Knowledge Discovery in …, 2020"],"snippet":"… trained embedding model. We use the implementation off Spacy library for Python4 with the pre-trained model called 'en vectors web lg', which has 300 dimensions and it's trained over common crawl texts. With Spacy we have …","url":["https://link.springer.com/content/pdf/10.1007/978-3-030-43887-6_24.pdf"]}
{"year":"2020","title":"Determining Event Outcomes: The Case of# fail","authors":["S Murugan, D Chinnappa, E Blanco - Proceedings of the 2020 Conference on …, 2020"],"snippet":"… has variable length). Additionally, the word embeddings (GloVe embeddings pre-trained with CommonCrawl) allow us to leverage a distributional representation of tags, including those not seen during training. The second …","url":["https://www.aclweb.org/anthology/2020.findings-emnlp.359.pdf"]}
{"year":"2020","title":"Developing a Twitter bot that can join a discussion using state-of-the-art architectures","authors":["YM Çetinkaya, İH Toroslu, H Davulcu - Social Network Analysis and Mining, 2020"],"snippet":"… requests. Radford et al. (2019) construct an auto-regressive feed-forward model instead of seq2seq-RNN as a language model using Common Crawl as a dataset and generate sentences with predicting next word. Generative …","url":["https://link.springer.com/article/10.1007/s13278-020-00665-4"]}
{"year":"2020","title":"Developing an online hate classifier for multiple social media platforms","authors":["J Salminen, M Hopf, SA Chowdhury, S Jung… - Human-centric Computing …, 2020"],"snippet":"The proliferation of social media enables people to express their opinions widely online. However, at the same time, this has resulted in the emergence of conflict and hate, making online...","url":["https://link.springer.com/article/10.1186/s13673-019-0205-6"]}
{"year":"2020","title":"Development and evaluation of a Polish Automatic Speech Recognition system using the TLK toolkit","authors":["NU Roselló Beneitez - 2020"],"snippet":"… 12 2.8 Trellis representing the decoding step . . . . . 15 3.1 Examples of sentences extracted from the Common Crawl corpus . . . . . 21 4.1 Actions performed to create the final acoustic models . . . . . 24 4.2 Phonetic transcription of a Polish word …","url":["https://riunet.upv.es/bitstream/handle/10251/150495/Rosell%C3%B3%20-%20Desarrollo%20y%20evaluaci%C3%B3n%20de%20un%20sistema%20de%20Reconocimiento%20Autom%C3%A1tico%20del%20Habla%20en%20Polaco%20....pdf?sequence=1"]}
{"year":"2020","title":"Development of a Search Engine to Answer Comparative Queries","authors":["J Huck"],"snippet":"… extraction and tuning the retrieval model. Page 6. References 1. Bevendorff, J., Stein, B., Hagen, M., Potthast, M.: Elastic chatnoir: Search engine for the clueweb and the common crawl. In: ECIR (2018) 2. Bondarenko, A., Fröbe …","url":["http://www.dei.unipd.it/~ferro/CLEF-WN-Drafts/CLEF2020/paper_178.pdf"]}
{"year":"2020","title":"Development of Word Embeddings for Uzbek Language","authors":["B Mansurov, A Mansurov - arXiv preprint arXiv:2009.14384, 2020"],"snippet":"… variant of Uzbek. As far as we're aware, only fastText [5] word embeddings exist for the Latin variant. However, fastText was trained on the relatively low quality Uzbek Wikipedia and noisy Common Crawl corpus. In this paper …","url":["https://arxiv.org/pdf/2009.14384"]}
{"year":"2020","title":"Dialog Response Generation Using Adversarially Learned Latent Bag-of-Words","authors":["K Khan - 2020"],"snippet":"Page 1. Dialog Response Generation Using Adversarially Learned Latent Bag-of-Words by Kashif Khan A thesis presented to the University of Waterloo in fulfillment of the thesis requirement for the degree of Master of Mathematics in Computer Science …","url":["https://uwspace.uwaterloo.ca/bitstream/handle/10012/16188/Khan_Kashif.pdf?sequence=3&isAllowed=y"]}
{"year":"2020","title":"Dictionary for Computer-Assisted Text Analysis of Cyber Security (TACS)","authors":["A Levordashka, A Joinson, S Jones"],"snippet":"… algorithm [372] implemented via the Python package textacy [7]. We then grouped the terms by semantic similarity, with the help of a vector space model available via the Python library spacy ('en_core_web_lg'), with 685k …","url":["https://nbviewer.jupyter.org/github/anidroid/tacs/blob/master/tacs-soups.pdf"]}
{"year":"2020","title":"Dictionary-based Data Augmentation for Cross-Domain Neural Machine Translation","authors":["W Peng, C Huang, T Li, Y Chen, Q Liu - arXiv preprint arXiv:2004.02577, 2020"],"snippet":"… The OOD data used for pre-training for the baseline model are extracted from WMT 144 including Eu- roparl V7, New-commentary V9 and Common Crawl corpora … Train Dataset (OOD) Europarl,News-commentary, Common …","url":["https://arxiv.org/pdf/2004.02577"]}
{"year":"2020","title":"Differences Beyond Identity: Perceived Construal Distance and Interparty Animosity in the United States","authors":["A van Loon, A Goldberg, S Srivastava - SocArXiv. July, 2020"],"snippet":"Page 1. Differences Beyond Identity: Perceived Construal Distance and Interparty Animosity in the United States ∗ Austin van Loon Stanford University Amir Goldberg Stanford University Sameer B. Srivastava …","url":["https://osf.io/j2f6u/download"]}
{"year":"2020","title":"Dilated Convolution Networks for Classification of ICD-9 based Clinical Summaries","authors":["M Morisio, N Kanwal, I Tutor, DG Rizzo - 2020","N Kanwal - 2020"],"snippet":"… This architecture uses multiple dilation layers with a label-specific dot-based attention mechanism. We have extracted the embeddings from Common Crawl Glove (Global Vector). The architecture of the model is designed to calculate attention to words and their context …","url":["https://webthesis.biblio.polito.it/14400/","https://webthesis.biblio.polito.it/14400/1/tesi.pdf"]}
{"year":"2020","title":"Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures","authors":["J Launay, I Poli, F Boniface, F Krzakala - arXiv preprint arXiv:2006.12878, 2020"],"snippet":"Page 1. Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures Julien Launay1,2 Iacopo Poli1 François Boniface1 Florent Krzakala1,2 1LightOn 2École Normale Supérieure {julien, iacopo, francois, florent}@lighton.ai Abstract …","url":["https://arxiv.org/pdf/2006.12878"]}
{"year":"2020","title":"Discovering key topics from short, real-world medical inquiries via natural language processing and unsupervised learning","authors":["A Ziletti, C Berns, O Treichel, T Weber, J Liang… - arXiv preprint arXiv …, 2020"],"snippet":"… Table IIA1 presents a qualitative comparison of a standard embedding (en core web lg, trained on the Common Crawl) and a specialized biomedical embedding (scispaCy en core sci lg, trained also on PubMed). Specifically …","url":["https://arxiv.org/pdf/2012.04545"]}
{"year":"2020","title":"Discovering Relational Intelligence in Online Social Networks","authors":["L Tan, T Pham, HK Ho, TS Kok - International Conference on Database and Expert …, 2020"],"snippet":"… 100 mil tweets. 283. \\(^\\text {a}\\)https://archive.ics.uci.edu/ml/datasets /bag+of+words. \\(^\\text {b}\\)http://commoncrawl.org/2014/07/april-2014-crawldata-available/. \\(^text {c}\\)https://developer.twitter.com/en/docs.html. \\(^\\text …","url":["https://link.springer.com/chapter/10.1007/978-3-030-59003-1_22"]}
{"year":"2020","title":"Discovering web services in social web service repositories using deep variational autoencoders","authors":["I Lizarralde, C Mateos, A Zunino, TA Majchrzak… - Information Processing & …, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0306457319310878"]}
{"year":"2020","title":"Disentangling semantic composition and semantic association in the left temporal lobe Abbreviated title: Semantic composition versus association","authors":["J Li, S Island, A Dhabi, L Pylkkänen"],"snippet":"… derived using the well-known GloVe word embeddings model (Pennington et al., 2014; freely available at https://nlp.stanford.edu/projects/glove/) trained on Common Crawl (https://commoncrawl.org/), which contains petabytes …","url":["https://www.biorxiv.org/content/10.1101/2020.08.17.254482v2.full.pdf"]}
{"year":"2020","title":"Distractor Analysis and Selection for Multiple-Choice Cloze Questions for Second-Language Learners","authors":["L Gao, K Gimpel, A Jensson"],"snippet":"… form pair. For features that require embedding words, we use the 300-dimensional GloVe word embedPage 4. dings (Pennington et al., 2014) pretrained on the 42 billion token Common Crawl corpus. The GloVe embeddings …","url":["https://ttic.uchicago.edu/~kgimpel/papers/gao+etal.bea2020.pdf"]}
{"year":"2020","title":"Distributed frameworks for approximate data analytics","authors":["G Hu - 2020"],"snippet":"… analytical queries can be expensive, eg, the Google book Ngrams dataset contains 2.2 TB of text data [17], and CommonCrawl corpus petabytes of web pages [18]. The above challenge is exacerbated when it is desirable to run different types of …","url":["https://rucore.libraries.rutgers.edu/rutgers-lib/65036/PDF/1/play/"]}
{"year":"2020","title":"Distributed Training of Graph Convolutional Networks using Subgraph Approximation","authors":["A Angerd, K Balasubramanian, M Annavaram - arXiv preprint arXiv:2012.04930, 2020"],"snippet":"… The vertex label is the community a post belongs to. The features consist of an embedding of post information, created using GloVe CommonCrawl (Pennington et al., 2014). The first 20 days of posts are used for training, while the rest are used for testing and validation …","url":["https://arxiv.org/pdf/2012.04930"]}
{"year":"2020","title":"Distributional and Lexical Exploration of Semantics of Derivational Morphology","authors":["UC Kunter, GN Özdemir, C Bozşahin"],"snippet":"… using Wikipedia datasets. The second one is presented in Grave et al. (2018), covering 157 languages including Turkish. Their models used Common Crawl and Wikipedia datasets and trained on fastText. The models were …","url":["http://www.academia.edu/download/63487208/Distributional_and_Lexical_Exploration_of_Semantics_of_DM20200531-40903-1x0rwv8.pdf"]}
{"year":"2020","title":"Distributional Models in the Task of Hypernym Discovery","authors":["V Yadrintsev, A Ryzhova, I Sochenkov - Russian Conference on Artificial Intelligence, 2020"],"snippet":"… Most likely, the largest text corpus was used for the first model, which includes Wikipedia and Common Crawl (we do not know the exact volume of crawl-data for the Russian, but roughly 24 terabytes of plain text was used …","url":["https://link.springer.com/chapter/10.1007/978-3-030-59535-7_25"]}
{"year":"2020","title":"Distributional semantic modeling: a revised technique to train term/word vector space models applying the ontology-related approach","authors":["O Palagin, V Velychko, K Malakhov, O Shchurov - arXiv preprint arXiv:2003.03350, 2020"],"snippet":"… Ac- cessed: 2020-03-03. [42] Firefly documentation. https://rorodata.github. io/firefly/. Accessed: 2020-03-03. [43] Common crawl. http://commoncrawl org/. Accessed: 2020-03-03. [44] Google dataset search. https://datasetsearch …","url":["https://arxiv.org/pdf/2003.03350"]}
{"year":"2020","title":"Do Neural Language Models Show Preferences for Syntactic Formalisms?","authors":["A Kulmizev, V Ravishankar, M Abdou, J Nivre - arXiv preprint arXiv:2004.14096, 2020"],"snippet":"… Che et al. (2018). These models are trained on 20 million words randomly sampled from the concatenation of WikiDump and CommonCrawl datasets for 44 different languages, including our 13 languages. Each model features …","url":["https://arxiv.org/pdf/2004.14096"]}
{"year":"2020","title":"Document Representations for Fast and Accurate Retrieval of Mathematical Information","authors":["V Novotný"],"snippet":"Page 1. Masaryk University Faculty of Informatics Document Representations for Fast and Accurate Retrieval of Mathematical Information Rigorous Thesis Vít Novotný Advisor: Doc. RNDr. Petr Sojka, Ph. D. Brno, Fall 2019 Signature …","url":["https://is.muni.cz/th/x86jd/thesis-with-papers.pdf"]}
{"year":"2020","title":"Domain Name System Security and Privacy: A Contemporary Survey","authors":["A Khormali, J Park, H Alasmary, A Anwar, D Mohaisen - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Domain Name System Security and Privacy: A Contemporary Survey Aminollah Khormali, Jeman Park, Hisham Alasmary, Afsah Anwar, David Mohaisen University of Central Florida Abstract The domain name system …","url":["https://arxiv.org/pdf/2006.15277"]}
{"year":"2020","title":"Domain Specific Complex Sentence (DCSC) Semantic Similarity Dataset","authors":["D Chandrasekaran, V Mago - arXiv preprint arXiv:2010.12637, 2020"],"snippet":"… trained. While BERT was trained on Book Corpus and Wikipedia corpus, RoBERTa model was trained on four different corpora namely Book Corpus, Common Crawl News dataset, OpenWebText dataset and the Stories dataset …","url":["https://arxiv.org/pdf/2010.12637"]}
{"year":"2020","title":"Domain-Specific Meta-Embedding with Latent Semantic Structures","authors":["Q Liu, J Lu, G Zhang, T Shen, Z Zhang, H Huang - Information Sciences, 2020"],"snippet":"… For example, GloVe is trained on aggregated global word-word co-occurrence statistics from a corpus of over 840B tokens and fastText [2] pre-trained word representations for 157 languages on Common Crawl and the Wikipedia Corpora …","url":["https://www.sciencedirect.com/science/article/pii/S002002552031029X"]}
{"year":"2020","title":"DOMINANCE STYLE AND VOCAL COMMUNICATION IN NON-HUMAN PRIMATES","authors":["ZC CHEN-KRAUS, C COYE10, M EMERY… - LANGUAGE of, 2020"],"snippet":"Page 441. DOMINANCE STYLE AND VOCAL COMMUNICATION IN NON-HUMAN PRIMATES K KATIE SLOCOMBE* 1, EITHNE KAVANAGH11,, SALLY STREET2, FELIX O. ANGWELA3, THORE J. BERGMAN4, MARYJKA …","url":["https://pure.mpg.de/rest/items/item_3190925/component/file_3219601/content#page=441"]}
{"year":"2020","title":"Don't Stop Pretraining: Adapt Language Models to Domains and Tasks","authors":["S Gururangan, A Marasović, S Swayamdipta, K Lo… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks Suchin Gururangan† Ana Marasovic†♦ Swabha Swayamdipta†♦ Kyle Lo† Iz Beltagy† Doug Downey† Noah A. Smith†♦ †Allen Institute for Artificial …","url":["https://arxiv.org/pdf/2004.10964"]}
{"year":"2020","title":"DRIVING INTENT EXPANSION VIA ANOMALY DETECTION IN A MODULAR CONVERSATIONAL SYSTEM","authors":["NR Mallinar, TK HO - US Patent App. 16/180,613, 2020"],"snippet":"… In various embodiments, the dataset builder 365 uses one or more of word2vec, the enwiki 2015 document vectorizer, ppdb paragram sentence embeddings, common-crawl uncased GloVe word embeddings, and enwiki …","url":["http://www.freepatentsonline.com/y2020/0142959.html"]}
{"year":"2020","title":"Drug-Drug Interaction Classification Using Attention Based Neural Networks","authors":["D Zaikis, I Vlahavas - 11th Hellenic Conference on Artificial Intelligence, 2020"],"snippet":"… word in a given sentence. The large English statistical model was used, which is trained with GloVe vectors on the OntoNotes 5 Common Crawl corpus and has a POS syntax accuracy of 97.22 percent. The unique POS tags …","url":["https://dl.acm.org/doi/abs/10.1145/3411408.3411461"]}
{"year":"2020","title":"Dual Conditional Cross Entropy Scores and LASER Similarity Scores for the WMT20 Parallel Corpus Filtering Shared Task","authors":["F Koerner, P Koehn"],"snippet":"… sentences. The Pashto language model was trained on a concatenation of the CommonCrawl and Wikipedia corpora, with the CommonCrawl oversampled by a factor of 64 to produce a dataset of 9,273,763 sentences. The …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.109.pdf"]}
{"year":"2020","title":"Dynamic Data Selection and Weighting for Iterative Back-Translation","authors":["ZY Dou, A Anastasopoulos, G Neubig - arXiv preprint arXiv:2004.03672, 2020"],"snippet":"Page 1. Dynamic Data Selection and Weighting for Iterative Back-Translation Zi-Yi Dou, Antonios Anastasopoulos, Graham Neubig Language Technologies Institute, Carnegie Mellon University {zdou, aanastas, gneubig}@cs.cmu.edu Abstract …","url":["https://arxiv.org/pdf/2004.03672"]}
{"year":"2020","title":"DynE: Dynamic Ensemble Decoding for Multi-Document Summarization","authors":["C Hokamp, DG Ghalandari, NT Pham, J Glover - arXiv preprint arXiv:2006.08748, 2020"],"snippet":"… Ghalandari et al. (2020) presented the WCEP dataset, which is a large-scale collection of clusters of news articles with a corresponding summary, constructed using the Wikipedia Current Events Portal, with additional articles gathered from CommonCrawl …","url":["https://arxiv.org/pdf/2006.08748"]}
{"year":"2020","title":"EEGdenoiseNet: A benchmark dataset for deep learning solutions of EEG denoising","authors":["H Zhang, M Zhao, C Wei, D Mantini, Z Li, Q Liu - arXiv preprint arXiv:2009.11662, 2020"],"snippet":"Page 1. EEGdenoiseNet: A benchmark dataset for deep learning solutions of EEG denoising Haoming Zhang1,†, Mingqi Zhao1,2,†, Chen Wei1, Dante Mantini2,3, Zherui Li1, Quanying Liu1,∗ September 25, 2020 1 Department …","url":["https://arxiv.org/pdf/2009.11662"]}
{"year":"2020","title":"Effect of Character and Word Features in Bidirectional LSTM-CRF for NER","authors":["C Ronran, S Lee - 2020 IEEE International Conference on Big Data and …, 2020"],"snippet":"… CRF), respectively, using public word embedding, character features and word features [6,4]. We explore existing word embeddings that is distinct from the previous studies including (1) Glove 300 embedding of 42B, 840B …","url":["https://ieeexplore.ieee.org/abstract/document/9070329/"]}
{"year":"2020","title":"Efficient and High-Quality Neural Machine Translation with OpenNMT","authors":["G Klein, D Zhang, C Chouteau, JM Crego, J Senellart - Proceedings of the Fourth …, 2020"],"snippet":"… Europarl v9 1,838,568 Common Crawl corpus 2,399,123 News Commentary v14 338,285 Wiki Titles v1 1,305,141 Document-split Rapid 1,531,261 ParaCrawl v3 31,358,551 Total 38,770,929 news-crawl 2007-2018 …","url":["https://www.aclweb.org/anthology/2020.ngt-1.25.pdf"]}
{"year":"2020","title":"Efficient strategies for hierarchical text classification: External knowledge and auxiliary tasks","authors":["KR Rojas, G Bustamante, MAS Cabezudo, A Oncevay - arXiv preprint arXiv …, 2020"],"snippet":"… of tokens in the input document. We use pre-trained word embeddings from Common Crawl (Grave et al., 2018) for the weights of this layer, and we do not fine-tune them during training time. Encoder: It is a bidirectional GRU …","url":["https://arxiv.org/pdf/2005.02473"]}
{"year":"2020","title":"Efficient Transfer Learning for Quality Estimation with Bottleneck Adapter Layer","authors":["H Yang, M Wang, N Xie, Y Qin, Y Deng - Proceedings of the 22nd Annual Conference …, 2020"],"snippet":"… BPE is used for tokenizing, where 32000 tokens are reserved. We use UN corpus and Common Crawl parallel corpus with the size of 1https://github.com/pytorch/fairseq Page 5. Total Params Training Params …","url":["https://www.aclweb.org/anthology/2020.eamt-1.4.pdf"]}
{"year":"2020","title":"Efficiently Reusing Old Models Across Languages via Transfer Learning","authors":["T Kocmi, O Bojar - Proceedings of the 22nd Annual Conference of the …, 2020"],"snippet":"… EN - Russian 12.6M News Commentary, Yandex, and UN Corpus WMT 2012 WMT 2018 EN - French 34.3M Commoncrawl, Europarl, Giga FREN, News commentary, UN corpus WMT 2013 WMT dis. 2015 Table 2: Corpora used for each language pair …","url":["https://www.aclweb.org/anthology/2020.eamt-1.3.pdf"]}
{"year":"2020","title":"Embedding Compression with Isotropic Iterative Quantization","authors":["S Liao, J Chen, Y Wang, Q Qiu, B Yuan - arXiv preprint arXiv:2001.05314, 2020"],"snippet":"… We perform experiments with the GloVe embedding (Pennington et al. 2014) and the HDC embedding (Sun et al. 2015). The GloVe embedding is trained from 42B tokens of Common Crawl data. The HDC Table 1: Experiment …","url":["https://arxiv.org/pdf/2001.05314"]}
{"year":"2020","title":"Embedding Compression with Right Triangle Similarity Transformations","authors":["H Song, D Zou, L Hu, J Yuan - International Conference on Artificial Neural Networks, 2020"],"snippet":"… model. 4.1 Experimental Setup. Pre-trained Continuous Embeddings. We conduct experiments on GloVe [16] and fasttext [1]. GloVe embeddings have been trained on 42B tokens of Common Crawl data with 400k words. Fasttext …","url":["https://link.springer.com/chapter/10.1007/978-3-030-61616-8_62"]}
{"year":"2020","title":"EmoDet2: Emotion Detection in English Textual Dialogue using BERT and BiLSTM Models","authors":["H Al-Omari, MA Abdullah, S Shaikh - 2020 11th International Conference on …, 2020"],"snippet":"… Moreover, we have encoded the words in the conversation using Word2vec, Glove Wiki, and Glove Common Crawl packages … The hyperparameters as follow: Dropout = 0.4, the text in Word2Vec and Glove Wiki are lowered …","url":["https://ieeexplore.ieee.org/abstract/document/9078946/"]}
{"year":"2020","title":"Emotion Aided Dialogue Act Classification for Task-Independent Conversations in a Multi-modal Framework","authors":["T Saha, D Gupta, S Saha, P Bhattacharyya - Cognitive Computation"],"snippet":"… To extract textual features, a convolutional neural network (CNN) [48]–based approach is used. Pretrained GloVe [49] embeddings trained on the CommonCrawl corpus of dimension 300 have been used to represent words as word vectors …","url":["https://link.springer.com/article/10.1007/s12559-019-09704-5"]}
{"year":"2020","title":"Employing distributional semantics to organize task-focused vocabulary learning","authors":["HS Ponnusamy, D Meurers - arXiv preprint arXiv:2011.11115, 2020"],"snippet":"… graph, we start with a distributional semantic vector representation of each word, which we obtain from the pre-trained model of GloVe (Pennington et al., 2014) based on the co-occurrence statistics of the the words form a large …","url":["https://arxiv.org/pdf/2011.11115"]}
{"year":"2020","title":"Empowering Architects and Designers: A Classification of What Functions to Accelerate in Storage","authors":["C Zou, AA Chien"],"snippet":"Page 1. Empowering Architects and Designers: A Classification of What Functions to Accelerate in Storage Chen Zou chenzou@uchicago.edu University of Chicago Andrew A. Chien achien@cs.uchicago.edu University of Chicago …","url":["https://newtraell.cs.uchicago.edu/files/tr_authentic/TR-2020-02.pdf"]}
{"year":"2020","title":"End to end approach for i2b2 2012 challenge based on Cross-lingual models","authors":["EA Santamaría - 2020"],"snippet":"… Joshi et al., 2019). Unlike mBERT who has been trained on Wikipedia, XLM-RoBERT uses the CommonCrawl(Conneau et al., 2019a) corpus for its training. In this section we explain step by step our approach. First we adapt …","url":["https://addi.ehu.es/bitstream/handle/10810/48623/MAL-Edgar_Andres.pdf?sequence=1"]}
{"year":"2020","title":"End-to-End Simultaneous Translation System for IWSLT2020 Using Modality Agnostic Meta-Learning","authors":["HJ Han, MA Zaidi, SR Indurthi, NK Lakumarapu, B Lee… - Proceedings of the 17th …, 2020"],"snippet":"… We evaluate our system on the MuST-C Dev set. Our parallel corpus of WMT19 consists of Europarl v9, ParaCrawl v3, Common Crawl, News Commentary v14, Wiki Titles v1 and Documentsplit Rapid for the German-English language pair …","url":["https://www.aclweb.org/anthology/2020.iwslt-1.5.pdf"]}
{"year":"2020","title":"Energy-Based Models for Text","authors":["A Bakhtin, Y Deng, S Gross, M Ott, MA Ranzato… - arXiv preprint arXiv …, 2020"],"snippet":"… (2015); Kiros et al. (2015), which consists of fiction books in 16 different genres, totaling about half a billion words. • CCNews: We collect a de-duplicated subset of the English portion of the CommonCrawl news …","url":["https://arxiv.org/pdf/2004.10188"]}
{"year":"2020","title":"English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too","authors":["J Phang, PM Htut, Y Pruksachatkun, H Liu, C Vania… - arXiv preprint arXiv …, 2020"],"snippet":"… 5XLM-R Large (Conneau et al., 2019) is a 550m-parameter variant of the RoBERTa masked language model (Liu et al., 2019b) trained on a cleaned version of CommonCrawl on 100 languages. 6Excluded in this draft due to implementation issues. Page 5 …","url":["https://arxiv.org/pdf/2005.13013"]}
{"year":"2020","title":"Enhanced-RCNN: An Efficient Method for Learning Sentence Similarity","authors":["S Peng, H Cui, N Xie, S Li, J Zhang, X Li - Proceedings of The Web Conference 2020, 2020"],"snippet":"… Base model. Model parameter size time (s/batch) BERT-Base 102.2M 0.23 ± 0.20 Enhanced-RCNN 7.7M 0.02 ± 0.01 from the 840B Common Crawl corpus [21]. We set the hidden size as 192 for all BiGRU layers. Ant Financial …","url":["https://dl.acm.org/doi/pdf/10.1145/3366423.3379998"]}
{"year":"2020","title":"Enhancing Word Embeddings with Knowledge Extracted from Lexical Resources","authors":["M Biesialska, B Rafieian, MR Costa-jussà - arXiv preprint arXiv:2005.10048, 2020"],"snippet":"… Dinu et al., 2015; Artetxe et al., 2017, 2018). Moreover, GloVe vectors for English were trained on Common Crawl (Pennington et al., 2014). Linguistic Constraints. To perform semantic specialization of word vector spaces, we …","url":["https://arxiv.org/pdf/2005.10048"]}
{"year":"2020","title":"Entity-Switched Datasets: An Approach to Auditing the In-Domain Robustness of Named Entity Recognition Models","authors":["O Agarwal, Y Yang, BC Wallace, A Nenkova - arXiv preprint arXiv:2004.04123, 2020"],"snippet":"… al., 2019). For the first two, we used 300-d cased GloVe (Pennington et al., 2014) vectors trained on Common Crawl.7 For BERT, we use the public large8 uncased9 model and apply the default fine-tuning strategy. We use …","url":["https://arxiv.org/pdf/2004.04123"]}
{"year":"2020","title":"Entrepreneurial Organizations and the Use of Strategic Silence","authors":["W Shi, M Weber - Proceedings of the 54th Hawaii International …"],"snippet":"… number of competing apps for a particular keyword), chart rankings (current ranking position for a keyword), difficulty (the popularity of apps including reviews and ratings) and traffic (eg, autosuggestion when typing in the store …","url":["https://scholarspace.manoa.hawaii.edu/bitstream/10125/71247/0506.pdf"]}
{"year":"2020","title":"Establishing a New State-of-the-Art for French Named Entity Recognition","authors":["PJO Suárez, Y Dupont, B Muller, L Romary, B Sagot - LREC 2020-12th Language …, 2020"],"snippet":"… They use zero to three of the following vector representations: FastText non-contextual embeddings (Bojanowski et al., 2017), the FrELMo contextual language model ob- tained by training the ELMo architecture on the OSCAR …","url":["https://hal.inria.fr/hal-02617950/document"]}
{"year":"2020","title":"Estimating educational outcomes from students' short texts on social media","authors":["I Smirnov - EPJ Data Science, 2020"],"snippet":"… We obtained significantly better results with a model that used word-embeddings (see Methods). We also find that embeddings trained on the VK corpus outperform models trained on the Wikipedia and Common Crawl corpora (Table 1). Page 6 …","url":["https://link.springer.com/content/pdf/10.1140/epjds/s13688-020-00245-8.pdf"]}
{"year":"2020","title":"Estimating Mutual Information Between Dense Word Embeddings","authors":["V Zhelezniak, A Savkov, N Hammerla - Proceedings of the 58th Annual Meeting of …, 2020"],"snippet":"… Our focus here is on fastText vectors (Bojanowski et al., 2017) trained on Common Crawl (600B tokens), as previous literature suggests that among unsupervised vectors fastText yields the best performance for all tasks and …","url":["https://www.aclweb.org/anthology/2020.acl-main.741.pdf"]}
{"year":"2020","title":"Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks","authors":["F Schröder, C Biemann"],"snippet":"Page 1. Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks Fynn Schröder Language Technology Group Universität Hamburg Hamburg, Germany fschroeder@informatik.uni-hamburg.de …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/publications/2020-schroeder-biemann-acl20-dataset-similarity.pdf"]}
{"year":"2020","title":"Evaluating cross-lingual textual similarity on dictionary alignment problem","authors":["Y Sever, G Ercan - Language Resources and Evaluation, 2020"],"snippet":"… 2018) provides embeddings for 157 different languages trained on Wikipedia and Common Crawl Footnote 3 . For efficiency concerns, each embedding set of the 7 languages are pruned to the the most frequent \\(500\\times 10^3\\) words in each language …","url":["https://link.springer.com/article/10.1007/s10579-020-09498-1"]}
{"year":"2020","title":"Evaluating German Transformer Language Models with Syntactic Agreement Tests","authors":["K Zaczynska, N Feldhus, R Schwarzenberg…"],"snippet":"… 1 The first model which we refer to as GBERTlarge is a community model provided by the Bavarian State Library.2 It was trained on multiple German corpora including a recent Wikipedia dump, EU Bookshop corpus …","url":["http://ceur-ws.org/Vol-2624/paper7.pdf"]}
{"year":"2020","title":"Evaluating Multilingual BERT for Estonian","authors":["C Kittask, K Milintsevich, K Sirts - arXiv preprint arXiv:2010.00454, 2020"],"snippet":"… 1Corresponding Author: Claudia Kittask; E-mail: claudiakittask@gmail.com Page 2. and cross-lingual RoBERTa (XLM-RoBERTa) [3], which was trained on much larger CommonCrawl corpora and also includes 100 languages …","url":["https://arxiv.org/pdf/2010.00454"]}
{"year":"2020","title":"Evaluating Sentence Representations for Biomedical Text: Methods and Experimental Results","authors":["NS Tawfik, MR Spruit - Journal of Biomedical Informatics, 2020"],"snippet":"… 3.2. Embedding Methods. GloVe We use the pre-trained embeddings consisting of 2.2 million vocabulary words available at https://nlp.stanford.edu/projects/glove/ which were trained on the Common Crawl (840B tokens) dataset …","url":["https://www.sciencedirect.com/science/article/pii/S1532046420300253"]}
{"year":"2020","title":"Evaluating Word Embeddings on Low-Resource Languages","authors":["N Stringham, M Izbicki - Proceedings of the First Workshop on Evaluation and …, 2020"],"snippet":"… Grave et al. (2018) trained FastText embeddings on 157 languages using data from the Common Crawl project. But they were only able to explicitly evaluate 10 of these language models using the analogy task due to the …","url":["https://www.aclweb.org/anthology/2020.eval4nlp-1.17.pdf"]}
{"year":"2020","title":"Evaluation of related news recommendations using document similarity methods","authors":["M Pranjic, V Podpecan, M Robnik-Šikonja, S Pollak"],"snippet":"… RoBERTa (Liu et al., 2019). It uses the sentence piece tokenizer and it is trained with the masked language model objective (MLM) on the CommonCrawl data in 100 languages, including Croatian. Similar to the mBERT, all …","url":["http://nl.ijs.si/jtdh20/pdf/JT-DH_2020_Pranjic-et-al_Evaluation-of-related-news-recommendations-using-document-similarity-methods.pdf"]}
{"year":"2020","title":"Event Detection on Literature by Utilizing Word Embedding","authors":["J Chun, C Kim - International Conference on Database Systems for …, 2020"],"snippet":"… On the contrary, Neural-based methods have the limitation that they ignore semantic relationships in a text. 2.3 Facebook Pre-trained Word Vectors. We chose pre-trained word vectors published by Facebook, trained on …","url":["https://link.springer.com/chapter/10.1007/978-3-030-59413-8_21"]}
{"year":"2020","title":"Evidence Integration for Multi-hop Reading Comprehension with Graph Neural Networks","authors":["L Song, Z Wang, M Yu, Y Zhang, R Florian, D Gildea - IEEE Transactions on …, 2020"],"snippet":"Page 1. 1041-4347 (c) 2020 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/9051845/"]}
{"year":"2020","title":"Examining the rhetorical capacities of neural language models","authors":["Z Zhu, C Pan, M Abdalla, F Rudzicz - arXiv preprint arXiv:2010.00153, 2020"],"snippet":"… Non-contextualized word embeddings We consider two popular word embeddings here: • GloVe (Pennington et al., 2014) contains 2.2M vocabulary items and produces 300dimensional word vectors. The GloVe embedding …","url":["https://arxiv.org/pdf/2010.00153"]}
{"year":"2020","title":"Experience Grounds Language","authors":["Y Bisk, A Holtzman, J Thomason, J Andreas, Y Bengio… - arXiv preprint arXiv …, 2020"],"snippet":"… (2013) trained on 1.6 billion tokens, while Pennington et al. (2014) scaled up to 840 billion tokens from Common Crawl. Recent approaches have made progress by substantially increasing the number of model parameters …","url":["https://arxiv.org/pdf/2004.10151"]}
{"year":"2020","title":"Experiencers, Stimuli, or Targets: Which Semantic Roles Enable Machine Learning to Infer the Emotions?","authors":["L Oberländer, K Reich, R Klinger - arXiv preprint arXiv:2011.01599, 2020"],"snippet":"… 1For ET, 90% of the annotated experiencers are the authors of the tweets without corresponding span annotation. 2We use 42B tokens, pretrained on CommonCrawl (Pennington et al., 2014), https://nlp.stanford.edu …","url":["https://arxiv.org/pdf/2011.01599"]}
{"year":"2020","title":"Experiments on Paraphrase Identification Using Quora Question Pairs Dataset","authors":["A Chandra, R Stefanus - arXiv preprint arXiv:2006.02648, 2020"],"snippet":"… matching result into a fix-length matching vector and continued to last layer of the model which is a fully connected layer. The paper use GloVe as a pretrained word vector from 840B Common Crawl corpus and apply it to Quora …","url":["https://arxiv.org/pdf/2006.02648"]}
{"year":"2020","title":"Explicit Alignment Objectives for Multilingual Bidirectional Encoders","authors":["J Hu, M Johnson, O Firat, A Siddhant, G Neubig - arXiv preprint arXiv:2010.07972, 2020"],"snippet":"… Sentence Alignment Our first proposed objective encourages cross-lingual alignment of sentence 1 AMBER is trained on 26GB parallel data and 80GB monolingual Wikipedia data, while XLM-R-large is trained on 2.5TB …","url":["https://arxiv.org/pdf/2010.07972"]}
{"year":"2020","title":"Explicit Sentence Compression for Neural Machine Translation","authors":["Z Li, R Wang, K Chen, M Utiyama, E Sumita, Z Zhang… - arXiv preprint arXiv …, 2019"],"snippet":"… for NMT evaluation. For the EN-DE translation task, 4.43 M bilingual sentence pairs from the WMT14 dataset were used as training data, including Common Crawl, News Commentary, and Europarl v7. The newstest2013 and …","url":["https://arxiv.org/pdf/1912.11980"]}
{"year":"2020","title":"Exploiting Categorization of Online News for Profiling City Areas","authors":["A Bondielli, P Ducange, F Marcelloni - 2020 IEEE Conference on Evolving and …, 2020"],"snippet":"… FastText is created by Facebook and is based on Neural Networks. Pre-trained word vectors are available for 157 languages, trained on Common Crawl and Wikipedia. More specifically, we chose to use the pre-trained Italian model …","url":["https://ieeexplore.ieee.org/abstract/document/9122777/"]}
{"year":"2020","title":"Exploiting Class Labels to Boost Performance on Embedding-based Text Classification","authors":["A Zubiaga - arXiv preprint arXiv:2006.02104, 2020"],"snippet":"… 4.2 Word Embedding Models & Classifiers We tested four word embedding models: (1) Google's Word2Vec model (gw2v), (2) a Twitter Word2Vec model5 (tw2v) [10], (3) GloVe embeddings trained from Common Crawl (cglove) …","url":["https://arxiv.org/pdf/2006.02104"]}
{"year":"2020","title":"Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning","authors":["T Shen, Y Mao, P He, G Long, A Trischler, W Chen - arXiv preprint arXiv:2004.14224, 2020"],"snippet":"Page 1. EXPLOITING STRUCTURED KNOWLEDGE IN TEXT VIA GRAPH-GUIDED REPRESENTATION LEARNING Tao Shen∗ University of Technology Sydney tao.shen@student.uts.edu.au Yi Mao, Pengcheng …","url":["https://arxiv.org/pdf/2004.14224"]}
{"year":"2020","title":"Exploring Different Methods for Solving Analogies with Portuguese Word Embeddings","authors":["T Sousa, H Gonçalo Oliveira, A Alves - 9th Symposium on Languages, Applications …, 2020"],"snippet":"… from several sources. Such sources include raw text (ie, an ensemble of Google News word2vec, Common Crawl GloVe, Open Subtitles fastText) combined with the ConceptNet semantic network with retrofitting. 1 https://github …","url":["https://drops.dagstuhl.de/opus/volltexte/2020/13022/pdf/OASIcs-SLATE-2020-9.pdf"]}
{"year":"2020","title":"Exploring Event Extraction Across Languages","authors":["S Prabhu - 2020"],"snippet":"Page 1. Exploring Event Extraction Across Languages Thesis submitted in partial fulfillment of the requirements for the degree of Master of Science in Computational Linguistics by Research by Suhan Prabhu 201525118 suhan.prabhuk@research.iiit.ac.in …","url":["http://web2py.iiit.ac.in/research_centres/publications/download/mastersthesis.pdf.b481c850852a12a8.737568616e5f66696e616c5f7468657369732e706466.pdf"]}
{"year":"2020","title":"Exploring Neural Network Approaches in Automatic Personality Recognition of Filipino Twitter Users","authors":["E Tighe, O Aran, C Cheng"],"snippet":"… FastText differs by learning character-grams, as opposed to word-grams. We utilize the embeddings trained on Common Crawl and Wikipedia data2 – specifically, the embeddings for English and Tagalog (both 300 dimensions) …","url":["https://www.researchgate.net/profile/Edward_Tighe/publication/343189230_Exploring_Neural_Network_Approaches_in_Automatic_Personality_Recognition_of_Filipino_Twitter_Users/links/5f1ae2aea6fdcc9626ad4c4d/Exploring-Neural-Network-Approaches-in-Automatic-Personality-Recognition-of-Filipino-Twitter-Users.pdf"]}
{"year":"2020","title":"Exploring Swedish & English fastText Embeddings with the Transformer","authors":["TP Adewumi, F Liwicki, M Liwicki - arXiv preprint arXiv:2007.16007, 2020"],"snippet":"… We obtain better performance in both languages on the downstream task with far smaller training data, compared to recently released, common crawl versions and character n-grams appear useful for Swedish, a morphologically rich language …","url":["https://arxiv.org/pdf/2007.16007"]}
{"year":"2020","title":"Exploring the Dominance of the English Language on the Websites of EU Countries","authors":["A Giannakoulopoulos, M Pergantis, N Konstantinou… - Future Internet, 2020"],"snippet":"… For this purpose, we used information obtained from Common Crawl, a “repository of web crawl data that is universally accessible and analyzable” [34]. Among the data Common Crawl offers is an index of every available webpage …","url":["https://www.mdpi.com/1999-5903/12/4/76/pdf"]}
{"year":"2020","title":"Extended Overview of CLEF HIPE 2020: Named Entity Processing on Historical Newspapers","authors":["A Flückiger, S Clematide"],"snippet":"Page 1. Extended Overview of CLEF HIPE 2020: Named Entity Processing on Historical Newspapers Maud Ehrmann1[0000−0001−9900−2193], Matteo Romanello1[0000−0002− 1890−2577], Alex Flückiger2, and Simon Clematide2[0000−0003−1365−0662] …","url":["http://ceur-ws.org/Vol-2696/paper_255.pdf"]}
{"year":"2020","title":"Extended study on using pretrained language models and YiSi-1 for machine translation evaluation","authors":["C Lo - Proceedings of the Fifth Conference on Machine …, 2020"],"snippet":"… The differencesbetweenXLM-RandBERTare1)XLM-Ris trained on the CommonCrawl corpus which is significantly larger than the Wikipedia training data used by BERT; 2) instead of a uniform data sampling rate used in BERT …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.99.pdf"]}
{"year":"2020","title":"Extending Tables using a Web Table Corpus","authors":["S Sarabchi - 2020"],"snippet":"… Lehmberg et al. [12] gathered a web table corpus containing 233 million tables from the 2015 version of the CommonCrawl2, using a table extraction method similar to that of [11]. The table corpus … and 2https://commoncrawl.org/about …","url":["https://era.library.ualberta.ca/items/4f9f40b8-69ba-4c24-85b4-41f17517cc59/view/dfd3938b-a7d1-4b4c-85e0-4087c36d5713/Sarabchi_Saeed_202002_MSc.pdf"]}
{"year":"2020","title":"Extracting Family History of Patients From Clinical Narratives: Exploring an End-to-End Solution With Deep Learning Models","authors":["X Yang, H Zhang, X He, J Bian, Y Wu - JMIR Medical Informatics, 2020"],"snippet":"… We screened 4 different word embeddings following a similar procedure reported in our previous study [46] and found that the Common Crawl embeddings—released by Facebook and trained using the fastText on the …","url":["https://medinform.jmir.org/2020/12/e22982/"]}
{"year":"2020","title":"Extracting Training Data from Large Language Models","authors":["N Carlini, F Tramer, E Wallace, M Jagielski… - arXiv preprint arXiv …, 2020"],"snippet":"… In particular, we select samples from a subset of Common Crawl6 to feed as context to the model.7 6http://commoncrawl.org/ 7It is possible there is some intersection between these two datasets, effectively allowing this strategy to “cheat” …","url":["https://arxiv.org/pdf/2012.07805"]}
{"year":"2020","title":"Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation","authors":["I Chung, B Kim, Y Choi, SJ Kwon, Y Jeon, B Park… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. arXiv:2009.07453v1 [cs.LG] 16 Sep 2020 Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation Insoo Chung∗ Byeongwook Kim∗ Yoonjung Choi Se Jung Kwon Yongkweon …","url":["https://arxiv.org/pdf/2009.07453"]}
{"year":"2020","title":"Facebook AI's WMT20 News Translation Task Submission","authors":["PJ Chen, A Lee, C Wang, N Goyal, A Fan… - arXiv preprint arXiv …, 2020"],"snippet":"… we use all the available monolingual data, eg NewsCrawl + CommonCrawl + Wikipedia dumps for Tamil, and CommonCrawl for Inuktitut … unconstrained track, we use Tamil monolingual data and Tamil-English mined bitext data …","url":["https://arxiv.org/pdf/2011.08298"]}
{"year":"2020","title":"Factors affecting sentence similarity and paraphrasing identification","authors":["M Alian, A Awajan - International Journal of Speech Technology, 2020"],"snippet":"… Grave et al. (2018) contributed in a pre-trained word vector representation for 157 languages including Arabic. The word vectors have been trained on Wikipedia and the Common Crawl corpus using an extension of the FastText model with subword information …","url":["https://link.springer.com/article/10.1007/s10772-020-09753-4"]}
{"year":"2020","title":"Fairness in AI-based Recruitment and Career Pathway Optimization","authors":["DF Mujtaba - 2020"],"snippet":"Page 1. FAIRNESS IN AI-BASED RECRUITMENT AND CAREER PATHWAY OPTIMIZATION By Dena Freshta Mujtaba A THESIS Submitted to Michigan State University in partial fulfillment of the requirements for the degree of …","url":["http://search.proquest.com/openview/f2938ed72cda2c3b656a0db1b2be7320/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2020","title":"Fake News Data Collection and Classification: Iterative Query Selection for Opaque Search Engines with Pseudo Relevance Feedback","authors":["A Elyashar, M Reuben, R Puzis - arXiv preprint arXiv:2012.12498, 2020"],"snippet":"Page 1. FAKE NEWS DATA COLLECTION AND CLASSIFICATION: ITERATIVE QUERY SELECTION FOR OPAQUE SEARCH ENGINES WITH PSEUDO RELEVANCE FEEDBACK APREPRINT Aviad Elyashar, Maor Reuben …","url":["https://arxiv.org/pdf/2012.12498"]}
{"year":"2020","title":"Fake News Detection","authors":["IP Marín, D Arroyo - Conference on Complex, Intelligent, and Software …, 2020"],"snippet":"… 4. https://spacy.io/ [Last accessed 26 Jan 2020]. 5. en_core_web_lg, pre-trained English statistical models. English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. 6. https …","url":["https://link.springer.com/chapter/10.1007/978-3-030-57805-3_22"]}
{"year":"2020","title":"Fake news spreader detection using neural tweet aggregation","authors":["O Bakhteev, A Ogaltsov, P Ostroukhov"],"snippet":"… As a preprocessing step we lowercased tweets and removed stop-words and punctuation. We did not use any special preprocessing. For the word embeddings we used fastText [1] trained on Common Crawl and Wikipedia with dimension set to 100 …","url":["https://pan.webis.de/downloads/publications/papers/bakhteev_2020.pdf"]}
{"year":"2020","title":"Fake News Spreader Identification in Twitter using Ensemble Modeling","authors":["A Hashemi, MR Zarei, MR Moosavi, M Taheri"],"snippet":"… Page 4. The sources used in the English model for training data are OntoNotes 51 and GloVe Common Crawl2 and the Spanish model utilizes UD Spanish AnCora v2.53, WikiNER4, OSCAR (Common Crawl)5 and Wikipedia …","url":["https://pan.webis.de/downloads/publications/papers/hashemi_2020.pdf"]}
{"year":"2020","title":"Fashion-IQ 2020 Challenge 2nd Place Team's Solution","authors":["M Shin, Y Cho, S Hong - arXiv preprint arXiv:2007.06404, 2020"],"snippet":"… For training the LSTM and the GRU from scratch, we initialize the word embedding with the concatenation of three GloVe vectors2 learned from Wikipedia, Twitter, and Common Crawl that results in 900-dimensional input …","url":["https://arxiv.org/pdf/2007.06404"]}
{"year":"2020","title":"Fast entity linking in noisy text environments","authors":["SM Shah, MD Conover, PN Skomoroch, MT Hayes… - US Patent 10,733,383, 2020"],"snippet":"… entry). In some embodiments, the candidate dictionary is determined by using the hyperlinks on a Wikipedia page, or a Common Crawl page to identify surface forms (the hyperlink anchor text) that point to a specific page. Each …","url":["http://www.freepatentsonline.com/10733383.html"]}
{"year":"2020","title":"Fast Indexes for Gapped Pattern Matching","authors":["M Cáceres, SJ Puglisi, B Zhukova - International Conference on Current Trends in …, 2020"],"snippet":"… Open image in new window Fig. 2. Fig. 2. Time to search a 2GiB subset of the Common Crawl web collection (commoncrawl.org). for 20 VLG patterns (\\(k=2\\), \\(delta _i,\\varDelta _i = \\langle 100,110\\rangle \\)), composed of very …","url":["https://link.springer.com/chapter/10.1007/978-3-030-38919-2_40"]}
{"year":"2020","title":"FinBERT: A Pre-trained Financial Language Representation Model for Financial Text Mining","authors":["Z Liu, D Huang, K Huang, Z Li, J Zhao"],"snippet":"… sizes, totaling over 61 GB text: • English Wikipedia1 and BooksCorpus (Zhu et al., 2015), which are the original training data used to train BERT (totaling 13GB, 3.31B words); • FinancialWeb (totaling 24GB, 6.38B words), which …","url":["https://www.ijcai.org/Proceedings/2020/0622.pdf"]}
{"year":"2020","title":"Finding of asymmetric relation between words","authors":["M Muraoka, T Nasukawa, KMA Salam - US Patent App. 16/287,326, 2020"],"snippet":"US20200272696A1 - Finding of asymmetric relation between words - Google Patents. Finding of asymmetric relation between words. Download PDF Info. Publication number US20200272696A1. US20200272696A1 US16/287,326 …","url":["https://patents.google.com/patent/US20200272696A1/en"]}
{"year":"2020","title":"Finding the needle in the haystack: Fine-tuning transformers to classify protest events in a sea ofnews articles, with Bayesian uncertainty measures","authors":["C Ghai - 2020"],"snippet":"Page 1. Finding the needle in the haystack: Fine-tuning transformers to classify protest events in a sea of news articles, with Bayesian uncertainty measures Chris Ghai Master's Thesis, Spring 2020 Page 2. This master's thesis …","url":["https://www.duo.uio.no/bitstream/handle/10852/79984/1/chris_ghai_thesis.pdf"]}
{"year":"2020","title":"Findings of the 2020 conference on machine translation (wmt20)","authors":["L Barrault, M Biesialska, O Bojar, MR Costa-jussà… - Proceedings of the Fifth …, 2020"],"snippet":"… Distinct words – 76,013 – 6,165 178,453 85,189 Common Crawl Parallel Corpus German ↔ English Czech ↔ English Russian ↔ English French ↔ German … Common Crawl Language Model Data English German Czech Russian Polish Sent …","url":["https://www.aclweb.org/anthology/2020.wmt-1.1.pdf"]}
{"year":"2020","title":"Findings of the WMT 2020 Biomedical Translation Shared Task: Basque, Italian and Russian as New Additional Languages","authors":["R Bawden, G Di Nunzio, C Grozea, I Unanue, A Yepes… - 5th Conference on Machine …, 2020"],"snippet":"Page 1. HAL Id: hal-02986356 https://hal.inria.fr/hal-02986356 Submitted on 2 Nov 2020 HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not …","url":["https://hal.inria.fr/hal-02986356/document"]}
{"year":"2020","title":"Findings of the WMT 2020 shared task on parallel corpus filtering and alignment","authors":["P Koehn, V Chaudhary, A El-Kishky, N Goyal, PJ Chen… - Proceedings of the Fifth …, 2020"],"snippet":"… Noisy parallel documents and parallel sentences were sourced from the CCAligned2 dataset (El- Kishky et al., 2020a), a massive collection of cross-lingual web documents covering over 8k language pairs aligned from …","url":["https://www.aclweb.org/anthology/2020.wmt-1.78.pdf"]}
{"year":"2020","title":"Fine-Grained Argument Unit Recognition and Classification","authors":["D Trautmann, J Daxenberger, C Stab, H Schütze…"],"snippet":"… Table 1). This also increases comparability with related work. The topics are general enough to have good coverage in Common Crawl … 4http://commoncrawl.org/2016/02/ february-2016-crawlarchive-now-available/ 5https://www.elastic.co/products/elasticsearch …","url":["https://www.researchgate.net/profile/Dietrich_Trautmann/publication/332590723_Robust_Argument_Unit_Recognition_and_Classification/links/5e32b02ca6fdccd96576e059/Robust-Argument-Unit-Recognition-and-Classification.pdf"]}
{"year":"2020","title":"Fine-grained entity type classification using GRU with self-attention","authors":["K Dhrisya, G Remya, A Mohan - International Journal of Information Technology, 2020"],"snippet":"… The dictionary for the proposed model is built with GLoVe vectors of 300 dimensions. It is a pre-trained word embedding created by utilizing common Crawl 840B. These vectors on various corpora can be downloaded from Stanford GLoVe Website. Parameter settings …","url":["https://link.springer.com/article/10.1007/s41870-020-00499-5"]}
{"year":"2020","title":"FLERT: Document-Level Features for Named Entity Recognition","authors":["S Schweter, A Akbik - arXiv preprint arXiv:2011.06993, 2020"],"snippet":"… Conneau et al. (2019). We use the xlm-roberta-large model in our experiments, trained on 2.5TB of data from a cleaned Common Crawl corpus (Wenzek et al., 2020) for 100 different languages Embeddings (+WE). For each setup …","url":["https://arxiv.org/pdf/2011.06993"]}
{"year":"2020","title":"Forum Duplicate Question Detection by Domain Adaptive Semantic Matching","authors":["Z Xu, H Yuan - IEEE Access, 2020"],"snippet":"… B. MODEL IMPLEMENTATION The word embedding was initialized with 300-dimensional GloVe [33] vectors which are pretrained in the 840B Common Crawl corpus. The Embedding was set to be trainable. The …","url":["https://ieeexplore.ieee.org/iel7/6287639/8948470/09043551.pdf"]}
{"year":"2020","title":"Four dimensions characterizing comprehensive trait judgments of faces","authors":["C Lin, U Keles, R Adolphs - 2020"],"snippet":"… and text classi cation using a neural network provided within the FastText library40; this neural network had been trained on Common Crawl data of 600 billion words to predict the identity of a word given a context. We then applied …","url":["https://www.researchsquare.com/article/rs-41215/latest.pdf"]}
{"year":"2020","title":"French Contextualized Word-Embeddings with a sip of CaBeRnet: a New French Balanced Reference Corpus","authors":["M Fabre, PJO Suárez, B Sagot, ÉV de la Clergerie - CMLC-8-8th Workshop on the …, 2020","M Popa-Fabre, PJO Suárez, B Sagot… - Proceedings of the 8th …, 2020"],"snippet":"… al., 2019), we decided to include in our comparison a corpus of French text extracted from Common Crawl8. We … 8More information available at https://commoncrawl … OSCAR gathers a set of monolingual text extracted …","url":["https://hal.inria.fr/hal-02678358/document","https://www.aclweb.org/anthology/2020.cmlc-1.3.pdf"]}
{"year":"2020","title":"Frequency-dependent Regularization in Constituent Ordering Preferences","authors":["Z Liu, E Morgan"],"snippet":"… a total of around 9 billion tokens. This corpus consists of web page data from both Common Crawl and Wikipedia and is automatically parsed with UDPipe (Straka & Straková, 2017). Within this corpus, each token is represented …","url":["https://www.researchgate.net/profile/Zoey_Liu2/publication/341712949_Frequency-dependent_Regularization_in_Constituent_Ordering_Preferences/links/5ecffdb292851c9c5e65d021/Frequency-dependent-Regularization-in-Constituent-Ordering-Preferences.pdf"]}
{"year":"2020","title":"From Chest X-Rays to Radiology Reports: A Multimodal Machine Learning Approach","authors":["S Singh, S Karimi, K Ho-Shon, L Hamey - 2019 Digital Image Computing: Techniques …, 2019"],"snippet":"… Also, on the text side, we use the Glove [30] word embeddings having a 300-dimensional embedding vector for each word, and have been trained on a generic text corpus named Common Crawl having 42B tokens, 1.9M vocab …","url":["https://ieeexplore.ieee.org/abstract/document/8945819/"]}
{"year":"2020","title":"From Dataset Recycling to Multi-Property Extraction and Beyond","authors":["T Dwojak, M Pietruszka, Ł Borchmann, J Chłędowski… - arXiv preprint arXiv …, 2020"],"snippet":"… T5. Recently proposed T5 model (Raffel et al., 2020) is a Transformer model pretrained on a cleaned version of CommonCrawl. T5 is famous for achieving excellent performance on the SuperGLUE benchmark (Wang et al., 2019) …","url":["https://arxiv.org/pdf/2011.03228"]}
{"year":"2020","title":"From Hero to Z\\'eroe: A Benchmark of Low-Level Adversarial Attacks","authors":["S Eger, Y Benz - arXiv preprint arXiv:2010.05648, 2020"],"snippet":"… The reason may be that our noises are not always natural, in the sense of having high support in large datasets such as CommonCrawl or Wikipedia, but they are still within the limits of cognitive abilities of ordinary humans …","url":["https://arxiv.org/pdf/2010.05648"]}
{"year":"2020","title":"From Pixel to Patch: Synthesize Context-aware Features for Zero-shot Semantic Segmentation","authors":["Z Gu, S Zhou, L Niu, Z Zhao, L Zhang - arXiv preprint arXiv:2009.12232, 2020"],"snippet":"Page 1. 1 From Pixel to Patch: Synthesize Context-aware Features for Zero-shot Semantic Segmentation Zhangxuan Gu, Siyuan Zhou, Li Niu*, Zihan Zhao, Liqing Zhang* Abstract—Zero-shot learning has been actively studied …","url":["https://arxiv.org/pdf/2009.12232"]}
{"year":"2020","title":"From Syntactic Structure to Semantic Relationship: Hypernym Extraction from Definitions by Recurrent Neural Networks Using the Part of Speech Information","authors":["Y Tan, X Wang, T Jia - International Semantic Web Conference, 2020"],"snippet":"… In recent years, much research pay attention to extracting hypernyms from larger data resources via the high precise of pattern-based methods. [25] extract hypernymy relations from the CommonCrawl web corpus using lexico-syntactic patterns …","url":["https://link.springer.com/chapter/10.1007/978-3-030-62419-4_30"]}
{"year":"2020","title":"From Web Crawl to Clean Register-Annotated Corpora","authors":["V Laippala, S Rönnqvist, S Hellström, J Luotolahti… - … of the 12th Web as Corpus …, 2020"],"snippet":"… crawl or extracting data from existing crawl-based datasets, such as Common Crawl1. As … CommonCrawl is a free and openly available web crawl maintained by the CommonCrawl … Lately the Common Crawl dataset has …","url":["https://www.aclweb.org/anthology/2020.wac-1.3.pdf"]}
{"year":"2020","title":"From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers","authors":["A Lauscher, V Ravishankar, I Vulić, G Glavaš - arXiv preprint arXiv:2005.00633, 2020"],"snippet":"… It is trained on the CommonCrawl-100 data (Wenzek et al., 2019) of 100 languages … Interestingly, for both high7For XLM-R, we take the reported sizes of languagespecific portions of CommonCrawl-100 from Conneau et al …","url":["https://arxiv.org/pdf/2005.00633"]}
{"year":"2020","title":"From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers","authors":["A Lauscher, V Ravishankar, I Vulić, G Glavaš - … of the 2020 Conference on Empirical …, 2020"],"snippet":"… sampling. XLM on RoBERTa (XLM-R). XLM-R (Conneau et al., 2020) is an instance of RoBERTa, robustly trained on a large multilingual CommonCrawl-100 (CC-100) corpus (Wenzek et al., 2019) covering 100 languages …","url":["https://www.aclweb.org/anthology/2020.emnlp-main.363.pdf"]}
{"year":"2020","title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing","authors":["Z Dai, G Lai, Y Yang, QV Le - arXiv preprint arXiv:2006.03236, 2020"],"snippet":"… ablation studies. 5 Page 6. • Large scale: Pretraining models for 500K steps with batch size 8K on the five datasets used by XLNet [3] and ELECTRA [5] (Wikipedia + Book Corpus + ClueWeb + Gigaword + Common Crawl). We will …","url":["https://arxiv.org/pdf/2006.03236"]}
{"year":"2020","title":"Gated Semantic Difference Based Sentence Semantic Equivalence Identification","authors":["X Liu, Q Chen, X Wu, Y Hua, J Chen, D Li, B Tang… - IEEE/ACM Transactions on …, 2020"],"snippet":"… The word embeddings for the quora corpus are 300dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus [45] and for the LCQMC are 300-dimensional word vectors pre-trained from the Chinese 5[Online] …","url":["https://ieeexplore.ieee.org/abstract/document/9222233/"]}
{"year":"2020","title":"Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer","authors":["J Zhao, S Mukherjee, S Hosseini, KW Chang… - arXiv preprint arXiv …, 2020"],"snippet":"… is an OCCUPATION-TITLE” where name is recognized in each language by using the corresponding Named Entity Recognition model from spaCy.5 To control for the same time period for datasets across languages …","url":["https://arxiv.org/pdf/2005.00699"]}
{"year":"2020","title":"Gender Bias in Multilingual Embeddings","authors":["J Zhao, S Mukherjee, S Hosseini, KW Chang…"],"snippet":"… an OCCUPATION-TITLE” where name is recognized in each language by using the corresponding Named Entity Recognition model from spaCy.5 To control for the same time period for datasets across languages …","url":["https://www.researchgate.net/profile/Subhabrata_Mukherjee/publication/340660062_Gender_Bias_in_Multilingual_Embeddings/links/5e97428692851c2f52a6200a/Gender-Bias-in-Multilingual-Embeddings.pdf"]}
{"year":"2020","title":"Gender Detection on Social Networks using Ensemble Deep Learning","authors":["K Kowsari, M Heidarysafa, T Odukoya, P Potter… - arXiv preprint arXiv …, 2020"],"snippet":"… 25d, 50d, 100d, and 200d vectors. This word embedding is trained over even bigger corpora, including Wikipedia and Common Crawl content. The objective function is as follows: f(wi − wj, ˜wk) = Pik Pjk (2) where wi is refer to …","url":["https://arxiv.org/pdf/2004.06518"]}
{"year":"2020","title":"Gender stereotype reinforcement: Measuring the gender bias conveyed by ranking algorithms","authors":["A Fabris, A Purpura, G Silvello, GA Susto - Information Processing & Management, 2020"],"snippet":"… Corrado, Dean, 2013). Most frequently, they are learnt from large text corpora available online (such as Wikipedia, Google News and Common Crawl, capturing semantic relationships of words based on their usage. Recent work …","url":["https://arxiv.org/pdf/2009.01334"]}
{"year":"2020","title":"Gender stereotypes are reflected in the distributional structure of 25 languages","authors":["M Lewis, G Lupyan - Nature Human Behaviour, 2020"],"snippet":"Cultural stereotypes such as the idea that men are more suited for paid work and women are more suited for taking care of the home and family, may contribute to gender imbalances in science, technology, engineering and …","url":["https://www.nature.com/articles/s41562-020-0918-6"]}
{"year":"2020","title":"Generalisation of Cyberbullying Detection","authors":["K Richard, L Marc-André - arXiv preprint arXiv:2009.01046, 2020","MA Larochelle, R Khoury"],"snippet":"… We use FastText pre-trained on Common Crawl data featuring 300 dimensions and 2 million word vectors with subword information6 to convert the words into vector representations, of which we concatenate a 60-dimensional binary …","url":["https://arxiv.org/pdf/2009.01046","https://web.ntpu.edu.tw/~myday/doc/ASONAM2020/ASONAM2020_Proceedings/pdf/papers/047_034_296.pdf"]}
{"year":"2020","title":"Generalize Sentence Representation with Self-Inference","authors":["KC Yang, HY Kao"],"snippet":"… Our model is trained with the phrases in the parse trees and tested on the whole sentence. Experimental Settings We initialize word embeddings using the pretrained FastText common-crawl vectors (Mikolov et al. 2018) and freeze the weights during training …","url":["https://www.aaai.org/Papers/AAAI/2020GB/AAAI-YangKC.7098.pdf"]}
{"year":"2020","title":"Generating Categories for Sets of Entities","authors":["S Zhang, K Balog, J Callan - arXiv preprint arXiv:2008.08428, 2020"],"snippet":"… entity linking for tables and table schema to predicate matching. Ritze et al. [31] propose an iterative method for matching tables to DBpedia. They develop a manually annotated dataset for matching between a Web table corpus …","url":["https://arxiv.org/pdf/2008.08428"]}
{"year":"2020","title":"Generating Diverse Conversation Responses by Creating and Ranking Multiple Candidates","authors":["YP Ruan, ZH Ling, X Zhu, Q Liu, JC Gu - Computer Speech & Language, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0885230820300048"]}
{"year":"2020","title":"Generating Fact Checking Briefs","authors":["A Fan, A Piktus, F Petroni, G Wenzek, M Saeidi… - Proceedings of the 2020 …, 2020"],"snippet":"… We take the top search hit as the evidence and retrieve the text from CommonCrawl4. Finally, the generated question and retrieved evidence document is provided to the question answering model to generate an answer. 4.1 Question Generation …","url":["https://www.aclweb.org/anthology/2020.emnlp-main.580.pdf"]}
{"year":"2020","title":"Generating fake websites: WikiGen","authors":["M Longland - 2020"],"snippet":"… [2015]. This is a relatively small vector file. An alternative considered was the Common Crawl (840B tokens) vectors from Stanford NLP's GloVe [Pennington et al., 2014] but memory issues meant the smaller file was used instead …","url":["https://pdfs.semanticscholar.org/0776/aece84f01a6f593d1748657bf2ec4dec49b4.pdf"]}
{"year":"2020","title":"Generating Keyword Lists Related to Topics Represented by an Array of Topic Records, for Use in Targeting Online Advertisements and Other Uses","authors":["L Palaic, MH Gross, SA Schriber - US Patent App. 16/803,214, 2020"],"snippet":"… For data gathering purposes, a custom heuristic can be used that operates on a Common Crawl Corpus. Documents gathered from a Common Crawl process might be automatically annotated with appropriate topics tags so …","url":["https://patents.google.com/patent/US20200273069A1/en"]}
{"year":"2020","title":"Generating Personalized Product Descriptions from User Reviews","authors":["G Elad, K Radinsky, B Kimelfeld - 2019"],"snippet":"Page 1. Generating Personalized Product Descriptions from User Reviews Guy Elad Technion - Computer Science Department - M.Sc. Thesis MSC-2019-25 - 2019 Page 2. Technion - Computer Science Department …","url":["http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2019/MSC/MSC-2019-25.pdf"]}
{"year":"2020","title":"Generating Query Suggestions for Cross-language and Cross-terminology Health Information Retrieval","authors":["PM Santos, C Teixeira Lopes - Advances in Information Retrieval: 42nd European …, 2020"],"snippet":"… The English collection is provided by the Consumer Health Search Task in the 2018 edition of the CLEF eHealth Lab2. This task uses a set of 50 English queries and a document corpus with 5,535,120 web pages acquired from a CommonCrawl dump …","url":["https://link.springer.com/content/pdf/10.1007/978-3-030-45442-5_43.pdf"]}
{"year":"2020","title":"Generating Representative Headlines for News Stories","authors":["X Gu, Y Mao, J Han, J Liu, H Yu, Y Wu, C Yu, D Finnie… - arXiv preprint arXiv …, 2020"],"snippet":"… By fine-tuning the model on human-curated labels, we can combine the two sources of supervision and further improve performance. 6One can use CommonCrawl to fetch web articles. Page 5. Generating Representative Headlines for News Stories …","url":["https://arxiv.org/pdf/2001.09386"]}
{"year":"2020","title":"Generative Language Modeling for Automated Theorem Proving","authors":["S Polu, I Sutskever - arXiv preprint arXiv:2009.03393, 2020"],"snippet":"… We pre-train our models on both GPT-3's post-processed version of CommonCrawl as well as a more reasoning-focused mix of Github, arXiv and Math StackExchange. 7 … 5.3 Pre-training Models are pre-trained on …","url":["https://arxiv.org/pdf/2009.03393"]}
{"year":"2020","title":"Generative Models are Unsupervised Predictors of Page Quality: A Colossal-Scale Study","authors":["D Bahri, Y Tay, C Zheng, D Metzler, C Brunk… - arXiv preprint arXiv …, 2020"],"snippet":"… 3.1 Datasets This section describes the datasets used in our experiments. • Web500M. The core corpora used in our experiments consists of a random sample of 500 million English web documents obtained from the Common Crawl1. • GPT-2-Output …","url":["https://arxiv.org/pdf/2008.13533"]}
{"year":"2020","title":"Geographically-Balanced Gigaword Corpora for 50 Language Varieties","authors":["J Dunn, B Adams - Proceedings of The 12th Language Resources and …, 2020"],"snippet":"… 3. Collecting Geo-Referenced Documents The data for this paper comes from the Common Crawl,2 as processed in the Corpus of Global Language Use (henceforth, CGLU). This project includes the Common Crawl data from …","url":["https://www.aclweb.org/anthology/2020.lrec-1.308.pdf"]}
{"year":"2020","title":"Geoparsing the historical Gazetteers of Scotland: accurately computing location in mass digitised texts","authors":["R Filgueira, C Grover, M Terras, B Alex - Proceedings of the 8th Workshop on …, 2020"],"snippet":"… Small size model (11MB). • en core web md: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl … en core web lg: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl …","url":["https://www.aclweb.org/anthology/2020.cmlc-1.4.pdf"]}
{"year":"2020","title":"GeoVectors: a Linked Open Corpus of OpenStreetMap Embeddings","authors":["N Tempelmeier, S Gottschalk, E Demidova - 2020"],"snippet":"… As most of the OSM keys are in English, we chose the 300-dimensional English word vectors trained on the Common Crawl and Wikipedia [5]. Encoding: To encode an OSM entity o, we utilise the individual word em …","url":["https://openreview.net/pdf?id=EibPtOjZUn"]}
{"year":"2020","title":"German's Next Language Model","authors":["B Chan, S Schweter, T Möller - arXiv preprint arXiv:2010.10906, 2020"],"snippet":"… The XLM-RoBERTa model is trained on 2.5TB of data from a cleaned Common Crawl corpus (Wenzek et al., 2020) for 100 different languages … OSCAR (Ortiz Suárez et al., 2019) is a set of monolingual corpora extracted from Common Crawl …","url":["https://arxiv.org/pdf/2010.10906"]}
{"year":"2020","title":"Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks","authors":["AROKM Downey, A Rumshisky"],"snippet":"… Similarity-based approaches. The first baseline represents a given text, a question, and each of the choices as the average of 300-dimensional CommonCrawl FastText word embeddings (Bojanowski et al. 2016) of its constituent words …","url":["https://www.aaai.org/Papers/AAAI/2020GB/AAAI-RogersA.7778.pdf"]}
{"year":"2020","title":"Getting Passive Aggressive About False Positives: Patching Deployed Malware Detectors","authors":["E Raff, B Filar, J Holt - arXiv preprint arXiv:2010.12080, 2020"],"snippet":"… We base our testing and results on a representative sample of industry data. Our corpus consisted of 1,101,407 Microsoft Office documents that contained macros. Similar to [33] data was collected from Common Crawl (CC) [34] and VirusTotal [35] …","url":["https://arxiv.org/pdf/2010.12080"]}
{"year":"2020","title":"Getting Structured Data from the Internet","authors":["BDP Scale, JM Patel"],"snippet":"… WARC file format ..... 278 Common crawl index ..... 282 … 331 Processing parquet files for a common crawl index ..... 334 …","url":["https://link.springer.com/content/pdf/10.1007/978-1-4842-6576-5.pdf"]}
{"year":"2020","title":"Give your Text Representation Models some Love: the Case for Basque","authors":["R Agerri, IS Vicente, JA Campos, A Barrena, X Saralegi… - arXiv preprint arXiv …, 2020"],"snippet":"… Common Crawl word vectors (FastText-officialcommon-crawl) were trained on Common Crawl and Wikipedia using CBOW with position-weights … train our systems to perform the following comparisons: (i) FastText official models …","url":["https://arxiv.org/pdf/2004.00033"]}
{"year":"2020","title":"GLEAKE: Global and Local Embedding Automatic Keyphrase Extraction","authors":["JR Asl, JM Banda - arXiv preprint arXiv:2005.09740, 2020"],"snippet":"… doc2vec_news_dbow AP News glove.6B Wikipedia + Gigaword GloVe 50-300 [28] glove.twitter.27B Twitter 25-200 glove.840B Common Crawl 300 TABLE 1 DIFFERENT PRE-TRAINED EMBEDDINGS USED BY GLEAKE Page 5. 5 …","url":["https://arxiv.org/pdf/2005.09740"]}
{"year":"2020","title":"Global Under-Resourced MEedia Translation (GoURMET)","authors":["MAAS BBC, JW BBC, B Haddow, AM Barone…"],"snippet":"Page 1. GoURMET H2020–825299 D5.3 Initial Integration Report Global Under-Resourced MEedia Translation (GoURMET) H2020 Research and Innovation Action Number: 825299 D5.3 – Initial Integration Report Nature Report Work Package WP5 …","url":["https://gourmet-project.eu/wp-content/uploads/2020/07/GoURMET_D5_3_Initial_Integration_Report.pdf"]}
{"year":"2020","title":"Going Back in Time to Find What Existed on the Web and How much has been Preserved: How much of Palestinian Web has been Archived?","authors":["T Sammar, H Khalilia - مؤتمرات الآداب والعلوم الانسانية والطبيعية, 2020"],"snippet":"‎… References 1. Common crawl url index. url (http://index.commoncrawl.org/). 2. International internet preservation consortium (iipc).http://www.netpreserve.org. 3. Internet archive (https://archive.org/). 4. Internet archive wayback machine. url (https://archive.org/web/) …","url":["http://proceedings.sriweb.org/akn/index.php/art/article/viewFile/410/466"]}
{"year":"2020","title":"Goku's Participation in WAT 2020","authors":["D Wang, O Htun - Proceedings of the 7th Workshop on Asian Translation, 2020"],"snippet":"… Secondly, we fine-tuned on the JPO patent corpus using the mBART auto-encoder model (Liu et al., 2020), which has been pre-trained on largescale monolingual CommonCrawl (CC) corpus in 25 languages using the BART objective (Lewis et al., 2020) …","url":["https://www.aclweb.org/anthology/2020.wat-1.16.pdf"]}
{"year":"2020","title":"GoodReads Book Recommendation Service","authors":["Y Tian, V Bai, Z Doganata"],"snippet":"… Common Crawl: https://commoncrawl.org/ Models: ● Howard, Jeremy and Ruder, Sebastian. \"Universal Language Model Fine-tuning for Text Classification.\" Paper presented at the meeting of the ACL, 2018. ● Conneau …","url":["http://tianyijun.com/files/GoodReads_Recommendation.pdf"]}
{"year":"2020","title":"GottBERT: a pure German Language Model","authors":["R Scheible, F Thomczyk, P Tippmann, V Jaravine… - arXiv preprint arXiv …, 2020"],"snippet":"… dbmz BERT used as source data a German Wikipedia dump, EU Bookshop corpus, Open Subtitles, CommonCrawl, ParaCrawl and News Crawl which … than mBERT, the multilingual XLM-RoBERTa (Conneau et al., 2019) was …","url":["https://arxiv.org/pdf/2012.02110"]}
{"year":"2020","title":"GPT-3 AI language tool calls for cautious optimism","authors":["Oxford Analytica - Emerald Expert Briefings"],"snippet":"… The training process leveraged this by exposing GPT-3 to historical sweeps of the internet, known as 'crawls'. One substantial component was the Common Crawl dataset, a multilingual capture of almost 1 trillion words …","url":["https://www.emerald.com/insight/content/doi/10.1108/OXAN-DB256373/full/html"]}
{"year":"2020","title":"GPT-3 Creative Fiction","authors":["G Branwen - 2020"],"snippet":"… For GPT-2, I saw finetuning as doing 2 things: Fixing ignorance: missing domain knowledge. GPT-2 didn't know many things about most things—it was just a handful (1.5 billion) of parameters trained briefly on …","url":["https://www.gwern.net/GPT-3"]}
{"year":"2020","title":"Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses","authors":["S Flachs, O Lacroix, H Yannakoudakis, M Rei… - arXiv preprint arXiv …, 2020"],"snippet":"… matical errors. The source texts are randomly se- lected from the first 18 dumps of the CommonCrawl4 dataset and represent a wide range of data seen online such as blogs, magazines, corporate or educational websites. These …","url":["https://arxiv.org/pdf/2010.07574"]}
{"year":"2020","title":"Graph Attention Network with Memory Fusion for Aspect-level Sentiment Analysis","authors":["L Yuan, J Wang, LC Yu, X Zhang - Proceedings of the 1st Conference of the Asia …, 2020"],"snippet":"Page 1. Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 27–36 December 4 - 7, 2020 …","url":["https://www.aclweb.org/anthology/2020.aacl-main.4.pdf"]}
{"year":"2020","title":"Graph Policy Network for Transferable Active Learning on Graphs","authors":["S Hu, Z Xiong, M Qu, X Yuan, MA Côté, Z Liu, J Tang - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Graph Policy Network for Transferable Active Learning on Graphs Shengding Hu Tsinghua University hsd16@mails.tsinghua.edu.cn Zheng Xiong Tsinghua University harryfootball@163.com Meng Qu MILA meng.qu@umontreal.ca …","url":["https://arxiv.org/pdf/2006.13463"]}
{"year":"2020","title":"Graphical User Interface Auto-Completion with Element Constraints","authors":["L Brückner - 2020"],"snippet":"Page 1. Aalto University School of Science Master's Programme in ICT Innovation Lukas Brückner Graphical User Interface Auto-Completion with Element Constraints Master's Thesis Espoo, September 25, 2020 Supervisor: Prof …","url":["https://aaltodoc.aalto.fi/bitstream/handle/123456789/47385/master_Br%C3%BCckner_Lukas_2020.pdf?sequence=1"]}
{"year":"2020","title":"GraphWalker: An I/O-Efficient and Resource-Friendly Graph Analytic System for Fast and Scalable Random Walks","authors":["R Wang, Y Li, H Xie, Y Xu, JCS Lui"],"snippet":"Page 1. GraphWalker: An I/O-Efficient and Resource-Friendly Graph Analytic System for Fast and Scalable Random Walks Rui Wang1, Yongkun Li1, Hong Xie2, Yinlong Xu1, John CS Lui3 1University of Science and Technology …","url":["https://www.cse.cuhk.edu.hk/~cslui/PUBLICATION/ATC2020.pdf"]}
{"year":"2020","title":"GREEK-BERT: The Greeks visiting Sesame Street","authors":["J Koutsikakis, I Chalkidis, P Malakasiotis… - arXiv preprint arXiv …, 2020"],"snippet":"… and (c) the Greek part of OSCAR [25], a clean version of Common Crawl.5 Accents and other diacritics were removed, and all words were … 5https://commoncrawl.org 6https://github.com/google-research/bert 7The …","url":["https://arxiv.org/pdf/2008.12014"]}
{"year":"2020","title":"Grounded Compositional Outputs for Adaptive Language Modeling","authors":["N Pappas, P Mulcaire, NA Smith - arXiv preprint arXiv:2009.11523, 2020"],"snippet":"Page 1. Grounded Compositional Outputs for Adaptive Language Modeling Nikolaos Pappas♣ Phoebe Mulcaire♣ Noah A. Smith♣♦ ♣Paul G. Allen School of Computer Science & Engineering, University of Washington ♦Allen …","url":["https://arxiv.org/pdf/2009.11523"]}
{"year":"2020","title":"Guided Generation of Cause and Effect","authors":["Z Li, X Ding, T Liu, JE Hu, B Van Durme"],"snippet":"… 3629 Page 2. Processed Common Crawl Corpus Causal Patterns Based Matching and Filtering … Thus we harvest a large causal dataset from the preprocessed large-scale English Common Crawl corpus (5.14 TB) [Buck et al., 2014] …","url":["https://www.ijcai.org/Proceedings/2020/0502.pdf"]}
{"year":"2020","title":"Harbsafe-162. A Domain-Specific Data Set for the Intrinsic Evaluation of Semantic Representations for Terminological Data","authors":["S Arndt, D Schnäpp - arXiv preprint arXiv:2005.14576, 2020"],"snippet":"Page 1. Harbsafe-162 – A Domain-Specific Data Set for the Intrinsic Evaluation of Semantic Representations for Terminological Data Susanne Arndt, MA∗ Technische Universität Braunschweig Dieter Schnäpp, MA∗∗ Technische Universität Braunschweig …","url":["https://arxiv.org/pdf/2005.14576"]}
{"year":"2020","title":"Hard-Coded Gaussian Attention for Neural Machine Translation","authors":["W You, S Sun, M Iyyer - arXiv preprint arXiv:2005.00742, 2020"],"snippet":"… 10As the full WMT14 En→Fr is too large for us to feasibly train on, we instead follow Akoury et al. (2019) and train on just the Europarl / Common Crawl subset, while evaluating using the full dev/test sets. 11https://github.com/dojoteef/synst Page 5 …","url":["https://arxiv.org/pdf/2005.00742"]}
{"year":"2020","title":"Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages","authors":["X Garcia, A Siddhant, O Firat, AP Parikh - arXiv preprint arXiv:2009.11201, 2020"],"snippet":"… In contrast, for an actual low-resource language, Gujarati, WMT only provides 500 thousand lines of monolingual data (in news domain) and an additional 3.7 million lines of monolingual data from Common Crawl (noisy, generaldomain) …","url":["https://arxiv.org/pdf/2009.11201"]}
{"year":"2020","title":"HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection","authors":["B Mathew, P Saha, SM Yimam, C Biemann, P Goyal… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection* Binny Mathew1†, Punyajoy Saha1†, Seid Muhie Yimam2 Chris Biemann2, Pawan Goyal1, Animesh Mukherjee1 1 Indian Institute of Technology …","url":["https://arxiv.org/pdf/2012.10289"]}
{"year":"2020","title":"HCA: Hierarchical Compare Aggregate model for question retrieval in community question answering","authors":["MS Zahedi, M Rahgozar, RA Zoroofi - Information Processing & Management, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S030645732030813X"]}
{"year":"2020","title":"Hidden in Plain Sight: Building a Global Sustainable Development Data Catalogue","authors":["J Hodson, A Spezzatti - ICT Analysis and Applications, 2020"],"snippet":"… We iteratively re-train our model using 300-dimensional word-embedding features trained on the CommonCrawl web-scale data set 7 with the GLobal VEctors for Word Representation (GloVe) procedure (see [7]). Table 2 shows …","url":["https://link.springer.com/content/pdf/10.1007/978-981-15-8354-4.pdf#page=795"]}
{"year":"2020","title":"Hierarchical models vs. transfer learning for document-level sentiment classification","authors":["J Barnes, V Ravishankar, L Øvrelid, E Velldal - arXiv preprint arXiv:2002.08131, 2020"],"snippet":"… Universal Language Model Fine-Tuning (ULMFIT): We use the AWD-LSTM architecture (Merity et al., 2018) and pretrain on Wikipedia data (or Common Crawl in the case of Norwegian) taken from the CONLL 2017 shared task (Zeman et al., 2017) …","url":["https://arxiv.org/pdf/2002.08131"]}
{"year":"2020","title":"Hierarchical Multimodal Attention for End-to-End Audio-Visual Scene-Aware Dialogue Response Generation","authors":["H Le, D Sahoo, NF Chen, SCH Hoi - Computer Speech & Language, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0885230820300280"]}
{"year":"2020","title":"High Accuracy Phishing Detection Based on Convolutional Neural Networks","authors":["SY Yerima, MK Alzaylaee - 2020"],"snippet":"… Their approach first encodes the URL strings using one-hot encoding and then inputs each encoded character vector into the LSTM neurons for training and testing. Their method achieved an accuracy of 0.935 on the Common …","url":["https://dora.dmu.ac.uk/bitstream/handle/2086/19450/HAPD-CNN-paper-accepted-version.pdf?sequence=1&isAllowed=y"]}
{"year":"2020","title":"HitAnomaly: Hierarchical Transformers for Anomaly Detection in System Log","authors":["S Huang, Y Liu, C Fung, R He, Y Zhao, H Yang, Z Luan - IEEE Transactions on …, 2020"],"snippet":"… matrix. Finally, we obtain the word vector of 'terminating' as [16, 2, 11]. LogRobust [10] leverages off-the-shelf word vectors, which were pre-trained on Common Crawl Corpus dataset using the Fast-Text [20] algorithm. We initialize …","url":["https://ieeexplore.ieee.org/abstract/document/9244088/"]}
{"year":"2020","title":"How “BERTology” Changed the State-of-the-Art also for Italian NLP","authors":["F Tamburini - Proceedings of the Seventh Italian Conference on …, 2020"],"snippet":"… Page 2. billions of tokens. Also for GilBERTo it is available only the uncased model. • UmBERTo4: the more recent model de- veloped explicitly for Italian, as far as we know, is UmBERTo ('Musixmatch/umbertocommoncrawl-cased-v1' – umC) …","url":["http://ceur-ws.org/Vol-2769/paper_79.pdf"]}
{"year":"2020","title":"How Furiously Can Colourless Green Ideas Sleep? Sentence Acceptability in Context","authors":["JH Lau, CS Armendariz, S Lappin, M Purver, C Shu - arXiv preprint arXiv:2004.00881, 2020"],"snippet":"… BERTUCS Transformer Bidir. 340M Uncased 13GB WordPiece Wikipedia, BookCorpus XLNET Transformer Hybrid 340M Cased 126GB SentenceWikipedia, BookCorpus, Giga5 Piece ClueWeb, Common Crawl Table 1: Language models and their configurations …","url":["https://arxiv.org/pdf/2004.00881"]}
{"year":"2020","title":"How Human is Machine Translationese? Comparing Human and Machine Translations of Text and Speech","authors":["J van Genabith, E Teich","Y Bizzoni, TS Juzek, C España-Bonet, KD Chowdhury… - Proceedings of the 17th …, 2020"],"snippet":"… German translation and interpreting are both from English. lines de tokens en tokens Ct Cs CommonCrawl 2,212,292 49,870,179 54,140,396 MultiUN 108,387 4,494,608 4,924,596 NewsCommentary 324,388 8,316,081 46,222,416 …","url":["http://www.sfb1102.uni-saarland.de/wp/wp-content/uploads/2020/06/IWSLT-b1-B7-final2020.pdf","https://www.aclweb.org/anthology/2020.iwslt-1.34.pdf"]}
{"year":"2020","title":"How Language Shapes Prejudice Against Women: An Examination Across 45 World Languages","authors":["D DeFranza, H Mishra, A Mishra - 2020"],"snippet":"… context in which it occurs. Using text data from Wikipedia and the Common Crawl project … discussing gender issues. Wikipedia and a corpus of web crawl data from over five billion web pages, known as the Common Crawl, serve as our data source …","url":["https://psyarxiv.com/mrbcf/download?format=pdf"]}
{"year":"2020","title":"How Many Pages? Paper Length Prediction from the Metadata","authors":["E Çano, O Bojar - arXiv preprint arXiv:2010.15924, 2020"],"snippet":"… We used static word embeddings of 300 dimensions from three sources: the 6 billion tokens collection of Common Crawl4 trained with Glove [23], the 840 billion tokens collection of Common Crawl trained with Glove, and …","url":["https://arxiv.org/pdf/2010.15924"]}
{"year":"2020","title":"How Much Self-attention Do We Need? Trading Attention for Feed-forward Layers","authors":["K Irie, A Gerstenberger, R Schlüter, H Ney - ICASSP, Barcelona, Spain, 2020"],"snippet":"… to 6 (in contrast to what we typically observe eg, on LibriSpeech [28]): this is in fact due to some overlap between the common crawl training subset … Then we fine-tune the model on the TED-LIUM 2 transcriptions (2 M words) …","url":["https://www-i6.informatik.rwth-aachen.de/publications/download/1126/Irie-ICASSP-2020.pdf"]}
{"year":"2020","title":"How Should Markup Tags Be Translated?","authors":["G Hanneman, G Dinu, AI Amazon"],"snippet":"… especially large or noisy data sets. For EN–DE, we begin with the training data released by the WMT 2020 news task, ignoring the Common Crawl and Paracrawl corpora and heavily filtering WikiMatrix. Our EN–FR training data …","url":["https://assets.amazon.science/fa/f2/640de7fd483a8c385db7a0b5c7cd/how-should-markup-tags-be-translated.pdf"]}
{"year":"2020","title":"Human-in-the-Loop AI for Analysis of Free Response Facial Expression Label Sets","authors":["C Butler, H Oster, J Togelius - Proceedings of the 20th ACM International Conference …, 2020"],"snippet":"… 1. GloVe, 300-dimensional vectors trained on Common Crawl [33]: distributional model that learns word vectors by examining word co- occurrences within a text corpus with logbilinear regression, global matrix factorization and local context window methods …","url":["https://dl.acm.org/doi/abs/10.1145/3383652.3423892"]}
{"year":"2020","title":"Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning","authors":["R Schuster, T Schuster, Y Meri, V Shmatikov - arXiv preprint arXiv:2001.04935, 2020"],"snippet":"Page 1. Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning* Roei Schuster Tel Aviv University † roeischuster@mail.tau.ac.il Tal Schuster CSAIL, MIT tals@csail.mit.edu Yoav Meri † 111yoav@gmail.com Vitaly …","url":["https://arxiv.org/pdf/2001.04935"]}
{"year":"2020","title":"Hungarian layer: A novel interpretable neural layer for paraphrase identification","authors":["H Xiao - Neural Networks, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0893608020302653"]}
{"year":"2020","title":"HW-TSC's Participation in the WMT 2020 News Translation Shared Task","authors":["D Wei, H Shang, Z Wu, Z Yu, L Li, J Guo, M Wang…"],"snippet":"… monolingual text from Common Crawl and news crawl 2018 for Km and En, respectively. 2.1.3 Ps/En Similar to Km/En, we also use the Para Crawl v5.1 (1M), Khmer and Pashto parallel data (0.03M) as bitext and select …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.31.pdf"]}
{"year":"2020","title":"Hybrid Feature Model for Emotion Recognition in Arabic Text","authors":["N Alswaidan, MEB Menai - IEEE Access, 2020"],"snippet":"… 5https://github.com/minimaxir/char-embeddings 6https://unicode.org/emoji/ charts/full-emoji-list.html • FastText [22]: 300-dimensional word vectors trained on Common Crawl7 using CBOW with position weights … 7https://commoncrawl …","url":["https://ieeexplore.ieee.org/iel7/6287639/8948470/09007420.pdf"]}
{"year":"2020","title":"HyCoNN: Hybrid Cooperative Neural Networks for Personalized News Discussion Recommendation","authors":["J Risch, V Künstler, R Krestel"],"snippet":"… For DeepCoNN and also HyCoNN, we use 300-dimensional fastText word embeddings, which were pre-trained on the English-language Common Crawl dataset [25]. The resulting embeddings function as input to a convolutional layer that consists of n neurons …","url":["https://hpi.de/fileadmin/user_upload/fachgebiete/naumann/people/risch/risch2020hyconn.pdf"]}
{"year":"2020","title":"Identifying Cognates in English-Dutch and French-Dutch by means of Orthographic Information and Cross-lingual Word Embeddings","authors":["E Lefever, S Labat, P Singh - the 12th Conference on Language Resources and …, 2020"],"snippet":"… The former approach was improved in the following way. Firstly, standard fastText word embeddings, which were pretrained on Common Crawl and Wikipedia and generated with the standard skip-gram model as proposed by Bo- janowski et al …","url":["https://biblio.ugent.be/publication/8662200/file/8662201"]}
{"year":"2020","title":"Identifying Phished Website Using Multilayer Perceptron","authors":["A Dev, V Jain - Advances in Distributed Computing and Machine …"],"snippet":"… Phishing Webpage Source: PhishTank, OpenPhish. Legitimate Webpage Source: Alexa, Common Crawl. The main process in the phishing webpage is to work on its features and how effectively it is handling the dataset. Each …","url":["https://link.springer.com/chapter/10.1007/978-981-15-4218-3_37"]}
{"year":"2020","title":"Identifying Sensitive URLs at Web-Scale","authors":["M Srdjan, I Costas, S Georgios, N Laoutaris - 2020","S Matic, C Iordanou, G Smaragdakis, N Laoutaris - studies"],"snippet":"… We then use our classifier to search for sensitive URLs in a corpus of 1 Billion URLs collected by the Common Crawl project. We identify more than 155 millions sensitive URLs in more than 4 million domains … Automated …","url":["http://eprints.networks.imdea.org/2187/1/imc20.pdf","http://laoutaris.info/wp-content/uploads/2020/09/imc2020.pdf"]}
{"year":"2020","title":"Identifying Tasks from Mobile App Usage Patterns","authors":["Y Tian, K Zhou, M Lalmas, D Pelleg - Proceedings of the 43rd International ACM …, 2020"],"snippet":"Page 1. Identifying Tasks from Mobile App Usage Patterns Yuan Tian University of Nottingham Nottingham, UK yuan.tian@nottingham.ac.uk Ke Zhou University of Nottingham Nottingham, UK ke.zhou@nottingham.ac …","url":["https://dl.acm.org/doi/abs/10.1145/3397271.3401441"]}
{"year":"2020","title":"Igbo-English Machine Translation: An Evaluation Benchmark","authors":["I Ezeani, P Rayson, I Onyenwe, C Uchechukwu… - arXiv preprint arXiv …, 2020"],"snippet":"… Page 2. Published as a conference paper at ICLR 2020 texts (eg Wikipedia, CommonCrawl, local government materials, local TV/Radio stations etc). Phase 2: Translation and correction In this phase, the 10,000 sentence pairs …","url":["https://arxiv.org/pdf/2004.00648"]}
{"year":"2020","title":"IIU: Specialized Architecture for Inverted Index Search","authors":["J Heo, J Won, Y Lee, S Bharuka, J Jang, TJ Ham…"],"snippet":"Page 1. IIU: Specialized Architecture for Inverted Index Search Jun Heo∗ Jaeyeon Won∗ Yejin Lee∗ Shivam Bharuka†§ Jaeyoung Jang‡ Tae Jun Ham∗ Jae W. Lee∗ ∗Seoul National University, †Facebook, Inc., ‡Sungkyunkwan University …","url":["https://www.cs.princeton.edu/~tae/iiu_asplos2020.pdf"]}
{"year":"2020","title":"Imitation Attacks and Defenses for Black-box Machine Translation Systems","authors":["E Wallace, M Stern, D Song - arXiv preprint arXiv:2004.15015, 2020"],"snippet":"… For English→German, we query the source side of the WMT14 training set (≈ 4.5M sentences).3 For Nepali→English, we query the Nepali Language Wikipedia (≈ 100,000 sentences) and approximately two million sentences from Nepali common crawl …","url":["https://arxiv.org/pdf/2004.15015"]}
{"year":"2020","title":"Impact of News on the Commodity Market: Dataset and Results","authors":["A Sinha, T Khandait - arXiv preprint arXiv:2009.04202, 2020"],"snippet":"… The GloVe pre-trained word-embeddings are known to capture the meaning of a word through a high dimensional vector [22]. For this research, we used the 300-dimensional vectors which were trained on 840 billion tokens through the common crawl …","url":["https://arxiv.org/pdf/2009.04202"]}
{"year":"2020","title":"Impact of sentence length on the readability of web for screen reader users","authors":["BB Kadayat, E Eika - International Conference on Human-Computer …, 2020"],"snippet":"… They used MapReduce for real-time calculation of the readability of more than a billion webpages. The datasets called Common Crawl included 61 million domain-names, 92 million PDF documents, and seven million Word documents …","url":["https://link.springer.com/chapter/10.1007/978-3-030-49282-3_18"]}
{"year":"2020","title":"Improved method of word embedding for efficient analysis of human sentiments","authors":["S Sagnika, BSP Mishra, SK Meher - Multimedia Tools and Applications, 2020"],"snippet":"… Designed by Pennington [23] in 2014, it creates a word vector space by training on word-word co-occurrence counts. The models are trained on Wikipedia dumps, Gigaword 5 and Common Crawl texts, and apply …","url":["https://link.springer.com/article/10.1007/s11042-020-09632-9"]}
{"year":"2020","title":"Improving Indonesian Text Classification Using Multilingual Language Model","authors":["IF Putra, A Purwarianti - arXiv preprint arXiv:2009.05713, 2020"],"snippet":"… The XLM-R Large also has substantially more parameters and was trained on a larger balanced dataset from the CommonCrawl corpus that contains 100 languages. The XLM-R variant that we use in the experiment is …","url":["https://arxiv.org/pdf/2009.05713"]}
{"year":"2020","title":"IMPROVING KNOWLEDGE ACCESSIBILITY ON THE WEB","authors":["R Yu"],"snippet":"Page 1. IMPROVING KNOWLEDGE ACCESSIBILITY ON THE WEB – from Knowledge Base Augmentation to Search as Learning Inaugural dissertation for the attainment of the title of doctor in the Faculty of Mathematics and …","url":["https://docserv.uni-duesseldorf.de/servlets/DerivateServlet/Derivate-56199"]}
{"year":"2020","title":"Improving Low Compute Language Modeling with In-Domain Embedding Initialisation","authors":["C Welch, R Mihalcea, JK Kummerfeld - arXiv preprint arXiv:2009.14109, 2020"],"snippet":"… We see that the value of additional data depends on the domain. Gigaword is also news text and is able to improve performance. The larger GloVe datasets use Wikipedia and CommonCrawl data, which is a poorer match and so does not improve performance …","url":["https://arxiv.org/pdf/2009.14109"]}
{"year":"2020","title":"Improving Network Security through Collaborative Sharing","authors":["CS Ardi - 2020"],"snippet":"Page 1. IMPROVING NETWORK SECURITY THROUGH COLLABORATIVE SHARING by Calvin Satiawan Ardi A Dissertation Presented to the FACULTY OF THE USC GRADUATE SCHOOL UNIVERSITY OF SOUTHERN CALIFORNIA …","url":["http://search.proquest.com/openview/9de70dfceaa27a03d073c17f5f071579/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2020","title":"Improving Personal Health Mention Detection on Twitter Using Permutation Based Word Representation Learning","authors":["PI Khan, I Razzak, A Dengel, S Ahmed - International Conference on Neural …, 2020"],"snippet":"… In: Invited Talk at the SIGIR 2012 Workshop on Open-Source Information Retrieval (2012)Google Scholar. 24. Common CrawlCommon crawl corpus (2019). http://commoncrawl.org. Copyright information. © Springer Nature …","url":["https://link.springer.com/chapter/10.1007/978-3-030-63830-6_65"]}
{"year":"2020","title":"Improving Ranking in Document based Search Systems","authors":["RRK Menon, J Kaartik, ETK Nambiar, AK TK, A Kumar - 2020 4th International …, 2020"],"snippet":"… The pre-trained word embedding models used were: 1. Fasttext i. – Wiki News 1M 16B Tokens ii. – Common Crawl 2M 600B Tokens 2. GloVe 2.2M 840B Tokens B. Evaluation Metrics The standard metrics include Precision, Recall, and F1 score …","url":["https://ieeexplore.ieee.org/abstract/document/9143047/"]}
{"year":"2020","title":"Increasing Accessibility of Electronic Theses and Dissertations (ETDs) Through Chapter-level Classification","authors":["PM Jude - 2020"],"snippet":"Page 1. Increasing Accessibility of Electronic Theses and Dissertations (ETDs) Through Chapter-level Classification Palakh Mignonne Jude Thesis submitted to the Faculty of the Virginia Polytechnic Institute and State University …","url":["https://vtechworks.lib.vt.edu/bitstream/handle/10919/99294/Jude_P_T_2020.pdf?sequence=1"]}
{"year":"2020","title":"INDEXING OF BIG TEXT DATA AND SEARCHING IN THE INDEXED DATA","authors":["BD KOZÁK"],"snippet":"… enhanced documents. Input data In [14, 8], the input data was CommonCrawl and Wikipedia. The English wikipedia … 8 Page 15. CommonCrawl3is a project that maintains an open repository of web crawl data. For the practical part of …","url":["https://dspace.vutbr.cz/bitstream/handle/11012/192492/final-thesis.pdf?sequence=3"]}
{"year":"2020","title":"Indic-Transformers: An Analysis of Transformer Language Models for Indian Languages","authors":["K Jain, A Deshpande, K Shridhar, F Laumann, A Dash - arXiv preprint arXiv …, 2020"],"snippet":"… The Open Super-large Crawled ALMAnaCH coRpus (OSCAR) dataset [48] is a filtered version of the CommonCrawl dataset and has monolingual corpora for 166 languages. Prior to training, we normalize the OSCAR dataset for …","url":["https://arxiv.org/pdf/2011.02323"]}
{"year":"2020","title":"IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages","authors":["D Kakwani, A Kunchukuttan, S Golla, A Bhattacharyya…"],"snippet":"… The OSCAR project (Ortiz Suarez et al., 2019), a recent processing of CommonCrawl, also contains much less data for most Indian languages than our crawls. The CCNet () and C4 () projects also provide tools to …","url":["https://indicnlp.ai4bharat.org/papers/arxiv2020_indicnlp_corpus.pdf"]}
{"year":"2020","title":"IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding","authors":["B Wilie, K Vincentio, GI Winata, S Cahyawijaya, X Li… - arXiv preprint arXiv …, 2020"],"snippet":"… Page 5. Dataset # Words # Sentences Size Style Source OSCAR (Ortiz Suárez et al., 2019) 2,279,761,186 148,698,472 14.9 GB mixed OSCAR CoNLLu Common Crawl (Ginter et al., 2017) 905,920,488 77,715,412 6.1 GB mixed LINDAT/CLARIAH-CZ …","url":["https://arxiv.org/pdf/2009.05387"]}
{"year":"2020","title":"Inducing Language-Agnostic Multilingual Representations","authors":["W Zhao, S Eger, J Bjerva, I Augenstein - arXiv preprint arXiv:2008.09112, 2020"],"snippet":"… XLM-R Contextualized word embeddings (Conneau et al., 2019) are pre-trained on the CommonCrawl corpora of 100 languages, which contain more monolingual data than Wikipedia corpora, with 1) a vocabulary size …","url":["https://arxiv.org/pdf/2008.09112"]}
{"year":"2020","title":"Inductive Learning on Commonsense Knowledge Graph Completion","authors":["B Wang, G Wang, J Huang, J You, J Leskovec… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Inductive Learning on Commonsense Knowledge Graph Completion Bin Wang1, Guangtao Wang2, Jing Huang2, Jiaxuan You3, Jure Leskovec3, C.-C. Jay Kuo1 1 University of Southern California 2 JD AI Research …","url":["https://arxiv.org/pdf/2009.09263"]}
{"year":"2020","title":"Inexpensive Domain Adaptation of Pretrained Language Models: A Case Study on Biomedical Named Entity Recognition","authors":["N Poerner, U Waltinger, H Schütze - arXiv preprint arXiv:2004.03354, 2020"],"snippet":"… 1 Introduction Pretrained Language Models such as BERT (De- vlin et al., 2019) have spearheaded advances on many NLP tasks. Usually, PTLMs are pretrained on unlabeled general-domain and/or mixed-domain text, such …","url":["https://arxiv.org/pdf/2004.03354"]}
{"year":"2020","title":"Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA","authors":["N Poerner, U Waltinger, H Schütze"],"snippet":"… Pretrained Language Models (PTLMs) such as BERT (Devlin et al., 2019) have spearheaded ad- vances on many NLP tasks. Usually, PTLMs are pretrained on unlabeled general-domain and/or mixed-domain text, such …","url":["https://web.iiit.ac.in/~rizwan.ali/papers/828.pdf"]}
{"year":"2020","title":"Infosys Machine Translation System for WMT20 Similar Language Translation Task","authors":["K Rathinasamy, A Singh, B Sivasambagupta… - Proceedings of WMT, 2020"],"snippet":"… data. 2.2.2 Synthetic data CommonCrawl n-grams raw monolingual files are processed1 to remove sentences with invalid characters, strip leading and trailing whitespaces, and remove duplicate sentences. 3 System Overview …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.52.pdf"]}
{"year":"2020","title":"iNLTK: Natural Language Toolkit for Indic Languages","authors":["G Arora - arXiv preprint arXiv:2009.12534, 2020"],"snippet":"… iNLTK results were compared against results reported in (Kunchukuttan et al., 2020) for pre-trained embeddings released by the FastText project trained on Wikipedia (FT-W) (Bo- janowski et al., 2016), Wiki+CommonCrawl …","url":["https://arxiv.org/pdf/2009.12534"]}
{"year":"2020","title":"INSET: Sentence Infilling with INter-SEntential Transformer","authors":["Y Huang, Y Zhang, O Elachqar, Y Cheng - Proceedings of the 58th Annual Meeting of …, 2020"],"snippet":"… For the effectiveness of human evaluation, we use the simplest strategy to mask sentences. The Recipe dataset is obtained from (https: //commoncrawl.org), where the metadata is formatted according to Schema.org (https:// schema.org/Recipe) …","url":["https://www.aclweb.org/anthology/2020.acl-main.226.pdf"]}
{"year":"2020","title":"Integrating Geospatial Data and Social Media in Bidirectional Long-Short Term Memory Models to Capture Human Nature Interactions","authors":["A Larkin, P Hystad - The Computer Journal, 2020"],"snippet":"… language processing [32]. Tweet texts were transformed into word vector arrays using the Stanford GloVe (Global Vectors for Word Representation) Common Crawl dictionary (https://nlp.stanford.edu/projects/glove/). The GloVe …","url":["https://academic.oup.com/comjnl/advance-article-abstract/doi/10.1093/comjnl/bxaa094/5893915"]}
{"year":"2020","title":"Intelligent phishing detection scheme using deep learning algorithms","authors":["MA Adebowale, KT Lwin, MA Hossain - Journal of Enterprise Information …, 2020"],"snippet":"… Half of the data set consisted of phishing sites from PhishTank, which is a site that is used as phishing URL depository, and half of the data set was comprised of legitimate sites from Common Crawl, a corpus of web crawl data …","url":["https://www.emerald.com/insight/content/doi/10.1108/JEIM-01-2020-0036/full/html"]}
{"year":"2020","title":"Intermediate Training of BERT for Product Matching","authors":["R Peeters, C Bizer, G Glavaš - small"],"snippet":"… uct Corpus for Large-Scale Product Matching [26]. These datasets are derived from schema.org annotations from thousands of webshops extracted from the Common Crawl. Relying on schema.org annotations of product identifiers …","url":["http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/papers/DI2KG2020_Peeters.pdf"]}
{"year":"2020","title":"Interpretability Analysis for Named Entity Recognition to Understand System Predictions and How They Can Improve","authors":["O Agarwal, Y Yang, BC Wallace, A Nenkova - arXiv preprint arXiv:2004.04564, 2020"],"snippet":"… representations are concatenated. We use 300 dimensional cased GloVe (Pennington et al., 2014) vectors trained on Common Crawl.2 We use the IO labeling scheme and evaluate the systems via micro-F1, at the token level. We use …","url":["https://arxiv.org/pdf/2004.04564"]}
{"year":"2020","title":"Interpretable & Time-Budget-Constrained Contextualization for Re-Ranking","authors":["S Hofstätter, M Zlabinger, A Hanbury - arXiv preprint arXiv:2002.01854, 2020"],"snippet":"… The first section contains the traditional baselines; the second contains the neural re-ranking baselines; in the third section we report the results of our TK model with three 6 42B CommonCrawl lower-cased: https://nlp.stanford.edu/projects/glove/ Page 5 …","url":["https://arxiv.org/pdf/2002.01854"]}
{"year":"2020","title":"Introduction to Cloud Computing and Amazon Web Services (AWS)","authors":["JM Patel - Getting Structured Data from the Internet, 2020"],"snippet":"… 5 examples. IAM and S3 sections are necessary for Chapters 6 and 7 since we will be using data compiled by a nonprofit called common crawl which is only publicly available on S3 through AWS open registry. You will have …","url":["https://link.springer.com/chapter/10.1007/978-1-4842-6576-5_3"]}
{"year":"2020","title":"Introduction to Common Crawl Datasets","authors":["JM Patel - Getting Structured Data from the Internet, 2020"],"snippet":"The Common Crawl Foundation (https://commoncrawl.org/) is a 501(c)(3) nonprofit involved in providing open access web crawl data going back to over eight years. They perform monthly web crawls which cover over 25 billion pages for each month. This …","url":["https://link.springer.com/chapter/10.1007/978-1-4842-6576-5_6"]}
{"year":"2020","title":"Introduction to Web Scraping","authors":["JM Patel - Getting Structured Data from the Internet, 2020"],"snippet":"… We will introduce natural language processing algorithms in Chapter 4, and we will put them into action in Chapters 6 and 7 on a Common Crawl dataset. The next step is loading the cleaned data from the preceding step into an appropriate database …","url":["https://link.springer.com/chapter/10.1007/978-1-4842-6576-5_1"]}
{"year":"2020","title":"Is Everything Fine, Grandma? Acoustic and Linguistic Modeling for Robust Elderly Speech Emotion Recognition","authors":["G Sogancıoglu, O Verkholyak, H Kaya, D Fedotov… - INTERSPEECH, Shanghai …, 2020","G Soğancıoğlu, O Verkholyak, H Kaya, D Fedotov… - arXiv preprint arXiv …, 2020"],"snippet":"… negative scores. For the SentiWordNet representation, an input text is tokenized ignoring the punctuation, and each token is looked up ac- 1https://cloud.google.com/ translate 2http://commoncrawl.org/ Page 3. cording to its POS. It …","url":["https://arxiv.org/pdf/2009.03432","https://indico2.conference4me.psnc.pl/event/35/contributions/3140/attachments/1218/1261/Wed-SS-1-4-12.pdf"]}
{"year":"2020","title":"Is language modeling enough? Evaluating effective embedding combinations","authors":["R Schneider, T Oberhauser, P Grundmann, FA Gers… - 2020"],"snippet":"… 2.1. Universal Text Embeddings Recently, researchers explore universal text embeddings trained on extensive Web corpora, such as the Common Crawl6 (Mikolov et al., 2018; Radford et al., 2019), the billion … 5https …","url":["https://eprints.soton.ac.uk/438613/1/LREC20_LM_TM_27_1_.pdf"]}
{"year":"2020","title":"Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation","authors":["B Eikema, W Aziz - arXiv preprint arXiv:2005.10283, 2020"],"snippet":"… For English-Nepali we also use a translated version of the Penn Treebank4 and for English-Sinhala we additionally use Open Subtitles (Lison et al., 2018). We use a filtered crawl of Wikipedia and Common Crawl released in Guzmán et al …","url":["https://arxiv.org/pdf/2005.10283"]}
{"year":"2020","title":"Is Wikipedia succeeding in reducing gender bias? Assessing changes in gender bias in Wikipedia using word embeddings","authors":["KG Schmahl, TJ Viering, S Makrodimitris, AN Jahfari… - Proceedings of the Fourth …, 2020"],"snippet":"… These categories have shown significant bias towards male or female words in embeddings from Google News corpora [Mikolov et al., 2013a], Google Books [Jones et al., 2020], as well as a 'Common Crawl' corpus [Caliskan et al., 2017] …","url":["https://www.aclweb.org/anthology/2020.nlpcss-1.11.pdf"]}
{"year":"2020","title":"It's the Best Only When It Fits You Most: Finding Related Models for Serving Based on Dynamic Locality Sensitive Hashing","authors":["L Zhou, Z Wang, A Das, J Zou - arXiv preprint arXiv:2010.09474, 2020"],"snippet":"… BAIR CORD-19 LSUN Bedroom iNaturalist (iNat) 2017 ImageNet OpenImagesV4 Wikipedia 1 Billion Word Benchmark CommonCrawl Multillingual Wikipedia Natural Questions 3 15 3 3 8 10 9 7 2 5 58 3 5 2 5 2 CelebA HQ iMet Collection 2019 …","url":["https://arxiv.org/pdf/2010.09474"]}
{"year":"2020","title":"Italian Transformers Under the Linguistic Lens","authors":["A Miaschip, G Sartim, D Brunato, F Dell'Orletta… - Proceedings of the Seventh …, 2020"],"snippet":"… For instance, we can notice that, for both the probing models, features related to the distribution of syntactic relations (SyntacticDep) are better predicted by GePpeTto, while GilBERTo and UmBERTo-Commoncrawl are the best …","url":["http://ceur-ws.org/Vol-2769/paper_56.pdf"]}
{"year":"2020","title":"JASS: Japanese-specific Sequence to Sequence Pre-training for Neural Machine Translation","authors":["Z Mao, F Cromieres, R Dabre, H Song, S Kurohashi - arXiv preprint arXiv:2005.03361, 2020"],"snippet":"… Mono Ja Common Crawl 22M En News Crawl 22M Ru News Crawl 22M … 5.1.2. Monolingual data We use monolingual data containing 22M Japanese, 22M English and 22M Russian sentences randomly sub-sampled from Common Crawl dataset and News crawl4 dataset …","url":["https://arxiv.org/pdf/2005.03361"]}
{"year":"2020","title":"Joint Multiclass Debiasing of Word Embeddings","authors":["R Popović, F Lemmerich, M Strohmaier - arXiv preprint arXiv:2003.11520, 2020"],"snippet":"… As in previous studies [7], evaluation was done on three pretrained Word Embedding models with vector dimension of 300: FastText2(English we- bcrawl and Wikipedia, 2 million words), GloVe3(Common Crawl, Wikipedia …","url":["https://arxiv.org/pdf/2003.11520"]}
{"year":"2020","title":"Joint translation and unit conversion for end-to-end localization","authors":["G Dinu, P Mathur, M Federico, S Lauly, Y Al-Onaizan - arXiv preprint arXiv …, 2020","GDPMMFSL YaserAl-Onaizan, AWS Amazon"],"snippet":"… Europarl (Koehn, 2005) and news commentary data from WMT En→De shared task 2019 totalling 2.2 million sentences.2 Standard translation test sets do not have, however, enough examples of unit conversions and in fact …","url":["https://arxiv.org/pdf/2004.05219","https://assets.amazon.science/b2/a7/e1ada6104b3587401b30ccc8637a/joint-translation-and-unit-conversion-for-end-to-end-localization.pdf"]}
{"year":"2020","title":"KBPearl: a knowledge base population system supported by joint entity and relation linking","authors":["X Lin, H Li, H Xin, Z Li, L Chen - Proceedings of the VLDB Endowment, 2020"],"snippet":"Page 1. KBPearl: A Knowledge Base Population System Supported by Joint Entity and Relation Linking Xueling Lin, Haoyang Li, Hao Xin, Zijian Li, Lei Chen Department of Computer Science and Engineering The Hong Kong …","url":["https://dl.acm.org/doi/pdf/10.14778/3384345.3384352"]}
{"year":"2020","title":"Keeping Models Consistent between Pretraining and Translation for Low-Resource Neural Machine Translation","authors":["W Zhang, X Li, Y Yang, R Dong, G Luo - Future Internet, 2020"],"snippet":"Recently, the pretraining of models has been successfully applied to unsupervised and semi-supervised neural machine translation. A cross-lingual language model uses a pretrained masked language model to initialize the …","url":["https://www.mdpi.com/1999-5903/12/12/215/pdf"]}
{"year":"2020","title":"Kernel compositional embedding and its application in linguistic structured data classification","authors":["H Ganji, MM Ebadzadeh, S Khadivi - Knowledge-Based Systems, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0950705120300460"]}
{"year":"2020","title":"Key Phrase Classification in Complex Assignments","authors":["M Ravikiran - arXiv preprint arXiv:2003.07019, 2020"],"snippet":"… Corpus and English Wikipedia used in BERT was found to be useful for training. The additional data included Common Crawl News dataset (76 GB), Web text corpus (38 GB) and Stories from Common Crawl (31 GB). This coupled …","url":["https://arxiv.org/pdf/2003.07019"]}
{"year":"2020","title":"Keynote speaker","authors":["M Benjamin"],"snippet":"Skip to content …","url":["https://asling.org/tc42/"]}
{"year":"2020","title":"Keyphrase Extraction as Sequence Labeling Using Contextualized Embeddings","authors":["D Sahrawat, D Mahata, H Zhang, M Kulkarni, A Sharma… - Advances in Information …, 2020"],"snippet":"… We also use 300 dimensional fixed embeddings from Glove [20], Word2Vec [19], and FastText [13] (common-crawl, wiki-news). We also compare the proposed architecture against four popular baselines …","url":["https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7148038/"]}
{"year":"2020","title":"KGvec2go--Knowledge Graph Embeddings as a Service","authors":["J Portisch, M Hladik, H Paulheim - arXiv preprint arXiv:2003.05809, 2020"],"snippet":"… knowledge graphs. 4.3. WebIsALOD The WebIsA database (Seitner et al., 2016) is a data set which consists of hypernymy relations ex- tracted from the Common Crawl8, a downloadable copy of the Web. The extraction was …","url":["https://arxiv.org/pdf/2003.05809"]}
{"year":"2020","title":"KIT's IWSLT 2020 SLT Translation System","authors":["NQ Pham, F Schneider, TN Nguyen, TL Ha, TS Nguyen… - Proceedings of the 17th …, 2020"],"snippet":"… Table 2: Text Training Data Dataset Sentences TED Talks (TED) 220K Europarl (EPPS) 2.2MK CommonCrawl 2.1M Rapid 1.21M ParaCrawl 25.1M OpenSubtitles 12.6M WikiTitle 423K Back-translated News 26M Page 2. 56 3 Simultaneous Speech Translation …","url":["https://www.aclweb.org/anthology/2020.iwslt-1.4.pdf"]}
{"year":"2020","title":"KLEJ: Comprehensive Benchmark for Polish Language Understanding","authors":["P Rybak, R Mroczkowski, J Tracz, I Gawlik - arXiv preprint arXiv:2005.00630, 2020"],"snippet":"… word vectors. To evaluate their impact on KLEJ tasks, we initialize word embeddings with fastText (Bojanowski et al., 2016) trained on Common Crawl and Wikipedia for Polish language (Grave et al., 2018). 4.1.3 ELMo ELMo …","url":["https://arxiv.org/pdf/2005.00630"]}
{"year":"2020","title":"KLUMSy@ KIPoS: Experiments on Part-of-Speech Tagging of Spoken Italian","authors":["T Proisl, G Lapesa"],"snippet":"… The PAISÀ corpus of Italian texts from the web (Lyding et al., 2014),5 the text of the Italian Wikimedia dumps,6 ie Wiki(pedia|books|news|versity|voyage), as ex- tracted by Wikipedia Extractor,7 and the Italian subset of OSCAR …","url":["http://ceur-ws.org/Vol-2765/paper140.pdf"]}
{"year":"2020","title":"Knowledge Augmented Aspect Category Detection for Aspect-based Sentiment Analysis","authors":["K Martinen - 2019"],"snippet":"Page 1. MASTERTHESIS Knowledge Augmented Aspect Category Detection for Aspect-based Sentiment Analysis Kai Martinen 01.12.2019 University of Hamburg MIN-Faculty Department of Computer Science Language Technologies …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2019-ma-martinen.pdf"]}
{"year":"2020","title":"Knowledge Efficient Deep Learning for Natural Language Processing","authors":["H Wang - arXiv preprint arXiv:2008.12878, 2020"],"snippet":"Page 1. Knowledge Efficient Deep Learning for Natural Language Processing by Hai Wang A thesis submitted in partial fulfillment for the degree of Doctor of Philosophy in Computer Science at the Toyota Technological Institute …","url":["https://arxiv.org/pdf/2008.12878"]}
{"year":"2020","title":"Knowledge Graphs Evolution and Preservation--A Technical Report from ISWS 2019","authors":["N Abbas, K Alghamdi, M Alinam, F Alloatti, G Amaral… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Knowledge Graphs Evolution and Preservation A Technical Report from ISWS 2019 December 23, 2020 Bertinoro, Italy arXiv:2012.11936v1 [cs.AI] 22 Dec 2020 Page 2. Authors Main Editors Valentina Anita Carriero …","url":["https://arxiv.org/pdf/2012.11936"]}
{"year":"2020","title":"KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding","authors":["J Ham, YJ Choe, K Park, I Choi, H Soh - arXiv preprint arXiv:2004.03289, 2020"],"snippet":"… 20 days). We also use XLM-R (Conneau and Lample, 2019), a publicly available cross-lingual language model that was pre-trained on 2.5TB of Common Crawl corpora in 100 languages including Korean (54GB). Note that …","url":["https://arxiv.org/pdf/2004.03289"]}
{"year":"2020","title":"LAMBERT: Layout-Aware language Modeling using BERT for information extraction","authors":["Ł Garncarek, R Powalski, T Stanisławek, B Topolski… - arXiv preprint arXiv …, 2020"],"snippet":"… Dataset pages EDGAR 119 088 RVL-CDIP 90 054 Common Crawl 389 469 cTDaR 782 private 151 074 Total 750 467 Table 1: Sizes of training datasets … Common Crawl PDFs This is a dataset produced by downloading PDF …","url":["https://arxiv.org/pdf/2002.08087"]}
{"year":"2020","title":"Language model domain adaptation for automatic speech recognition","authors":["A Prasad, P Motlicek, A Nanchen - 2020"],"snippet":"… By exploring and exploiting various datasets like Common Crawl, Europarl, news and TEDLIUM and by experimenting different techniques in training a model, we achieve the goal of adapting a general purpose LM to a domain like talks …","url":["https://infoscience.epfl.ch/record/275402"]}
{"year":"2020","title":"Language Models and Word Sense Disambiguation: An Overview and Analysis","authors":["D Loureiro, K Rezaee, MT Pilehvar… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Language Models and Word Sense Disambiguation: An Overview and Analysis Daniel Loureiro∗ LIAAD - INESC TEC Department of Computer Science - FCUP University of Porto, Portugal Kiamehr Rezaee∗ Department …","url":["https://arxiv.org/pdf/2008.11608"]}
{"year":"2020","title":"Language Models are Few-Shot Learners","authors":["TB Brown, B Mann, N Ryder, M Subbiah, J Kaplan… - arXiv preprint arXiv …, 2020"],"snippet":"… of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity. Details of the first two …","url":["https://arxiv.org/pdf/2005.14165"]}
{"year":"2020","title":"Language Models are Open Knowledge Graphs","authors":["C Wang, X Liu, D Song - arXiv preprint arXiv:2010.11967, 2020"],"snippet":"… In fact, these pre-trained LMs automatically acquire factual knowledge from large-scale corpora (eg, BookCorpus (Zhu et al., 2015), Common Crawl (Brown et al., 2020)) via pre-training. The learned knowledge in pre-trained LMs is the key to the current success …","url":["https://arxiv.org/pdf/2010.11967"]}
{"year":"2020","title":"Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling","authors":["S Bhosale, K Yee, S Edunov, M Auli - arXiv preprint arXiv:2011.07164, 2020"],"snippet":"… Big architecture. The model is trained on de-duplicated Romanian CommonCrawl data consisting of 623M sentences or 21.7B words after normalization and tokenization (Conneau et al., 2019; Wenzek et al., 2020). The German …","url":["https://arxiv.org/pdf/2011.07164"]}
{"year":"2020","title":"Language-agnostic BERT Sentence Embedding","authors":["F Feng, Y Yang, D Cer, N Arivazhagan, W Wang - arXiv preprint arXiv:2007.01852, 2020"],"snippet":"… The sentences are filtered using a sentence 1https://commoncrawl.org/ 2https://www.wikipedia.org/ 3Long lines are usually JavaScript or attempts at SEO … Finally, we pretrain on common crawl which is much larger, albeit …","url":["https://arxiv.org/pdf/2007.01852"]}
{"year":"2020","title":"Large Scale Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training","authors":["O Agarwal, H Ge, S Shakeri, R Al-Rfou - arXiv preprint arXiv:2010.12688, 2020"],"snippet":"… 3We use only one annotator at the moment but are expanding this evaluation to multiple annotators. Page 7. such as Wikipedia or common crawl. KGs are a rich source of factual information that can serve as additional succint informatopm …","url":["https://arxiv.org/pdf/2010.12688"]}
{"year":"2020","title":"Large-Scale Analysis of HTTP Response Headers","authors":["C Leyers, J Paytosh, N Worthy - 2020"],"snippet":"… The data come from the Common Crawl's monthly web crawls that collect responses from what we can consider to be the entire internet … The data come from the Common Crawl's monthly web crawls that collect responses …","url":["https://digitalcommons.winthrop.edu/source/SOURCE_2020/allpresentationsandperformances/101/"]}
{"year":"2020","title":"Latte-Mix: Measuring Sentence Semantic Similarity with Latent Categorical Mixtures","authors":["M Li, H Bai, L Tan, K Xiong, J Lin - arXiv preprint arXiv:2010.11351, 2020"],"snippet":"Page 1. Latte-Mix: Measuring Sentence Semantic Similarity with Latent Categorical Mixtures Minghan Li*1, 2 , He Bai1, 3 , Luchen Tan1 , Kun Xiong1 , Ming Li1, 3 , Jimmy Lin1, 3 1RSVP.ai, 2University of Toronto, 3David R. Cheriton …","url":["https://arxiv.org/pdf/2010.11351"]}
{"year":"2020","title":"LEAPME: Learning-based Property Matching with Embeddings","authors":["D Ayala Hernández, IC Hernández Salmerón… - ArXiv. org, arXiv …, 2020","D Ayala, I Hernández, D Ruiz, E Rahm - arXiv preprint arXiv:2010.01951, 2020"],"snippet":"… To compute embeddings, we use the pre-trained GloVe approach [43]1, specifically for the uncased Common Crawl corpus that includes 300-dimensional vectors for 1.9 million words, promising a good coverage …","url":["https://arxiv.org/pdf/2010.01951","https://idus.us.es/bitstream/handle/11441/105071/1/LEAPME%20Learning%20based%20Property%20Matching%20with%20Embeddings.pdf?sequence=1"]}
{"year":"2020","title":"Learning Accurate Integer Transformer Machine-Translation Models","authors":["E Wu - arXiv preprint arXiv:2001.00926, 2020"],"snippet":"… sor2Tensor v1.12 English-to-German translation task (translate_ende_wmt32k_packed). This dataset has 4.6 million sentence pairs drawn from three WMT18 [Bojar et al., 2018a] parallel corpora: News Commentary V13, Europarl V7, and Common Crawl …","url":["https://arxiv.org/pdf/2001.00926"]}
{"year":"2020","title":"Learning and Evaluating Emotion Lexicons for 91 Languages","authors":["S Buechel, S Rücker, U Hahn - arXiv preprint arXiv:2005.05672, 2020"],"snippet":"… We use the fastText embedding models from Grave et al. (2018) trained for 157 languages on the respective WIKIPEDIA and the respective part of COMMONCRAWL. These resources not only greatly facilitate our work …","url":["https://arxiv.org/pdf/2005.05672"]}
{"year":"2020","title":"Learning Dynamic Knowledge Graphs to Generalize on Text-Based Games","authors":["A Adhikari, X Yuan, MA Côté, M Zelinka, MA Rondeau… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Learning Dynamic Knowledge Graphs to Generalize on Text-Based Games Ashutosh Adhikari * 1 Xingdi Yuan * 2 Marc-Alexandre Côté * 2 Mikuláš Zelinka 3 Marc-Antoine Rondeau 2 Romain Laroche 2 Pascal Poupart …","url":["https://arxiv.org/pdf/2002.09127"]}
{"year":"2020","title":"Learning Engineering Properties with Bag-of-Tricks. For the Automated Evaluation of a Piping Design","authors":["WC Tan, KH Chua, CB Yan, IM Chen - 2020 IEEE 16th International Conference on …, 2020"],"snippet":"… and T. Mikolov, “Learning word vectors for 157 languages,” in Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018), 2018. [28] [Online]. Available: https://commoncrawl.org/ 1280","url":["https://ieeexplore.ieee.org/abstract/document/9217001/"]}
{"year":"2020","title":"LEARNING FROM MULTIMODAL WEB DATA","authors":["JM Hessel - 2020"],"snippet":"… (2018); Roemmele et al. (2011); De Marneffe et al. (2019); Clark et al. (2019). 7https://commoncrawl.org/ 5 Page 22. model; they achieve high performance on several video understanding tasks (eg, retrieval), and …","url":["https://jmhessel.com/files/2020/phd_thesis.pdf"]}
{"year":"2020","title":"Learning Geometric Word Meta-Embeddings","authors":["P Jawanpuria, NTV Dev, A Kunchukuttan, B Mishra - arXiv preprint arXiv:2004.09219, 2020"],"snippet":"… GloVe (Pennington et al., 2014): has 1 917 494 word embeddings trained on 42B tokens of web data from the common crawl. • fastText (Bojanowski et al., 2017): has 2 000 000 word embeddings trained on common crawl …","url":["https://arxiv.org/pdf/2004.09219"]}
{"year":"2020","title":"Learning hierarchical relationships for object-goal navigation","authors":["Y Qiu, A Pal, HI Christensen"],"snippet":"Page 1. Learning hierarchical relationships for object-goal navigation Yiding Qiu ∗ UC San Diego yiqiu@eng.ucsd.edu Anwesan Pal ∗ UC San Diego a2pal@eng.ucsd.edu Henrik I. Christensen UC San Diego hichristensen@eng.ucsd.edu …","url":["https://www.researchgate.net/profile/Anwesan_Pal/publication/346061932_Learning_hierarchical_relationships_for_object-goal_navigation/links/5fb9b05fa6fdcc6cc659d1b2/Learning-hierarchical-relationships-for-object-goal-navigation.pdf"]}
{"year":"2020","title":"Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task","authors":["T Sellam, A Pu, HW Chung, S Gehrmann, Q Tan… - arXiv preprint arXiv …, 2020"],"snippet":"… Details of MBERT-WMT pre-training We trained MBERT-WMT model with an MLM loss (Devlin et al., 2019), using a combination of public datasets: Wikipedia, the WMT 2019 News Crawl (Barrault et al.), the C4 variant of Com …","url":["https://arxiv.org/pdf/2010.04297"]}
{"year":"2020","title":"Learning to Segment Actions from Observation and Narration","authors":["D Fried, JB Alayrac, P Blunsom, C Dyer, S Clark… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Learning to Segment Actions from Observation and Narration Daniel Fried‡ Jean-Baptiste Alayrac† Phil Blunsom† Chris Dyer† Stephen Clark† Aida Nematzadeh† †DeepMind, London, UK ‡Computer Science Division, UC Berkeley …","url":["https://arxiv.org/pdf/2005.03684"]}
{"year":"2020","title":"Learning to summarize from human feedback","authors":["N Stiennon, L Ouyang, J Wu, DM Ziegler, R Lowe… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Learning to summarize from human feedback Nisan Stiennon∗ Long Ouyang∗ Jeff Wu∗ Daniel M. Ziegler∗ Ryan Lowe∗ Chelsea Voss∗ Alec Radford Dario Amodei Paul Christiano∗ OpenAI Abstract As language …","url":["https://arxiv.org/pdf/2009.01325"]}
{"year":"2020","title":"Learning unbiased zero-shot semantic segmentation networks via transductive transfer","authors":["H Liu, Y Wang, J Zhao, G Yang, F Lv - arXiv preprint arXiv:2007.00515, 2020"],"snippet":"… IV. EXPERIMENTS Following [7], we use the concatenation of two different word vectors, ie word2vec trained on Google News [12] and fastText trained on Common Crawl [12], to construct the semantic space shared by source and target classes …","url":["https://arxiv.org/pdf/2007.00515"]}
{"year":"2020","title":"Learning User Representations for Open Vocabulary Image Hashtag Prediction","authors":["T Durand - Proceedings of the IEEE/CVF Conference on Computer …, 2020"],"snippet":"… We train our model using ADAM [26] during 20 epochs with a start learning rate 5e-5. We use ResNet-50 [22] as the ConvNet and GloVe embeddings [34] as pre-trained word em- beddings. GloVe was trained on Common …","url":["http://openaccess.thecvf.com/content_CVPR_2020/papers/Durand_Learning_User_Representations_for_Open_Vocabulary_Image_Hashtag_Prediction_CVPR_2020_paper.pdf"]}
{"year":"2020","title":"Learning Word and Sub-word Vectors for Amharic (Less Resourced Language)","authors":["A Eshetu, G Teshome, T Abebe"],"snippet":"… The large text collection from Wikipedia and common crawl are commonly used data source to train and learn word vectors (Al-Rfou et al., 2013; Bojanowski et al … (2014) released GloVe models trained on Wikipedia …","url":["https://www.academia.edu/download/64390049/39IJAERS-08202035-Learning.pdf"]}
{"year":"2020","title":"Leveraging ParsBERT and Pretrained mT5 for Persian Abstractive Text Summarization","authors":["M Farahani, M Gharachorloo, M Manthouri - arXiv preprint arXiv:2012.11204, 2020"],"snippet":"… T5, on the other hand, is a unified Seq2Seq framework that employs Text-to- Text format to address NLP text-based problems. A multilingual variation of the T5 model is called mT5 [16] that covers 101 different …","url":["https://arxiv.org/pdf/2012.11204"]}
{"year":"2020","title":"Leveraging Structured Metadata for Improving Question Answering on the Web","authors":["X Du, A Hassan, A Fourney, R Sim, P Bennett… - … of the 1st Conference of the …, 2020"],"snippet":"… website content. The Web Data Commons project (Mühleisen and Bizer, 2012) estimates that 0.9 billion HTML pages out of the 2.5 billion pages (37.1%) in the Common Crawl web corpus1 contain structured metadata. Figure …","url":["https://www.aclweb.org/anthology/2020.aacl-main.55.pdf"]}
{"year":"2020","title":"LIG-Health at Adhoc and Spoken IR Consumer Health Search: expanding queries using UMLS and FastText.","authors":["P Mulhem, GG Saez, A Mannion, D Schwab, J Frej - Conference and Labs of the …, 2020"],"snippet":"… The FastText embedding vector of a word is the sum of the vectors of its component ngrams. We used the pre-trained word vectors for English language, trained on Common Crawl and Wikipedia using FastText. The features of the model used are as follows; …","url":["http://www.dei.unipd.it/~ferro/CLEF-WN-Drafts/CLEF2020/paper_129.pdf"]}
{"year":"2020","title":"LIMSI@ WMT 2020","authors":["SA Rauf, JC Rosales, I Paris, PM Quang, S Paris…"],"snippet":"… Domain Corpus sents. words words (en) (de) web Paracrawl 50,875 978 919 economy Tilde EESC 2,858 61 58 news Commoncrawl 2,399 51 47 Tilde rapid 940 20 19 News commentary 361 8 8 tourism Tilde tourism 7 …","url":["http://statmt.org/wmt20/pdf/2020.wmt-1.86.pdf"]}
{"year":"2020","title":"Linguistic Structure Guided Context Modeling for Referring Image Segmentation","authors":["F Zhang, J Han","T Hui, S Liu, S Huang, G Li, S Yu, F Zhang, J Han"],"snippet":"… rate. CNN is fixed during training. We use batch size 1 and stop training after 700K iterations. GloVe word embeddings [30] pretrained on Common Crawl with 840B tokens are used to replace randomly initialized ones. For fair …","url":["http://colalab.org/media/paper/Linguistic_Structure_Guided_Context_Modeling_for_Referring_Image_Segmentation.pdf","https://link.springer.com/content/pdf/10.1007/978-3-030-58607-2_4.pdf"]}
{"year":"2020","title":"Linguistically-aware Attention for Reducing the Semantic-Gap in Vision-Language Tasks","authors":["G KV, A Nambiar, KS Srinivas, A Mittal - arXiv preprint arXiv:2008.08012, 2020"],"snippet":"… The pre-trained word-to-vector networks such as Glove [29] and Bert [30] are inexpensive and rich in making linguistic correlations (since they are already trained on a large textual corpus such as Common Crawl and Wikipedia2014) …","url":["https://arxiv.org/pdf/2008.08012"]}
{"year":"2020","title":"LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon Induction Through Non-Linear Mapping in Latent Space","authors":["T Mohiuddin, MS Bari, S Joty - arXiv preprint arXiv:2004.13889, 2020"],"snippet":"… English, Italian, and German em- beddings were trained on WacKy crawling corpora using CBOW (Mikolov et al., 2013b), while Spanish and Finnish embeddings were trained on WMT News Crawl and Common Crawl, respectively. 4.2 Baseline Methods …","url":["https://arxiv.org/pdf/2004.13889"]}
{"year":"2020","title":"Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine Translation","authors":["M Moradshahi, G Campagna, SJ Semnani, S Xu… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine Translation Mehrad Moradshahi Giovanni Campagna Sina J. Semnani Silei Xu Monica S. Lam Computer Science Department Stanford University …","url":["https://arxiv.org/pdf/2010.05106"]}
{"year":"2020","title":"Localizing Q&A Semantic Parsers for Any Language in a Day","authors":["M Moradshahi, G Campagna, S Semnani, S Xu, M Lam - Proceedings of the 2020 …, 2020"],"snippet":"Page 1. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5970–5983, November 16–20, 2020. c 2020 Association for Computational Linguistics 5970 Localizing Open-Ontology …","url":["https://www.aclweb.org/anthology/2020.emnlp-main.481.pdf"]}
{"year":"2020","title":"Locally Constructing Product Taxonomies from Scratch Using Representation Learning","authors":["M Kejriwal, RK Selvam, CC Ni, N Torzec"],"snippet":"… The WDC schema.org project, which relies on the webpages in the Common Crawl, is able to automatically extract schema.org data from webpages due to its unique syntax and make it available as a dataset in Resource Description Framework (RDF) …","url":["https://web.ntpu.edu.tw/~myday/doc/ASONAM2020/ASONAM2020_Proceedings/pdf/papers/080_098_507.pdf"]}
{"year":"2020","title":"LOREM: Language-consistent Open Relation Extraction from Unstructured Text","authors":["T Harting, S Mesbah, C Lofi"],"snippet":"… Since these sentences are automatically tagged, we do expect a higher noise level than in the manually tagged test sets. For the language-individual model, we use FastText word em- beddings [11] which are trained on Common Crawl and Wikipedia dataset …","url":["https://pure.tudelft.nl/portal/files/69221720/2020_WWW_LOREM.pdf"]}
{"year":"2020","title":"Lost in Embedding Space: Explaining Cross-Lingual Task Performance with Eigenvalue Divergence","authors":["H Dubossarsky, I Vulić, R Reichart, A Korhonen - arXiv preprint arXiv:2001.11136, 2020"],"snippet":"… We prefer Wikipedia as the main embedding training corpus over the larger Common Crawl corpus, because the text in Wikipedia is much cleaner or even hand-curated, and adheres to the rules of standard language (Grave et al., 2018) …","url":["https://arxiv.org/pdf/2001.11136"]}
{"year":"2020","title":"Low-Resource Knowledge-Grounded Dialogue Generation","authors":["X Zhao, W Wu, C Tao, C Xu, D Zhao, R Yan - arXiv preprint arXiv:2002.10348, 2020"],"snippet":"Page 1. Published as a conference paper at ICLR 2020 LOW-RESOURCE KNOWLEDGE-GROUNDED DIALOGUE GENERATION Xueliang Zhao1,2, Wei Wu3, Chongyang Tao1, Can Xu3, Dongyan Zhao1,2, Rui Yan1,2,4∗ 1Wangxuan …","url":["https://arxiv.org/pdf/2002.10348"]}
{"year":"2020","title":"Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning","authors":["X Li, Z Li, J Sheng, W Slamu"],"snippet":"… XLM - R shows the possibility of training one model for many languages while not sacrificing per-language performance. It is trained on 2.5TB of CommonCrawl data, in 100 languages and uses a large vocabulary size of CCL 2020 …","url":["http://www.cips-cl.org/static/anthology/CCL-2020/CCL-20-092.pdf"]}
{"year":"2020","title":"LREC 2020 Workshop Language Resources and Evaluation Conference 11–16 May 2020","authors":["M Kupietz, H Lungen, I Pisetta"],"snippet":"Page 1. LREC 2020 Workshop Language Resources and Evaluation Conference 11–16 May 2020 8th Workshop on Challenges in the Management of Large Corpora (CMLC-8) PROCEEDINGS Editors: Piotr Ba´nski, Adrien Barbaresi, Simon Clematide …","url":["https://ids-pub.bsz-bw.de/files/9811/Banski_Barbaresi_Clematide_Kupietz_Luengen_Pisetta_Proceedings_LREC_2020.pdf"]}
{"year":"2020","title":"Machine Bias and Fundamental Rights","authors":["D Amilevičius - Smart Technologies and Fundamental Rights, 2020"],"snippet":"Jump to Content Jump to Main Navigation. English; 中文; français; Deutsch. Access via: Google Googlebot - Web Crawler SEO. Login to my Brill account Create Brill Account. Publications. Subjects. African Studies American Studies …","url":["https://brill.com/view/book/edcoll/9789004437876/BP000019.xml"]}
{"year":"2020","title":"Machine Translation for English–Inuktitut with Segmentation, Data Acquisition and Pre-Training","authors":["C Roest, L Edman, G Minnema, K Kelly, J Spenader… - Proceedings of the Fifth …, 2020"],"snippet":"… 2), we train XLM models with the News Crawl data for English and Common Crawl data for Inuktitut, as specified in Table 2. We also use Hansards and Newsdevtrain oversampled 5 times for parallel data. We try both tagging …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.29.pdf"]}
{"year":"2020","title":"Machine Translation in Natural Language Processing by Implementing Artificial Neural Network Modelling Techniques: An Analysis","authors":["FA Khan, A Abubakar - International Journal on Perceptive and Cognitive …, 2020"],"snippet":"… The experiment for the proposed model has followed with BERT, to BookCorpus [35] and Wikipedia for English as an initializing point for pretraining. Similarly, other text includes, Giga5 (16Gb) ClueWeb 2012-B and Common Crawl respectively …","url":["https://journals.iium.edu.my/kict/index.php/IJPCC/article/download/134/96"]}
{"year":"2020","title":"Machine Translation Reference-less Evaluation using YiSi-2 with Bilingual Mappings of Massive Multilingual Language Model","authors":["C Lo, S Larkin - Proceedings of the Fifth Conference on Machine …, 2020"],"snippet":"… The differencesbetweenXLM-RandBERTare1)XLM-Ris trained on the CommonCrawl corpus which is significantly larger than the Wikipedia training data used by BERT; 2) instead of a uniform data sampling rate used in BERT …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.100.pdf"]}
{"year":"2020","title":"Machine Translation System Selection from Bandit Feedback","authors":["J Naradowsky, X Zhang, K Duh - arXiv preprint arXiv:2002.09646, 2020"],"snippet":"… Specifically, we in- clude OpenSubtitles2018 [Lison and Tiedemann, 2016] and WMT 2017 [Bojar et al., 2017], which contains data from eg parliamentary proceedings (Europarl, UN), political/economic news, and web-crawled parallel corpus (Common Crawl) …","url":["https://arxiv.org/pdf/2002.09646"]}
{"year":"2020","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer","authors":["J Pfeiffer, I Vulić, I Gurevych, S Ruder - arXiv preprint arXiv:2005.00052, 2020"],"snippet":"… It is a Transformer-based model pretrained for one hundred languages on large cleaned Common Crawl corpora (Wenzek et al., 2019). For efficiency purposes, we use the XLM-R Base configuration as the basis for all of our experiments …","url":["https://arxiv.org/pdf/2005.00052"]}
{"year":"2020","title":"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation","authors":["N Reimers, I Gurevych - arXiv preprint arXiv:2004.09813, 2020"],"snippet":"Page 1. Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation Nils Reimers and Iryna Gurevych Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science …","url":["https://arxiv.org/pdf/2004.09813"]}
{"year":"2020","title":"MantisTable SE: an Efficient Approach for the Semantic Table Interpretation","authors":["M Cremaschi, R Avogadro, A Barazzetti, D Chieregato - Semantic Web Challenge on …, 2020"],"snippet":"… Web Tables: The WebTables system [1] extracts 14.1 billion HTML tables and finds 154 million are high-quality tables (1.1%); – Web Tables: Lehmberg et al. [5] extract 233 million content tables from Common Crawl 2015 (2.25 …","url":["http://ceur-ws.org/Vol-2775/paper8.pdf"]}
{"year":"2020","title":"Mapping crime descriptions to law articles using deep learning","authors":["M Vink, N Netten, MS Bargh, S van den Braak… - Proceedings of the 13th …, 2020"],"snippet":"… This is a popular word embedding created by Facebook and available in many languages. The FastText embeddings are trained on Wikipedia texts and the data from the common crawl project. The word embeddings have a vector size of 300 …","url":["https://dl.acm.org/doi/abs/10.1145/3428502.3428507"]}
{"year":"2020","title":"Mapping Languages: The Corpus of Global Language Use","authors":["J Dunn"],"snippet":"… language) and 156 countries (again with over 1 million words from each country), all distilled from Common Crawl web data … region: (i) the number of sites indexed by the Common Crawl; (ii) the population's degree of access …","url":["https://publicdata.canterbury.ac.nz/Research/Geocorpus/Documentation/!Paper.Corpus_of_Global_Language_Use.pdf"]}
{"year":"2020","title":"Mapping the market for remanufacturing: An application of “Big Data” analytics","authors":["JQF Netoa, M Dutordoira - International Journal of Production Economics, 2020"],"snippet":"… The vectors are created with Global Vectors for Word Representation (GloVe), one of the most well-known word embedding methods (Pennington et al., 2014), and are based on a data set obtained from Common Crawl, a nonprofit …","url":["https://www.sciencedirect.com/science/article/pii/S092552732030181X"]}
{"year":"2020","title":"MASK: A flexible framework to facilitate de-identification of clinical texts","authors":["N Milosevic, G Kalappa, H Dadafarin, M Azimaee… - arXiv preprint arXiv …, 2020"],"snippet":"… The first of these ap- proaches, used GLoVe (Global Vector) word embeddings [14]. We used GLoVe embeddings trained on common crawl data containing 840 billion tokens, 2.2 million unique tokens in vocabulary, and 300dimensional vectors …","url":["https://arxiv.org/pdf/2005.11687"]}
{"year":"2020","title":"Masked ELMo: An evolution of ELMo towards fully contextual RNN language models","authors":["G Senay, E Salin - arXiv preprint arXiv:2010.04302, 2020"],"snippet":"… It should be noted that ELMo 5.5B is trained on a larger corpus than ELMo and Masked ELMo (Wikipedia: 1.9B and the common crawl from WMT 2008-2012: 3.6B). Moreover, a BERT baseline (BERT*) trained on the same …","url":["https://arxiv.org/pdf/2010.04302"]}
{"year":"2020","title":"Massive vs. Curated Embeddings for Low-Resourced Languages: the Case of Yorùbá and Twi","authors":["J Alabi, K Amponsah-Kaakyire, D Adelani… - Proceedings of The 12th …, 2020"],"snippet":"… The resource par excellence is Wikipedia2, an online encyclopedia currently available in 307 languages3. Other initiatives such as Common Crawl4 or the Jehovahs Witnesses site5 are also repositories for multilingual …","url":["https://www.aclweb.org/anthology/2020.lrec-1.335.pdf"]}
{"year":"2020","title":"Massively Multilingual Document Alignment with Cross-lingual Sentence-Mover's Distance","authors":["A El-Kishky, F Guzmán - arXiv preprint arXiv:2002.00761, 2020"],"snippet":"… selected for evaluation. Baseline Methods. For comparison, we implemented two existing and intuitive document scoring baselines previously evaluated on this URL-Aligned CommonCrawl dataset [11]. The first method dubbed …","url":["https://arxiv.org/pdf/2002.00761"]}
{"year":"2020","title":"Master Thesis: Developing a Cross-Lingual Named Entity Recognition Model","authors":["J Podolak, P Zeinert - 2020"],"snippet":"Page 1. Master Thesis: Developing a Cross-Lingual Named Entity Recognition Model Jowita Podolak1, Philine Zeinert2 1jopo@itu.dk 2phze@itu.dk June 1, 2020 Course Code: KISPECI1SE Page 2. Abstract To build a Cross …","url":["https://www.derczynski.com/itu/docs/xling-ner_jopo_phze.pdf"]}
{"year":"2020","title":"Matching Job Applicants to Free Text Job Ads Using Semantic Networks and Natural Language Inference","authors":["A Thun - 2020"],"snippet":"Page 1. IN DEGREE PROJECT COMPUTER SCIENCE AND ENGINEERING, SECOND CYCLE, 30 CREDITS , STOCKHOLM SWEDEN 2020 Matching Job Applicants to Free Text Job Ads Using Semantic Networks and Natural Language Inference ANTON THUN …","url":["https://www.diva-portal.org/smash/get/diva2:1467916/FULLTEXT01.pdf"]}
{"year":"2020","title":"MDD@ AMI: Vanilla Classifiers for Misogyny Identification","authors":["S El Abassi, S Nisioi - Proceedings of Sixth Evaluation Campaign of Natural …, 2020"],"snippet":"… CBOW embeddings pre-trained on Wikipedia and OSCAR (Common Crawl)3. The second run is trained on English glove em- beddings that surprisingly contain the representation of more than half of our Italian …","url":["http://ceur-ws.org/Vol-2765/paper149.pdf"]}
{"year":"2020","title":"MEASURING DIVERGENT THINKING ORIGINALITY WITH HUMAN RATERS AND TEXT-MINING MODELS: A PSYCHOMETRIC COMPARISON OF METHODS","authors":["D Dumasa, P Organisciaka, M Dohertyb"],"snippet":"Page 1. Running Head: MEASURING ORIGINALITY 1 MEASURING DIVERGENT THINKING ORIGINALITY WITH HUMAN RATERS AND TEXT-MINING MODELS: A PSYCHOMETRIC COMPARISON OF METHODS …","url":["https://www.researchgate.net/profile/Denis_Dumas/publication/339364072_Measuring_Divergent_Thinking_Originality_with_Human_Raters_and_Text-Mining_Models_A_Psychometric_Comparison_of_Methods/links/5e4d686892851c7f7f46b607/Measuring-Divergent-Thinking-Originality-with-Human-Raters-and-Text-Mining-Models-A-Psychometric-Comparison-of-Methods.pdf"]}
{"year":"2020","title":"Measuring prominence of scientific work in online news as a proxy for impact","authors":["J Ravenscroft, A Clare, M Liakata - arXiv preprint arXiv:2007.14454, 2020"],"snippet":"… In our task we employ pre-trained GloVe4 feature embeddings trained on the Common Crawl dataset5, a multi-petabyte archive of content scraped from the world wide web containing 42 billion tokens and a vocabulary 1.9 million words …","url":["https://arxiv.org/pdf/2007.14454"]}
{"year":"2020","title":"Media-Analytics. org: A Resource to Research Language Usage by News Media Outlets","authors":["D Rozado - ITM Web of Conferences, 2020"],"snippet":"… news media outlets. News articles textual content are available in outlet-specific domains and Internet cache repositories such as the Internet Archive Wayback Machine, Google Cache and Common Crawl. Articles' headlines …","url":["https://www.itm-conferences.org/articles/itmconf/pdf/2020/03/itmconf_ictessh2020_03004.pdf"]}
{"year":"2020","title":"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?","authors":["S Hisamoto, M Post, K Duh"],"snippet":"… Page 3. CommonCrawl subcorpus … We now describe how Carol prepares the data for Alice and Bob. First, Carol se- lects 4 subcorpora for the training data of Al- ice, namely CommonCrawl, Europarl v7, News Commentary v13, and Rapid 2016 …","url":["http://www.cs.jhu.edu/~kevinduh/t/membership-inference.pdf"]}
{"year":"2020","title":"Meta-Learning for Few-Shot NMT Adaptation","authors":["A Sharaf, H Hassan, H Daumé III - arXiv preprint arXiv:2004.02745, 2020"],"snippet":"… In all cases, the baseline machine translation system is a neural English to German (En-De) transformer model (Vaswani et al., 2017), initially trained on 5.2M sentences filtered from the the standard parallel data …","url":["https://arxiv.org/pdf/2004.02745"]}
{"year":"2020","title":"Method and apparatus for improved automatic subtitle segmentation using an artificial neural network model","authors":["P WILKEN, E Matusov - US Patent App. 16/876,780, 2020"],"snippet":"… These data included all other publicly available training data, including ParaCrawl, CommonCrawl, EUbookshop, JRCAcquis, EMEA, and other corpora from the OPUS collection … This may be done to avoid oversampling …","url":["https://patentimages.storage.googleapis.com/d1/56/2b/7b6e0c087c851d/US20200364402A1.pdf"]}
{"year":"2020","title":"Method and system for interactive keyword optimization for opaque search engines","authors":["R Puzis, A ELYASHAR, M REUBEN - US Patent App. 16/840,538, 2020"],"snippet":"… 2018]. The model was trained on Common Crawl (http://commoncrawl.org/) and Wikipedia (https://www.wikipedia.org/) using fastText library (https://fasttext.cc/). For the distance measure, the simple Euclidean distance was used …","url":["https://patents.google.com/patent/US20200327120A1/en"]}
{"year":"2020","title":"Method for automatically generating a wrapper for extracting web data, and a computer system","authors":["G Gottlob, E SALLINGER, R FAYZRAKHMANOV… - US Patent App. 16/630,485, 2020"],"snippet":"US20200167393A1 - Method for automatically generating a wrapper for extracting web data, and a computer system - Google Patents. Method for automatically generating a wrapper for extracting web data, and a computer system. Download PDF Info …","url":["https://patents.google.com/patent/US20200167393A1/en"]}
{"year":"2020","title":"Methods for morphology learning in low (er)-resource scenarios","authors":["T Bergmanis - 2020"],"snippet":"Page 1. This thesis has been submitted in fulfilment of the requirements for a postgraduate degree (eg PhD, MPhil, DClinPsychol) at the University of Edinburgh. Please note the following terms and conditions of use: This work …","url":["https://era.ed.ac.uk/bitstream/handle/1842/37115/Bergmanis2020_Redacted.pdf?sequence=3&isAllowed=y"]}
{"year":"2020","title":"Metrics and tools for exploring toxicity in social media","authors":["PMFN da Silva - 2020"],"snippet":"Page 1. FACULDADE DE ENGENHARIA DA UNIVERSIDADE DO PORTO Metrics and tools for exploring toxicity in social media Pedro Silva Mestrado Integrado em Engenharia Informática e Computação Supervisor: Sérgio …","url":["https://repositorio-aberto.up.pt/bitstream/10216/128545/2/412412.pdf"]}
{"year":"2020","title":"Mis-shapes, Mistakes, Misfits: An Analysis of Domain Classification Services","authors":["P Vallina, V Le Pochat, Á Feal, M Paraschiv, J Gamba…"],"snippet":"… service. Their popularity is further reflected by the fact that 47% of the 4.4M domains are indexed in the Chrome User Experience Report [71] and 0.5% by Common Crawl [72], both generated between August and October 2019 …","url":["https://lepoch.at/files/domain-classification-imc20.pdf"]}
{"year":"2020","title":"Mitigating Bias in Deep Nets with Knowledge Bases: the Case of Natural Language Understanding for Robots","authors":["M Mensio, E Bastianelli, I Tiddi, G Rizzo"],"snippet":"… The connections in green represent highway connections be- tween the first and the third layer. over the Common Crawl resource3 … For this reason, it can intrinsically provide an ex- planation for the model behavior, as it summarizes a much 3http://commoncrawl.org …","url":["http://ceur-ws.org/Vol-2600/paper20.pdf"]}
{"year":"2020","title":"Mitigating Gender Bias in Machine Learning Data Sets","authors":["S Leavy, G Meaney, K Wade, D Greene - arXiv preprint arXiv:2005.06898, 2020"],"snippet":"… evaluation of system accuracy and learned associations in machine learning technologies that underlie many search and recommendation systems [9]. Implicit Association Tests (IATs) were found to be effective in uncovering …","url":["https://arxiv.org/pdf/2005.06898"]}
{"year":"2020","title":"Modeling Recurring Concepts in Single-label and Multi-label Streams","authors":["Z Ahmadi"],"snippet":"Page 1. Modeling Recurring Concepts in Single-label and Multi-label Streams A thesis submitted for the degree of DN at the Department of Physics, Mathematics and Computer Science at the Johannes …","url":["https://publications.ub.uni-mainz.de/theses/volltexte/2019/100003220/pdf/100003220.pdf"]}
{"year":"2020","title":"Modeling remotely collected speech data: Applications for psychiatry","authors":["TB Holmlund - 2020"],"snippet":"… To base the analysis on a corpus with a wide variety of animal-word sources, we used a set of pretrained word vectors calculated from approximately 42 billion tokens from the entire internet, courtesy of the Common Crawl project (Pennington et al., 2014) …","url":["https://munin.uit.no/bitstream/handle/10037/17098/paper_III.pdf?sequence=8"]}
{"year":"2020","title":"Modeling the Music Genre Perception across Language-Bound Cultures","authors":["EV Epure, G Salha, M Moussallam, R Hennequin - arXiv preprint arXiv:2010.06325, 2020"],"snippet":"… representations as described next. Multilingual Static Word Embeddings. The classical word embeddings we study are the multilingual fastText word vectors trained on Wikipedia and Common Crawl (Grave et al., 2018). The model is an …","url":["https://arxiv.org/pdf/2010.06325"]}
{"year":"2020","title":"Modeling Word Formation in English–German Neural Machine Translation","authors":["M Weller-Di Marco, A Fraser - Proceedings of the 58th Annual Meeting of the …, 2020"],"snippet":"… We compare four training settings: small (248,730 sentences: newscommentary), large2M (1,956,444 sentences: Europarl + news-commentary), large4M (4,116,215 sentences: Europarl + news-commentary + …","url":["https://www.aclweb.org/anthology/2020.acl-main.389.pdf"]}
{"year":"2020","title":"Moral Concerns are Differentially Observable in Language","authors":["B Kennedy, M Atari, AM Davani, J Hoover, A Omrani… - 2020"],"snippet":"… lexical semantic relatedness. We compute text representations by averaging GloVe word embedding vectors (Pennington et al., 2014) which were trained on text from the Common Crawl3. In 2 https://github.com/lda-project/lda …","url":["https://psyarxiv.com/uqmty/download?format=pdf"]}
{"year":"2020","title":"Moral Framing and Ideological Bias of News","authors":["K Lerman - … : 12th International Conference, SocInfo 2020, Pisa …","N Mokhberian, A Abeliuk, P Cummings, K Lerman - arXiv preprint arXiv:2009.12979, 2020"],"snippet":"… then and V the m− denote semantic to axis set corresponding to this MF dimension is: Am= mean (V m+)− mean (V m−)(1) For the computations of this part, the embeddings of words are obtained from the pretrained GloVe …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=6tIBEAAAQBAJ&oi=fnd&pg=PA206&dq=commoncrawl&ots=3yMGfdMsr5&sig=MyXXhdRNotNI087l9_nvaNHNjhI","https://arxiv.org/pdf/2009.12979"]}
{"year":"2020","title":"Morphological and pseudomorphological effects in English visual word processing: How much can we attribute the statistical structure of the language?","authors":["P Stevens, D Plaut"],"snippet":"… Real-valued 300dimensional semantic vectors generated from the Common Crawl internet text corpus were converted to 200-dimensional binary vectors using a binary multidimensional scaling algorithm (Rohde, 2002) …","url":["https://cognitivesciencesociety.org/cogsci20/papers/0399/0399.pdf"]}
{"year":"2020","title":"Morphological Skip-Gram: Using morphological knowledge to improve word representation","authors":["F Santos, H Macedo, T Bispo, C Zanchetting - arXiv preprint arXiv:2007.10055, 2020"],"snippet":"… Keeping the quality of word embeddings and decreasing training time is very important because usually, a corpus to training embeddings is composed of 1B tokens. For example, the Common Crawl corpora contain 820B tokens …","url":["https://arxiv.org/pdf/2007.10055"]}
{"year":"2020","title":"mT5: A massively multilingual pre-trained text-to-text transformer","authors":["A Roberts, A Barua, A Siddhant, C Raffel, L Xue… - 2021","L Xue, N Constant, A Roberts, M Kale, R Al-Rfou… - arXiv preprint arXiv …, 2020"],"snippet":"… wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified …","url":["https://arxiv.org/pdf/2010.11934","https://research.google/pubs/pub50316/"]}
{"year":"2020","title":"Multi-Label Sentiment Analysis on 100 Languages with Dynamic Weighting for Label Imbalance","authors":["SF Yilmaz, EB Kaynak, A Koç, H Dibeklioğlu, SS Kozat - arXiv preprint arXiv …, 2020"],"snippet":"… 2, we use XLM-RoBERTa pretrained tokenizer and pretrained model [17]. XLM-RoBERTa is pretrained on CommonCrawl corpora of 100 different languages. We first tokenize the input sentence si into subword units via …","url":["https://arxiv.org/pdf/2008.11573"]}
{"year":"2020","title":"Multi-model transfer and optimization for cloze task","authors":["J Tang, L Ling, C Ma, H Zhang, J Huang - … on Artificial Intelligence and Robotics 2020, 2020"],"snippet":"… Transformer Enc. PLM ≈ BERT WikiEn+BookCorpus+Giga5 +ClueWeb+Common Crawl RoBERTa Transformer Enc. MLM 355M BookCorpus+CCNews +OpenWebText+STORIES XLM-R Transformer Enc. MLM 550M CommonCrawl ALBERT Transformer Enc …","url":["https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11574/115740T/Multi-model-transfer-and-optimization-for-cloze-task/10.1117/12.2579412.short"]}
{"year":"2020","title":"Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity","authors":["I Vulić, S Baker, EM Ponti, U Petti, I Leviant, K Wing… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity https://multisimlex.com/ Ivan Vulic ∗♠ LTL, University of Cambridge Simon Baker ∗♠ LTL, University of Cambridge …","url":["https://arxiv.org/pdf/2003.04866"]}
{"year":"2020","title":"Multilingual AMR-to-Text Generation","authors":["A Fan, C Gardent - Proceedings of the 2020 Conference on Empirical …, 2020"],"snippet":"… 4.1 Data Pretraining For encoder pretraining on silver AMR, we take thirty million sentences from the English portion of CCNET 2 (Wenzek et al., 2019), a cleaned version of Common Crawl (an open source version of the web) …","url":["https://www.aclweb.org/anthology/2020.emnlp-main.231.pdf"]}
{"year":"2020","title":"Multilingual Denoising Pre-training for Neural Machine Translation","authors":["Y Liu, J Gu, N Goyal, X Li, S Edunov, M Ghazvininejad… - arXiv preprint arXiv …, 2020"],"snippet":"… 2 Multilingual Denoising Pre-training We use a large-scale common crawl (CC) corpus (§2.1) to pre-train BART models (§2.2). Our ex- periments in the later sections involve finetuning a range of models pre-trained on different subsets of the CC languages §2.3) …","url":["https://arxiv.org/pdf/2001.08210"]}
{"year":"2020","title":"Multilingual Dependency Parsing from Universal Dependencies to Sesame Street","authors":["J Nivre - International Conference on Text, Speech, and …, 2020"],"snippet":"… pre-trained models provided by Che et al. [3], who train ELMo on 20 million words randomly sampled from raw WikiDump and Common Crawl datasets for 44 languages. For BERT, we employ the pretrained multilingual cased …","url":["https://link.springer.com/chapter/10.1007/978-3-030-58323-1_2"]}
{"year":"2020","title":"Multilingual Factual Knowledge Retrieval from Pretrained Language Models","authors":["Z Jiang, A Anastasopoulos, J Araki, H Ding, G Neubig - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models Zhengbao Jiang†, Antonios Anastasopoulos♣,∗, Jun Araki‡, Haibo Ding‡, Graham Neubig† †Languages Technologies Institute …","url":["https://arxiv.org/pdf/2010.06189"]}
{"year":"2020","title":"Multilingual Legal Information Retrieval System for Mapping Recitals and Normative Provisions","authors":["AK JOHN - Legal Knowledge and Information Systems: JURIX …, 2020"],"snippet":"… cc/docs/en/crawl-vectors. html): pre-trained on Common Crawl and Wikipedia. We used the word-average method, which divides the sum of the word embeddings in a legal norm by the norm length. The embedding di- mension size was set to 128 …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=fy4NEAAAQBAJ&oi=fnd&pg=PA123&dq=commoncrawl&ots=BUOuNY0sH2&sig=nWyRhj_URXdbdS30qkO5Z2kfHcY"]}
{"year":"2020","title":"Multilingual Probing Tasks for Word Representations","authors":["G Gül Şahin, C Vania, I Kuznetsov, I Gurevych - Computational Linguistics"],"snippet":"Page 1. Computational Linguistics Just Accepted MS. https://doi.org/10.1162/ COLI_a_00376 © Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license LINSPECTOR Multilingual Probing Tasks for Word Representations …","url":["https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00376"]}
{"year":"2020","title":"Multilingual Stance Detection in Tweets: The Catalonia Independence Corpus","authors":["E Zotova, R Agerri, M Nuñez, G Rigau - … of The 12th Language Resources and …, 2020"],"snippet":"… Initial experimentation showed that the Common Crawl5 models performed better for our particular task. The Common Crawl models are trained using a Continuous Bag-of-Words (CBOW) architecture with position-weights and …","url":["https://www.aclweb.org/anthology/2020.lrec-1.171.pdf"]}
{"year":"2020","title":"Multilingual Stance Detection: The Catalonia Independence Corpus","authors":["E Zotova, R Agerri, M Nuñez, G Rigau - arXiv preprint arXiv:2004.00050, 2020"],"snippet":"… task. The Common Crawl models are trained using a Continuous Bag-of-Words (CBOW) architecture with position-weights and 300 di- mensions on a vocabulary of 2M words … information. 5http://commoncrawl.org/ Page 4. 3.5 …","url":["https://arxiv.org/pdf/2004.00050"]}
{"year":"2020","title":"Multilingual Translation with Extensible Multilingual Pretraining and Finetuning","authors":["Y Tang, C Tran, X Li, PJ Chen, N Goyal, V Chaudhary… - arXiv preprint arXiv …, 2020"],"snippet":"… challenging to train from scratch. In contrast, monolingual data exists even for low resource languages, particularly in resources such as Wikipedia or Commoncrawl, a version of the web. Thus, leveraging this monolingual …","url":["https://arxiv.org/pdf/2008.00401"]}
{"year":"2020","title":"Multilingual Unsupervised Sentence Simplification","authors":["L Martin, A Fan, É de la Clergerie, A Bordes, B Sagot - arXiv preprint arXiv …, 2020"],"snippet":"… We mine a large quantity of paraphrases from the Common Crawl using libraries such as LASER (Artetxe et al., 2018) and faiss (Johnson et al … CCNET is an extraction of Common Crawl,2 an open source snapshot of the web …","url":["https://arxiv.org/pdf/2005.00352"]}
{"year":"2020","title":"Multilingual Zero-shot Constituency Parsing","authors":["T Kim, S Lee - arXiv preprint arXiv:2004.13805, 2020"],"snippet":"Page 1. Multilingual Zero-shot Constituency Parsing Taeuk Kim and Sang-goo Lee Department of Computer Science and Engineering Seoul National University, Seoul, Korea {taeuk,sglee}@europa.snu.ac.kr Abstract Zero-shot …","url":["https://arxiv.org/pdf/2004.13805"]}
{"year":"2020","title":"MultiMix: A Robust Data Augmentation Strategy for Cross-Lingual NLP","authors":["MS Bari, MT Mohiuddin, S Joty - arXiv preprint arXiv:2004.13240, 2020"],"snippet":"… samples around each selected sample. XLM-R is a multilingual LM that is trained on massive multilingual corpora (2.5 TB of refined Common-Crawl data in 100 languages) with a masked LM (MLM) objective. We chose XLM-R …","url":["https://arxiv.org/pdf/2004.13240"]}
{"year":"2020","title":"Multiple Knowledge GraphDB (MKGDB)","authors":["S Faralli, P Velardi, F Yusifli - Proceedings of The 12th Language Resources and …, 2020"],"snippet":"… Crawl11 Web corpus. 7https://www.objectivity.com/products/ thingspan/ thingspanfeatures/ 8https://titan.thinkaurelius.com/ 9https://neo4j.com/ 10the Neo4j platform provides an interface for the development and inclusion of …","url":["https://www.aclweb.org/anthology/2020.lrec-1.283.pdf"]}
{"year":"2020","title":"Multiscale System for Alzheimer's Dementia Recognition through Spontaneous Speech","authors":["E Edwards, C Dognin, B Bollepalli, M Singh…"],"snippet":"… Deep Random Forest Setting: We extract features using three pre-trained embeddings: Word2Vec (CBOW) with subword information [29] (pre-trained on Common Crawl), GloVe [30] pre-trained on Common Crawl and Sent2Vec …","url":["https://indico2.conference4me.psnc.pl/event/35/contributions/3302/attachments/1227/1271/Wed-SS-1-6-9.pdf"]}
{"year":"2020","title":"MuSe 2020 Challenge and Workshop: Multimodal Sentiment Analysis, Emotion-target Engagement and Trustworthiness Detection in Real-life Media: Emotional Car …","authors":["L Stappen, A Baird, G Rizos, P Tzirakis, X Du, F Hafner… - Proceedings of the 1st …, 2020"],"snippet":"… 4.3 Language FastText [5] is a library for efficient learning of word embeddings. It is based on the skipgram model where a vector representation is associated to each character n-gram. The model is trained on the English Common Crawl corpus (600B tokens) …","url":["https://dl.acm.org/doi/abs/10.1145/3423327.3423673"]}
{"year":"2020","title":"MuSe 2020--The First International Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop","authors":["L Stappen, A Baird, G Rizos, P Tzirakis, X Du, F Hafner… - arXiv preprint arXiv …, 2020"],"snippet":"… 4.3 Language FastText [5] is a library for efficient learning of word embeddings. It is based on the skipgram model where a vector representation is associated to each character n-gram. The model is trained on the English Common Crawl corpus (600B tokens) …","url":["https://arxiv.org/pdf/2004.14858"]}
{"year":"2020","title":"MWPD2020: Semantic Web Challenge on Mining the Web of HTML-embedded Product Data","authors":["Z Zhang, C Bizer, R Peeters, A Primpeli"],"snippet":"Page 1. MWPD2020: Semantic Web Challenge on Mining the Web of HTML-embedded Product Data Ziqi Zhang1[0000−0002−8587−8618], Christian Bizer2[0000−0003−2367−0237], Ralph Peeters2[0000−0003 …","url":["http://ceur-ws.org/Vol-2720/paper1.pdf"]}
{"year":"2020","title":"Névszói kötőhangzók variabilitásának korpuszalapú vizsgálata Corpus-based analysis of the variability of linking vowels in nouns and adjectives","authors":["R Péter, L Dániel"],"snippet":"… 3.1 Corpus The corpus on which we conducted our measurements is the prepublished version of the Webcorpus 2 (Nemeskey, 2020). It is based on the Common Crawl webcorpus, which is a collection of pages …","url":["https://hlt.bme.hu/media/pdf/thesis_levai_ma.pdf"]}
{"year":"2020","title":"Named Entity Recognition for Code-Mixed Indian Corpus using Meta Embedding","authors":["R Priyadharshini, BR Chakravarthi, M Vegupatti… - 2020 6th International …, 2020"],"snippet":"… IV. EXPERIMENT We use FastText word embedding trained from Common Crawl and Wikipedia [30] for English and Hindi-Devanagari script (native script for Hindi). We also add the English Twitter GloVe word embeddings since the NER data is from Twitter …","url":["https://ieeexplore.ieee.org/abstract/document/9074379/"]}
{"year":"2020","title":"Naming unrelated words reliably predicts creativity","authors":["JA Olson, J Nahas, D Chmoulevitch, ME Webb - PsyArXiv. December, 2020"],"snippet":"… We trained the GloVe model with the Common Crawl corpus, which contains the text of billions of web pages … We chose the GloVe algorithm and the Common Crawl corpus; this combination correlates best with …","url":["https://psyarxiv.com/qvg8b/download/?format=pdf"]}
{"year":"2020","title":"Narrative Origin Classification of Israeli-Palestinian Conflict Texts","authors":["J Wei, E Santos Jr - The Thirty-Third International Flairs Conference, 2020"],"snippet":"… For training and testing, we converted text inputs into nu- merical representations using 300-dimensional distributed embeddings pre-trained on the Common Crawl database with the GloVe method (Pennington, Socher, and Manning 2014) …","url":["https://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS20/paper/download/18443/17596"]}
{"year":"2020","title":"Natural Language Correction-Thesis Proposal","authors":["J Náplava"],"snippet":"… considered. This table displays top 15 languages with worst results on dictionary baseline. a new pipeline utilizing both clean data from Wikipedia and also not that clean data from general web (utilizing CommonCrawl corpus). The …","url":["http://ufal.mff.cuni.cz/~zabokrtsky/pgs/thesis_proposal/jakub-naplava-proposal.pdf"]}
{"year":"2020","title":"Natural Language Generation Using Transformer Network in an Open-Domain Setting","authors":["AAM Gopinath, P Bhattacharyya","D Varshney, A Ekbal, GP Nagaraja, M Tiwari… - International Conference on …, 2020"],"snippet":"… The embeddings used in our model are trained on Common Crawl dataset with 840B tokens and 2.2M vocab. We use 300-dimensional sized vectors. 3.3 Baseline Models. We formulate our task of response generation as a machine translation problem …","url":["https://link.springer.com/chapter/10.1007/978-3-030-51310-8_8","https://www.researchgate.net/profile/Deeksha-Varshney/publication/342238636_Natural_Language_Generation_Using_Transformer_Network_in_an_Open-Domain_Setting/links/60432b74299bf1e0785aff2f/Natural-Language-Generation-Using-Transformer-Network-in-an-Open-Domain-Setting.pdf"]}
{"year":"2020","title":"Natural Language Processing (NLP) and Text Analytics","authors":["JM Patel - Getting Structured Data from the Internet, 2020"],"snippet":"In the preceding chapters, we have solely relied on the structure of the HTML documents themselves to scrape information from them, and that is a powerful method to extract information.","url":["https://link.springer.com/chapter/10.1007/978-1-4842-6576-5_4"]}
{"year":"2020","title":"Natural Language Processing Model for Managing Maintenance Requests in Buildings","authors":["Y Bouabdallaoui, Z Lafhaj, P Yim, L Ducoulombier… - Buildings, 2020"],"snippet":"… In order to overcome the limited corpus of vocabulary in our dataset, a pre-trained word embedding was used. It is based on the FastText model [44] and trained on a large corpus of French vocabulary from Wikipedia and Common Crawl [45] …","url":["https://www.mdpi.com/2075-5309/10/9/160/pdf"]}
{"year":"2020","title":"Natural Language Transfer Learning for Physiological Textual Similarity","authors":["V Awatramani, P Gupta - 2020 10th International Conference on Cloud …, 2020"],"snippet":"… Moreover, RoBERTa is trained over 160 GB of text that includes English Wikipedia and BooksCorporus used earlier in BERT and additionally, CommonCrawl News (CC-NEWS) dataset consisting of 63 million articles …","url":["https://ieeexplore.ieee.org/abstract/document/9058216/"]}
{"year":"2020","title":"Naver Labs Europe's Participation in the Robustness, Chat, and Biomedical Tasks at WMT 2020","authors":["A Bérard, V Nikoulina, I Calapodescu, J Philip - … of the Fifth Conference on Machine …, 2020"],"snippet":"… Corpus Sents Docs Paracrawl 33.9M – Rapid2019 965k 48.3k Europarl 1.75M 6.7k Commoncrawl 1.97M – Wikimatrix 5.68M – Wikititles … of large monolingual English and German datasets (100M lines in total per language …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.57.pdf"]}
{"year":"2020","title":"Nearest Neighbor Machine Translation","authors":["U Khandelwal, A Fan, D Jurafsky, L Zettlemoyer… - arXiv preprint arXiv …, 2020"],"snippet":"… The parallel sentences are mined from cleaned monolingual commoncrawl data created using the ccNet pipeline (Wenzek et al., 2019) … with only the kNN distribution (λ = 1) with beam size 1, retrieving k = 8 neighbors from …","url":["https://arxiv.org/pdf/2010.00710"]}
{"year":"2020","title":"NestMSA: a new multiple sequence alignment algorithm","authors":["M Kayed, AA Elngar - The Journal of Supercomputing, 2020"],"snippet":"Multiple sequence alignment (MSA) is a core problem in many applications. Various optimization algorithms such as genetic algorithm and particle swarm opti.","url":["https://link.springer.com/article/10.1007/s11227-020-03206-0"]}
{"year":"2020","title":"Neural Aspect-based Text Generation","authors":["H Hayashi - 2020"],"snippet":"Page 1. November 25, 2020 DRAFT Thesis Proposal Neural Aspect-based Text Generation Hiroaki Hayashi November 25, 2020 Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15123 Thesis Committee …","url":["https://hiroakih.me/thesis_proposal.pdf"]}
{"year":"2020","title":"Neural Databases","authors":["J Thorne, M Yazdani, M Saeidi, F Silvestri, S Riedel… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Neural Databases James Thorne University of Cambridge Facebook AI jt719@cam.ac.uk Majid Yazdani Facebook AI myazdani@fb.com Marzieh Saeidi Facebook AI marzieh@fb.com Fabrizio Silvestri Facebook AI fsilvestri@fb.com …","url":["https://arxiv.org/pdf/2010.06973"]}
{"year":"2020","title":"NEURAL MACHINE TRANSLATION WITH UNIVERSAL VISUAL REPRESENTATION","authors":["Z Li, H Zhao"],"snippet":"… We used newsdev2016 as the dev set and newstest2016 as the test set. 2) For the EN-DE translation task, 4.43M bilingual sentence pairs of the WMT14 dataset were used as training data, including Common Crawl, News Commentary, and Europarl v7 …","url":["https://www.researchgate.net/profile/Zhuosheng_Zhang4/publication/339375656_Neural_Machine_Translation_with_Universal_Visual_Representation/links/5e4e27a4299bf1cdb938db20/Neural-Machine-Translation-with-Universal-Visual-Representation.pdf"]}
{"year":"2020","title":"Neural Simultaneous Speech Translation Using Alignment-Based Chunking","authors":["P Wilken, T Alkhouli, E Matusov, P Golik - arXiv preprint arXiv:2005.14489, 2020"],"snippet":"Page 1. arXiv:2005.14489v1 [cs.CL] 29 May 2020 Neural Simultaneous Speech Translation Using Alignment-Based Chunking Patrick Wilken, Tamer Alkhouli, Evgeny Matusov, Pavel Golik Applications Technology (AppTek), Aachen …","url":["https://arxiv.org/pdf/2005.14489"]}
{"year":"2020","title":"Neural Text Segmentation and Its Application to Sentiment Analysis","authors":["J Li, B Chiu, S Shang, L Shao - IEEE Transactions on Knowledge and Data …, 2020"],"snippet":"Page 1. 1041-4347 (c) 2020 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/9051834/"]}
{"year":"2020","title":"Neural Word Embeddings for Sentiment Analysis","authors":["B Naderalvojoud - 2020"],"snippet":"Page 1. 1 Page 2. NEURAL WORD EMBEDDINGS FOR SENTIMENT ANALYSIS DUYGU ANAL˙IZ˙I˙IC¸˙IN S˙IN˙IRSEL S¨OZC¨UK¨OZ YERLES¸˙IKLER˙I BEHZAD NADERALVOJOUD PROF. DR. EBRU SEZER Supervisor Submitted to …","url":["http://www.openaccess.hacettepe.edu.tr:8080/xmlui/bitstream/handle/11655/22784/10350958.pdf?sequence=1"]}
{"year":"2020","title":"NewB: 200,000+ Sentences for Political Bias Detection","authors":["J Wei - arXiv preprint arXiv:2006.03051, 2020"],"snippet":"… When inputting sentences into the model, we tokenize each sentence at the word-level and convert each word into a vector using 300dimensional distributed embeddings trained on the Common Crawl database with the GloVe method (Pennington et al., 2014) …","url":["https://arxiv.org/pdf/2006.03051"]}
{"year":"2020","title":"News topic classification as a first step towards diverse news recommendation","authors":["O De Clercq, L De Bruyne, V Hoste - Computational Linguistics in the Netherlands …, 2020"],"snippet":"… The RobBERT model was trained on the Dutch part of the 39GB OSCAR corpus, a part of the Common Crawl corpus (Suárez et al. 2019). As sub-word token input, BERTje uses WordPiece, whereas RobBERT uses byte-level Byte Pair Encoding (BPE) …","url":["https://www.clinjournal.org/clinj/article/download/103/92"]}
{"year":"2020","title":"NLNDE at CANTEMIST: Neural Sequence Labeling and Parsing Approaches for Clinical Concept Extraction","authors":["L Lange, X Dai, H Adel, J Strötgen - arXiv preprint arXiv:2010.12322, 2020"],"snippet":"… In particular, we use pre-trained fastText embeddings [22] that were trained on articles from Wikipedia and the Common Crawl, as well as domain-speci c fastText embeddings [23] that were pretrained on articles of the Spanish …","url":["https://arxiv.org/pdf/2010.12322"]}
{"year":"2020","title":"NLP North at WNUT-2020 Task 2: Pre-training versus Ensembling for Detection of Informative COVID-19 English Tweets","authors":["AG Møller, R van der Goot, B Plank - Proceedings of the 6th Workshop on Noisy …, 2020"],"snippet":"… twitter. ArXiv, abs/2005.07503. Sebastian Nagel. 2016. https://commoncrawl org/2016/10/news-dataset-available/. Dat Quoc Nguyen, Thanh Vu, Afshin Rahimi, Mai Hoang Dao, Linh The Nguyen, and Long Doan. 2020. WNUT …","url":["http://www.robvandergoot.com/doc/wnut2020.pdf"]}
{"year":"2020","title":"No computation without representation: Avoiding data and algorithm biases through diversity","authors":["C Kuhlman, L Jackson, R Chunara - arXiv preprint arXiv:2002.11836, 2020"],"snippet":"… Page 3. Dataset Description Sensitive Attribute race gender age other Adult: US Census income data [36]. [43, 57, 72, 77] [2, 3, 25, 27, 43, 57, 64, 77] [57] [2, 57] Common Crawl: Occupation biographies [42]. [34] Comm …","url":["https://arxiv.org/pdf/2002.11836"]}
{"year":"2020","title":"Noise Pollution in Hospital Readmission Prediction: Long Document Classification with Reinforcement Learning","authors":["L Xu, J Hogan, RE Patzer, JD Choi - arXiv preprint arXiv:2005.01259, 2020"],"snippet":"… Averaged Word Embedding For the averaged word embedding encoder (AWE; Section 4.2), em- beddings generated by FastText trained on the Common Crawl and the English Wikipedia with the 300 dimension is …","url":["https://arxiv.org/pdf/2005.01259"]}
{"year":"2020","title":"Not All Swear Words Are Used Equal: Attention over Word n-grams for Abusive Language Identification","authors":["HJ Jarquín-Vásquez, M Montes-y-Gómez… - Mexican Conference on …, 2020","L Villasenor-Pineda - Pattern Recognition: 12th Mexican Conference, MCPR …"],"snippet":"… On the other hand, for word representation we used pre-trained fastText embeddings [3], trained with subword information on Common Crawl. Table 2. Proposed attention-based deep neural network hyperparameters. Layer …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=UO_rDwAAQBAJ&oi=fnd&pg=PA282&dq=commoncrawl&ots=4BGirDzhwW&sig=0HRX8HVtoXlgmfN76pD6wgHh1_c","https://link.springer.com/chapter/10.1007/978-3-030-49076-8_27"]}
{"year":"2020","title":"NOTICE TO BORROWERS","authors":["C Li"],"snippet":"Page 1. DISTRIBUTION AGREEMENT In presenting this thesis as a partial fulfillment of the requirements for an advanced degree from Emory University, I agree that the Library of the University shall make it available for inspection …","url":["https://franklicm.github.io/files/thesis_final.pdf"]}
{"year":"2020","title":"Novel Entity Discovery from Web Tables","authors":["S Zhang, E Meij, K Balog, R Reinanda - arXiv preprint arXiv:2002.00206, 2020"],"snippet":"Page 1. Novel Entity Discovery from Web Tables Shuo Zhang Bloomberg London, United Kingdom szhang611@bloomberg.net Edgar Meij Bloomberg London, United Kingdom emeij@bloomberg.net Krisztian Balog University …","url":["https://arxiv.org/pdf/2002.00206"]}
{"year":"2020","title":"Novel Opinion mining System for Movie Reviews","authors":["AH AbdulHafiz - International Journal of Intelligent Systems and …, 2020"],"snippet":"… We have adopted the Word2Vec feature representation, the CSG in particular, in our work. It has a pre-trained word vector for the English language trained on 1 million common crawl and Wikipedia documents. Word2vec is a two-layer neural net …","url":["https://151.80.211.128/IJISAE/article/download/1090/621"]}
{"year":"2020","title":"NRC Systems for the 2020 Inuktitut–English News Translation Task","authors":["R Knowles, D Stewart, S Larkin, P Littell - Proceedings of the Fifth Conference on …, 2020"],"snippet":"… org/wmt20/translation-task.html Page 3. 157 Wiki Titles or Common Crawl Inuktitut data.9 We incorporated the news portion of the development data in training our models to alleviate the domain mismatch issue (Section 5.1). 4 Preprocessing and Postprocessing …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.13.pdf"]}
{"year":"2020","title":"NUIG-DSI at the WebNLG+ challenge: Leveraging Transfer Learning for RDF-to-text generation","authors":["N Pasricha, M Arcan, P Buitelaar - Proceedings of the 3rd WebNLG Workshop on …, 2020"],"snippet":"… middle) and reference lexicalisation (bottom). (Vaswani et al., 2017) and is pre-trained using un- supervised learning on a large corpus of unlabeled data obtained from the Web using the Common Crawl project. It is trained using …","url":["https://webnlg-challenge.loria.fr/files/2020.webnlg-papers.15.pdf"]}
{"year":"2020","title":"ODArchive–Creating an Archive for Structured Data from Open Data Portals","authors":["T Weber, J Mitöhner"],"snippet":"… We deem this project particularly useful as a resource for experiments on real-world structured data: to name an example, while large corpora of tabular data from Web tables have been made available via CommonCrawl [6], the …","url":["https://aic.ai.wu.ac.at/~polleres/publications/webe-etal-2020ISWC.pdf"]}
{"year":"2020","title":"On Finding Similar Verses from the Holy Quran using Word Embeddings","authors":["S Saeed, S Haider, Q Rajput - 2020 International Conference on Emerging Trends in …, 2020"],"snippet":"… It is an English multitask Convolution Neural Network (CNN)[4] trained on OntoNotes[14], with GloVe[11] vectors trained on Common Crawl[3]. It contains 685,000 keys, 20,000 unique words and each word …","url":["https://ieeexplore.ieee.org/abstract/document/9080691/"]}
{"year":"2020","title":"On Multilingual Word Embeddings & their applications in machine translation","authors":["N Jain"],"snippet":"… crosslingual signals. The hard/challenging dataset comprise of English-Italian, English-German, English-Finnish and English-Spanish pairs. These embeddings are trained on Wacky crawling corpora/common crawl corpora. We notice …","url":["https://naman-ntc.github.io/data/Seminar.pdf"]}
{"year":"2020","title":"On revealing shared conceptualization among open datasets","authors":["M Bogdanović, N Veljković, MF Gligorijević, D Puflović… - Journal of Web Semantics, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S1570826820300573"]}
{"year":"2020","title":"On the comparability of Pre-trained Language Models","authors":["M Aßenmacher, C Heumann - arXiv preprint arXiv:2001.00781, 2020"],"snippet":"… Wikipedia data sets are available in the Tensorflow Datasets-module3. CommonCrawl Among other resources, Yang et al. (2019) used data from CommonCrawl … CommonCrawl https://commoncrawl.org/ Unclear Fully available XLNet ClueWeb 2012-B Callan et al …","url":["https://arxiv.org/pdf/2001.00781"]}
{"year":"2020","title":"On the diminishing return of labeling clinical reports","authors":["JB Lamare, T Olatunji, L Yao - arXiv preprint arXiv:2010.14587, 2020"],"snippet":"… linear classifiers. Recent NLP ad- vance pushes the envelop much further by leveraging web-scale data – for instance, the Common Crawl project 1 that produces 20TB of textual data from the Internet each month. To cope …","url":["https://arxiv.org/pdf/2010.14587"]}
{"year":"2020","title":"On the Effectiveness of Behavior-Based Ransomware Detection","authors":["J Han, Z Lin, DE Porter - International Conference on Security and Privacy in …, 2020"],"snippet":"… To measure whether partial encryption is effective at withholding user data, we collected 200 different PDF documents from the web using Common Crawl Document Download [4]. We choose PDF documents with a minimum of 10 pages …","url":["https://link.springer.com/chapter/10.1007/978-3-030-63095-9_7"]}
{"year":"2020","title":"On the evaluation of retrofitting for supervised short-text classification","authors":["K GHAZI, A TCHECHMEDJIEV, S HARISPE…"],"snippet":"… we considered the 300-dimensional word vectors: (i) Paragram [16], learned from the text content in the paraphrase database PPDB, (ii) Glove [22] learned from Wikipedia and Common Crawl data, (iii) MUSE, a fastText embedding …","url":["http://ceur-ws.org/Vol-2708/donlp2.pdf"]}
{"year":"2020","title":"On the impact of publicly available news and information transfer to financial markets","authors":["M Jazbec, B Pásztor, F Faltings, N Antulov-Fantulin… - arXiv preprint arXiv …, 2020"],"snippet":"… To address ihttps://commoncrawl.org iiDetailed statistics about the Common Crawl can found here: https://commoncrawl.github.io/cc-crawl-statistics iiiWe omitted the domain www.nbonews.com. While the most frequently occurring …","url":["https://arxiv.org/pdf/2010.12002"]}
{"year":"2020","title":"On the importance of pre-training data volume for compact language models","authors":["V Micheli, M D'Hoffschmidt, F Fleuret - arXiv preprint arXiv:2010.03813, 2020"],"snippet":"… OSCAR 2 (Ortiz Suárez et al., 2019) is a large-scale multilingual open source collection of corpora ob- tained by language classification and filtering of the Common Crawl corpus 3. The whole French part amounts to 138 GB …","url":["https://arxiv.org/pdf/2010.03813"]}
{"year":"2020","title":"On the Language Neutrality of Pre-trained Multilingual Representations","authors":["J Libovický, R Rosa, A Fraser - arXiv preprint arXiv:2004.05160, 2020"],"snippet":"… XLM-RoBERTa. Conneau et al. (2019) claim that the original mBERT is under-trained and train a similar model on a larger dataset that consists of two terabytes of plain text extracted from CommonCrawl (Wenzek et al., 2019) …","url":["https://arxiv.org/pdf/2004.05160"]}
{"year":"2020","title":"On the Persistence of Persistent Identifiers of the Scholarly Web","authors":["M Klein, L Balakireva - arXiv preprint arXiv:2004.03011, 2020"],"snippet":"… These findings were confirmed in a large scale study by Thompson and Jian [16] based on two samples of the web taken from Common Crawl6 datasets … Thompson, HS, Tong, J.: Can common crawl reliably track persistent identifier (PID) use over time …","url":["https://arxiv.org/pdf/2004.03011"]}
{"year":"2020","title":"On the synthesis of metadata tags for HTML files","authors":["P Jiménez, JC Roldán, FO Gallego, R Corchuelo - Software: Practice and Experience"],"snippet":"… Recently, an analysis of the 32.04 million domains in the November 2019 Common Crawl has revealed that only 11.92 million domains provide metadata tags,1 which clearly argues for a method that helps software agents deal with the documents provided by the remaining …","url":["https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2886"]}
{"year":"2020","title":"On using Product-Specific Schema. org from Web Data Commons: An Empirical Set of Best Practices","authors":["R Kiran Selvam, M Kejriwal - arXiv e-prints, 2020","RK Selvam, M Kejriwal - arXiv preprint arXiv:2007.13829, 2020"],"snippet":"… on e-commerce websites. The Web Data Commons (WDC) project has extracted schema.org data at scale from webpages in the Common Crawl and made it available as an RDF 'knowledge graph' at scale. The portion of this …","url":["https://arxiv.org/pdf/2007.13829","https://ui.adsabs.harvard.edu/abs/2020arXiv200713829K/abstract"]}
{"year":"2020","title":"On-The-Fly Information Retrieval Augmentation for Language Models","authors":["H Wang, D McAllester - Proceedings of the First Joint Workshop on Narrative …, 2020"],"snippet":"… News etc. For language modelling we use the NY Times portion because it is written by native English speakers. Since GPT 2.0 is trained on Common Crawl which contains news collections started from 2008. To avoid testing …","url":["https://www.aclweb.org/anthology/2020.nuse-1.14.pdf"]}
{"year":"2020","title":"One Belt, One Road, One Sentiment? A Hybrid Approach to Gauging Public Opinions on the New Silk Road Initiative","authors":["JK Chandra, E Cambria, A Nanetti"],"snippet":"… ABSA. We used the Common Crawl GloVe version [44], a pre-trained 300-dimension vector representation database of 840 billion tokens and 2.2 million vocabulary, to convert our preprocessed tweets into word embeddings …","url":["https://sentic.net/one-belt-one-road-one-sentiment.pdf"]}
{"year":"2020","title":"Open Information Extraction as Additional Source for Kazakh Ontology Generation","authors":["N Khairova, S Petrasova, O Mamyrbayev, K Mukhsina - Asian Conference on …, 2020"],"snippet":"… also for many others. For example, an experiment was conducted in [19] for assessing the adequacy of measuring the factual density of 50 randomly selected Spanish documents in the CommonCrawl corpus. In a recent study …","url":["https://link.springer.com/chapter/10.1007/978-3-030-41964-6_8"]}
{"year":"2020","title":"Open Intent Extraction from Natural Language Interactions","authors":["N Vedula, N Lipka, P Maneriker, S Parthasarathy - Proceedings of The Web …, 2020"],"snippet":"… 2A commercial Customer Relationship Management (CRM) software. Implementation: We use the 300-dimensional GloVe embeddings [50] pre-trained on the Common Crawl dataset3, and character embeddings as per Ma et al [42] …","url":["https://dl.acm.org/doi/pdf/10.1145/3366423.3380268"]}
{"year":"2020","title":"Open science-based framework to reveal open data publishing: an experience from using Common Crawl","authors":["A Correa, I Fernandes - ELPUB 24rd edition of the International Conference on …, 2020"],"snippet":"The publishing of open data is considered a key element for civic participation paving the way to the 'public value', a term which underpins the social contribution. A result of that can be seen through the popularity of data portals published all around …","url":["https://hal.archives-ouvertes.fr/hal-02544245/document"]}
{"year":"2020","title":"Open source speech recognition on edge devices","authors":["R Peinl, B Rizk, R Szabad - 2020 10th International Conference on Advanced …, 2020"],"snippet":"… To make the comparison as fair as possible we used the KenLM 6-gram language model for all ASR models except DS2, which came with its own word-level language model trained on CommonCrawl (en-00) from the Common Crawl Corpus7 …","url":["https://ieeexplore.ieee.org/abstract/document/9208978/"]}
{"year":"2020","title":"Open-Domain Question Answering Goes Conversational via Question Rewriting","authors":["R Anantha, S Vakulenko, Z Tu, S Longpre, S Pulman… - arXiv preprint arXiv …, 2020"],"snippet":"… relevant pages with randomly sampled web pages that constitute 1% of the Common Crawl dataset identified … Wayback Machine and 9.9M random web pages from the Common Crawl dataset … 3 https://commoncrawl.org/2019 …","url":["https://arxiv.org/pdf/2010.04898"]}
{"year":"2020","title":"Optimal Subarchitecture Extraction For BERT","authors":["A de Wynter, DJ Perry - arXiv preprint arXiv:2010.10499, 2020"],"snippet":"… In order to have a sufficiently diverse dataset to pre-train Bort, we combined corpora obtained from Wikipedia7, Wiktionary8, OpenWebText (Gokaslan and Cohen, 2019), UrbanDictionary9, One Billion Words (Chelba et al., 2014) …","url":["https://arxiv.org/pdf/2010.10499"]}
{"year":"2020","title":"Optimizing Distributed Computing Systems via Machine Learning","authors":["H Wang - 2020"],"snippet":"Page 1. Optimizing Distributed Computing Systems via Machine Learning by Hao Wang A thesis submitted in conformity with the requirements for the degree of Doctor of Philosophy Graduate Department of Electrical …","url":["https://tspace.library.utoronto.ca/bitstream/1807/103710/1/Wang_Hao_202011_PhD_thesis.pdf"]}
{"year":"2020","title":"OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings","authors":["S Dev, T Li, JM Phillips, V Srikumar - arXiv preprint arXiv:2007.00049, 2020"],"snippet":"… Our code for reproducing experiments will be released upon publication. Debiasing contextualized embeddings. The operations above are described for a noncontextualized embedding; we use one of the largest such …","url":["https://arxiv.org/pdf/2007.00049"]}
{"year":"2020","title":"Out-of-the-Box and into the Ditch? Multilingual Evaluation of Generic Text Extraction Tools","authors":["A Barbaresi, G Lejeune - Proceedings of the 12th Web as Corpus Workshop, 2020"],"snippet":"… Recently, approaches using the CommonCrawl1 have flourished as they allow for faster download and processing by skipping (or more precisely outsourcing) the crawling phase (Habernal et al., 2016; Schäfer, 2016) … 1https://commoncrawl.org …","url":["https://www.aclweb.org/anthology/2020.wac-1.2.pdf"]}
{"year":"2020","title":"Overview of the CLEF eHealth 2020 task 2: consumer health search with ad hoc and spoken queries","authors":["L Goeuriot, Z Liu, G Pasi, GG Saez, M Viviani, C Xu - … Notes of Conference and Labs of …, 2020"],"snippet":"… 2.1 Documents The 2018 CLEF eHealth Consumer Health Search document collection was used in this year's IR challenge. As detailed in [17], this collection consists of web pages acquired from the CommonCrawl. An …","url":["http://www.dei.unipd.it/~ferro/CLEF-WN-Drafts/CLEF2020/paper_260.pdf"]}
{"year":"2020","title":"Overview of the CLEF eHealth Evaluation Lab 2020","authors":["C Xu - … IR Meets Multilinguality, Multimodality, and Interaction …"],"snippet":"… Task 2. The 2018 CLEF eHealth Consumer Health Search document collection was used in this year's IR challenge. As detailed in [14], this collection consists of web pages acquired from the CommonCrawl. An initial list of websites was identified for acquisition …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=IxP9DwAAQBAJ&oi=fnd&pg=PA255&dq=commoncrawl&ots=BCbV87DfTS&sig=y0JIuiOfa-DKv4aFVa2ZXlCYBic"]}
{"year":"2020","title":"Overview of the seventh Dialog System Technology Challenge: DSTC7","authors":["LF D'Haro, K Yoshino, C Hori, TK Marks… - Computer Speech & …, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0885230820300012"]}
{"year":"2020","title":"Overview of the Transformer-based Models for NLP Tasks","authors":["A Gillioz, J Casas, E Mugellini, O Abou Khaled"],"snippet":"… they contain. The tokenization is done with SentencePiece [15]. In a few cases, for example, in [16], the authors only used a subset of those datasets (eg Stories [17] is a subset of CommonCrawl dataset). IV. BENCHMARKS …","url":["https://annals-csis.org/Volume_21/drp/pdf/20.pdf"]}
{"year":"2020","title":"Overview of Touché 2020: Argument Retrieval","authors":["A Bondarenko, M Fröbe, M Beloucif, L Gienapp… - Working Notes Papers of the …, 2020","H Wachsmuth, M Potthast, M Hagen - … IR Meets Multilinguality, Multimodality, and Interaction …","Y Ajjour, A Panchenko, C Biemann, B Stein… - Experimental IR Meets …"],"snippet":"… machine CAM [25], a system for argument retrieval in comparative search, tries to support decision making in comparison scenarios based on billions of sentences from the Common Crawl but still lacks a … This method is shown …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=IxP9DwAAQBAJ&oi=fnd&pg=PA384&dq=commoncrawl&ots=BCbV87DfTS&sig=590FLiaMF0QksqxDhusBxoZQuK4","https://link.springer.com/content/pdf/10.1007/978-3-030-58219-7.pdf#page=395","https://webis.de/downloads/publications/papers/stein_2020v.pdf"]}
{"year":"2020","title":"ParaCrawl: Web-Scale Acquisition of Parallel Corpora","authors":["M Banón, P Chen, B Haddow, K Heafield, H Hoang…"],"snippet":"… In an ex- ploratory study, only 5% of a collection of web pages with useful content were found in CommonCrawl. This may have improved with recent more extensive crawls by CommonCrawl but there is still a strong argument for targeted crawling. 4 Crawling …","url":["https://www.neural.mt/papers/edinburgh/paracrawl.pdf"]}
{"year":"2020","title":"Parallelograms revisited: Exploring the limitations of vector space models for simple analogies","authors":["JC Peterson, D Chen, TL Griffiths - Cognition, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0010027720302596"]}
{"year":"2020","title":"Pedro Javier Ortiz Suárez1, 3 [0000− 0003− 0343− 8852], Yoann Dupont2","authors":["G Lejeune, T Tian"],"snippet":"… For the fixed word embeddings we used the Common Crawl-based FastText embeddings [10] originally trained by Facebook as opposed to the embeddings provided by the HIPE shared task, as we obtained better dev …","url":["http://ceur-ws.org/Vol-2696/paper_203.pdf"]}
{"year":"2020","title":"Persistent metadata catalog","authors":["GS Mcpherson, Y Mikhaylyuta, TD Baker, RJ Cole - US Patent 10,853,356, 2020"],"snippet":"… sources. In one embodiment metadata catalog service 120 may host metadata for a collection of data sets that are available in the public domain, such as the 1000 genome, NASA NEX, and the Common Crawl Corpus data sets …","url":["https://www.freepatentsonline.com/10853356.html"]}
{"year":"2020","title":"Phishing Detection Using Machine Learning Technique","authors":["J Rashid, T Mahmood, MW Nisar, T Nazir - 2020 First International Conference of …, 2020"],"snippet":"… and June 2017. In particular, we selected 5000 phishing web pages, and all web pages are more stable, especially based on URLs. The fish tank is based entirely on the Alexa URL and Common Crawl archives. B. Step 2 Vocabulary …","url":["https://ieeexplore.ieee.org/abstract/document/9283771/"]}
{"year":"2020","title":"PhishingLine: Hybrid Phishing Classifier with Logo Detection","authors":["K Vohra - 2020"],"snippet":"… 29 5.5 Web Capture . . . . . 30 5.5.1 WARC . . . . . 30 5.5.2 Common Crawl . . . . . 30 5.6 Client Server Architecture . . . . . 30 5.6.1 TCP …","url":["https://www.ka.beer/pdf/project.pdf"]}
{"year":"2020","title":"Phonemer at WNUT-2020 Task 2: Sequence Classification Using COVID Twitter BERT and Bagging Ensemble Technique based on Plurality Voting","authors":["A Wadhawan - arXiv preprint arXiv:2010.00294, 2020"],"snippet":"… Table 2. 4.2 System Settings For training the CNN, LSTM and BiLSTMs, word vectors for english language pre-trained on Common Crawl2 and Wikipedia3 are downloaded4 and used using FastText5 library. These word vectors …","url":["https://arxiv.org/pdf/2010.00294"]}
{"year":"2020","title":"Photo Stream Question Answer","authors":["W Zhang, S Tang, Y Cao, J Xiao, S Pu, F Wu, Y Zhuang - Proceedings of the 28th …, 2020"],"snippet":"Page 1. Photo Stream Question Answer Wenqiao Zhang Zhejiang University wenqiaozhang@zju.edu.cn Siliang Tang* Zhejiang Universety siliang@zju.edu.cn Yanpeng Cao,Jun Xiao Zhejiang University caoyp,junx@zju.edu.cn …","url":["https://dl.acm.org/doi/abs/10.1145/3394171.3413745"]}
{"year":"2020","title":"PMap: Ensemble Pre-training Models for Product Matching","authors":["N Kertkeidkachorn, R Ichise"],"snippet":"… In this section, we explain the pre-train models and how to fine-tune them. 5 http://webdatacommons.org/largescaleproductcorpus/v2/index. html 6 http://webdatacommons.org/structureddata/ 7 https://commoncrawl …","url":["http://ceur-ws.org/Vol-2720/paper2.pdf"]}
{"year":"2020","title":"PoKED: A Semi-Supervised System for Word Sense Disambiguation","authors":["F Wei"],"snippet":"Page 1. PoKED: A Semi-Supervised System for Word Sense Disambiguation Feng Wei 1 Abstract In this paper, we propose a semi-supervised neural system, named Position-wise Orthogonal Knowledge-Enhanced Disambiguator …","url":["https://proceedings.icml.cc/static/paper_files/icml/2020/1929-Paper.pdf"]}
{"year":"2020","title":"Practical Data Science for Information Professionals","authors":["D Stuart - 2020"],"snippet":"Page 1. Practical Data Science for Information Professionals Page 2. Every purchase of a Facet book helps to fund CILIP's advocacy, awareness and accreditation programmes for information professionals. Page 3. Practical Data …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=2TjzDwAAQBAJ&oi=fnd&pg=PP3&dq=commoncrawl&ots=kN5yF0ztQ5&sig=n_c9XhBzAQWnpO6G9vyAev9qRJY"]}
{"year":"2020","title":"Pragmatic Aspects of Discourse Production for the Automatic Identification of Alzheimer's Disease","authors":["A Pompili, A Abad, DM de Matos, IP Martins - IEEE Journal of Selected Topics in …, 2020"],"snippet":"… For this purpose, we rely on a pre-trained model of word vector representations containing 2 million word vectors, in 300 dimensions, trained with fastText on Common Crawl [42]. In the process of converting a sentence into its vector …","url":["https://ieeexplore.ieee.org/abstract/document/8963723/"]}
{"year":"2020","title":"Pre-indexing Pruning Strategies","authors":["S Altin, R Baeza-Yates, BB Cambazoglu - International Symposium on String …, 2020"],"snippet":"… 4 Experimental Setup. 4.1 Document Collection. As web document collection, we mostly use the open source web collection provided by Common Crawl, CC, in November 2017 … 5 Experimental Results. 5.1 Common Crawl and BM25 …","url":["https://link.springer.com/chapter/10.1007/978-3-030-59212-7_13"]}
{"year":"2020","title":"Pre-trained Models for Natural Language Processing: A Survey","authors":["X Qiu, T Sun, Y Xu, Y Shao, N Dai, X Huang - arXiv preprint arXiv:2003.08271, 2020"],"snippet":"Page 1. .Invited Review . Pre-trained Models for Natural Language Processing: A Survey Xipeng Qiu*, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai & Xuanjing Huang School of Computer Science, Fudan University, Shanghai …","url":["https://arxiv.org/pdf/2003.08271"]}
{"year":"2020","title":"Pre-training Polish Transformer-based Language Models at Scale","authors":["S Dadas, M Perełkiewicz, R Poświata - arXiv preprint arXiv:2006.04229, 2020"],"snippet":"… the size of the training corpus. 2) We proposed a method for collecting and pre-processing the data from the Common Crawl database to obtain clean, high-quality text corpora. 3) We conducted a comprehensive evaluation …","url":["https://arxiv.org/pdf/2006.04229"]}
{"year":"2020","title":"Pre-training via Leveraging Assisting Languages and Data Selection for Neural Machine Translation","authors":["H Song, R Dabre, Z Mao, F Cheng, S Kurohashi… - arXiv preprint arXiv …, 2020"],"snippet":"… Additionally, we used Common Crawl4 monolingual corpora for pre-training … We filled the CurrentDistribution line by line in Common Crawl file if the ratio of the length of current line had been less than the ratio of this length in target length distribution …","url":["https://arxiv.org/pdf/2001.08353"]}
{"year":"2020","title":"Pre-training via Leveraging Assisting Languages for Neural Machine Translation","authors":["H Song, R Dabre, Z Mao, F Cheng, S Kurohashi…"],"snippet":"… We used Common Crawl3 monolingual corpora for pre-training … We created a shared sub-word vocabulary us- ing Japanese and English data from ASPEC mixing with Japanese, English, Chinese and French data from Common Crawl …","url":["https://shyyhs.github.io/files/ACL2020SRW_Song_paper.pdf"]}
{"year":"2020","title":"Predicting Consumers' Brand Sentiment Using Text Analysis on Reddit","authors":["P Cen"],"snippet":"… with a F-score of 86.25 for NER tasks. The en_core_web_md model is an English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl For NER tasks, it assigns various labels to identified entities, such as “CARDINAL,” “ORG,” …","url":["https://repository.upenn.edu/cgi/viewcontent.cgi?article=1097&context=joseph_wharton_scholars"]}
{"year":"2020","title":"Predicting Themes within Complex Unstructured Texts: A Case Study on Safeguarding Reports","authors":["A Edwards, D Rogers, J Camacho-Collados… - arXiv preprint arXiv …, 2020"],"snippet":"… Pre-trained word embeddings We leverage two pre-trained 300-dimensional word embedding models: Word2vec (Mikolov et al., 2013) trained on Google news dataset and fastText (Bojanowski et al., 2017) trained with subword information on Common Crawl …","url":["https://arxiv.org/pdf/2010.14584"]}
{"year":"2020","title":"Predicting Twitter Engagement With Deep Language Models","authors":["M Volkovs, Z Cheng, M Ravaut, H Yang, K Shen…"],"snippet":"… text comprehension tasks. The majority of published language models are pre-trained on large text corpora such as Wikipedia or CommonCrawl, that typically contain longer and properly worded pieces of text. Tweets on the …","url":["http://www.cs.toronto.edu/~mvolkovs/recsys2020_challenge.pdf"]}
{"year":"2020","title":"Prior Guided Feature Enrichment Network for Few-Shot Segmentation","authors":["Z Tian, H Zhao, M Shu, Z Yang, R Li, J Jia - IEEE Annals of the History of Computing, 2020"],"snippet":"… Therefore the prior is not,applicable and we only verify FEM on the baseline with,VGG-16 backbone in the zero-shot setting.,Structural Change,Embeddings of Word2Vec [24] and,FastText [22] are trained …","url":["https://www.computer.org/csdl/journal/tp/5555/01/09154595/1lZzPRFhQqY"]}
{"year":"2020","title":"Privacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy Policies","authors":["M Srinath, S Wilson, CL Giles - arXiv preprint arXiv:2004.11131, 2020"],"snippet":"… As a consequence, 2https://commoncrawl.org/ Page 3 … Thus, we selected those URLs which had the word “privacy” or the words “data” and “protection” from the Common Crawl URL archive. We were able to extract 3.9 million URLs that fit this selection criterion …","url":["https://arxiv.org/pdf/2004.11131"]}
{"year":"2020","title":"Privacy Policies over Time: Curation andAnalysis of a Million-Document Dataset","authors":["R Amos, G Acar, E Lucherini, M Kshirsagar… - arXiv preprint arXiv …, 2020"],"snippet":"… app privacy policy URLs [23]. Concurrent to this work, Srinath et al. contribute PrivaSeer, a dataset of over 1 million English privacy policies extracted from May 2019 Common Crawl data [14]. Our work advances this area of …","url":["https://arxiv.org/pdf/2008.09159"]}
{"year":"2020","title":"Privacy-Preserving Passive DNS","authors":["P Papadopoulos, N Pitropakis, WJ Buchanan, O Lo… - Computers, 2020"],"snippet":"… These sources include but are not limited to Public Blacklists, the Alexa ranking, the Common Crawl project, and various Top Level Domain (TLD) zone files. This system's output is a refined dataset that can be …","url":["https://www.mdpi.com/2073-431X/9/3/64/pdf"]}
{"year":"2020","title":"Privacy-Preserving Visual Content Tagging using Graph Transformer Networks","authors":["XS Vu, DT Le, C Edlund, L Jiang, HD Nguyen"],"snippet":"… that local knowledge can be derived from data observations including label semantics or multimedia content se- mantics (eg, optical character recognition); whereas, global knowledge can be drawn from publicly available corpora …","url":["https://people.cs.umu.se/sonvx/files/ACMMM2020_SGTN_CAMREADY_1.pdf"]}
{"year":"2020","title":"Probing Task-Oriented Dialogue Representation from Language Models","authors":["CS Wu, C Xiong - arXiv preprint arXiv:2010.13912, 2020"],"snippet":"… is to maximize left-to-right generation likelihood. To ensure diverse and nearly unlimited text sources, they use Common Crawl to obtain 8M documents as its training data. Budzianowski and Vulic (2019) trained GPT2 on task …","url":["https://arxiv.org/pdf/2010.13912"]}
{"year":"2020","title":"Probing Tasks for Noised Back-Translation","authors":["N Spring - 2020"],"snippet":"Page 1. Bachelor's thesis presented to the Faculty of Arts and Social Sciences of the University of Zurich for the degree of Bachelor of Arts UZH Probing Tasks for Noised Back-Translation Author: Nicolas Spring Student …","url":["https://www.cl.uzh.ch/dam/jcr:34ea0877-26f8-405b-88a5-1191536986db/spring_ba_probing_tasks.pdf"]}
{"year":"2020","title":"Probing Text Models for Common Ground with Visual Representations","authors":["G Ilharco, R Zellers, A Farhadi, H Hajishirzi - arXiv preprint arXiv:2005.00619, 2020"],"snippet":"… GloVe embeddings (Pennington et al., 2014). For such, we use embeddings trained on 840 billion tokens of web data from Common Crawl, with dL = 300 and a vocabulary size of 2.2 million2. Models trained on text and images …","url":["https://arxiv.org/pdf/2005.00619"]}
{"year":"2020","title":"Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding","authors":["S Weigelt, V Steurer, T Hey, WF Tichy - Proceedings of the 58th Annual Meeting of …, 2020"],"snippet":"… on the Common Crawl dataset4 by Facebook Research (Mikolov et al., 3Note that we do not discuss the influence of varying epoch numbers, since we used early stopping, ie the training stops when the validation loss stops …","url":["https://www.aclweb.org/anthology/2020.acl-main.395.pdf"]}
{"year":"2020","title":"Projecting Heterogeneous Annotations for Named Entity Recognition","authors":["R Agerri, G Rigau","R Agerri, G Rigau - Proceedings of the Iberian Languages Evaluation …, 2020"],"snippet":"… Page 3. of Common Crawl text … The biggest update that XLM-Roberta offers is a significantly increased amount of training data, 2.5TB of Common Crawl clean data [6]. As for BERT, in this paper we use the base version of XLM-RoBERTa …","url":["http://ceur-ws.org/Vol-2664/capitel_paper2.pdf","https://ragerri.github.io/files/ixaera-capitel2020.pdf"]}
{"year":"2020","title":"Projecting named entity recognizers from resource-rich to resource-poor languages without annotated or parallel corpora","authors":["J Hou"],"snippet":"Page 1. Projecting named entity recognizers from resource-rich to resourcepoor languages without annotated or parallel corpora Hou, Jue Helsinki October 20, 2019 UNIVERSITY OF HELSINKI Department of Computer …","url":["https://helda.helsinki.fi/bitstream/handle/10138/310012/Jue_Hou-Master_s_Thesis-v2.1.pdf?sequence=2&isAllowed=y"]}
{"year":"2020","title":"PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data","authors":["D Carmo, M Piau, I Campiotti, R Nogueira, R Lotufo - arXiv preprint arXiv:2008.09144, 2020"],"snippet":"… The original T5 vocabulary uses the SentencePiece library [7] using English, German, French, and Romanian web pages from Common Crawl.1 We use a similar procedure to create our Portuguese vocabulary: we train …","url":["https://arxiv.org/pdf/2008.09144"]}
{"year":"2020","title":"PublishInCovid19 at the FinSBD-2 Task: Sentence and List Extraction in Noisy PDF Text Using a Hybrid Deep Learning and Rule-Based Approach","authors":["J Singh - Proceedings of the Second Workshop on Financial …, 2020"],"snippet":"… For the pretrained word embeddings, we use GloVE 6 which are trained on large Common Crawl dataset and can effectively represent 84 billion cased tokens. To train the BERT for our task, we fine-tune a pre-trained model, namely bert-base-cased …","url":["https://www.aclweb.org/anthology/2020.finnlp-1.pdf#page=63"]}
{"year":"2020","title":"Punctuation Prediction in Spontaneous Conversations: Can We Mitigate ASR Errors with Retrofitted Word Embeddings?","authors":["Ł Augustyniak, P Szymanski, M Morzy, P Zelasko… - arXiv preprint arXiv …, 2020"],"snippet":"… In this work, we use pre-trained GloVe embeddings trained on the Common Crawl dataset consisting of 2.6 billion textual documents … In our case, the punctuation in conversational transcripts is substantially different from the …","url":["https://arxiv.org/pdf/2004.05985"]}
{"year":"2020","title":"PyChain: A Fully Parallelized PyTorch Implementation of LF-MMI for End-to-End ASR","authors":["Y Shao, Y Wang, D Povey, S Khudanpur - arXiv preprint arXiv:2005.09824, 2020"],"snippet":"… We hope that our experience with PYCHAIN will inspire other efforts to build next-generation hybrid ASR tools. 812k hours AM train set and common crawl LM. 9Data augmentation and pre-trained on LibriSpeech …","url":["https://arxiv.org/pdf/2005.09824"]}
{"year":"2020","title":"Quality and Relevance Metrics for Selection of Multimodal Pretraining Data","authors":["R Rao, S Rao, E Nouri, D Dey, A Celikyilmaz, B Dolan - Proceedings of the IEEE/CVF …, 2020"],"snippet":"… The GloVe vectors used are pretrained on 840 billion tokens from Common Crawl. Let o ∈ Oi be the set of objects de- tected by the RCNN for a given image i, w ∈ d be the set 0 100 200 300 400 500 ConceptualCaptions Ngram …","url":["http://openaccess.thecvf.com/content_CVPRW_2020/papers/w56/Rao_Quality_and_Relevance_Metrics_for_Selection_of_Multimodal_Pretraining_Data_CVPRW_2020_paper.pdf"]}
{"year":"2020","title":"Quality Estimation for Machine Translation with Multi-granularity Interaction⋆","authors":["K Tian, J Zhang"],"snippet":"… (7) 4 Experiments 4.1 Dataset The bilingual parallel corpus that we use for pre-trained multilingual BERT is officially released by the WMT17 Shared Task: Machine Translation of News1, in- cluding Europarl v7, Common Crawl …","url":["http://sc.cipsc.org.cn/mt/conference/2020/papers/T20-1005.pdf"]}
{"year":"2020","title":"Quality Evaluation","authors":["JM Gomez-Perez, R Denaux, A Garcia-Silva - A Practical Guide to Hybrid Natural …, 2020"],"snippet":"… Besides the embeddings trained by us, we also include, as part of our study, several pre-trained embeddings, notably the GloVe embeddings for CommonCrawl— code glove_840B provided by Stanford11 —fastText …","url":["https://link.springer.com/chapter/10.1007/978-3-030-44830-1_7"]}
{"year":"2020","title":"Query focused abstractive summarization using BERTSUM model","authors":["DM Abdullah - 2020"],"snippet":"… Conneau et al. (2019) have introduced a multilingual masked language model from Facebook AI. This model has been trained on 2.5 TB of newly created clean CommonCrawl (Wenzek et al., 2019) data in 100 languages. The model has shown state-of-the-art results …","url":["https://opus.uleth.ca/bitstream/handle/10133/5760/ABDULLAH_DEEN_MOHAMMAD_MSC_2020.pdf?sequence=1"]}
{"year":"2020","title":"Question Answering for Comparative Questions with GPT-2","authors":["B Sievers"],"snippet":"… https://www.microsoft.com/en-us/research/blog/ turing-nlg-a-17-billion-parameter-languagemodel-by-microsoft/, accessed: 6.3.2020 3. Bevendorff, J., Stein, B., Hagen, M., Potthast, M.: Elastic chatnoir: search engine for the clueweb and the common crawl …","url":["http://ceur-ws.org/Vol-2696/paper_213.pdf"]}
{"year":"2020","title":"Question Answering When Knowledge Bases are Incomplete","authors":["C Pradel, D Sileo, Á Rodrigo, A Peñas, E Agirre - International Conference of the …, 2020","E Agirre - … IR Meets Multilinguality, Multimodality, and Interaction …"],"snippet":"… with bag of word embeddings. We use FastText CommonCrawl word embeddings [10] 4 and a max pooling to produce the continuous bag of word representations of table columns and the question text. The 4 We used Magnitude …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=IxP9DwAAQBAJ&oi=fnd&pg=PA43&dq=commoncrawl&ots=BCbV87DfTS&sig=kVIo_AYLn9xgMPpxB-rDuk1jzEg","https://link.springer.com/chapter/10.1007/978-3-030-58219-7_4"]}
{"year":"2020","title":"Question Type Classification Methods Comparison","authors":["T Seidakhmetov - arXiv preprint arXiv:2001.00571, 2020"],"snippet":"… The GLoVe vectors were pre-trained using 840 billion tokens from Common Crawl, and each token is mapped into a 300-dimensional vector [3]. Xembeddings = GloveEmbedding( Xword) ∈ RNxDword where Dword is a number of dimensions of a word vector …","url":["https://arxiv.org/pdf/2001.00571"]}
{"year":"2020","title":"Questioning the Use of Bilingual Lexicon Induction as an Evaluation Task for Bilingual Word Embeddings","authors":["B Marie, A Fujita"],"snippet":"… gual word embeddings. In fact, this corpus was significantly smaller than the Wikipedia corpora for all the other languages, and than the Finnish Common Crawl corpus used to train Finnish Vecmap-emb. Another finding is …","url":["https://www.anlp.jp/proceedings/annual_meeting/2020/pdf_dir/P5-14.pdf"]}
{"year":"2020","title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models","authors":["S Gehman, S Gururangan, M Sap, Y Choi, NA Smith - arXiv preprint arXiv …, 2020","SGS Gururangan, MSY Choi, NA Smith"],"snippet":"… GPT-2 (specifically, GPT-2-small; Radford et al., 2019), is a similarly sized model pretrained on OPENAI-WT, which contains 40GB of English web text and is described in §6.7 GPT-3 (Brown et al., 2020) is pretrained on a mix …","url":["https://arxiv.org/pdf/2009.11462","https://homes.cs.washington.edu/~msap/pdfs/gehman2020realtoxicityprompts.pdf"]}
{"year":"2020","title":"Recent Trends in the Use of Deep Learning Models for Grammar Error Handling","authors":["M Naghshnejad, T Joshi, VN Nair - arXiv preprint arXiv:2009.02358, 2020"],"snippet":"Page 1. 1 Recent Trends in the Use of Deep Learning Models for Grammar Error Handling Mina Naghshnejad1, Tarun Joshi, and Vijayan N. Nair Corporate Model Risk, Wells Fargo2 Abstract Grammar error handling (GEH) is …","url":["https://arxiv.org/pdf/2009.02358"]}
{"year":"2020","title":"Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation","authors":["AC Stickland, X Li, M Ghazvininejad - arXiv preprint arXiv:2004.14911, 2020"],"snippet":"Page 1. Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation Asa Cooper Stickland♣ Xian Li♠ ♣ University of Edinburgh, ♠ Facebook AI a.cooper.stickland@ed.ac.uk, {xianl,ghazvini}@fb.com Marjan Ghazvininejad♠ Abstract …","url":["https://arxiv.org/pdf/2004.14911"]}
{"year":"2020","title":"ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning","authors":["W Yu, Z Jiang, Y Dong, J Feng - arXiv preprint arXiv:2002.04326, 2020"],"snippet":"Page 1. Published as a conference paper at ICLR 2020 RECLOR: AREADING COMPREHENSION DATASET REQUIRING LOGICAL REASONING Weihao Yu∗, Zihang Jiang∗, Yanfei Dong & Jiashi Feng National University …","url":["https://arxiv.org/pdf/2002.04326"]}
{"year":"2020","title":"Recognai's Working Notes for CANTEMIST-NER Track","authors":["DC Fidalgo, D Vila-Suero, FA Montes - … of the Iberian Languages Evaluation Forum …, 2020"],"snippet":"… light-weight solution regarding the pretrained component. For this reason we chose the pretrained Spanish word vectors provided by FastText[4]. These vectors encompass 2 million words that were trained on Common Crawl4 …","url":["http://ceur-ws.org/Vol-2664/cantemist_paper4.pdf"]}
{"year":"2020","title":"Reducing Language Biases in Visual Question Answering with Visually-Grounded Question Encoder","authors":["G KV, A Mittal - arXiv preprint arXiv:2007.06198, 2020"],"snippet":"Page 1. Reducing Language Biases in Visual Question Answering with Visually-Grounded Question Encoder Gouthaman KV and Anurag Mittal Indian Institute of Technology Madras, India {gkv,amittal}@cse.iitm.ac.in Abstract …","url":["https://arxiv.org/pdf/2007.06198"]}
{"year":"2020","title":"Referring Image Segmentation via Cross-Modal Progressive Comprehension","authors":["S Huang, T Hui, S Liu, G Li, Y Wei, J Han, L Liu, B Li - … of the IEEE/CVF Conference on …, 2020"],"snippet":"Page 1. Referring Image Segmentation via Cross-Modal Progressive Comprehension Shaofei Huang1,2∗ Tianrui Hui1,2∗ Si Liu3† Guanbin Li4 Yunchao Wei5 Jizhong Han1,2 Luoqi Liu6 Bo Li3 1 Institute of Information Engineering …","url":["http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Referring_Image_Segmentation_via_Cross-Modal_Progressive_Comprehension_CVPR_2020_paper.pdf"]}
{"year":"2020","title":"Refinement of Unsupervised Cross-Lingual Word Embeddings","authors":["M Biesialska, MR Costa-jussà - arXiv preprint arXiv:2002.09213, 2020"],"snippet":"… Finnish. Monolingual embeddings of 300 di- mensions were created using Word2Vec3 [18] and were trained on WMT News Crawl (Spanish), WacKy crawling corpora (English, German), and Common Crawl (Finnish). To evaluate …","url":["https://arxiv.org/pdf/2002.09213"]}
{"year":"2020","title":"ReINTEL: A Multimodal Data Challenge for Responsible Information Identification on Social Network Sites","authors":["DT Le, XS Vu, ND To, HQ Nguyen, TT Nguyen, L Le… - arXiv preprint arXiv …, 2020"],"snippet":"… Word2VecVN (Vu, 2016) x Trained on 7GB texts of Vietnamese news FastText (Vietnamese version) (Joulin et al., 2016) x Trained on Vietnamese texts of the CommonCrawl corpus ETNLP (Vu et al., 2019) x Trained on 1GB texts of Vietnamese Wikipedia …","url":["https://arxiv.org/pdf/2012.08895"]}
{"year":"2020","title":"Related Tasks can Share! A Multi-task Framework for Affective language","authors":["KS Deep, MS Akhtar, A Ekbal, P Bhattacharyya - arXiv preprint arXiv:2002.02154, 2020"],"snippet":"… 3. Character-level embeddings2: Character-level embeddings are trained over common crawl glove corpus providing 300 dimensional vectors for each character (used in case if word is not present in other two embeddings) …","url":["https://arxiv.org/pdf/2002.02154"]}
{"year":"2020","title":"Relational and Fine-Grained Argument Mining","authors":["D Trautmann, M Fromm, V Tresp, T Seidl, H Schütze - Datenbank-Spektrum, 2020"],"snippet":"… Crowdworkers had the task of selecting argumentative spans for a given set of topics and topic related sentences. The sentences were from textual data extracted from Common Crawl Footnote 6 for a predefined list of eight …","url":["https://link.springer.com/article/10.1007/s13222-020-00341-z"]}
{"year":"2020","title":"Relational Databases and SQL Language","authors":["JM Patel - Getting Structured Data from the Internet, 2020"],"snippet":"… itself. We are mainly discussing PostgreSQL here so that you can scale up to 64 TB if you decide to index large portions of common crawl datasets for creating a backlinks and news database in Chapters 6 and 7, respectively …","url":["https://link.springer.com/chapter/10.1007/978-1-4842-6576-5_5"]}
{"year":"2020","title":"Representation learning for input classification via topic sparse autoencoder and entity embedding","authors":["D Li, J Zhang, P Li - US Patent App. 16/691,554, 2020"],"snippet":"US20200184339A1 - Representation learning for input classification via topic sparse autoencoder and entity embedding - Google Patents. Representation learning for input classification via topic sparse autoencoder and entity embedding. Download PDF Info …","url":["https://patents.google.com/patent/US20200184339A1/en"]}
{"year":"2020","title":"Reproducible Extraction of Cross-lingual Topics (rectr)","authors":["CH Chan, J Zeng, H Wessler, M Jungblut, K Welbers… - … Methods and Measures, 2020"],"snippet":"… 17 Researchers at Facebook Research created these aligned word embeddings by first training fastText word embeddings (Bojanowski et al., 2017) using Wikipedia and Common Crawl articles of each individual …","url":["https://www.tandfonline.com/doi/abs/10.1080/19312458.2020.1812555"]}
{"year":"2020","title":"Research Challenges in Designing Differentially Private Text Generation Mechanisms","authors":["O Feyisetan, A Aggarwal, Z Xu, N Teissier - arXiv preprint arXiv:2012.05403, 2020"],"snippet":"… A natural way to “bias” an exponential mechanism without changing its privacy properties is to modulate it with a public “prior” µ(z). For example, such a prior can be constructed over a publicly available corpus such as Wikipedia or Common Crawl …","url":["https://arxiv.org/pdf/2012.05403"]}
{"year":"2020","title":"Residual Energy-Based Models for Text Generation","authors":["Y Deng, A Bakhtin, M Ott, A Szlam, MA Ranzato - arXiv preprint arXiv:2004.11714, 2020"],"snippet":"… Page 6. Published as a conference paper at ICLR 2020 genres, totaling about half a billion words. The latter is a de-duplicated subset of the English portion of the CommonCrawl news dataset (Nagel, 2016), which totals around 16 Billion words …","url":["https://arxiv.org/pdf/2004.11714"]}
{"year":"2020","title":"Rethinking embedding coupling in pre-trained language models","authors":["HW Chung, T Févry, H Tsai, M Johnson, S Ruder - arXiv preprint arXiv:2010.12821, 2020"],"snippet":"Page 1. Preprint. Under review. RETHINKING EMBEDDING COUPLING IN PRE-TRAINED LANGUAGE MODELS Hyung Won Chung∗† Google Research hwchung@google.com Thibault Févry∗† thibaultfevry@gmail …","url":["https://arxiv.org/pdf/2010.12821"]}
{"year":"2020","title":"Rethinking Evaluation in ASR: Are Our Models Robust Enough?","authors":["T Likhomanenko, Q Xu, V Pratap, P Tomasello, J Kahn… - arXiv preprint arXiv …, 2020"],"snippet":"… only; for TL – both train transcriptions and provided LM data. We also train a 4-gram LM on Common Crawl (CC) data with 200k top words and pruning of all 3,4-grams appearing once. Perplexity of all LMs is shown in Table 2 …","url":["https://arxiv.org/pdf/2010.11745"]}
{"year":"2020","title":"Retrieving Comparative Arguments using Deep Pre-trained Language Models and NLU","authors":["V Chekalina, A Panchenko"],"snippet":"… ChatNoir is an Elasticsearch-based5 engine providing access to nearly 3 billion web pages from ClueWeb and Common Crawl corpora … ACM. 2. J. Bevendorff, B. Stein, M. Hagen, and M. Potthast. Elastic ChatNoir: Search …","url":["http://ceur-ws.org/Vol-2696/paper_210.pdf"]}
{"year":"2020","title":"Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT","authors":["A Chronopoulou, D Stojanovski, A Fraser - arXiv preprint arXiv:2009.07610, 2020"],"snippet":"… We use 68M En sentences from NewsCrawl, 2.4M Mk and 4M Sq, both from CommonCrawl and Wikipedia … Second, in En- De, we use high-quality corpora for both languages (NewsCrawl), whereas Mk and Sq are trained on low-quality CommonCrawl data …","url":["https://arxiv.org/pdf/2009.07610"]}
{"year":"2020","title":"Review of the Recent Techniques for Learning Commonsense Knowledge applied to the Winograd Schema Challenge","authors":["A Koleva"],"snippet":"… 4 A Combined Approach Prakash et al. [2], to the best of our knowledge, are the first ones to combine methods from KRR with methods from ML such as language models. They propose a framework in which four different …","url":["http://ecai2020.eu/papers/1513_paper.pdf"]}
{"year":"2020","title":"Review rating prediction framework using deep learning","authors":["BH Ahmed, AS Ghabayen - Journal of Ambient Intelligence and Humanized …, 2020"],"snippet":"… word representation) embedding. There are several Glove embeddings from different sources, such as Twitter, Wikipedia or the common crawl. We utilized we utilize the Glove embedding trained by (Pennington et al. 2014) on …","url":["https://link.springer.com/article/10.1007/s12652-020-01807-4"]}
{"year":"2020","title":"Revisiting Round-Trip Translation for Quality Estimation","authors":["J Moon, H Cho, EL Park - arXiv preprint arXiv:2004.13937, 2020"],"snippet":"Page 1. Revisiting Round-Trip Translation for Quality Estimation Jihyung Moon Naver Papago Hyunchang Cho Naver Papago {jihyung.moon, hyunchang.cho, lucypark}@navercorp.com Eunjeong L. Park Naver Papago Abstract …","url":["https://arxiv.org/pdf/2004.13937"]}
{"year":"2020","title":"RobBERT: a Dutch RoBERTa-based Language Model","authors":["P Delobelle, T Winters, B Berendt - arXiv preprint arXiv:2001.06286, 2020"],"snippet":"… 3.1 Data We pre-trained our model on the Dutch section of the OSCAR corpus, a large multilingual corpus which was obtained by language classification in the Common Crawl corpus (Ortiz Suárez et al., 2019). This Dutch corpus …","url":["https://arxiv.org/pdf/2001.06286"]}
{"year":"2020","title":"Robust Cross-lingual Embeddings from Parallel Sentences","authors":["A Sabet, P Gupta, JB Cordonnier, R West, M Jaggi - arXiv preprint arXiv:1912.12481, 2019"],"snippet":"… MUSE 0.38 0.30 0.74 0.64 RCSLS 0.38 0.30 0.74 0.64 FASTTEXTCommon Crawl 0.49 0.32 0.75 0.57 BIVEC 0.40 0.36 0.70 0.60 … We also include FASTTEXT monolingual vectors trained on CommonCrawl data (Grave et …","url":["https://arxiv.org/pdf/1912.12481"]}
{"year":"2020","title":"Robust Prediction of Punctuation and Truecasing for Medical ASR","authors":["M Sunkara, S Ronanki, K Dixit, S Bodapati, K Kirchhoff - Proceedings of the First …, 2020","MSSRK Dixit, SBK Kirchhoff - ACL 2020, 2020"],"snippet":"… But just like any other model, these Language Models are biased by their training data. In particular, they are typically trained on data that is easily available in large quantities on the internet eg Wikipedia, CommonCrawl etc …","url":["https://www.aclweb.org/anthology/2020.nlpmc-1.8.pdf","https://www.aclweb.org/anthology/2020.nlpmc-1.pdf#page=65"]}
{"year":"2020","title":"Robust Prediction of Punctuation and Truecasingfor Medical ASR","authors":["M Sunkara, S Ronanki, K Dixit, S Bodapati, K Kirchhoff - arXiv preprint arXiv …, 2020"],"snippet":"… But just like any other model, these Language Models are biased by their training data. In particular, they are typically trained on data that is easily available in large quantities on the internet eg Wikipedia, CommonCrawl etc …","url":["https://arxiv.org/pdf/2007.02025"]}
{"year":"2020","title":"Russian-English Bidirectional Machine Translation System","authors":["A Xv, W Chao - Ariel"],"snippet":"… For the monolingual data we use English and Russian Newscrawl as well as a filtered part of Commoncrawl in Russian … Russian is relatively smaller than that of English, we have to augment the Newscrawl data for Russian …","url":["http://statmt.org/wmt20/pdf/2020.wmt-1.35.pdf"]}
{"year":"2020","title":"Samsung R&D Institute Poland submission to WMT20 News Translation Task","authors":["M Krubinski, M Chochowski, B Boczek, M Koszowski… - Proceedings of the Fifth …, 2020"],"snippet":"… Pretraining a complete encoder-decoder model allows for later direct fine-tuning on the translation ob- jective, with parallel corpora. In our experiment, we sampled 250M sentences from CommonCrawl for Czech, English and Polish (ie 750M in total) …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.16.pdf"]}
{"year":"2020","title":"SandiDoc at CLEF 2020-Consumer Health Search: AdHoc IR Task","authors":["S Seneviratne, E Daskalaki, M Zakir - Conference and Labs of the Evaluation (CLEF) …, 2020"],"snippet":"… respectively. 2 Resources 2.1 Dataset The document collection used in the document retrieval task was acquired by the common crawl dump of 2018-19. This included web pages of the formats such as HTML, XHTML, XML. The …","url":["http://www.dei.unipd.it/~ferro/CLEF-WN-Drafts/CLEF2020/paper_160.pdf"]}
{"year":"2020","title":"SardiStance@ EVALITA2020: Overview of the Task on Stance Detection in Italian Tweets","authors":["AT Cignarella, M Lai, C Bosco, V Patti, P Rosso - … of the 7th Evaluation Campaign of …, 2020"],"snippet":"… In particular, they trained three classifiers based respectively on SENTIPOLC 2016 (Barbieri et al., 2016) for sentiment analysis classification, on HaSpeeDe 2018 (Bosco et al., 2018) 4https://huggingface.co/Musixmatch/ umberto-commoncrawl-cased-v1 …","url":["http://ceur-ws.org/Vol-2765/paper159.pdf"]}
{"year":"2020","title":"SberQuAD–Russian Reading Comprehension Dataset: Description and Analysis","authors":["P Braslavski - … IR Meets Multilinguality, Multimodality, and Interaction …"],"snippet":"… We tokenized text using spaCy. 12 To initialize the embedding layer for BiDAF, DocQA, DrQA, and R-Net we use Russian casesensitive fastText embeddings trained on Common Crawl and Wikipedia. 13 This initialization is used for both questions and paragraphs …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=IxP9DwAAQBAJ&oi=fnd&pg=PA3&dq=commoncrawl&ots=BCbV87DfTS&sig=E3mWPymZbBAvDLn8azUpJimpaMs"]}
{"year":"2020","title":"Scalable Cross Lingual Pivots to Model Pronoun Gender for Translation","authors":["K Webster, E Pitler - arXiv preprint arXiv:2006.08881, 2020"],"snippet":"… glish was most recently contested at WMT'132 (Bojaretal., 2013), which offered participants 14,980,513 sentence pairs from Eu- roparl3 (Habash et al., 2017), Common Crawl (Smith et al., 2013), the United Nations cor …","url":["https://arxiv.org/pdf/2006.08881"]}
{"year":"2020","title":"Scalable, Multi-Constraint, Complex-Objective Graph Partitioning","authors":["GM Slota, C Root, K Devine, K Madduri… - IEEE Transactions on …, 2020"],"snippet":"Page 1. Scalable, Multi-Constraint, Complex-Objective Graph Partitioning GeorgeM.Slota ,CameronRoot,KarenDevine ,KameshMadduri , and Sivasankaran Rajamanickam Abstract—We introduce XTRAPULP, a distributed-memory …","url":["https://ieeexplore.ieee.org/abstract/document/9115834/"]}
{"year":"2020","title":"Scaling Laws for Neural Language Models","authors":["J Kaplan, S McCandlish, T Henighan, TB Brown… - arXiv preprint arXiv …, 2020","OEIAY Need"],"snippet":"… We reserve 6.6 × 108 of these tokens for use as a test set, and we also test on similarlyprepared samples of Books Corpus [ZKZ+15], Common Crawl [Fou], English Wikipedia, and a collection of publicly-available Internet Books …","url":["https://arxiv.org/pdf/2001.08361","https://deepai.org/publication/scaling-laws-for-neural-language-models"]}
{"year":"2020","title":"Scientific Question Answering with AAN","authors":["K Mueller"],"snippet":"… have easy to understand language as well as little background knowledge required. They then used web data provided by Common Crawl to find supporting web documents from which to draw information and construct …","url":["https://zoo.cs.yale.edu/classes/cs290/19-20a/mueller.keaton.kim6/Keaton_Mueller_CPSC290_Final_Report.pdf"]}
{"year":"2020","title":"Scones: Towards Conversational Authoring of Sketches","authors":["F Huang, D Ha, E Schoop, J Canny"],"snippet":"… detail in Section 4.1. For each text token t, we use a 300-dimensional GLoVe vector trained on 42B tokens from the Common Crawl dataset [22] to semantically represent these words in the instructions. To train the Transformer …","url":["http://people.eecs.berkeley.edu/~eschoop/docs/scones.pdf"]}
{"year":"2020","title":"Scoring Dimension-Level Job Performance From Narrative Comments: Validity and Generalizability When Using Natural Language Processing","authors":["AB Speer - Organizational Research Methods, 2020"],"snippet":"Performance appraisal narratives are qualitative descriptions of employee job performance. This data source has seen increased research attention due to the ability to efficiently derive insights u...","url":["https://journals.sagepub.com/doi/abs/10.1177/1094428120930815"]}
{"year":"2020","title":"SDRS: A new lossless dimensionality reduction for text corpora","authors":["IV de Mendizabal, V Basto-Fernandes, E Ezpeleta… - Information Processing & …, 2020"],"snippet":"… Clueweb 12, Web (html) pages, English, unknown, 870M. Common Crawl Data, Web (html) pages, multilingual, 100% spam, 9 Billion in 2014 and increasing. YouTube Comments Dataset, Youtube comments, multilingual, 7% spam, 6M …","url":["https://www.sciencedirect.com/science/article/pii/S0306457319314694"]}
{"year":"2020","title":"Searching the Web for Cross-lingual Parallel Data","authors":["A El-Kishky, P Koehn, H Schwenk - Proceedings of the 43rd International ACM SIGIR …, 2020"],"snippet":"… and Tokenization – CommonCrawl Preprocessing Code • Open-source Code for Generating Cross-lingual Datasets – Code for Generating High-quality Monolingual Data from CommonCrawl – Code for Generating …","url":["https://dl.acm.org/doi/abs/10.1145/3397271.3401417"]}
{"year":"2020","title":"SEDAR: a Large Scale French-English Financial Domain Parallel Corpus","authors":["A Ghaddar, P Langlais - Proceedings of The 12th Language Resources and …, 2020"],"snippet":"… It contains 40.8M sentence pairs extracted from five datasets that cover various domains: EUROPARL V7 (Koehn, 2005), UNITED NATIONS CORPUS (Eisele and Chen, 2010), COMMON CRAWL CORPUS, NEWS COMMENTARY, and 109 FRENCH-ENGLISH corpus …","url":["https://www.aclweb.org/anthology/2020.lrec-1.442.pdf"]}
{"year":"2020","title":"Seeking Meaning: Examining a Cross-situational Solution to Learn Action Verbs Using Human Simulation Paradigm","authors":["Y Zhang, A Amatuni, E Cain, C Yu"],"snippet":"… of words in a given corpus. We used the GloVe model pretrained on 840B tokens of Common Crawl text to create semantic distance measures (Pennington, Socher & Manning, 2014). We discovered that both semantic knowledge …","url":["https://cognitivesciencesociety.org/cogsci20/papers/0705/0705.pdf"]}
{"year":"2020","title":"Self-training Improves Pre-training for Natural Language Understanding","authors":["J Du, E Grave, B Gunel, V Chaudhary, O Celebi, M Auli… - arXiv preprint arXiv …, 2020"],"snippet":"… As a large-scale external bank of unannotated sentences, we extract and filter text from CommonCrawl 1 (Wenzek et al., 2019) … CommonCrawl data contains a wide variety of domains and text styles which makes it a good general-purpose corpus …","url":["https://arxiv.org/pdf/2010.02194"]}
{"year":"2020","title":"Semantic image retrieval","authors":["T Berg, PN Belhumeur - US Patent 10,769,502, 2020","T Berg, PN Belhumeur - US Patent App. 16/999,616, 2020"],"snippet":"… GloVe. Common Crawl is a public repository of web crawl data (eg, blogs, news, and comments) available on the internet in the commoncrawl.org domain, the entire contents of which is hereby incorporated by reference. GloVe …","url":["https://patents.google.com/patent/US20200380320A1/en","https://www.freepatentsonline.com/10769502.html"]}
{"year":"2020","title":"Semantic Matching: Dynamic Composition of Matcher Ensembles for Ontology Alignment","authors":["A Vennesland - 2020"],"snippet":"Page 1. ISBN 978-82-326-4842-9 (printed ver.) ISBN 978-82-326-4843- 6 (electronic ver.) ISSN 1503-8181 Doctoral theses at NTNU, 2020:247 Audun Vennesland Semantic Matching Dynamic Composition of Matcher …","url":["https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/2674337/Audun%20Vennesland_PhD.pdf?sequence=1"]}
{"year":"2020","title":"Semantic Networks for Engineering Design: A Survey","authors":["J Han, S Sarica, F Shi, J Luo - arXiv preprint arXiv:2012.07060, 2020"],"snippet":"… relations No Pre-trained word2vec (Mikolov et al., 2013) Unsupervised Google News Cosine similarity No Pre-trained GloVe (Pennington et al., 2014) Unsupervised Wikipedia, Gigaword, Common Crawl Cosine similarity No B-Link …","url":["https://arxiv.org/pdf/2012.07060"]}
{"year":"2020","title":"Semantic Norm Extrapolation is a Missing Data Problem","authors":["B Snefjella, I Blank - 2020"],"snippet":"Page 1. DRAFT Running head: SEMANTIC NORM EXTRAPOLATION 1 Semantic Norm Extrapolation is a Missing Data Problem Bryor Snefjella & Idan Blank University of California, Los Angeles Department of Psychology Author Note …","url":["https://psyarxiv.com/y2gav/download?format=pdf"]}
{"year":"2020","title":"Semantic Recommendations of Books Using Recurrent Neural Networks","authors":["M Nitu, S Ruseti, M Dascalu, S Tomescu - Ludic, Co-design and Tools Supporting Smart …"],"snippet":"… We conducted the experiments using pre-trained FastText embeddings for Romanian language. The embedding model consists of 2 million word vectors trained on Common Crawl and Wikipedia (approx. 600 billion tokens). Page 239. 240 M. Nitu et al. Fig …","url":["https://link.springer.com/content/pdf/10.1007/978-981-15-7383-5.pdf#page=234"]}
{"year":"2020","title":"Semantic-Based Algorithm for Scoring Alternative Uses Tests (AUT)","authors":["C Stevenson"],"snippet":"… Page 6. Each response was mapped to a word vector in 300 dimensions using a fastText pretrained model for Dutch. This model was trained on Wikipedia and Common Crawl data, using CBOW model with character …","url":["http://modelingcreativity.org/blog/wp-content/uploads/2020/07/Tsai_Y_BDS_Thesis_report_11695986_PML.pdf"]}
{"year":"2020","title":"Semantical Search Term Clustering for Performance Prediction","authors":["R Coenders"],"snippet":"Page 1. Eindhoven University of Technology MASTER Semantical search term clustering for performance prediction Coenders, R. Award date: 2019 Link to publication Disclaimer This document contains a student thesis (bachelor's …","url":["https://research.tue.nl/files/139495213/Thesis_RikCoenders_Aug2019.pdf"]}
{"year":"2020","title":"Semi-autonomous methodology to validate and update customer needs database through text data analytics","authors":["AM Bigorra, O Isaksson, M Karlberg - International Journal of Information …, 2020"],"snippet":"… Two different pre-trained word vectors based on 1 and 2 million English words from Wikipedia and Common Crawl are considered and they are referred along the rest of the presented paper as emb1 and emb2, respectively 3 …","url":["https://www.sciencedirect.com/science/article/pii/S0268401219300817"]}
{"year":"2020","title":"SemSeq: A Regime for Training Widely-Applicable Word-Sequence Encoders","authors":["H Tsuyuki, TY Ogawa, HTB Kobayashi - … : 16th International Conference of the Pacific …, 2020"]}
{"year":"2020","title":"Sense Inventories for Arabic Texts","authors":["M Alian, A Awajan"],"snippet":"… E. Fasttext pre-trained embeddings Arabic Fasttext embeddings are provided by Grave et al. [16]. These embeddings are resulted from training on Wikipedia and Common Crawl corpus. They have used an extension of the Fasttext model with subword information …","url":["https://www.researchgate.net/profile/Marwah_Alian/publication/346785930_Sense_Inventories_for_Arabic_Texts/links/5fd0a6a745851568d14da099/Sense-Inventories-for-Arabic-Texts.pdf"]}
{"year":"2020","title":"Sentence Matching with Deep Self-attention and Co-attention Features","authors":["Z Wang, D Yan - 2020","Z Wang, D Yan - … Conference on Knowledge Science, Engineering and …, 2021"],"snippet":"… 4.1 Implementation Details. In our experiments, word embedding vectors are initialized with 300d GloVe vectors pre-trained from the 840B Common Crawl corpus. Embeddings of out of the vocabulary of GloVe is initialized …","url":["https://link.springer.com/chapter/10.1007/978-3-030-82147-0_45","https://openreview.net/pdf?id=EEV7-ruXM5H"]}
{"year":"2020","title":"Sentence-Embedding and Similarity via Hybrid Bidirectional-LSTM and CNN Utilizing Weighted-Pooling Attention","authors":["D HUANG, A AHMED, SY ARAFAT, KI RASHID… - IEICE TRANSACTIONS on …, 2020"],"snippet":"… bag-of-words architecture. • A GloVe is a 300-dimensional word embedding model learned on aggregated global word co-occurrence statistics from Common Crawl (840 billion to- kens) [32]. 4.2 Datasets The datasets are concisely …","url":["https://www.jstage.jst.go.jp/article/transinf/E103.D/10/E103.D_2018EDP7410/_pdf"]}
{"year":"2020","title":"Sentiment Analysis Approach Based on Combination of Word Embedding Techniques","authors":["I Kaibi, H Satori - Embedded Systems and Artificial Intelligence, 2020"],"snippet":"… The fastText pre-trained word vectors is a high-quality word representation for 157 languages, two sources of data are used to train fastText pre-trained models: the free online encyclopedia Wikipedia and data from the common crawl project …","url":["https://link.springer.com/chapter/10.1007/978-981-15-0947-6_76"]}
{"year":"2020","title":"Sentiment Analysis based Multi-person Multi-criteria Decision Making Methodology: Using Natural Language Processing and Deep Learning for Decision Aid","authors":["C Zuheros, E Martínez-Cámara, E Herrera-Viedma… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. arXiv:2008.00032v1 [cs.CL] 31 Jul 2020 Sentiment Analysis based Multi-person Multi-criteria Decision Making Methodology: Using Natural Language Processing and Deep Learning for Decision Aid Cristina Zuheros1 …","url":["https://arxiv.org/pdf/2008.00032"]}
{"year":"2020","title":"Sentiment analysis for customer relationship management: an incremental learning approach","authors":["N Capuano, L Greco, P Ritrovato, M Vento - Applied Intelligence, 2020"],"snippet":"… text corpora. In particular, Universal Dependencies and WikiNER corpora [31] were used for Italian, while OntoNotes [32] and Common Crawl Footnote 3 corpora were used for English. The classification model. Once the WEs …","url":["https://link.springer.com/article/10.1007/s10489-020-01984-x"]}
{"year":"2020","title":"Sentiment Analysis for Hinglish Code-mixed Tweets by means of Cross-lingual Word Embeddings","authors":["P Singh, E Lefever - LREC 2020–4th Workshop on Computational …, 2020"],"snippet":"… This can probably be attributed to the quality of the monolingual embeddings, since the English embeddings were trained on the vast Common Crawl data while the Code-Mixed embeddings were trained on a little more than 100,000 scraped tweets …","url":["https://biblio.ugent.be/publication/8662137/file/8662140"]}
{"year":"2020","title":"Sentiment Aware Word Embeddings Using Refinement and Senti-Contextualized Learning Approach","authors":["B Naderalvojoud, EA Sezer - Neurocomputing, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0925231220304811"]}
{"year":"2020","title":"Sentiment detection with FedMD: Federated Learning via Model Distillation","authors":["PTG Momcheva"],"snippet":"… value due to the nature of tweets - they are short, have little context and contain misspelled and shortened words, all of which stands in general contrast to the GloVe training data and logic, which was based on structured …","url":["http://ceur-ws.org/Vol-2656/paper24.pdf"]}
{"year":"2020","title":"Seq2Seq Models for Recommending Short Text Conversations","authors":["J Torres, C Vaca, L Terán, CL Abad - Expert Systems with Applications, 2020"],"snippet":"… to a lower-dimensional representation ( w ∈ R d ). For the initialization of the word embeddings, we use the pre-trained vectors provided by Mikolov, Grave, Bojanowski, Puhrsch, and Joulin (2018), which consist of 2 …","url":["https://www.sciencedirect.com/science/article/pii/S0957417420300956"]}
{"year":"2020","title":"Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue","authors":["B Kim, J Ahn, G Kim - arXiv preprint arXiv:2002.07510, 2020"],"snippet":"Page 1. Published as a conference paper at ICLR 2020 SEQUENTIAL LATENT KNOWLEDGE SELECTION FOR KNOWLEDGE-GROUNDED DIALOGUE Byeongchang Kim Jaewoo Ahn Gunhee Kim Department of Computer …","url":["https://arxiv.org/pdf/2002.07510"]}
{"year":"2020","title":"Sequential Neural Networks for Noetic End-to-End Response Selection","authors":["Q Chen, W Wang - Computer Speech & Language, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S088523082030005X"]}
{"year":"2020","title":"Sequential Transfer Learning for Event Detection and Key Sentence Extraction","authors":["A Ollagnier, H Williams"],"snippet":"… Training is performed on the base version trained on cased text. XLNet [22] 12-layer, 768-hidden, 12-heads 110M parameters Pre-trained models are based on English texts from Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl …","url":["https://www.researchgate.net/profile/Ollagnier_Anais/publication/344440133_Sequential_Transfer_Learning_for_Event_Detection_and_Key_Sentence_Extraction/links/5f75ad1a299bf1b53e0397ce/Sequential-Transfer-Learning-for-Event-Detection-and-Key-Sentence-Extraction.pdf"]}
{"year":"2020","title":"Shopify in Germany: An analysis of a Canadian e-commerce platform's marketing strategy and activities in an international market.","authors":["K Howe-Patterson, I Schuiling"],"snippet":"Page 1. Available at: http://hdl.handle.net/2078.1/thesis:25661 [Downloaded 2020/09/25 at 09:14:22 ] \"Shopify in Germany: An analysis of a Canadian e-commerce platform's marketing strategy and activities in an international …","url":["https://dial.uclouvain.be/downloader/downloader.php?pid=thesis%3A25661&datastream=PDF_01&cover=cover-mem"]}
{"year":"2020","title":"Sidecar: Augmenting Word Embedding Models with Expert Knowledge","authors":["M Lemay, D Shapiro, MK MacPherson, K Yee… - Future of Information and …, 2020"],"snippet":"… spaCy's en_core_web_lg GloVe model 8 , trained on Common Crawl 9. Facebook's crawl-300d-2M fastText model 10 , also trained on Common Crawl. All three models produce vectors of size 300. To generate vectors for the …","url":["https://link.springer.com/chapter/10.1007/978-3-030-39442-4_39"]}
{"year":"2020","title":"Sights, titles and tags: mining a worldwide photo database for sightseeing","authors":["A Luberg, J Pindis, T Tammet"],"snippet":"… Explosion AI spaCy pretrained model [7]. We use a medium sized web code model. The model is based on Common Crawl and OntoNotes 5 [22] sources … Facebook fastText pretrained model [8]. The model is pretrained on Common Crawl and Wikipedia data …","url":["http://wims2020.sigappfr.org/wp-content/uploads/2020/06/WIMS'20/p149-Luberg.pdf"]}
{"year":"2020","title":"SimAlign: High Quality Word Alignments without Parallel Training Data using Static and Contextualized Embeddings","authors":["MJ Sabet, P Dufter, H Schütze - arXiv preprint arXiv:2004.08728, 2020"],"snippet":"… In addition, we use XLM-RoBERTa base (Conneau et al., 2019), which is pretrained on 100 languages on CommonCrawl data. We denote alignments obtained using the embeddings from the i-th layer by XLM-R[i] …","url":["https://arxiv.org/pdf/2004.08728"]}
{"year":"2020","title":"Similarity judgment within and across categories: A comprehensive model comparison","authors":["R Richie, S Bhatia - 2020"],"snippet":"… Google News 100B 300 None Magnitude Librarya fastText 600B Common Crawl FastText with Continuous Bag of Words (CBOW) … GloVe 840B Common Crawl GloVe Common Crawl 840B 300 None Magnitude Library Glove 840B Common Crawl, Paragram …","url":["https://psyarxiv.com/5pa9r/download"]}
{"year":"2020","title":"Simulation Induces Durable, Extensive Changes to Self-knowledge","authors":["J Rubin-McGregor, Z Zhao, D Tamir - PsyArXiv. December, 2020"],"snippet":"Page 1. SIMULATION CHANGES SELF-KNOWLEDGE 1 1 2 3 4 5 6 Simulation Induces Durable, Extensive Changes to Self-Knowledge 7 Jordan Rubin-McGregora, Zidong Zhaoa, and Diana Tamira 8 aDepartment of …","url":["https://psyarxiv.com/m2wgk/download/?format=pdf"]}
{"year":"2020","title":"SINAI at eHealth-KD Challenge 2020: Combining Word Embeddings for Named Entity Recognition in Spanish Medical Records","authors":["P López-Úbedaa, JM Perea-Ortegab…"],"snippet":"… we have used two specific pre-trained word embeddings: BETO [28], which follows a BERT model trained on a big Spanish corpus, and XLM-RoBERTa [29], which were generated by using a large multilingual language model …","url":["http://ceur-ws.org/Vol-2664/eHealth-KD_paper7.pdf"]}
{"year":"2020","title":"Siva at WNUT-2020 Task 2: Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets","authors":["S Sai - Proceedings of the Sixth Workshop on Noisy User …, 2020"],"snippet":"… The base version of RoBERTa has 125M parameters, and the large version has 355M parameters. XLM-RoBERTa XLM-RoBERTa(Conneau et al., 2019) is a multilingual model trained on 2.5 TB data from CommonCrawl. This …","url":["https://www.aclweb.org/anthology/2020.wnut-1.45.pdf"]}
{"year":"2020","title":"SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task","authors":["Z Li, H Zhao, R Wang, K Chen, M Utiyama, E Sumita - arXiv preprint arXiv …, 2020"],"snippet":"… In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model …","url":["https://arxiv.org/pdf/2010.05122"]}
{"year":"2020","title":"SMAN: Stacked Multi-Modal Attention Network for Cross-Modal Image-Text Retrieval","authors":["BR Loss"],"snippet":"Page 1. warwick.ac.uk/lib-publications Manuscript version: Author's Accepted Manuscript The version presented in WRAP is the author's accepted manuscript and may differ from the published version or Version of Record. Persistent …","url":["https://pdfs.semanticscholar.org/7588/90bef9a1a85a25a1f6831a58f00a462476af.pdf"]}
{"year":"2020","title":"SML: Semantic Meta-learning for Few-shot Semantic Segmentation","authors":["AK Pambala, T Dutta, S Biswas - arXiv preprint arXiv:2009.06680, 2020"],"snippet":"… Word2vec (Mikolov et al. 2013) is trained on Google News dataset (Wang, Ye, and Gupta 2018) which contains 3-million words; (2) FastText (Joulin et al. 2016) is trained on Common-Crawl dataset (Mikolov et al. 2018). We use these …","url":["https://arxiv.org/pdf/2009.06680"]}
{"year":"2020","title":"SNK@ DANKMEMES: Leveraging Pretrained Embeddings for Multimodal Meme Detection","authors":["S Fiorucci - Proceedings of Seventh Evaluation Campaign of …, 2020"],"snippet":"… et al., 2018). Word embeddings are trained on Common Crawl and Wikipedia, using CBOW with positionweights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We calculated the …","url":["http://ceur-ws.org/Vol-2765/paper121.pdf"]}
{"year":"2020","title":"Social biases in word embeddings and their relation to human cognition","authors":["A Caliskan, M Lewis - 2020"],"snippet":"… state-of-the-art word embeddings is the vast amount of training data available. GloVe is trained on 840 billion tokens and more than 2 million unique words of Common Crawl data which is a crawl of the entire world wide web. Similarly, Word2vec is trained on a …","url":["https://psyarxiv.com/d84kg/download?format=pdf"]}
{"year":"2020","title":"Social Media Attributions in the Context of Water Crisis","authors":["R Sarkar, H Sarkar, S Mahinder, AR KhudaBukhsh - arXiv preprint arXiv:2001.01697, 2020"],"snippet":"… used embedding in this preprocessing step. We used the 300 dimensional GloVe model trained on 840 billion tokens of the CommonCrawl corpus, having a vocabulary size of 2.2 million. While calculating the embedding of …","url":["https://arxiv.org/pdf/2001.01697"]}
{"year":"2020","title":"Sociolinguistic Properties of Word Embeddings","authors":["A Arseniev-Koehler, JG Foster - SocArXiv. August, 2020"],"snippet":"… These studies use large, commonly available pre-trained embeddings or their training corpora, such as Google News, web data (Common Crawl), and Google Books … They replicated results using a pretrained model on Common Crawl data …","url":["https://osf.io/b8kud/download"]}
{"year":"2020","title":"Software for creating and analyzing semantic representations","authors":["FÅ Nielsen, LK Hansen - Statistical Semantics, 2020"],"snippet":"… This package provides models for the tagger, parser, named-entity recognizer and distributional semantic vectors trained on OntoNotes Release 5 and the Common Crawl dataset … 10 K–50 K. 300. 29 languages. GloVe. Common …","url":["https://link.springer.com/chapter/10.1007/978-3-030-37250-7_3"]}
{"year":"2020","title":"Spoken words as biomarkers: using machine learning to gain insight into communication as a predictor of anxiety","authors":["G Demiris, KL Corey Magan, D Parker Oliver… - Journal of the American …, 2020"],"snippet":"… The validity of using cosine distance in an embedding space to measure text similarity depends largely on how well the embedding space represents the semantic concepts present in the text. In our case, the word embeddings …","url":["https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocaa049/5831105"]}
{"year":"2020","title":"SPONTANEOUS STEREOTYPE CONTENT: MEASUREMENT AIMING TOWARD THEORETICAL INTEGRATION AND DISCOVERY","authors":["G Nicolas Ferreira - 2020","GN Ferreira - 2020"],"snippet":"Page 1. SPONTANEOUS STEREOTYPE CONTENT: MEASUREMENT AIMING TOWARD THEORETICAL INTEGRATION AND DISCOVERY GANDALF NICOLAS FERREIRA A DISSERTATION PRESENTED TO THE FACULTY OF PRINCETON UNIVERSITY IN …","url":["http://search.proquest.com/openview/41d33da8e87d459690442733f719668f/1?pq-origsite=gscholar&cbl=18750&diss=y","https://dataspace.princeton.edu/bitstream/88435/dsp01zp38wg55d/1/NicolasFerreira_princeton_0181D_13366.pdf"]}
{"year":"2020","title":"Stanza: A Python Natural Language Processing Toolkit for Many Human Languages","authors":["P Qi, Y Zhang, Y Zhang, J Bolton, CD Manning - arXiv preprint arXiv:2003.07082, 2020"],"snippet":"… For the character-level language models in the NER component, we pretrained them on a mix of the Common Crawl and Wikipedia dumps, and the news corpora released by the WMT19 Shared Task (Barrault et al., 2019), with …","url":["https://arxiv.org/pdf/2003.07082"]}
{"year":"2020","title":"STIL--Simultaneous Slot Filling, Translation, Intent Classification, and Language Identification: Initial Results using mBART on MultiATIS++","authors":["JGM FitzGerald - arXiv preprint arXiv:2010.00760, 2020"],"snippet":"… The mBART.cc25 model was trained on 25 languages for 500k steps using a 1.4 TB corpus of scraped website data taken from Common Crawl (Wenzek et al., 2019). The model was trained to reconstruct masked tokens and to rearrange scrambled sentences …","url":["https://arxiv.org/pdf/2010.00760"]}
{"year":"2020","title":"STILTool: A Semantic Table Interpretation evaLuation Tool","authors":["E Jimenez-Ruiz, A Maurino - The Semantic Web: ESWC 2020 Satellite Events …","M Cremaschi, A Siano, R Avogadro, E Jimenez-Ruiz…"],"snippet":"… In order to size the spread of tabular data, 2.5 M tables have been identified within the Common Crawl repository1 [3]. The current snapshot of Wikipedia contains more than 3.23 M tables from more than 520k Wikipedia articles …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=C0UIEAAAQBAJ&oi=fnd&pg=PA61&dq=commoncrawl&ots=OcUKD8orbe&sig=5EUZjTQOLRGuwqaWXWRmrck1S50","https://preprints.2020.eswc-conferences.org/posters_demos/paper_293.pdf"]}
{"year":"2020","title":"Structured deep neural network with low complexity","authors":["S Liao - 2020"],"snippet":"Page 1. STRUCTURED DEEP NEURAL NETWORK WITH LOW COMPLEXITY By SIYU LIAO A dissertation submitted to the School of Graduate Studies Rutgers, The State University of New Jersey in partial fulfillment of the …","url":["https://rucore.libraries.rutgers.edu/rutgers-lib/64996/PDF/1/"]}
{"year":"2020","title":"Study and Creation of Datasets for Comparative Questions Classification","authors":["S Stahlhacke"],"snippet":"… The data used by the system is a preprocessed version of the Common Crawl Text Corpus8, which crawled from the world wide web … Which one is better suited for me, Xbox One or PS4? 8https://commoncrawl.org/ 4 Page 11. CHAPTER 1. INTRODUCTION …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2020-ma-stahlhacke.pdf"]}
{"year":"2020","title":"Studying the Evolution of Greek Words via Word Embeddings","authors":["V Barzokas, E Papagiannopoulou, G Tsoumakas - 11th Hellenic Conference on …, 2020"],"snippet":"… Despite the limited size of the Greek corpus compared to Common Crawl and Wikipedia used for the pre-trained fastText embeddings, we didn't detect any notable difference in the quality of our models in comparison with the pre-trained one …","url":["https://dl.acm.org/doi/abs/10.1145/3411408.3411425"]}
{"year":"2020","title":"Substance over Style: Document-Level Targeted Content Transfer","authors":["A Hegel, S Rao, A Celikyilmaz, B Dolan - arXiv preprint arXiv:2010.08618, 2020"],"snippet":"Page 1. Substance over Style: Document-Level Targeted Content Transfer Allison Hegel1∗ Sudha Rao2 Asli Celikyilmaz2 Bill Dolan2 1Lexion, Seattle, WA, USA 2Microsoft Research, Redmond, WA, USA allison@lexion.ai {sudhra,aslicel,billdol}@microsoft.com Abstract …","url":["https://arxiv.org/pdf/2010.08618"]}
{"year":"2020","title":"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation","authors":["A Rios, M Müller, R Sennrich - arXiv preprint arXiv:2011.01703, 2020","AR Gonzales, M Müller, R Sennrich - Proceedings of the Fifth Conference on …, 2020"],"snippet":"… Page 3. corpora training dev test Language Pairs with English: de↔en Commoncrawl, Europarl-v9, Wikititles-v1 5M 250 2000 cs↔en Europarl-v9, CzEng1.7 5M 250 2000 fr↔en Commoncrawl, Europarl-v7 …","url":["https://arxiv.org/pdf/2011.01703","https://www.aclweb.org/anthology/2020.wmt-1.64.pdf"]}
{"year":"2020","title":"Suggesting Citations for Wikidata Claims based on Wikipedia's External References","authors":["P Curotto, A Hogan"],"snippet":"… Offline: Given that some Wikidata items do not have an associated Wikipedia article, that many Wikipedia articles have few references, etc., it would be interesting to develop a broader corpus with more documents from the Web, perhaps from the Common Crawl …","url":["http://aidanhogan.com/docs/wikidata-references.pdf"]}
{"year":"2020","title":"Supervised Understanding of Word Embeddings","authors":["HZ Yerebakan, P Bhatia, Y Shinagawa"],"snippet":"… In our experiments, we have used scikit-learn linear logistic regression model with a positive class weight of 2 to enhance the effect of positive words. We have used top 250k words of Fasttext Common Crawl word …","url":["https://rcqa-ws.github.io/papers/paper8.pdf"]}
{"year":"2020","title":"Surface pattern-enhanced relation extraction with global constraints","authors":["H Jiang, JT Liu, S Zhang, D Yang, Y Xiao, W Wang - Knowledge and Information …, 2020"],"snippet":"Relation extraction is one of the most important tasks in information extraction. The traditional works either use sentences or surface patterns (ie, the.","url":["https://link.springer.com/article/10.1007/s10115-020-01502-y"]}
{"year":"2020","title":"Survey on RNN and CRF models for de-identification of medical free text","authors":["JL Leevy, TM Khoshgoftaar, F Villanustre - Journal of Big Data, 2020"],"snippet":"The increasing reliance on electronic health record (EHR) in areas such as medical research should be addressed by using ample safeguards for patient privacy. These records often tend to be big data, and given that a significant …","url":["https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00351-4"]}
{"year":"2020","title":"SYMPTOM EXTRACTION FROM ATRIAL FIBRILLATION PATIENT CLINICAL NOTES USING DEEP LEARNING","authors":["TET van Putten"],"snippet":"Page 1. Eindhoven University of Technology MASTER Symptom extraction from atrial fibrillation patient clinical notes using deep learning van Putten, TE Award date: 2020 Link to publication Disclaimer This document contains …","url":["https://pure.tue.nl/ws/portalfiles/portal/163432620/Master_Thesis_Tim_van_Putten.pdf"]}
{"year":"2020","title":"Syntax Role for Neural Semantic Role Labeling","authors":["Z Li, H Zhao, S He, J Cai - arXiv preprint arXiv:2009.05737, 2020"],"snippet":"Page 1. Syntax Role for Neural Semantic Role Labeling Zuchao Li Shanghai Jiao Tong University Department of Computer Science and Engineering charlee@sjtu. edu.cn Hai Zhao∗ Shanghai Jiao Tong University Department …","url":["https://arxiv.org/pdf/2009.05737"]}
{"year":"2020","title":"System and method for model derivation for entity prediction","authors":["FI Wyss, A Ganapathiraju, P Buduguppa - US Patent App. 16/677,989, 2020"],"snippet":"… The dense representation can be used to capture the contextual information. For an NER system, the information can be encoded in the form of “world knowledge' by using a corpus such as the Wikipedia corpus or Google's common crawl data …","url":["https://patents.google.com/patent/US20200151248A1/en"]}
{"year":"2020","title":"Systematic Mapping on Embedded Semantic Markup Validated with Data Mining Techniques","authors":["R Navarrete, C Montenegro, L Recalde - … Conference on Applied Human Factors and …, 2020"],"snippet":"… Markup format: microdata, rdfa, jsonld. Approach of the Research: ads, commerce, commoncrawl, crawl, deploy, education, egovernment, entity, error, extract, extraction, fix, government, learning, lod, mistake, owl, pld, plds, rdf, video, wdc, webdatacommons …","url":["https://link.springer.com/chapter/10.1007/978-3-030-51328-3_53"]}
{"year":"2020","title":"SYSTEMS AND METHODS FOR LEARNING USER REPRESENTATIONS FOR OPEN VOCABULARY DATA SETS","authors":["T Durand, G Mori - US Patent App. 16/826,215, 2020"],"snippet":"Systems and methods adapted for training a machine learning model to predict data labels are described. The approach includes receiving a first data set comprising first data objects and associated fi.","url":["https://www.freepatentsonline.com/y2020/0302340.html"]}
{"year":"2020","title":"TabEAno: Table to Knowledge Graph Entity Annotation","authors":["P Nguyen, N Kertkeidkachorn, R Ichise, H Takeda - arXiv preprint arXiv:2010.01829, 2020"],"snippet":"… Note that, tables in this study refer to relational vertical tables. A 3 Open Data Vision: https://opendatabarometer.org 4 Common Crawl: http://commoncrawl.org/ arXiv:2010.01829v1 [cs.AI] 5 Oct 2020 Page 2. 2 Phuc Nguyen et al …","url":["https://arxiv.org/pdf/2010.01829"]}
{"year":"2020","title":"TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data","authors":["P Yin, G Neubig, W Yih, S Riedel - arXiv preprint arXiv:2005.08314, 2020"],"snippet":"… Page 5. interesting avenue for future work. Specifically, we collect tables and their surrounding NL text from English Wikipedia and the WDC WebTable Corpus (Lehmberg et al., 2016), a large-scale table collection from CommonCrawl …","url":["https://arxiv.org/pdf/2005.08314"]}
{"year":"2020","title":"Tagging Reading Comprehension Materials with Document Extraction Attention Networks","authors":["B Sun, Y Zhu, R Xiao, Y Xiao, YG Wei - IEEE Transactions on Learning …, 2020"],"snippet":"Page 1. 1939-1382 (c) 2020 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/9079601/"]}
{"year":"2020","title":"Tagging with Weak Labels. Paper presented at AAAI Conference on","authors":["E Simpson, J Pfeiffer, I Gurevych"],"snippet":"… For FAMULUS, we use 300-dimensional German fastText embeddings (Grave et al. 2018), and for NER and PICO we use 300-dimensional English GloVe 3 embeddings trained on 840 billion tokens from Common Crawl. To …","url":["https://research-information.bris.ac.uk/files/225055068/AAAI_Low_Resource_Sequence_Tagging_with_Weak_Labels.pdf"]}
{"year":"2020","title":"Tailored retrieval of health information from the web for facilitating communication and empowerment of elderly people","authors":["M Alfano, B Lenzitti, D Taibi, M Helfert - 2020"],"snippet":"… contain all Microformat, Microdata and RDFa (Resource Description Framework in Attributes) data extracted from the open repository of Web crawl data named Common Crawl (CC)9 … 8 http://webdatacommons.org/ 9 …","url":["http://doras.dcu.ie/24469/2/ICT4AWE_2020_40_CR.pdf"]}
{"year":"2020","title":"Target driven visual navigation exploiting object relationships","authors":["Y Qiu, A Pal, HI Christensen - arXiv preprint arXiv:2003.06749, 2020"],"snippet":"… For the word embeddings, we used the 300-D GloVe vectors pretrained on 840 billion tokens of Common Crawl [49]. The A3C model is based on [50], and the model hyperparameters used were: learning rate …","url":["https://arxiv.org/pdf/2003.06749"]}
{"year":"2020","title":"Targeted Poisoning Attacks on Black-Box Neural Machine Translation","authors":["C Xu, J Wang, Y Tang, F Guzman, BIP Rubinstein… - arXiv preprint arXiv …, 2020"],"snippet":"… 3We find that the crawling services commonly used for parallel data collection, eg, Common Crawl (commoncrawl.org), are also fetching news articles from self-publishing sources like blogs (eg, with a subdomain of blogspot.com) …","url":["https://arxiv.org/pdf/2011.00675"]}
{"year":"2020","title":"Tell and guess: cooperative learning for natural image caption generation with hierarchical refined attention","authors":["W Zhang, S Tang, J Su, J Xiao, Y Zhuang - Multimedia Tools and Applications, 2020"],"snippet":"… Implementation details. We use Tensorflow to implement our model and its variants. Given a textual caption, we employ the word2vec model (ie, GloVe word embedding [22] ) which is pre-trained on the Common Crawl dataset [25] …","url":["https://link.springer.com/article/10.1007/s11042-020-08832-7"]}
{"year":"2020","title":"Tell Me Why You Feel That Way: Processing Compositional Dependency for Tree-LSTM Aspect Sentiment Triplet Extraction (TASTE)","authors":["A Sutherland, S Bensch, T Hellström, S Magg… - International Conference on …, 2020"],"snippet":"… aligned}$$. (6). $$\\begin{aligned}&{h}_{j} = o_j \\odot tanh(c_j). \\end{aligned}$$. (7). Words in a sentence are represented as Word Embeddings from the pre-trained Common-Crawl 840 B data 2 before they are fed to the DTLSTM. To …","url":["https://link.springer.com/chapter/10.1007/978-3-030-61609-0_52"]}
{"year":"2020","title":"Tencent AI Lab machine translation systems for the WMT20 chat translation task","authors":["L Wang, Z Tu, X Wang, L Ding, L Ding, S Shi - Proceedings of the Fifth Conference on …, 2020"],"snippet":"… Out-of-domain Parallel Data The participants are allowed to use all the training data in the News shared task.4 Thus, we combine six corpora including Euporal, ParaCrawl, CommonCrawl, TildeRapid, NewsCommentary and WikiMatrix …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.60.pdf"]}
{"year":"2020","title":"Testing pre-trained Transformer models for Lithuanian news clustering","authors":["L Stankevičius, M Lukoševičius - arXiv preprint arXiv:2004.03461, 2020"],"snippet":"… Specifically, we will use well known baselines – multilingual BERT and recently published XLM-R, trained on more than two terabytes of filtered CommonCrawl data. We chose clustering task to also try to advance the field of data mining …","url":["https://arxiv.org/pdf/2004.03461"]}
{"year":"2020","title":"Text as data: a machine learning-based approach to measuring uncertainty","authors":["R Nyman, P Ormerod - arXiv preprint arXiv:2006.06457, 2020"],"snippet":"… The authors assemble a very large corpus of words from various sources. We use the one described on the GloVe website as Common Crawl (glove.42B.300d.zip). A co-occurrence matrix is constructed, which describes …","url":["https://arxiv.org/pdf/2006.06457"]}
{"year":"2020","title":"Text Classification: Exploiting the Social Network","authors":["SBM Alkhereyf"],"snippet":"Page 1. Text Classification: Exploiting the Social Network Sakhar Alkhereyf Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy under the Executive Committee of the Graduate School of …","url":["https://academiccommons.columbia.edu/doi/10.7916/d8-t2jv-sb09/download"]}
{"year":"2020","title":"Text mining-based construction site accident classification using hybrid supervised machine learning","authors":["MY Cheng, D Kusoemo, RA Gosno - Automation in Construction, 2020"],"snippet":"… There are various pre-trained databases in the GloVe website that is open for public, such as Wikipedia database consists of 6 billion words and 100 dimension, Common Crawl database consists of 42 billion words and 300 …","url":["https://www.sciencedirect.com/science/article/pii/S092658051931341X"]}
{"year":"2020","title":"Text-based classification of interviews for mental health--juxtaposing the state of the art","authors":["JV Wouts - arXiv preprint arXiv:2008.01543, 2020"],"snippet":"… Model name Pretrain corpus Tokenizer type Acc Sentiment analysis belabBERT Common Crawl Dutch (non-shuffled) BytePairEncoding 95.92∗ % RobBERT Common Crawl Dutch (shuffled) BytePairEncoding 94.42 …","url":["https://arxiv.org/pdf/2008.01543"]}
{"year":"2020","title":"TextSETTR: Label-Free Text Style Extraction and Tunable Targeted Restyling","authors":["P Riley, N Constant, M Guo, G Kumar, D Uthus… - arXiv preprint arXiv …, 2020"],"snippet":"… Furthermore, we demonstrate that a single model trained on unlabeled Common Crawl data is capable of transferring along multiple dimensions including dialect, emotiveness, formality, politeness, and sentiment. 1 INTRODUCTION …","url":["https://arxiv.org/pdf/2010.03802"]}
{"year":"2020","title":"TF-CR: Weighting Embeddings for Text Classification","authors":["A Zubiaga - arXiv preprint arXiv:2012.06606, 2020"],"snippet":"… Page 6. • cglove: GloVe embeddings trained from Common Crawl. • wglove: GloVe embeddings trained from Wikipedia.6 We use two different classifiers for these experiments, SVM and Logistic Regression, which are known …","url":["https://arxiv.org/pdf/2012.06606"]}
{"year":"2020","title":"The 2019 BBN Cross-lingual Information Retrieval System","authors":["DK Le Zhang, W Hartmann, M Srivastava, L Tarlin… - LREC 2020 Language Resources …","L Zhang, D Karakos, W Hartmann, M Srivastava… - Proceedings of the …, 2020"],"snippet":"… 4.1. Training Data The primary data source for constructing MT models is parallel data from the build language pack, augmented with a variety of web data, such as CommonCrawl2 and open parallel corpus (Tiedemann …","url":["http://www.lrec-conf.org/proceedings/lrec2020/workshops/CLSSTS2020/CLSSTS-2020.pdf#page=49","https://www.aclweb.org/anthology/2020.clssts-1.8.pdf"]}
{"year":"2020","title":"The 2020 bilingual, bi-directional webnlg+ shared task overview and evaluation results (webnlg+ 2020)","authors":["TC Ferreira, C Gardent, C van der Lee, N Ilinykh… - Proceedings of the 3rd …, 2020"],"snippet":"… 3.3 Mono-task, Bilingual Approaches cuni-ufal. The mBART model (Liu et al., 2020) is pre-trained for multilingual denoising on the large-scale multilingual CC25 corpus extracted from Common Crawl, which contains …","url":["https://webnlg-challenge.loria.fr/files/2020.webnlg-papers.7.pdf"]}
{"year":"2020","title":"THE ABILITY OF WORD EMBEDDINGS TO CAPTURE WORD SIMILARITIES","authors":["M Toshevska, F Stojanovska, J Kalajdjieski"],"snippet":"… architectures [25]. In our experiments, we have used pre-trained models both trained with subword information on Wikipedia 2017 (16B tokens) and trained with subword information on Common Crawl (600B tokens)4. 2https …","url":["http://www.academia.edu/download/63915170/120200714-10552-nn915u.pdf"]}
{"year":"2020","title":"The ADAPT Centre's neural MT systems for the WAT 2020 document-level translation task","authors":["W Jooste, R Haque, A Way - 2020"],"snippet":"… Finally, source-language monolingual data with n-grams similar to that of the documents in the test set was mined from the Common Crawl Corpus6 to be used as a source-side original synthetic corpus (SOSC) for fine-tuning the NMT model parameters …","url":["http://doras.dcu.ie/25205/1/WAT_2020.pdf"]}
{"year":"2020","title":"The afrl wmt20 news-translation systems","authors":["J Gwinnup, T Anderson - Proceedings of the Fifth Conference on Machine …, 2020"],"snippet":"… Page 2. 207 corpus unfiltered lines filtered lines percent remain commoncrawl 723,256 655,069 90.57% newscommentaryv15 319,242 286,947 89.88% yandex 1,000,000 901,318 90.13 … 2013. Dirt cheap webscale parallel text from the common crawl …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.20.pdf"]}
{"year":"2020","title":"The Art of Reproducible Machine Learning","authors":["V Novotný - RASLAN 2020 Recent Advances in Slavonic Natural …, 2020"],"snippet":"… We then use the initializations to reproduce 14 the results of Mikolov et al.(2018)[19, Table 2] us- ing the subword cbow model of Bojanowski et al.(2017)[2] and the 2017 English Wikipedia 15 training corpus (4% of the …","url":["https://nlp.fi.muni.cz/raslan/raslan20.pdf#page=63"]}
{"year":"2020","title":"The birth of Romanian BERT","authors":["SD Dumitrescu, AM Avram, S Pyysalo - arXiv preprint arXiv:2009.08712, 2020"],"snippet":"… In total, the OPUS corpus contains around 4GB of Romanian text. OSCAR OSCAR (Ortiz Suárez et al., 2019), or Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language …","url":["https://arxiv.org/pdf/2009.08712"]}
{"year":"2020","title":"The Case For Alternative Web Archival Formats To Expedite The Data-To-Insight Cycle","authors":["X Wang, Z Xie - arXiv preprint arXiv:2003.14046, 2020"],"snippet":"… Large-scale, comprehensive web archiving initiatives include the Internet Archive [8], the Common Crawl [16], and many programs at national libraries and archives. These … 5.2 Data We chose to use Common Crawl's web …","url":["https://arxiv.org/pdf/2003.14046"]}
{"year":"2020","title":"The Challenge of Diacritics in Yoruba Embeddings","authors":["TP Adewumi, F Liwicki, M Liwicki - arXiv preprint arXiv:2011.07605, 2020"],"snippet":"… (2018) are tabulated: Wiki, U_Wiki, C3 & CC, representing embeddings from the cleaned Wikipedia dump, its undiacritized (normalized) version, the diacritized data from Alabi et al. (2020) and the Common Crawl embedding by Grave et al. (2018), respectively …","url":["https://arxiv.org/pdf/2011.07605"]}
{"year":"2020","title":"The Danish Gigaword Project","authors":["L Strømberg-Derczynski, R Baglini, MH Christiansen… - arXiv preprint arXiv …, 2020"],"snippet":"… Similarly, other huge monolithic datasets such as the Common Crawl Danish data suffer from large amounts of non-Danish content, possibly due to the pervasive confusion between Danish and Norwegian Bokmål … Common …","url":["https://arxiv.org/pdf/2005.03521"]}
{"year":"2020","title":"The ELTE. DH Pilot Corpus–Creating a Handcrafted Gigaword Web Corpus with Metadata","authors":["B Indig, Á Knap, Z Sárközi-Lindner, M Timári, G Palkó - … of the 12th Web as Corpus …, 2020"],"snippet":"… Nowadays, large corpora are utilising the Common Crawl archive like the OSCAR corpus (Ortiz Suárez et al., 2019) with 5.16 billion (2.33 … All of these corpora – except the ones based on Common Crawl – have the same …","url":["https://www.aclweb.org/anthology/2020.wac-1.5.pdf"]}
{"year":"2020","title":"The Emergence, Advancement and Future of Textual Answer Triggering","authors":["KN Acheampong, W Tian, EB Sifah… - Science and Information …, 2020"],"snippet":"… A similar observation is realized (\\(A_1 \\approx 0.8986\\); \\(A_2 \\approx 0.7352\\); difference in margin \\(\\approx 0.1634 \\)) when the model the 300-dimensional word vectors trained on Common Crawl with GloVe from spaCy is used …","url":["https://link.springer.com/chapter/10.1007/978-3-030-52246-9_50"]}
{"year":"2020","title":"The Geometry of Distributed Representations for Better Alignment, Attenuated Bias, and Improved Interpretability","authors":["S Dev - arXiv preprint arXiv:2011.12465, 2020"],"snippet":"… 39 5 RMSE variation with word frequency in (a) GloVe Wiki to GloVe Common Crawl and (b) word2vec to GloVe evaluated for Wiki dataset. All words were used for tests in lower case as listed in the table. . . . . 40 …","url":["https://arxiv.org/pdf/2011.12465"]}
{"year":"2020","title":"The NiuTrans Machine Translation Systems for WMT20","authors":["Y Zhang, Z Wang, R Cao, B Wei, W Shan, S Zhou… - Proceedings of the Fifth …, 2020"],"snippet":"… mentary, Common Crawl , TED Talks 4 Japanese monolingual data corpus about 1.7 billion. After the data filter, 12 million parallel data was left and 11 million selected by the neural language model was used as training data …","url":["https://www.aclweb.org/anthology/2020.wmt-1.37.pdf"]}
{"year":"2020","title":"The POLAR Framework: Polar Opposites Enable Interpretability of Pre-Trained Word Embeddings","authors":["B Mathew, S Sikdar, F Lemmerich, M Strohmaier - arXiv preprint arXiv:2001.09876, 2020"],"snippet":"… (2) GloVe embeddings [27]3 trained on Web data from Common Crawl … 3We used the Common Crawl embeddings with 42B tokens: https://nlp. stanford.edu/ projects/glove/ 4The datasets are available here: https://github …","url":["https://arxiv.org/pdf/2001.09876"]}
{"year":"2020","title":"The POLUSA Dataset: 0.9 M Political News Articles Balanced by Time and Outlet Popularity","authors":["L Gebhard, F Hamborg - arXiv preprint arXiv:2005.14024, 2020"],"snippet":"… RoBERTa: A Robustly Optimized BERT Pretraining Ap- proach. arXiv: 1907.11692 [cs] [6] Sebastian Nagel. 2016. Common Crawl – News Dataset Available. Retrieved May 8, 2020 from https://commoncrawl.org/2016/10 …","url":["https://arxiv.org/pdf/2005.14024"]}
{"year":"2020","title":"The presence of occupational structure in online texts based on word embedding NLP models","authors":["Z Kmetty, J Koltai, T Rudas - arXiv preprint arXiv:2005.08612, 2020"],"snippet":"… We used three pre-trained vector spaces in the analysis. The first vector model we used was trained on the English language texts of the Common Crawl (CC) corpus1, a huge web archive … 2016) 1 http://commoncrawl.org 2 …","url":["https://arxiv.org/pdf/2005.08612"]}
{"year":"2020","title":"The role of affective meaning, semantic associates, and orthographic neighbours in modulating the N400 in single words","authors":["F Blomberg, M Roll, J Frid, M Lindgren, M Horne - The Mental Lexicon, 2020"],"snippet":"… of fastText compared to other popular implementations (such as Word2Vec and Glove) is that it already has a model for Swedish trained on millions of words taken from the Swedish version of the free online encyclopedia …","url":["https://www.jbe-platform.com/content/journals/10.1075/ml.19021.blo"]}
{"year":"2020","title":"The Two-Pass Softmax Algorithm","authors":["M Dukhan, A Ablavatski - arXiv preprint arXiv:2001.04438, 2020"],"snippet":"Page 1. The Two-Pass Softmax Algorithm Marat Dukhan ∗1,2 and Artsiom Ablavatski1 1Google Research 2Georgia Institute of Technology Abstract The softmax (also called softargmax) function is widely used in machine learning …","url":["https://arxiv.org/pdf/2001.04438"]}
{"year":"2020","title":"The University of Edinburgh's English-Tamil and English-Inuktitut Submissions to the WMT20 News Translation Task","authors":["R Bawden, A Birch, R Dobreva, A Oncevay… - 5th Conference on Machine …, 2020"],"snippet":"… The only additional monolingual Inuktitut data was 163k sentences of common-crawl data, which we backtranslated for the English→Inuktitut system … Synthetic (from en Europarl) en-iu 650k Synthetic (from en News …","url":["https://hal.archives-ouvertes.fr/hal-02981153/document"]}
{"year":"2020","title":"The University of Edinburgh's submission to the German-to-English and English-to-German Tracks in the WMT 2020 News Translation and Zero-shot Translation …","authors":["U Germann - 2020"],"snippet":"… High-quality parallel data Europarl ca. 1.79 M Rapid ca. 1.45 M News Commentary ca. 0.35 M Crawled parallel data ParaCrawl 5.1 ca. 34.37 M CommonCrawl ca. 2.40 M WikiMatrix ca. 6.22 M WikiTitles ca. 1.38 M Monolingual crawled news data German ca …","url":["http://statmt.org/wmt20/pdf/2020.wmt-1.18.pdf"]}
{"year":"2020","title":"The University of Helsinki and Aalto University submissions to the WMT 2020 news and lowresource translation tasks","authors":["Y Scherrer, SA Grönroos, S Virpioja - the Fifth Conference on Machine Translation, 2020"],"snippet":"… NewsDiscuss 2019 2 000 000 1 000 000 CommonCrawl 80 244 80 244 … In terms of monolingual Inuktitut data, besides the unaligned NH data, the organizers only provided a CommonCrawl dump. This corpus was again backtranslated to English and filtered …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.134.pdf"]}
{"year":"2020","title":"The University of Helsinki submission to the IWSLT2020 Offline Speech Translation Task","authors":["R Vázquez, M Aulamo, U Sulubacak, J Tiedemann - The 17th International …, 2020"],"snippet":"… filter out noisy translations. OpenSubtitles2018, which consists of subtitle translations, and corpora gathered by crawling the internet, Common Crawl and ParaCrawl, are especially likely to contain noisy data. For filtering the …","url":["https://tuhat.helsinki.fi/ws/files/137272405/uni_helsinki_submission_iwslt_2020.pdf"]}
{"year":"2020","title":"The Unreasonable Effectiveness of Machine Learning in Moldavian versus Romanian Dialect Identification","authors":["M Găman, RT Ionescu - arXiv preprint arXiv:2007.15700, 2020"],"snippet":"… We note that these representations are learned from Romanian corpora, such as the corpus for contemporary Romanian language (CoRoLa) (Mititelu, Tufis, and Irimia 2018; Pais and Tufis 2018), Common Crawl (CC) and Wikipedia (Grave et al …","url":["https://arxiv.org/pdf/2007.15700"]}
{"year":"2020","title":"The Voice and Speech Processing within Language Technology Applications: Perspective of the Russian Data Protection Law","authors":["I Ilin"],"snippet":"… of collecting, systematizing and annotating language data are various language datasets such as Open Subtitles43, the Common Crawl da- taset44, the … 43 Available at: https://www.opensubtitles.org/ru (accessed: 18.05.2020) …","url":["https://www.researchgate.net/profile/Ilya_Ilin/publication/345237982_The_Voice_and_Speech_Processing_within_Language_Technology_Application_Perspective_of_the_Russian_Data_Protection_Law/links/5fa11750458515b7cfb5ce68/The-Voice-and-Speech-Processing-within-Language-Technology-Application-Perspective-of-the-Russian-Data-Protection-Law.pdf"]}
{"year":"2020","title":"The Volctrans Machine Translation System for WMT20","authors":["L Wu, X Pan, Z Lin, Y Zhu, M Wang, L Li - arXiv preprint arXiv:2010.14806, 2020"],"snippet":"… We use all parallel data available: Eu- roparl v10, ParaCrawl v5.1, Common Crawl corpus, News Commentary v15, Wiki Titles v2, Tilde Rapid corpus and WikiMatrix corpus … Each part contains 10M common crawl sentences and 3M Newscrawl sentences …","url":["https://arxiv.org/pdf/2010.14806"]}
{"year":"2020","title":"TheNorth@ HaSpeeDe 2: BERT-based Language Model Fine-tuning for Italian Hate Speech Detection","authors":["E Lavergne, R Saini, G Kovács, K Murphy"],"snippet":"… ERT. AlBERTo was pretrained on TWITA, that is a collection of Italian tweets (Polignano et al., 2019b). UmBERTo was pretrained on Commoncrawl ITA exploiting OSCAR Italian large corpus (Parisi et al., 2020). Finally, PoliBERT …","url":["http://ceur-ws.org/Vol-2765/paper135.pdf"]}
{"year":"2020","title":"This is a post-peer-review, pre-copyedit version of an article in press in Motivation and Emotion. The final authenticated version is available online at: http://dx. doi. org …","authors":["JS Pang, H Ring"],"snippet":"… datasets. Based on these experiments we decided to use Facebook's FastText subword embeddings of 300 dimensions trained on Common Crawl (600 billion tokens).5 This is the set of pre-trained vectors that we …","url":["http://www.academia.edu/download/63571037/Pang_Ring-2020-Automating_implicit_motive_coding-ME_AAM.pdf"]}
{"year":"2020","title":"This is a post-peer-review, pre-copyedit version of an article in press in Motivation and Emotion. The final authenticated version will be available online at: http://dx. doi …","authors":["JS Pang, H Ring"],"snippet":"… datasets. Based on these experiments we decided to use Facebook's FastText subword embeddings of 300 dimensions trained on Common Crawl (600 billion tokens).5 This is the set of pre-trained vectors that we …","url":["https://osf.io/b7d96/download"]}
{"year":"2020","title":"Tight Integrated End-to-End Training for Cascaded Speech Translation","authors":["P Bahar, T Bieschke, R Schlüter, H Ney - arXiv preprint arXiv:2011.12167, 2020"],"snippet":"… For MT training on En→De, we utilize the parallel data allowed for the IWSLT 2020. After filtering the noisy corpora, namely ParaCrawl, CommonCrawl, Rapid and OpenSubtitles2018, we end up with almost 27M bilingual text sentences …","url":["https://arxiv.org/pdf/2011.12167"]}
{"year":"2020","title":"Tilde at WMT 2020: News Task Systems","authors":["R Krišlauks, M Pinnis - arXiv preprint arXiv:2010.15423, 2020"],"snippet":"… translation. In order to make use of the Polish CommonCrawl corpus, we scored sentences using the in-domain language models and selected top-scoring sentences as additional monolingual data for back-translation. Many …","url":["https://arxiv.org/pdf/2010.15423"]}
{"year":"2020","title":"Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!","authors":["S Sia, A Dalmia, SJ Mielke - arXiv preprint arXiv:2004.14914, 2020"],"snippet":"… 0.177 FastText 2B (Wikipedia) -0.561 -0.657 -0.419 0.225 0.142 0.196 -0.382 -0.187 0.212 0.235 0.240 0.253 Glove 840B (Common Crawl) -0.436 -0.111 -0.299 0.182 0.213 0.155 -0.043 0.179 0.233 0.219 0.237 0.240 BERT …","url":["https://arxiv.org/pdf/2004.14914"]}
{"year":"2020","title":"To BERT or Not to BERT: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging","authors":["K Bhattacharjee, M Ballesteros, R Anubhai, S Muresan… - arXiv preprint arXiv …, 2020","KBMBR Anubhai, S Muresan, JMFLY Al, OA AI"],"snippet":"… Cloze (Baevski et al., 2019) and BERT-MRC+DSC (Li et al., 2019) are SOTA baselines for CONLL-2003 and CONLL-2012, respectively, for this task. Baevski et al. (2019) also use subsampled Common Crawl and News Crawl …","url":["https://arxiv.org/pdf/2010.14042","https://assets.amazon.science/79/37/7a3f91804693baaaadc5062a9821/to-bert-or-not-to-bert-comparing-task-specific-and-task-agnostic-semi-supervised-approaches-for-sequence-tagging.pdf"]}
{"year":"2020","title":"Tohoku-AIP-NTT at WMT 2020 News Translation Task","authors":["S Kiyono, T Ito, R Konno, M Morishita, J Suzuki - … of the Fifth Conference on Machine …, 2020"],"snippet":"… 2.2 Monolingual Corpus The origins of the monolingual corpus in our system are the Europarl, NewsCommentary, and en- tire NewsCrawl (2008-2019) corpora for English and German, and the Europarl …","url":["http://www.statmt.org/wmt20/pdf/2020.wmt-1.12.pdf"]}
{"year":"2020","title":"Topics in Sequence-to-Sequence Learning for Natural Language Processing","authors":["R Aharoni"],"snippet":"Page 1. Topics in Sequence-to-Sequence Learning for Natural Language Processing Roee Aharoni Ph.D. Thesis Submitted to the Senate of Bar-Ilan University Ramat Gan, Israel May 2020 Page 2. This work was …","url":["http://www.roeeaharoni.com/Phd_Thesis.pdf"]}
{"year":"2020","title":"Touché: First Shared Task on Argument Retrieval","authors":["A Bondarenko, M Hagen, M Potthast, H Wachsmuth…"],"snippet":"… Systems. pp. 44–52 (2012) 2. Bevendorff, J., Stein, B., Hagen, M., Potthast, M.: Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl. In: Proceedings of the 40th European Conference on IR Research (ECIR). pp …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/publications/2020-bondarenkoetal-ecir-touche.pdf"]}
{"year":"2020","title":"Toward building recommender systems for the circular economy: Exploring the perils of the European Waste Catalogue","authors":["G van Capelleveen, C Amrit, H Zijm, DM Yazan, A Abdi - Journal of Environmental …"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0301479720313554"]}
{"year":"2020","title":"Towards Context-Aware Opinion Summarization for Monitoring Social Impact of News","authors":["A Ramón-Hernández, A Simón-Cuevas, MMG Lorenzo… - Information, 2020"],"snippet":"… Specifically, those vectors are generated by using the word2vec pre-trained model included in the es_core_news_md model of the spaCy library, which includes 300-dimensional vectors trained using FastText CBOW on Wikipedia …","url":["https://www.mdpi.com/2078-2489/11/11/535/pdf"]}
{"year":"2020","title":"Towards countering hate speech against journalists on social media","authors":["P Charitidis, S Doropoulos, S Vologiannidis… - Online Social Networks and …, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S2468696420300124"]}
{"year":"2020","title":"Towards Effective Utilization of Pretrained Language Models—Knowledge Distillation from BERT","authors":["L Liu - 2020"],"snippet":"… For non-contextual embeddings, there are multiple pre-trained word vectors, such as word2vec trained on Google News, GloVe [60] trained on Wikipedia/Gigaword/Common Crawl, and fastText[67] trained on Wikipedia/Common Crawl …","url":["https://uwspace.uwaterloo.ca/bitstream/handle/10012/16225/Liu_Linqing.pdf?sequence=3"]}
{"year":"2020","title":"Towards Efficient and Reproducible Natural Language Processing","authors":["J Dodge - 2020"],"snippet":"… multiple epochs is standard). For example, the July 2019 Common Crawl contains 242 TB of uncompressed data,8 so even storing the data is expensive … 7https://opensource.google.com/projects/open-images-dataset …","url":["https://www.lti.cs.cmu.edu/sites/default/files/dodge%2C%20jesse%20-%20May%202020.pdf"]}
{"year":"2020","title":"Towards Generalized Neural Semantic Parsing","authors":["P Yin - 2020"],"snippet":"Page 1. April 27, 2020 DRAFT Thesis Proposal Towards Generalized Neural Semantic Parsing Pengcheng Yin April 27, 2020 Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15123 Thesis Committee …","url":["http://pcyin.me/thesis_proposal.pdf"]}
{"year":"2020","title":"Towards IP-based Geolocation via Fine-grained and Stable Webcam Landmarks","authors":["Z Wang, Q Li, J Song, H Wang, L Sun - Proceedings of The Web Conference 2020, 2020"],"snippet":"Page 1. Towards IP-based Geolocation via Fine-grained and Stable Webcam Landmarks Zhihao Wang Institute of Information Engineering Chinese Academy of Sciences School of Cyber Security, University of Chinese Academy …","url":["https://dl.acm.org/doi/pdf/10.1145/3366423.3380216"]}
{"year":"2020","title":"Towards Orthographic and Grammatical Clinical Text Correction: a First Approach","authors":["S Lima López - 2020"],"snippet":"… Their application to GEC is based on the idea that correct sequences are bound to have a higher probability score than incorrect ones. They are very dependent on the data that is used to build them, and so large corpora …","url":["https://addi.ehu.es/bitstream/handle/10810/48624/MAL-Salvador_Lima.pdf?sequence=1"]}
{"year":"2020","title":"Towards Useful Word Embeddings","authors":["V Novotný, M Štefánik, D Lupták, P Sojka"],"snippet":"… The size of our dataset is only 4% of the Common Crawl dataset used by Mikolov et al … We will also train our word vector models using larger corpora such as Common Crawl to enable meaningful comparison to sota results. Acknowledgments …","url":["https://www.fi.muni.cz/usr/sojka/papers/raslan-2020-novotny-stefanik-luptak-sojka.pdf"]}
{"year":"2020","title":"Towards Visual Dialog for Radiology","authors":["O Kovaleva, C Shivade, S Kashyap, K Kanjaria, J Wu… - Proceedings of the 19th …, 2020"],"snippet":"… the models, (b) domain-independent GloVe Common Crawl embeddings (Pennington et al., 2014), and (c) domain-specific fastText embeddings trained by (Romanov and Shivade, 2018). The latter are initialized with GloVe …","url":["https://www.aclweb.org/anthology/2020.bionlp-1.6.pdf"]}
{"year":"2020","title":"Traceability Support for Multi-Lingual Software Projects","authors":["Y Liu, J Lin, J Cleland-Huang - arXiv preprint arXiv:2006.16940, 2020"],"snippet":"Page 1. Traceability Support for Multi-Lingual Software Projects Yalin Liu, Jinfeng Lin, Jane Cleland-Huang University of Notre Dame Notre Dame, IN yliu26@nd.edu, jlin6@nd.edu,JaneHuang@nd.edu ABSTRACT Software …","url":["https://arxiv.org/pdf/2006.16940"]}
{"year":"2020","title":"Tracing the emergence of gendered language in childhood","authors":["B Prystawski, E Grant, A Nematzadeh, SWS Lee…"],"snippet":"… We used three commonly-used sets of pre-trained word em- beddings: the word2vec embeddings trained on the Google News corpus (Mikolov et al., 2013a), the GloVe embeddings trained on the Common Crawl corpus, and …","url":["https://cognitivesciencesociety.org/cogsci20/papers/0190/0190.pdf"]}
{"year":"2020","title":"Train Hard, Finetune Easy: Multilingual Denoising for RDF-to-Text Generation","authors":["Z Kasner, O Dušek - Proceedings of the 3rd WebNLG Workshop on Natural …, 2020"],"snippet":"… al., 2019). Adopting BART's objective and architecture, mBART (Liu et al., 2020) is pre-trained on the large-scale CC25 corpus extracted from Common Crawl, which contains data in 25 languages (Wenzek et al., 2020). The …","url":["https://webnlg-challenge.loria.fr/files/2020.webnlg-papers.20.pdf"]}
{"year":"2020","title":"Transformer based Deep Intelligent Contextual Embedding for Twitter sentiment analysis","authors":["U Naseem, I Razzak, K Musial, M Imran - Future Generation Computer Systems, 2020"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0167739X2030306X"]}
{"year":"2020","title":"Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals","authors":["M Popel, M Tomkova, J Tomek, Ł Kaiser, J Uszkoreit… - Nature Communications, 2020"],"snippet":"The quality of human translation was long thought to be unattainable for computer translation systems. In this study, we present a deep-learning system, CUBBITT, which challenges this view. In a context-aware blind evaluation …","url":["https://www.nature.com/articles/s41467-020-18073-9"]}
{"year":"2020","title":"Translation Artifacts in Cross-lingual Transfer Learning","authors":["M Artetxe, G Labaka, E Agirre - arXiv preprint arXiv:2004.04721, 2020"],"snippet":"… We first collect the premises from a filtered version of CommonCrawl (Buck et al., 2014), taking a subset of 5 websites that represent a diverse set of genres: a newspaper, an economy forum, a celebrity magazine, a literature blog, and a consumer magazine …","url":["https://arxiv.org/pdf/2004.04721"]}
{"year":"2020","title":"Translation System and Method","authors":["N Bertoldi, D Caroselli, MA Farajian, M Federico… - US Patent App. 16/118,273, 2020"],"snippet":"US20200073947A1 - Translation System and Method - Google Patents. Translation System and Method. Download PDF Info. Publication number US20200073947A1. US20200073947A1 US16/118,273 US201816118273A US2020073947A1 …","url":["https://patents.google.com/patent/US20200073947A1/en"]}
{"year":"2020","title":"TransQuest: Translation Quality Estimation with Cross-lingual Transformers","authors":["T Ranasinghe, C Orasan, R Mitkov - arXiv preprint arXiv:2011.01536, 2020"],"snippet":"… to acquire. Instead, XLM-R trains RoBERTa(Liu et al., 2019) on a huge, multilingual dataset at an enormous scale: unlabelled text in 104 languages is extracted from CommonCrawl datasets, totalling 2.5TB of text. It is trained …","url":["https://arxiv.org/pdf/2011.01536"]}
{"year":"2020","title":"Triclustering in Big Data Setting","authors":["D Egurnov, DI Ignatov, D Tochilkin - arXiv preprint arXiv:2010.12933, 2020"],"snippet":"Page 1. Triclustering in Big Data Setting Dmitry Egurnov, Dmitry I. Ignatov, and Dmitry Tochilkin Abstract In this paper, we describe versions of triclustering algorithms adapted for efficient calculations in distributed environments …","url":["https://arxiv.org/pdf/2010.12933"]}
{"year":"2020","title":"Triple E-Effective Ensembling of Embeddings and Language Models for NER of Historical German.","authors":["S Schweter, L März"],"snippet":"… We use the FastText embeddings trained on Wikipedia (FastText Wiki) and Common Crawl (FastText CC) in a ”classic” word embeddings manner, that means we do not use subwords … BPE MultiBPEmb Wikipedia < 7000 …","url":["http://ceur-ws.org/Vol-2696/paper_173.pdf"]}
{"year":"2020","title":"TULIP: A Five-Star Table and List-from Machine-Readable to Machine-Understandable Systems","authors":["J Nandakwang, P Chongstitvatana - Linked Open Data-Applications, Trends and …, 2020"],"snippet":"Currently, Linked Data is increasing at a rapid rate as the growth of the Web. Aside from new information that has been created exclusively as Semantic Web-ready, part of them comes from the transformation of existing structural …","url":["https://www.intechopen.com/online-first/tulip-a-five-star-table-and-list-from-machine-readable-to-machine-understandable-systems"]}
{"year":"2020","title":"TweetBERT: A Pretrained Language Representation Model for Twitter Text Analysis","authors":["MMA Qudar, V Mago - arXiv preprint arXiv:2010.11091, 2020"],"snippet":"… It has been pre-trained on an extremely large, five different types of corpora: BookCorpus, English Wikipedia, CC-News (collected from CommonCrawl News) dataset, OpenWebText, a WebText corpus [23], and Stories, a dataset containing story-like content [23] …","url":["https://arxiv.org/pdf/2010.11091"]}
{"year":"2020","title":"Two-Level Transformer and Auxiliary Coherence Modeling for Improved Text Segmentation","authors":["G Glavaš, S Somasundaran - arXiv preprint arXiv:2001.00891, 2020"],"snippet":"… In all our experiments we use 300dimensional monolingual FASTTEXT word embeddings pretrained on the Common Crawl corpora of respective languages: EN, CS, FI, and TR.9 We induce a cross-lingual word embedding …","url":["https://arxiv.org/pdf/2001.00891"]}
{"year":"2020","title":"UHH-LT & LT2 at SemEval-2020 Task 12: Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection","authors":["G Wiedemann, SM Yimam, C Biemann - arXiv preprint arXiv:2004.11493, 2020"],"snippet":"… languages at once (Conneau et al., 2019). The model itself is equivalent to RoBERTa, but the training data consists of texts from more than 100 languages filtered from the CommonCrawl1 dataset. ALBERT – A Lite BERT for Self …","url":["https://arxiv.org/pdf/2004.11493"]}
{"year":"2020","title":"Uncertainty-Aware Machine Support for Paper Reviewing on the Interspeech 2019 Submission Corpus","authors":["L Stappen, G Rizos, M Hasan, T Hain, BW Schuller"],"snippet":"… compressed to 300 dimensions. FastText and GloVe are based on the Common Crawl (1.9 M unique words, 840 B tokens) and Word2Vec on the GoogleNews (3 M unique words, 100 B total) dataset. We additionally experimented …","url":["https://indico2.conference4me.psnc.pl/event/35/contributions/3133/attachments/305/328/Tue-1-9-1.pdf"]}
{"year":"2020","title":"Underlying Cause of Death Identification from Death Certificates using Reverse Coding to Text and a NLP Based Deep Learning Approach","authors":["V Della Mea, MH Popescu, K Roitero - Informatics in Medicine Unlocked, 2020"],"snippet":"… XLM-R (a variation of XLM), trained on one hundred languages using more than two terabytes of filtered CommonCrawl data, outperformed multilingual BERT (mBERT) on a variety of cross-lingual benchmarks [5]. 3.3.4. XLNet …","url":["https://www.sciencedirect.com/science/article/pii/S2352914820306067"]}
{"year":"2020","title":"Understanding phishers' strategies of mimicking uniform resource locators to leverage phishing attacks: A machine learning approach","authors":["JS Tharani, NAG Arachchilage - arXiv preprint arXiv:2007.00489, 2020"],"snippet":"… webpage data downloaded from PhishTank2, OpenPhish3 and Legitimate ones are downloaded from Alexa4 and Common Crawl5 According … (4) 4 https://www.alexa.com/topsites/category/Computers/Internet/OntheW eb/W …","url":["https://arxiv.org/pdf/2007.00489"]}
{"year":"2020","title":"Understanding Word Embeddings and Language Models","authors":["JM Gomez-Perez, R Denaux, A Garcia-Silva - A Practical Guide to Hybrid Natural …, 2020"],"snippet":"… 1) pre-trained contextualized word embeddings (ELMo), (2) pre-trained context-independent word embeddings learnt from Common Crawl (fastText), Twitter … Another version of this classifier using in addition fastText embeddings …","url":["https://link.springer.com/chapter/10.1007/978-3-030-44830-1_3"]}
{"year":"2020","title":"UniBO@ KIPoS: Fine-tuning the Italian “BERTology” for PoS-tagging Spoken Data","authors":["F Tamburini"],"snippet":"… project. Also for GilBERTo it is available only the uncased model. • UmBERTo4: the more recent model de- veloped explicitly for Italian, as far as we know, is UmBERTo ('Musixmatch/umbertocommoncrawl-cased-v1' – umC). As …","url":["http://ceur-ws.org/Vol-2765/paper94.pdf"]}
{"year":"2020","title":"UninaStudents@ SardiStance: Stance Detection in Italian Tweets-Task A","authors":["M Moraca, G Sabella, S Morra - Proceedings of the 7th Evaluation Campaign of …, 2020"],"snippet":"… As Master students, we approached these NLP topics for the first time. Therefore, we are aware 5https://huggingface.co/Musixmatch/umbertocommoncrawl-cased-v1 that our results are not at the state of the art in the field. However …","url":["http://ceur-ws.org/Vol-2765/paper146.pdf"]}
{"year":"2020","title":"Unit Test Case Generation with Transformers","authors":["M Tufano, D Drain, A Svyatkovskiy, SK Deng… - arXiv preprint arXiv …, 2020"],"snippet":"… It has been pre-trained on the Common Crawl dataset [32] constituting nearly a trillion words, an expanded version of the WebText [33] dataset, two internet-based books corpora (Books1 and Books2), and English-language Wikipedia …","url":["https://arxiv.org/pdf/2009.05617"]}
{"year":"2020","title":"Uniting Plain Language, Cognitive Fluency, and Believability","authors":["SI Johnson - 2020"],"snippet":"… considering at least four linguistic features (Romanyshyn, 2018). Similar to Randall (2019), Romanyshyn (2018) considers the frequency of a word using Common Crawl, a large corpus of web content. Taking this approach one step further, she lemmatizes the 1 …","url":["http://search.proquest.com/openview/9b0f9b3644e2372cabfc4aedb4849573/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2020","title":"UNITOR@ Sardistance2020: Combining Transformer-based Architectures and Transfer Learning for Robust Stance Detection","authors":["S Giorgioni, M Politi, S Salman, D Croce, R Basili - … of the 7th Evaluation Campaign of …, 2020"],"snippet":"… with <negativo>3. 2https://huggingface.co/Musixmatch/ umberto-commoncrawlcased-v1 3We discarded the few available messages with mixed po- larity, to simplify the final classification task. Irony Detection. We speculate …","url":["http://ceur-ws.org/Vol-2765/paper99.pdf"]}
{"year":"2020","title":"Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios","authors":["R Eskander, S Muresan, M Collins - Proceedings of the 2020 Conference on …, 2020"],"snippet":"… essential when the domain of the training data is different from the one of the pre-trained em- beddings, which is the case in our learning setup, where we use the Bible data for training, while the XLM-R model is trained on text …","url":["https://www.aclweb.org/anthology/2020.emnlp-main.391.pdf"]}
{"year":"2020","title":"Unsupervised Cross-lingual Representation Learning for Speech Recognition","authors":["A Conneau, A Baevski, R Collobert, A Mohamed… - arXiv preprint arXiv …, 2020"],"snippet":"… For comparison with [29] only, we train 4-gram n-gram language models on CommonCrawl data [25, 50] for Assamese (140MiB of text data), Swahili (2GiB), Tamil (4.8GiB) and Lao (763MiB); for this experiment only we report word error rate (WER). 3.2 Training details …","url":["https://arxiv.org/pdf/2006.13979"]}
{"year":"2020","title":"Unsupervised Domain Clusters in Pretrained Language Models","authors":["R Aharoni, Y Goldberg - arXiv preprint arXiv:2004.02105, 2020"],"snippet":"… exact requirements from such data with respect to all the aforementioned aspects. On top of that, domain labels are usually unavailable – eg in large-scale web-crawled data like Common Crawl1 which was recently used to …","url":["https://arxiv.org/pdf/2004.02105"]}
{"year":"2020","title":"Unsupervised Evaluation of Human Translation Quality","authors":["Y Zhou, D Bollegala - 2019"],"snippet":"… publicly available monolingual word embeddings. Specifically, we first use the monolingual word embeddings, which are trained on Wikipedia and Common Crawl using fastText (Grave et al., 2018). Because our dataset contains …","url":["https://pdfs.semanticscholar.org/2735/60e715ee0dfaae2dee75fbb7484f811816d2.pdf"]}
{"year":"2020","title":"Unsupervised Label Refinement Improves Dataless Text Classification","authors":["Z Chu, K Stratos, K Gimpel - arXiv preprint arXiv:2012.04194, 2020"],"snippet":"… We use the 300 dimensional GloVe vectors7 trained on Common Crawl.8 We experiment with two distance functions when using GloVe: cosine and L2 … 7http://nlp.stanford.edu/ data/glove.840B.300d.zip 8https://commoncrawl.org/ ROBERTA Dual Encoder …","url":["https://arxiv.org/pdf/2012.04194"]}
{"year":"2020","title":"Unsupervised Question Decomposition for Question Answering","authors":["E Perez, P Lewis, W Yih, K Cho, D Kiela"],"snippet":"… Specifically, by leveraging >10M questions from Common Crawl, we learn to map from the distribution of multi-hop questions to the distribution of single-hop subquestions … We retrieve candidates from a corpus of 10M simple …","url":["https://rcqa-ws.github.io/papers/paper9.pdf"]}
{"year":"2020","title":"UPB at GermEval-2020 Task 3: Assessing Summaries for German Texts using BERTScore and Sentence-BERT","authors":["A Paraschiv"],"snippet":"… bert-base-german-europeana-uc 2 Uncased Europeana newspapers bert-base-germanuc2 Uncased Wikipedia, Subtitles, News, Commoncrawl literary-german-bert3 Uncased German Fiction Literature bert-adapted-german-press4 Uncased Newspapers …","url":["http://ceur-ws.org/Vol-2624/germeval-task3-paper2.pdf"]}
{"year":"2020","title":"Upgrading the Newsroom: An Automated Image Selection System for News Articles","authors":["F Liu, R Lebret, D Orel, P Sordet, K Aberer - arXiv preprint arXiv:2004.11449, 2020"],"snippet":"Page 1. 1 Upgrading the Newsroom: An Automated Image Selection System for News Articles FANGYU LIU∗, Language Technology Lab (LTL), University of Cambridge, United Kingdom RÉMI LEBRET, Distributed Information …","url":["https://arxiv.org/pdf/2004.11449"]}
{"year":"2020","title":"UR NLP@ HaSpeeDe 2 at EVALITA 2020: Towards Robust Hate Speech Detection with Contextual Embeddings","authors":["J Hoffmann, U Kruschwitz"],"snippet":"… XLM-R is based on XLM and RoBERTa. It is trained on data covering 100 languages in a very large (2TB) CommonCrawl. Transformer document embeddings are obtained from (the large version of) XLM-R. In addition Page 3 …","url":["http://ceur-ws.org/Vol-2765/paper105.pdf"]}
{"year":"2020","title":"Urban Dictionary Embeddings for Slang NLP Applications","authors":["S Wilson, W Magdy, B McGillivray, K Garimella… - Proceedings of The 12th …, 2020"],"snippet":"… with the goal of producing generally applicable word embeddings, many popular pre-trained word embeddings have been fit to large and diverse corpora of text from the web such as the Common Crawl.3 In … 2 http://smash …","url":["https://www.aclweb.org/anthology/2020.lrec-1.586.pdf"]}
{"year":"2020","title":"URL-based Phishing Attack Detection by Convolutional Neural Networks","authors":["J Nowak, M Korytkowski, P Najgebauer, M Wozniak…"],"snippet":"… The database downloaded during the article writing contained 10,604 records. To obtain legitimate websites, the second part of the training dataset was downloaded from the Common Crawl Foundation (http://commoncrawl.org/) …","url":["http://ajiips.com.au/papers/V15.2/v15n2_64-71.pdf"]}
{"year":"2020","title":"Using Natural Language Preprocessing Architecture (NLPA) for Big Data Text Sources","authors":["M Novo-Lourés, R Pavón, R Laza, D Ruano-Ordas… - Scientific Programming, 2020"],"snippet":"Journals; Publish with us; Publishing partnerships; About us; Blog. Scientific Programming. +Journal Menu. PDF. Journal overview. For authorsFor reviewersFor editorsTable of Contents Special Issues.","url":["https://www.hindawi.com/journals/sp/2020/2390941/"]}
{"year":"2020","title":"Using Natural Language Processing to Identify Similar Patent Documents","authors":["J Navrozidis, H Jansson - LU-CS-EX, 2020"],"snippet":"Page 1. MASTER'S THESIS 2020 Using Natural Language Processing to Identify Similar Patent Documents Hannes Jansson, Jakob Navrozidis ISSN 1650-2884 LU-CS-EX: 2020-05 DEPARTMENT OF COMPUTER …","url":["https://lup.lub.lu.se/student-papers/record/9008699/file/9026407.pdf"]}
{"year":"2020","title":"Using Probabilistic Soft Logic to Improve Information Extraction in the Legal Domain","authors":["B Kirsch, S Giesselbach, T Schmude, M Völkening…"],"snippet":"… spaCy Classifier: This architecture is based on a CNN with mean pooling and a final feed-forward layer. The network is fed with pretrained word embeddings trained on the German Wikipedia and the German common crawl (Ortiz Suárez et al., 2019).9 …","url":["http://ceur-ws.org/Vol-2738/LWDA2020_paper_29.pdf"]}
{"year":"2020","title":"Using Publisher Partisanship for Partisan News Detection","authors":["CL Yeh"],"snippet":"Page 1. Using Publisher Partisanship for Partisan News Detection A Comparison of Performance between Annotation Levels Chia-Lun Yeh Page 2. Using Publisher Partisanship for Partisan News Detection A …","url":["https://pdfs.semanticscholar.org/604f/233a21249d44085e41e7415ed9741fc69d5e.pdf"]}
{"year":"2020","title":"Using Sentences as Semantic Representations in Large Scale Zero-Shot Learning","authors":["YL Cacheux, HL Borgne, M Crucianu - arXiv preprint arXiv:2010.02959, 2020"],"snippet":"… For the same reason, we used FastText and Glove models pre-trained on Common Crawl We used a 300-dimension version for all three … Fasttext: https://fasttext.cc/docs/en/englishvectors.html (version trained on Common Crawl with 600B tokens, no subword information) …","url":["https://arxiv.org/pdf/2010.02959"]}
{"year":"2020","title":"Using Word Embeddings to Learn a Better Food Ontology. Front","authors":["J Youn, T Naravane, I Tagkopoulos - Artif. Intell, 2020"],"snippet":"… W ikinews W ikipedia 2017 + UMBC webbase + s tatmt.org 0.313 2.98 Crawl Common Crawl 0 .317 3.00 Word2vec (Mikolov …","url":["https://pdfs.semanticscholar.org/1c47/eb747f27eab42bc8e9e9ded83dd784eadf4c.pdf"]}
{"year":"2020","title":"ValNorm: A New Word Embedding Intrinsic Evaluation Method Reveals Valence Biases are Consistent Across Languages and Over Decades","authors":["A Toney, A Caliskan - arXiv preprint arXiv:2006.03950, 2020"],"snippet":"… We choose six widely used pre-trained word embedding sets, listed in Table 2, to compare ValNorm's performance on different algorithms (GloVe, fastText, word2vec) and training corpora (Common Crawl, Wikipedia, OpenSubtitles …","url":["https://arxiv.org/pdf/2006.03950"]}
{"year":"2020","title":"Vandalism Detection in Crowdsourced Knowledge Bases","authors":["S Heindorf - 2019"],"snippet":"… Manual OpenStreetMap, Uniprot, WordNet, MusicBrainz, IMDb Wikipedia, WikiHow, YouTube Wikia/FANDOM, StackExchange, Quora, Yahoo Answers DBpedia, YAGO, NELL dblp, BabelNet Internet Archive, Common Crawl NASA …","url":["https://pdfs.semanticscholar.org/e70f/b288ceb09fc244554a274f31cd1217663027.pdf"]}
{"year":"2020","title":"Variational Transformers for Diverse Response Generation","authors":["Z Lin, GI Winata, P Xu, Z Liu, P Fung - arXiv preprint arXiv:2003.12738, 2020"],"snippet":"… embeddings. The first is EMBFT (Liu et al., 2016) that calculates the average of word embeddings in a sentence using FastText (Mikolov et al., 2018) which is trained with Common Crawl and Wikipedia data. We use FastText …","url":["https://arxiv.org/pdf/2003.12738"]}
{"year":"2020","title":"VECO: Variable Encoder-decoder Pre-training for Cross-lingual Understanding and Generation","authors":["F Luo, W Wang, J Liu, Y Liu, B Bi, S Huang, F Huang… - arXiv preprint arXiv …, 2020"],"snippet":"… We adopt the same 250K vocabulary that is also used by XLM-R (Conneau et al., 2019) and mBART (Liu et al., 2020b). Pre-Training Datasets For monolingual training datasets, we reconstruct Common-Crawl Corpus used in XLM-R (Conneau et al., 2019) …","url":["https://arxiv.org/pdf/2010.16046"]}
{"year":"2020","title":"Video Question Answering on Screencast Tutorials","authors":["W Zhao, S Kim, N Xu, H Jin"],"snippet":"… visual cues, and graph embeddings. All the models have the word embeddings initialized with the 300-dimensional pretrained fastText [Bojanowski et al., 2017] vectors on Common Crawl dataset. The convolutional layer in …","url":["https://www.ijcai.org/Proceedings/2020/0148.pdf"]}
{"year":"2020","title":"Visual and Textual Deep Feature Fusion for Document Image Classification","authors":["S Bakkali, Z Ming, M Coustaty, M Rusinol - Proceedings of the IEEE/CVF Conference …, 2020"],"snippet":"… FastText algorithm we used was pretrained on 2 million word vectors trained on Common Crawl (600B tokens), and uses 1,999,996 word vectors. Bert: Bert [11] is a contextualized bidirectional word embedding based on the transformer architecture …","url":["http://openaccess.thecvf.com/content_CVPRW_2020/papers/w34/Bakkali_Visual_and_Textual_Deep_Feature_Fusion_for_Document_Image_Classification_CVPRW_2020_paper.pdf"]}
{"year":"2020","title":"Visual Relations Augmented Cross-modal Retrieval","authors":["Y Guo, J Chen, H Zhang, YG Jiang - … of the 2020 International Conference on …, 2020"],"snippet":"… vector with the corresponding visual feature. The label embedding vector is obtained with a learnable embedding layer initialized by GloVe [25] that pre-trained on the Common-Crawl dataset. Given a set of object categories …","url":["https://dl.acm.org/doi/pdf/10.1145/3372278.3390709"]}
{"year":"2020","title":"Visualizing and Interpreting RNN Models in URL-based Phishing Detection","authors":["T Feng, C Yue - Proceedings of the 25th ACM Symposium on Access …, 2020"],"snippet":"… The legitimate URLs came from the Common Crawl (www.commoncrawl.org) open web searching database, while the phishing URLs came from the popular PhishTank (www.phishtank.com) phishing website repository. In …","url":["https://dl.acm.org/doi/pdf/10.1145/3381991.3395602"]}
{"year":"2020","title":"Wat zei je? Detecting Out-of-Distribution Translations with Variational Transformers","authors":["TZ Xiao, AN Gomez, Y Gal - arXiv preprint arXiv:2006.08344, 2020"],"snippet":"… The following datasets were used in our experiments: • WMT EN ↔ DE: The training set for translation tasks between English (EN) and German (DE) composed of news-commentary-v13 with 284k sentences pairs, wmt13 …","url":["https://arxiv.org/pdf/2006.08344"]}
{"year":"2020","title":"Weakly-Supervised Multi-Level Attentional Reconstruction Network for Grounding Textual Queries in Videos","authors":["Y Song, J Wang, L Ma, Z Yu, J Yu - arXiv preprint arXiv:2003.07048, 2020"],"snippet":"… For each second we uniformly sample 16 frames as input to C3D, and obtain a 4096-dimentional visual feature from fc6 layer. Each word from the query is represented by GloVe [22] word embedding vector pre-trained on Common Crawl …","url":["https://arxiv.org/pdf/2003.07048"]}
{"year":"2020","title":"Web Crawl Processing on Big Data Scale","authors":["JM Patel - Getting Structured Data from the Internet, 2020"],"snippet":"… We got this domain ranks file and column names from the common crawl blog post (https://commoncrawl.org/2020/02/host-and-domain-level-webgraphs-novdecjan-2019-2020/); they publish new domain ranks about four …","url":["https://link.springer.com/chapter/10.1007/978-1-4842-6576-5_7"]}
{"year":"2020","title":"Web Table Extraction, Retrieval, and Augmentation: A Survey","authors":["S Zhang, K Balog - ACM Transactions on Intelligent Systems and …, 2020"],"snippet":"… Table corpora Type #tables Source WDC 2012 Web Table Corpus Web tables 147M Web crawl (Common Crawl) WDC 2015 Web Table Corpus Web tables 233M Web crawl (Common Crawl) Dresden Web Tables Corpus …","url":["https://dl.acm.org/doi/abs/10.1145/3372117"]}
{"year":"2020","title":"Webis at TREC 2019: Decision Track","authors":["A Bondarenko, M Fröbe, V Kasturia, M Völske, B Stein…"],"snippet":"… In Proceedings of SIGIR 2017. 1419–1420. [2] Janek Bevendor , Benno Stein, Ma hias Hagen, and Martin Po hast. 2018. Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl. In Proceedings of ECIR 2018. 820–824 …","url":["https://trec.nist.gov/pubs/trec28/papers/Webis.D.pdf"]}
{"year":"2020","title":"Webis at TREC 2020: Health Misinformation Track","authors":["A Bondarenko, M Fröbe, S Günther, M Hagen… - 2020","J Bevendor, A Bondarenko, M Fröbe, S Günther…"],"snippet":"… During retrieval, we used ChatNoirs existing weighting scheme for the two Common Crawl snapshots, which combines BM25 scores of multiple elds … we relax the precondition of documents' 1We have indexed a 2015 and …","url":["https://webis.de/downloads/publications/papers/stein_2020zb.pdf","https://webis.de/downloads/publications/slides/stein_2020zb.pdf"]}
{"year":"2020","title":"Webly Supervised Semantic Embeddings for Large Scale Zero-Shot Learning","authors":["YL Cacheux, A Popescu, HL Borgne - arXiv preprint arXiv:2008.02880, 2020"],"snippet":"… prototypes. These embeddings are extracted from generic large scale text collections such as Wikipedia [21,34] or Common Crawl [6,33] … use. For GloVe on ImageNet, the model pretrained on Common Crawl has the best performance …","url":["https://arxiv.org/pdf/2008.02880"]}
{"year":"2020","title":"WeChat Neural Machine Translation Systems for WMT20","authors":["F Meng, J Yan, Y Liu, Y Gao, X Zeng, Q Zeng, P Li… - arXiv preprint arXiv …, 2020"],"snippet":"… Commentary, Common Crawl and Gigaword corpus. The English monolingual data includes News crawl, News discussions, Europarl v10, News Commentary, Common Crawl, Wiki dumps and the Gigaword corpus. After …","url":["https://arxiv.org/pdf/2010.00247"]}
{"year":"2020","title":"WEFE: The Word Embeddings Fairness Evaluation Framework","authors":["P Badilla, F Bravo-Marquez, J Pérez"],"snippet":"… The following are the pre-trained em- bedding models that we consider: 1) conceptnet, 2) fasttextwikipedia, 3) glove-twitter, 4) glove-wikipedia, 5) lexveccommoncrawl, 6) word2vec-googlenews, and 7) word2vecgender-hard …","url":["https://felipebravom.com/publications/ijcai2020.pdf"]}
{"year":"2020","title":"WEmbSim: A Simple yet Effective Metric for Image Captioning","authors":["N Sharif, L White, M Bennamoun, W Liu, SAA Shah - arXiv preprint arXiv:2012.13137, 2020"],"snippet":"… Page 5. TABLE I T - . Name Source Dims Corpus Corpus Size Vocabulary Size GloVE 840B [12] 300 Common Crawl 8.4 · 1011 2 · 106 Word2vec [6] 300 Google News (100B) 1.0 · 1011 3 · 106 FastText [13] 300 Wikipedia 4.0 · 109 3 · 106 …","url":["https://arxiv.org/pdf/2012.13137"]}
{"year":"2020","title":"What determines the order of adjectives in English? Comparing efficiency-based theories using dependency treebanks","authors":["R Futrell, W Dyer, G Scontras - Proceedings of the 58th Annual Meeting of the …, 2020"],"snippet":"… sklearn.cluster.KMeans applied to a pretrained set of 1.9 million 300-dimension GloVe vectors2 generated from the Common Crawl corpus … Table 1a shows the accuracies of our predictors in predicting held-out …","url":["https://www.aclweb.org/anthology/2020.acl-main.181.pdf"]}
{"year":"2020","title":"What Sparks Joy: The AffectVec Emotion Database","authors":["S Raji, G de Melo - Proceedings of The Web Conference 2020, 2020"],"snippet":"… We consider the cosine similarity of word– emotion pairs in word2vec trained on the Google News corpus [18], GloVe [26] trained on Twitter (200-dim.) and CommonCrawl (840B, 300-dim.), as well as the counterfitted vectors by Mrksic et al. [24]. Results …","url":["https://dl.acm.org/doi/pdf/10.1145/3366423.3380068"]}
{"year":"2020","title":"What the [MASK]? Making Sense of Language-Specific BERT Models","authors":["D Nozza, F Bianchi, D Hovy - arXiv preprint arXiv:2003.02912, 2020"],"snippet":"… OSCAR (Open Super-large Crawled Almanach coRpus) (Or- tiz Suárez et al., 2019) is a huge multilingual corpus obtained by filtering the Common Crawl corpus, which is a parallel multilingual corpus comprised of crawled documents from the internet …","url":["https://arxiv.org/pdf/2003.02912"]}
{"year":"2020","title":"When and Why is Unsupervised Neural Machine Translation Useless?","authors":["Y Kim, M Graça, H Ney - arXiv preprint arXiv:2004.10581, 2020","YKM Graça, H Ney - 22nd Annual Conference of the European Association …"],"snippet":"… However, for low-resource language pairs, it is difficult to match the data domain of both sides on a large scale. For example, our monolingual data for Kazakh is mostly from Wikipedia and Common Crawl, while the English data is solely from News Crawl …","url":["https://arxiv.org/pdf/2004.10581","https://www.aclweb.org/anthology/2020.eamt-1.pdf#page=55"]}
{"year":"2020","title":"When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models","authors":["B Muller, A Anastasopoulos, B Sagot, D Seddah - arXiv preprint arXiv:2010.12858, 2020"],"snippet":"… al., 2019). OSCAR is a corpus extracted from a Common Crawl Web snapshot.3 It provides a significant 2Also see the discussion in Section §3.2 on the script distributions in mBERT. 3http://commoncrawl.org/ Language (iso …","url":["https://arxiv.org/pdf/2010.12858"]}
{"year":"2020","title":"When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?","authors":["K Joseph, JH Morgan - arXiv preprint arXiv:2004.12043, 2020"],"snippet":"Page 1. When do Word Embeddings Accurately Reflect Surveys on our Beliefs about People? Kenneth Joseph Computer Science and Engineering University at Buffalo Buffalo, NY, 14226 kjoseph@buffalo.edu Jonathan H. Morgan …","url":["https://arxiv.org/pdf/2004.12043"]}
{"year":"2020","title":"When Does Unsupervised Machine Translation Work?","authors":["K Marchisio, K Duh, P Koehn - arXiv preprint arXiv:2004.05516, 2020"],"snippet":"… “News crawl” (News) and “Common Crawl” (CC) settings determine whether the system can flexibly handle diverse datasets. Specifics of the datasets used are described in subsequent subsections … UN = United Nations …","url":["https://arxiv.org/pdf/2004.05516"]}
{"year":"2020","title":"Which* BERT? A Survey Organizing Contextualized Encoders","authors":["P Xia, S Wu, B Van Durme - arXiv preprint arXiv:2010.00854, 2020"],"snippet":"… Raffel et al. (2019) curate a 745GB subset of Common Crawl (CC),10 which starkly contrasts with the 13GB used in BERT … 9https://sites.google.com/ view/ sustainlp2020/shared-task 10https://commoncrawl.org/ scrapes publicly …","url":["https://arxiv.org/pdf/2010.00854"]}
{"year":"2020","title":"Who is asking? humans and machines experience","authors":["M Klein, L Balakireva, H Shankar"],"snippet":"… licenses/by/4.0/). Similarly, the motivation behind the recent study by Thompson and Jian [14] based on two Common Crawl samples of the web was to quantify the use of HTTP DOIs versus URLs of landing pages. They found …","url":["https://osf.io/pgxc3/download"]}
{"year":"2020","title":"Why are events important and how to compute them in geospatial research?","authors":["M Yuan"],"snippet":"… GPT-3 is a gigantic neural network with 175 billion input parameters and 96 layers of transformer decoders, each of which has 1.8 billion parameters, and is pre-trained with 45TB (499 billion tokens) compressed data from five …","url":["https://www.josis.org/index.php/josis/article/viewFile/723/300"]}
{"year":"2020","title":"Why Not Simply Translate? A First Swedish Evaluation Benchmark for Semantic Similarity","authors":["T Isbister, M Sahlgren - arXiv preprint arXiv:2009.03116, 2020"],"snippet":"… et al., 2018). We use the CBOW model that has been trained on Common Crawl and Wikipedia.6 As with Word2Vec, the vectors for sentences are obtained by averaging the embedding vector for each word. BERT: Deep Transformer …","url":["https://arxiv.org/pdf/2009.03116"]}
{"year":"2020","title":"Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries","authors":["M Zhang, Y Fujinuma, MJ Paul, J Boyd-Graber"],"snippet":"… We align English embeddings with six target languages: German (DE), Spanish (ES), French (FR), Italian (IT), Japanese (JA), and Chinese (ZH). We use 300-dimensional fastText vectors trained on Wikipedia and Common Crawl (Grave et al., 2018) …","url":["http://users.umiacs.umd.edu/~mozhi/pdf/retrofit.pdf"]}
{"year":"2020","title":"Wide range screening of algorithmic bias in word embedding models using large sentiment lexicons reveals underreported bias types","authors":["D Rozado - PLOS ONE, 2020"],"snippet":"… This work systematically analyzed 3 popular word embeddings methods: Word2vec (Skip-gram) [4], Glove [9] and FastText [10], externally pretrained on a wide array of corpora such as Google News, Wikipedia, Twitter or Common Crawl …","url":["https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231189"]}
{"year":"2020","title":"WikiAsp: A Dataset for Multi-domain Aspect-based Summarization","authors":["H Hayashi, P Budania, P Wang, C Ackerson… - arXiv preprint arXiv …, 2020"],"snippet":"… sections of Wikipedia from referenced web pages. Following the WikiSum data generation script,3 we first crawled cited references covered by CommonCrawl for each Wikipedia article. We then recover all the sections4 of …","url":["https://arxiv.org/pdf/2011.07832"]}
{"year":"2020","title":"Will it Unblend?","authors":["Y Pinter, CL Jacobs, J Eisenstein - arXiv preprint arXiv:2009.09123, 2020"],"snippet":"Page 1. Will it Unblend? Yuval Pinter School of Interactive Computing Georgia Institute of Technology Atlanta, GA, USA uvp@gatech.edu Cassandra L. Jacobs Department of Psychology University of Wisconsin Madison, WI, USA cjacobs2@wisc.edu …","url":["https://arxiv.org/pdf/2009.09123"]}
{"year":"2020","title":"Word associations and the distance properties of context-aware word embeddings","authors":["MA Rodriguez, P Merlo - Proceedings of the 24th Conference on Computational …, 2020"],"snippet":"… However, in this work, we used the pre-trained FASTTEXT embeddings provided by the official site of FASTTEXT, that we expressly do not modify.3 The embeddings are trained on 600-billion tokens from …","url":["https://www.aclweb.org/anthology/2020.conll-1.30.pdf"]}
{"year":"2020","title":"Word Embedding Evaluation for Sinhala","authors":["D Lakmal, S Ranathunga, S Peramuna, I Herath - Proceedings of The 12th Language …, 2020"],"snippet":"… Common Crawl can be considered as a precious starting point for building a cleaned large corpus for … Common Crawl monthly dataset only contains 0.007% of content in Sinhala4, however, this amount is still … 4https …","url":["https://www.aclweb.org/anthology/2020.lrec-1.231.pdf"]}
{"year":"2020","title":"WORD EMBEDDINGS IN ROMANIAN FOR THE RETAIL BANKING DOMAIN","authors":["I RAICU, N BOITOUT, R BOLOGA, MG STURZA"],"snippet":"… In addition, Facebook released a year later another version of FastText pre-trained word embeddings, trained on Common Crawl and Wikipedia [4]. Another pre-trained word embeddings in Romanian can be found at …","url":["https://www.researchgate.net/profile/Irina_Raicu2/publication/341553193_WORD_EMBEDDINGS_IN_ROMANIAN_FOR_THE_RETAIL_BANKING_DOMAIN/links/5ec6d768a6fdcc90d68c8596/WORD-EMBEDDINGS-IN-ROMANIAN-FOR-THE-RETAIL-BANKING-DOMAIN.pdf"]}
{"year":"2020","title":"Word Embeddings Inherently Recover the Conceptual Organization of the Human Mind","authors":["V Swift - arXiv preprint arXiv:2002.10284, 2020"],"snippet":"… Sub-word information was incorporated on the basis of n-grams (length = 5), with a window size of 5 and 10 negatives, and a step size of .05. The English model was trained on a Common Crawl corpus comprised of English text from 2.96 billion webpages …","url":["https://arxiv.org/pdf/2002.10284"]}
{"year":"2020","title":"Word meaning in minds and machines","authors":["BM Lake, GL Murphy - arXiv preprint arXiv:2008.01766, 2020"],"snippet":"… is illustrated in Figure 1A. CBOW has been trained on tremendous corpora; for instance, in this article, we analyze a large-scale CBOW model trained on the Common Crawl corpus of 630 billion words. CBOW learns a word …","url":["https://arxiv.org/pdf/2008.01766"]}
{"year":"2020","title":"Word Representations for Named Entity Recognition","authors":["R Agerri"],"snippet":"… Transformers: Bertin (Gigaword+Wikipedia), XLM-RoBERTa (Common Crawl) and mBERT (Wikipedia + books) • Project annotations (various strategies) Page 54 … BETO (various sources) – XLM-RoBERTa (Common Crawl 2.5TB) – mBERT (Wikipedia + books) …","url":["https://cit-ai.net/archive/CitAI_Seminar_11Nov20_Agerri.pdf"]}
{"year":"2020","title":"Word Representations for Neural Network Based Myanmar Text-to-Speech System","authors":["AM Hlaing, WP Pa"],"snippet":"… In [21], the size of word vectors is small, and it contains about 55K entries for Myanmar language and can be downloaded from the link [23]. In [22], the word vectors are trained on Common Crawl and Wikipedia using fastText …","url":["http://www.inass.org/2020/2020043023.pdf"]}
{"year":"2020","title":"Word Rotator's Distance","authors":["S Yokoi, R Takahashi, R Akama, J Suzuki, K Inui - Proceedings of the 2020 …, 2020"],"snippet":"Page 1. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2944–2960, November 16–20, 2020. c 2020 Association for Computational Linguistics 2944 Word Rotator's Distance …","url":["https://www.aclweb.org/anthology/2020.emnlp-main.236.pdf"]}
{"year":"2020","title":"Word Rotator's Distance: Decomposing Vectors Gives Better Representations","authors":["S Yokoi, R Takahashi, R Akama, J Suzuki, K Inui - arXiv preprint arXiv:2004.15003, 2020"],"snippet":"Page 1. Word Rotator's Distance: Decomposing Vectors Gives Better Representations Sho Yokoi1 Ryo Takahashi1,2 Reina Akama1,2 Jun Suzuki1,2 Kentaro Inui1,2 1 Tohoku University 2 RIKEN {yokoi, ryo.t, reina.a, jun.suzuki, inui}@ecei.tohoku.ac.jp Abstract …","url":["https://arxiv.org/pdf/2004.15003"]}
{"year":"2020","title":"Word Sense Disambiguation for 158 Languages using Word Embeddings Only","authors":["V Logacheva, D Teslenko, A Shelmanov, S Remus… - arXiv preprint arXiv …, 2020"],"snippet":"… The contributions of our work are the following: 1The full list languages is available at fasttext.cc and includes English and 157 other languages for which embeddings were trained on a combination of Wikipedia and CommonCrawl texts …","url":["https://arxiv.org/pdf/2003.06651"]}
{"year":"2020","title":"Word2Sent: A new learning sentiment‐embedding model with low dimension for sentence level sentiment classification","authors":["M Kasri, M Birjali, A Beni‐Hssane - Concurrency and Computation: Practice and …"],"snippet":"Abstract Word embedding models become an increasingly important method that embeds words into a high dimensional space. These models have been widely utilized to extract semantic and syntactic feat...","url":["https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6149"]}
{"year":"2020","title":"Words Matter: Gender, Jobs and Applicant Behavior in India","authors":["S Chaturvedi, K Mahajan, Z Siddique - 2020"],"snippet":"… Pennington et al., 2014). The 300 dimensional pretrained word vectors have been obtained by training the algorithm on web data from common crawl, and comprise 2.2 million unique words. Cosine similarity between any …","url":["https://www.dse.univr.it/documenti/Seminario/documenti/documenti102498.pdf"]}
{"year":"2020","title":"Words, constructions and corpora: Network representations of constructional semantics for Mandarin space particles","authors":["ACH Chen - Corpus Linguistics and Linguistic Theory, 2020"],"snippet":"Jump to Content Jump to Main Navigation Publications. Subjects. Architecture and Design Arts Asian and Pacific Studies Business and Economics Chemistry Classical and Ancient Near Eastern Studies Computer Sciences Cultural …","url":["https://www.degruyter.com/view/journals/cllt/ahead-of-print/article-10.1515-cllt-2020-0012/article-10.1515-cllt-2020-0012.xml"]}
{"year":"2020","title":"Wrestling with Complexity in Computational Social Science: Theory, Estimation and Representation","authors":["S de Marchi - The SAGE Handbook of Research Methods in Political …, 2020"]}
{"year":"2020","title":"WT5?! Training Text-to-Text Models to Explain their Predictions","authors":["S Narang, C Raffel, K Lee, A Roberts, N Fiedel… - arXiv preprint arXiv …, 2020"],"snippet":"… 1997; Ruder, 2017). In Raffel et al. (2019), this framework was used to pre-train Transformer (Vaswani et al., 2017) models on a large collection of unlabeled text drawn from the Common Crawl web scrape. We use the result …","url":["https://arxiv.org/pdf/2004.14546"]}
{"year":"2020","title":"X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models","authors":["Z Jiang, A Anastasopoulos, J Araki, H Ding, G Neubig - Proceedings of the 2020 …, 2020"],"snippet":"Page 1. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5943–5959, November 16–20, 2020. c 2020 Association for Computational Linguistics 5943 X-FACTR: Multilingual …","url":["https://www.aclweb.org/anthology/2020.emnlp-main.479.pdf"]}
{"year":"2020","title":"XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation","authors":["Y Liang, N Duan, Y Gong, N Wu, F Guo, W Qi, M Gong… - arXiv preprint arXiv …, 2020"],"snippet":"… 2.1.2 Large Corpus (LC) Multilingual Corpus Following Wenzek et al. (2019), we construct a clean version of Common Crawl (CC)3 as the multilingual corpus … 2https://github.com/ attardi/wikiextractor. 3https://commoncrawl.org/. available in English …","url":["https://arxiv.org/pdf/2004.01401"]}
{"year":"2020","title":"Xiaomi's Submissions for IWSLT 2020 Open Domain Translation Task","authors":["Y Sun, M Guo, X Li, J Cui, B Wang - Proceedings of the 17th International Conference …, 2020"],"snippet":"… And for unconstrained submission, we choose the largescale amounts of Commoncrawl Chinese10 and Japanese11 dataset as additional monolingual data for training LMs and executing BT to enhance our NMT systems …","url":["https://www.aclweb.org/anthology/2020.iwslt-1.18.pdf"]}
{"year":"2020","title":"YNU OXZ@ HaSpeeDe 2 and AMI: XLM-RoBERTa with Ordered Neurons LSTM for classification task at EVALITA 2020","authors":["X Ou, H Li - Proceedings of Sixth Evaluation Campaign of Natural …, 2020"],"snippet":"… scale multi-language pre-training model. It can be un- derstood as a combination of XLM and RoBER- Ta. It is trained on 2.5 TB of newly created clean CommonCrawl data in 100 languages. Because the training of the model …","url":["http://ceur-ws.org/Vol-2765/paper93.pdf"]}
{"year":"2020","title":"Zero Shot Domain Generalization","authors":["U Maniyar, AA Deshmukh, U Dogan… - arXiv preprint arXiv …, 2020"],"snippet":"… Thus using semantic space helps us in the visual classification task. We use word embeddings of classes - in particular, simple GloVe embeddings [28] trained on Common Crawl corpus - as the semantic space in this work …","url":["https://arxiv.org/pdf/2008.07443"]}
{"year":"2020","title":"Zero-shot semantic segmentation using relation network","authors":["Y Zhang - 2020"],"snippet":"Page 1. University of Jyväskylä Faculty of Information Technology Yindong Zhang Zero-shot Semantic Segmentation using Relation Network Master's thesis of information technology May 28, 2020 Page 2. i Author: Yindong …","url":["https://jyx.jyu.fi/bitstream/handle/123456789/69720/1/URN%3ANBN%3Afi%3Ajyu-202006043976.pdf"]}
{"year":"2021","title":"'I'm just feeling like it'. On the relationship between the use of the progressive and sentiment polarity in Italian","authors":["L Viola"],"snippet":"… art transformer-based machine learning model for emotion and sentiment classification in Italian which employs the Italian BERT model UmBERTo trained on Commoncrawl ITA (Parisi, Francia, and Magnani [2020] 2021). For …","url":["https://www.uib.no/sites/w3.uib.no/files/attachments/viola.pdf"]}
{"year":"2021","title":"4. Unlocking value from AI in financial services: strategic and organizational tradeoffs vs. media narratives","authors":["G Lanzolla, S Santoni, C Tucci - Artificial Intelligence for Sustainable Value Creation, 2021"],"snippet":"Page 87. 4. Unlocking value from AI in financial services: strategic and organizational tradeoffs vs. media narratives Gianvito Lanzolla, Simone Santoni and Christopher Tucci 1. INTRODUCTION In 1955, McCarthy wrote that …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=_9BCEAAAQBAJ&oi=fnd&pg=PA70&dq=commoncrawl&ots=Z-4LjY9D6U&sig=BHpJ4i9Wq18ZWZDIWoGm5BnHqSY"]}
{"year":"2021","title":"6 Data Collection and Representation for Similar Languages, Varieties and Dialects","authors":["T Samardžic, N Ljubešic - Similar Languages, Varieties, and Dialects: A …, 2021"],"snippet":"… Page 146. Data Collection and Representation for Similar Languages 127 Another project that should be mentioned in this brief overview is the CommonCrawl, a project performing crawls over the whole internet for textual data since 2013 with regular data updates …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=hhA5EAAAQBAJ&oi=fnd&pg=PA121&dq=commoncrawl&ots=2XimIiF4W6&sig=XlIzLoiwAxuodhBmJeC_iS9BSeg"]}
{"year":"2021","title":"\" Short is the Road that Leads from Fear to Hate\": Fear Speech in Indian WhatsApp Groups","authors":["P Saha, B Mathew, K Garimella, A Mukherjee - arXiv preprint arXiv:2102.03870, 2021"],"snippet":"Page 1. “Short is the Road that Leads from Fear to Hate”: Fear Speech in Indian WhatsApp Groups Punyajoy Saha punyajoys@iitkgp.ac.in Indian Institute of Technology Kharagpur, West Bengal, India Binny Mathew binnymathew …","url":["https://arxiv.org/pdf/2102.03870"]}
{"year":"2021","title":"A common framework for quantifying the learnability of nouns and verbs","authors":["Y Zhou, D Yurovsky - Proceedings of the Annual Meeting of the Cognitive …, 2021"],"snippet":"… We used pre-trained 300-dimensional semantic vectors derived from the the Common Crawl corpus composed of 840 billion tokens and 2.2 million words. For our analysis, we considered only the words that corresponded to the relevant 434 images. Procedure …","url":["https://escholarship.org/content/qt8dn6k82j/qt8dn6k82j.pdf"]}
{"year":"2021","title":"A Comparative Study on Word Embeddings in Deep Learning for Text Classification","authors":["C Wang, P Nulty, D Lillis"],"snippet":"… 3https://nlp.stanford.edu/projects/glove/ 4https://commoncrawl.org/ 5https://fasttext.cc/ 6https://allennlp.org/elmo 7We additionally experimented from the fourth-to-last (-4) layer to the last layer … 300s refers to the GloVe …","url":["https://lill.is/pubs/Wang2020a.pdf"]}
{"year":"2021","title":"A Comparison Framework for Product Matching Algorithms","authors":["J Foxcroft - 2021"],"snippet":"Page 1. A Comparison Framework for Product Matching Algorithms by Jeremy Foxcroft A Thesis presented to The University of Guelph In partial fulfilment of requirements for the degree of Master of Science in Computer Science Guelph, Ontario, Canada …","url":["https://atrium.lib.uoguelph.ca/xmlui/bitstream/handle/10214/26375/Foxcroft_Jeremy_202109_Msc.pdf?sequence=3"]}
{"year":"2021","title":"A Comparison of Approaches to Document-level Machine Translation","authors":["Z Ma, S Edunov, M Auli - arXiv preprint arXiv:2101.11040, 2021"],"snippet":"… WMT17 English-German (en-de). For this benchmark, we follow the setup of Müller et al. (2018) whose training data includes the Europarl, Common Crawl, News Commentary and Rapid corpora, totaling nearly 6M sentence pairs …","url":["https://arxiv.org/pdf/2101.11040"]}
{"year":"2021","title":"A Comprehensive Assessment of Dialog Evaluation Metrics","authors":["YT Yeh, M Eskenazi, S Mehri - arXiv preprint arXiv:2106.03706, 2021"],"snippet":"… RoBERTa, which is employed in USR (Mehri and Eskenazi, 2020b), improves the training techniques in BERT and trains the model on a much larger corpus which includes the CommonCrawl News dataset (Mackenzie et al., 2020) and text ex- tracted from Reddit …","url":["https://arxiv.org/pdf/2106.03706"]}
{"year":"2021","title":"A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate Nearest Neighbor Search","authors":["M Wang, X Xu, Q Yue, Y Wang - arXiv preprint arXiv:2101.12631, 2021"],"snippet":"Page 1. A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate Nearest Neighbor Search Mengzhao Wang1, Xiaoliang Xu1, Qiang Yue1, Yuxiang Wang1,∗ 1Hangzhou Dianzi University …","url":["https://arxiv.org/pdf/2101.12631"]}
{"year":"2021","title":"A Comprehensive Survey of Grammatical Error Correction","authors":["Y Wang, Y Wang, K Dang, J Liu, Z Liu - ACM Transactions on Intelligent Systems and …, 2021"],"snippet":"Grammatical error correction (GEC) is an important application aspect of natural language processing techniques, and GEC system is a kind of very important intelligent system that has long been explored both in academic and industrial …","url":["https://dl.acm.org/doi/abs/10.1145/3474840"]}
{"year":"2021","title":"A Computational Framework for Slang Generation","authors":["Z Sun, R Zemel, Y Xu - arXiv preprint arXiv:2102.01826, 2021"],"snippet":"… To compare with and compute the baseline em- bedding methods M for definition sentences, we used 300-dimensional fastText embeddings (Bo- janowski et al., 2017) pre-trained with subword information on 600 billion …","url":["https://arxiv.org/pdf/2102.01826"]}
{"year":"2021","title":"A coral-reef approach to extract information from HTML tables","authors":["P Jiménez Aguirre, JC Roldán Salvador… - Applied Soft Computing …, 2022","P Jiménez, JC Roldán, R Corchuelo - Applied Soft Computing, 2021"],"snippet":"… Unfortunately, a recent analysis of the 32.04 million domains in the November 2019 Common Crawl has revealed that only 11.92 million domains provide such semantic hints [10], which argues for a method to deal with the remaining 20.12 …","url":["https://idus.us.es/bitstream/handle/11441/131990/1/1-s2.0-S1568494621009029-main.pdf?sequence=1","https://www.sciencedirect.com/science/article/pii/S1568494621009029"]}
{"year":"2021","title":"A COVID-19 news coverage mood map of Europe","authors":["F Robertson, J Lagus, K Kajava - Proceedings of the EACL Hackashop on News …, 2021"],"snippet":"… Newscrawl is a web crawl provided by the Common Crawl organisation which is updated more frequently and contains only data from news websites2. In order to keep the size of the corpus manageable and the extraction …","url":["https://www.aclweb.org/anthology/2021.hackashop-1.15.pdf"]}
{"year":"2021","title":"A data quality approach to the identification of discrimination risk in automated decision making systems","authors":["A Vetrò, M Torchiano, M Mecati - Government Information Quarterly, 2021"],"snippet":"… Similarly, a scientific experiment on the search engine Common Crawl (De-Arteaga et al., 2019) revealed an unequal treatment due to gender imbalance in the input data (almost 400,000 biographies): authors compared …","url":["https://www.sciencedirect.com/science/article/pii/S0740624X21000551"]}
{"year":"2021","title":"A data-centric review of deep transfer learning with applications to text data","authors":["S Bashath, N Perera, S Tripathi, K Manjang, M Dehmer… - Information Sciences, 2021"],"snippet":"Abstract In recent years, many applications are using various forms of deep learning models. Such methods are usually based on traditional learning paradigms requiring the consistency of properties among the feature spaces of the training and …","url":["https://www.sciencedirect.com/science/article/pii/S002002552101183X"]}
{"year":"2021","title":"A deep learning-based bilingual Hindi and Punjabi named entity recognition system using enhanced word embeddings","authors":["A Goyal, V Gupta, M Kumar - Knowledge-Based Systems, 2021"],"snippet":"… Initially, we collect Facebook’s pre-trained FastText embeddings which are trained on Wikipedia and common crawl data with 300 dimensions for our Hindi and Punjabi datasets. But after experiments, we find many of the words in our dataset are …","url":["https://www.sciencedirect.com/science/article/pii/S0950705121008637"]}
{"year":"2021","title":"A Framework for Generating Extractive Summary from Multiple Malayalam Documents","authors":["K Manju, S David Peter, SM Idicula - Information, 2021"],"snippet":"… Semantically similar words are mapped to nearby points in the vector space. In this work the vectorization of the terms in the document are performed using the pretrained word embedding model FastText for Malayalam, trained on Common Crawl and Wikipedia …","url":["https://www.mdpi.com/2078-2489/12/1/41/pdf"]}
{"year":"2021","title":"A Framework for Quality Assessment of Semantic Annotations of Tabular Data","authors":["R Avogadro, M Cremaschi, E Jiménez-Ruiz, A Rula - International Semantic Web …, 2021"],"snippet":"… 1 Introduction. Much information is conveyed within tables. A prominent example is the large set of relational databases or tabular data present on the Web. To size the spread of tabular data, 2.5M tables have been …","url":["https://link.springer.com/chapter/10.1007/978-3-030-88361-4_31"]}
{"year":"2021","title":"A Fusion Approach for Paper Submission Recommendation System","authors":["ST Huynh, N Dang, PT Huynh, DH Nguyen, BT Nguyen - International Conference on …, 2021"],"snippet":"… Finally, we use crawl-300d-2M 3 as the pre-train embedding matrix, which has 600 billion tokens and 2 million word vectors trained on Common Crawl. It can make using crawl-300d-2M more efficiently in vectorization. As depicted in Fig …","url":["https://link.springer.com/chapter/10.1007/978-3-030-79463-7_7"]}
{"year":"2021","title":"A General Language Assistant as a Laboratory for Alignment","authors":["A Askell, Y Bai, A Chen, D Drain, D Ganguli… - arXiv preprint arXiv …, 2021"],"snippet":"… For language model pre-training, these models are trained for 400B tokens on a distribution consisting mostly of filtered common crawl … The natural language dataset was composed of 55% heavily filtered common crawl data (220B tokens), 32 …","url":["https://arxiv.org/pdf/2112.00861"]}
{"year":"2021","title":"A Heuristic-driven Ensemble Framework for COVID-19 Fake News Detection","authors":["SD Das, A Basak, S Dutta - arXiv preprint arXiv:2101.03545, 2021"],"snippet":"… of model-specific special tokens. Each model also has its corresponding vocabulary associated with its tokenizer, trained on a large corpus data like GLUE, wikitext-103, CommonCrawl data etc. During training, each model …","url":["https://arxiv.org/pdf/2101.03545"]}
{"year":"2021","title":"A Heuristic-driven Uncertainty based Ensemble Framework for Fake News Detection in Tweets and News Articles","authors":["SD Das, A Basak, S Dutta - arXiv preprint arXiv:2104.01791, 2021"],"snippet":"… Each model also has its corresponding vocabulary associated with its tokenizer, trained on a large corpus data like GLUE, wikitext-103, CommonCrawl data etc. During training, each model applies the tokenization …","url":["https://arxiv.org/pdf/2104.01791"]}
{"year":"2021","title":"A Human Being Wrote This Law","authors":["AB Cyphert"],"snippet":"… GPT-3 had an impressively large data training set: it was trained on the Common Crawl dataset, a nearly trillion-word dataset,22 which includes everything from traditional news sites like the New York Times to sites like Reddit.The Common …","url":["https://lawreview.law.ucdavis.edu/issues/55/1/articles/files/55-1_Cyphert.pdf"]}
{"year":"2021","title":"A Literature Survey of Recent Advances in Chatbots","authors":["G Caldarini, S Jaf, K McGarry - 2021"],"snippet":"… This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from transformers) [46] and GPT (Generative Pre-trained Transformer), which were trained with huge language datasets, such as Wikipedia …","url":["https://www.preprints.org/manuscript/202112.0265/download/final_file"]}
{"year":"2021","title":"A Mechanism for Producing Aligned Latent Spaces with Autoencoders","authors":["S Jain, A Radhakrishnan, C Uhler - arXiv preprint arXiv:2106.15456, 2021"],"snippet":"… 6.1 Alignment of GloVe Embeddings In this section, we apply our theory to align semantic/syntactic directions in GloVe word embeddings [21]. We use 300 dimensional GloVe vectors that were trained on Common Crawl with 840 billion tokens …","url":["https://arxiv.org/pdf/2106.15456"]}
{"year":"2021","title":"A Multi-Platform Analysis of Political News Discussion and Sharing on Web Communities","authors":["Y Wang, S Zannettou, J Blackburn, B Bradlyn… - arXiv preprint arXiv …, 2021"],"snippet":"… supported types of entities). The model re- lies on Convolutional Neural Networks (CNNs), trained on the OntoNotes dataset [90], as well as Glove vectors [62] trained on the Common Crawl dataset [17]. 2.3 News Stories Identification …","url":["https://arxiv.org/pdf/2103.03631"]}
{"year":"2021","title":"A Multi-Task Learning Model for Multidimensional Relevance Assessment","authors":["DGP Putri, M Viviani, G Pasi - International Conference of the Cross-Language …, 2021"],"snippet":"… 6 In particular, we focused on the ad-hoc retrieval subtask. The data consist of Web pages crawled by means of CommonCrawl, 7 related to the health-related domain. The data collections consider 50 topics/queries and associated documents …","url":["https://link.springer.com/chapter/10.1007/978-3-030-85251-1_9"]}
{"year":"2021","title":"A Multifactorial Approach to Crosslinguistic Constituent Orderings","authors":["Z Liu"],"snippet":"… The data for training these LMs was taken from the raw data of the CoNLL 2017 Shared Task on multilingual parsing (Ginter et al. 2017), which contains texts from Common Crawl and Wikipedia. The architecture of the LM was the same for every language …","url":["https://www.researchgate.net/profile/Zoey-Liu/publication/354204297_A_Multifactorial_Approach_to_Crosslinguistic_Constituent_Orderings/links/612c0095c69a4e487967c628/A-Multifactorial-Approach-to-Crosslinguistic-Constituent-Orderings.pdf"]}
{"year":"2021","title":"A Multitask Framework to Detect Depression, Sentiment and Multi-label Emotion from Suicide Notes","authors":["S Ghosh, A Ekbal, P Bhattacharyya - Cognitive Computation, 2021"],"snippet":"The significant rise in suicides is a major cause of concern in public health domain. Depression plays a major role in increasing suicide ideation among th.","url":["https://link.springer.com/article/10.1007/s12559-021-09828-7"]}
{"year":"2021","title":"A Novel Corpus of Discourse Structure in Humans and Computers","authors":["B Hemmatian, S Feucht, R Avram, A Wey, M Garg… - arXiv preprint arXiv …, 2021"],"snippet":"We present a novel corpus of 445 humanand computer-generated documents, comprising about 27,000 clauses, annotated for semantic clause types and coherence relations that allow for nuanced comparison of artificial and natural …","url":["https://arxiv.org/pdf/2111.05940"]}
{"year":"2021","title":"A novel fusion-based deep learning model for sentiment analysis of COVID-19 tweets","authors":["ME Basiri, S Nemati, M Abdar, S Asadi, UR Acharrya - Knowledge-Based Systems, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0950705121005049"]}
{"year":"2021","title":"A NOVEL TRILINGUAL DATASET FOR CRISIS NEWS CATEGORIZATION ACROSS LANGUAGES","authors":["K Kajava - 2021"],"snippet":"… Use Common Crawl instead • “the goal of democratizing access to web information by producing and maintaining an open repository of web crawl data that is universally accessible and analyzable” (https://commoncrawl.org/about/, accessed June 1 2021) …","url":["https://blogs.helsinki.fi/language-technology/files/2021/06/LT-Seminar-2021-06-03-Kaisla-Kajava.pdf"]}
{"year":"2021","title":"A Primer on Pretrained Multilingual Language Models","authors":["S Doddapaneni, G Ramesh, A Kunchukuttan, P Kumar… - arXiv preprint arXiv …, 2021"],"snippet":"… and they differ in the architecture (eg, number of layers, parameters, etc), objective functions used for training (eg, monolingual masked language modeling objective, translation language modeling objective, etc), data used …","url":["https://arxiv.org/pdf/2107.00676"]}
{"year":"2021","title":"A Probing Task on Linguistic Properties of Korean Sentence Embedding","authors":["A Ahn, BI Ko, D Lee, G Han, M Shin, J Nam - Annual Conference on Human and …, 2021"],"snippet":"Abstract 본 연구는 한국어 문장 임베딩 (embedding) 에 담겨진 언어적 속성을 평가 하기 위한 프로빙 태스크 (Probing Task) 를 소개한다. 프로빙 태스크는 임베딩 으로부터 문장의 표층적, 통사적, 의미적 속성을 구분하는 문제로 영어, 폴란드어 …","url":["https://www.koreascience.or.kr/article/CFKO202130060614813.pdf"]}
{"year":"2021","title":"A Residual Network Architecture for Hindi NER using Fasttext and BERT embedding layers","authors":["R Shelke, S Vanjale"],"snippet":"… It provides word embedding for Hindi (and 157 other languages) and is based on the CBOW (Continuous Bag-of-Words) model. The CBOW model learns by predicting the current word based on its context, and it was trained on Common Crawl and Wikipedia …","url":["https://www.novyimir.net/gallery/nmrj%202867f.pdf"]}
{"year":"2021","title":"A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models","authors":["F Alam, A Hasan, T Alam, A Khan, J Tajrin, N Khan… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models FIROJ ALAM, Qatar Computing Research Institute, HBKU, Qatar MD. ARID HASAN, Cognitive Insight Limited, Bangladesh …","url":["https://arxiv.org/pdf/2107.03844"]}
{"year":"2021","title":"A Review of Public Datasets in Question Answering Research","authors":["BB Cambazoglu, M Sanderson, F Scholer, B Croft"],"snippet":"… matching the question. The sentences are selected based on their tf-idf similarity to the question. The underlying web page collection contains pages from the July 2018 archive of the Common Crawl web repository. Task. Given a …","url":["http://www.sigir.org/wp-content/uploads/2020/12/p07.pdf"]}
{"year":"2021","title":"A Semi-supervised Multi-task Learning Approach to Classify Customer Contact Intents","authors":["L Dong, MC Spencer, A Biagi"],"snippet":"… We note that this ALBERT model is trained as a multiclass classification with only positive cases. 3.2.2 SS MT D/TAPT ALBERT The pretrained language models are mostly trained on well-known corpora, such as Wikipedia, Common Crawl, BookCorpus, Reddit, etc …","url":["https://assets.amazon.science/79/22/d9237534448293405083a73b896d/a-semi-supervised-multi-task-learning-approach-to-classify-customer-contact-intents.pdf"]}
{"year":"2021","title":"A Short Survey of LSTM Models for De-identification of Medical Free Text","authors":["JL Leevy, TM Khoshgoftaar - 2020 IEEE 6th International Conference on …, 2020"],"snippet":"… The training set was obtained from the 2014 i2b2 challenge, while the test set came from the University of Florida (UF) Health Integrated Data Repository 5. Word embeddings were sourced from GoogleNews [54] …","url":["https://ieeexplore.ieee.org/abstract/document/9319017/"]}
{"year":"2021","title":"A Simple Post-Processing Technique for Improving Readability Assessment of Texts using Word Mover's Distance","authors":["JM Imperial, E Ong - arXiv preprint arXiv:2103.07277, 2021"],"snippet":"… technique described in Section 4. For the word embeddings of English, German, and Filipino needed for the technique, we downloaded the resources from the fastText website5. The word embeddings in various …","url":["https://arxiv.org/pdf/2103.07277"]}
{"year":"2021","title":"A Simple Recipe for Multilingual Grammatical Error Correction","authors":["S Rothe, J Mallinson, E Malmi, S Krause, A Severyn - arXiv preprint arXiv:2106.03830, 2021"],"snippet":"… 2.1 mT5 Pre-training mT5 has been pre-trained on mC4 corpus, a subset of Common Crawl, covering 101 languages and composed of about 50 billion documents. For details on mC4, we refer the reader to the original paper (Xue et al., 2020) …","url":["https://arxiv.org/pdf/2106.03830"]}
{"year":"2021","title":"A Spontaneous Stereotype Content Model: Taxonomy, Properties, and Prediction.","authors":["G Nicolas, X Bai, ST Fiske"],"snippet":"… model trained on the Common Crawl (600 billion words obtained from various internet sources) … the Common Crawl (600 billion words), a Glove model trained using around 840 billion words from the Common Crawl (Pennington, Socher, & Manning, 2014; …","url":["https://www.nicolaslab.org/publication/sscm/SSCM.pdf"]}
{"year":"2021","title":"A Study of Analogical Density in Various Corpora at Various Granularity","authors":["R Fam, Y Lepage - Information, 2021"],"snippet":"In this paper, we inspect the theoretical problem of counting the number of analogies between sentences contained in a text. Based on this, we measure the analogical density of the text. We focus on analogy at the sentence level …","url":["https://www.mdpi.com/2078-2489/12/8/314/pdf"]}
{"year":"2021","title":"A Study of Analogical Density in Various Corpora at Various Granularity. Information 2021, 12, 314","authors":["R Fam, Y Lepage - 2021"],"snippet":"… Table 4 shows the statistics of Multi30K corpus. • CommonCrawl (available at: commoncrawl.org accessed on 20 September 2020) is a crawled web archive and dataset … Table 5 shows the statistics on the CommonCrawl corpus …","url":["https://search.proquest.com/openview/208b192bc36d7c71728c73989c304dea/1?pq-origsite=gscholar&cbl=2032384"]}
{"year":"2021","title":"A study on performance improvement considering the balance between corpus in Neural Machine Translation","authors":["C Park, K Park, H Moon, S Eo, H Lim - Journal of the Korea Convergence Society, 2021"],"snippet":"… 1. Concept of Corpus Weight Balance GPT3도 Common Crawl, WebText2, Books1, Books2, Wikipedia 등의 데이터를 합쳐 모델을 훈련하 게 된다. 그러나 말뭉치 간의 특성 및 특징(어투, 문체, 도메인 등)이 다름에도 하나의 데이터로 …","url":["https://www.koreascience.or.kr/article/JAKO202116954598769.pdf"]}
{"year":"2021","title":"A Survey of COVID-19 Misinformation: Datasets, Detection Techniques and Open Issues","authors":["AR Ullah, A Das, A Das, MA Kabir, K Shu - arXiv preprint arXiv:2110.00737, 2021"],"snippet":"Page 1. A Survey of COVID-19 Misinformation: Datasets, Detection Techniques and Open Issues AR Sana Ullaha, Anupam Dasa, Anik Dasb, Muhammad Ashad Kabirc,∗, Kai Shud aDepartment of Computer Science and Engineering …","url":["https://arxiv.org/pdf/2110.00737"]}
{"year":"2021","title":"A Survey of Machine Learning-Based Solutions for Phishing Website Detection","authors":["L Tang, QH Mahmoud - Machine Learning and Knowledge Extraction, 2021"],"snippet":"With the development of the Internet, network security has aroused people's attention. It can be said that a secure network environment is a basis for the rapid and sound development of the Internet. Phishing is an essential class …","url":["https://www.mdpi.com/2504-4990/3/3/34/pdf"]}
{"year":"2021","title":"A Survey of Recent Abstract Summarization Techniques","authors":["D Puspitaningrum - Proceedings of Sixth International Congress on …, 2021"],"snippet":"… For C4, taken from Common Crawl scrape from April 2019 and applied some cleansing filters, it results in a very clean 750GB text dataset of large pre-training datasets, more extensive than other pre-training datasets. 3.2 Pegasus-XSum (Pegasus) …","url":["https://hal.archives-ouvertes.fr/hal-03216381/document"]}
{"year":"2021","title":"A Survey on Bias in Deep NLP","authors":["I Garrido-Muñoz, A Montejo-Ráez, F Martínez-Santiago… - 2021"],"snippet":"… 2016 [26] Gender Word2Vec, GloVe GoogleNews corpus (w2vNEWS), Common Crawl English Analogies/Cosine Similarity Vector Space Manipulation After - 2017 [10] Gender, Ethnicity GloVe, Word2Vec Common …","url":["https://www.preprints.org/manuscript/202103.0049/download/final_file"]}
{"year":"2021","title":"A Survey on Data Augmentation for Text Classification","authors":["M Bayer, MA Kaufhold, C Reuter - arXiv preprint arXiv:2107.03158, 2021"],"snippet":"… CNN+LSTM/GRU HON RSN-1 RSN-2 Word2Vec Hate Speech FastText Wikipedia GoogleNews W2V GloVe Common Crawl GloVe Common Crawl GloVe Common Crawl -22.7 (Macro F1) +1.0 -3.3 +0.3 -0.2 0 [44] 1. Method …","url":["https://arxiv.org/pdf/2107.03158"]}
{"year":"2021","title":"A Survey on Low-Resource Neural Machine Translation","authors":["R Wang, X Tan, R Luo, T Qin, TY Liu - arXiv preprint arXiv:2107.04239, 2021"],"snippet":"Page 1. A Survey on Low-Resource Neural Machine Translation Rui Wang, Xu Tan, Renqian Luo, Tao Qin and Tie-Yan Liu Microsoft Research Asia {ruiwa, xuta, t-reluo, taoqin, tyliu}@microsoft.com Abstract Neural approaches …","url":["https://arxiv.org/pdf/2107.04239"]}
{"year":"2021","title":"A Survey On Neural Word Embeddings","authors":["E Sezerer, S Tekir - arXiv preprint arXiv:2110.01804, 2021"],"snippet":"… ivLBL/vLBL [95] 2013 100-600 Wiki LBL Performance - NCE [47] GloVe [109] 2014 300 Wiki, Gigaword, Commoncrawl LBL+coocurence Matrix Training - - DEPS [69] 2014 300 Wiki CBOW Training Stanford tagger[129] …","url":["https://arxiv.org/pdf/2110.01804"]}
{"year":"2021","title":"A Survey on Statistical Approaches for Abstractive Summarization of Low Resource Language Documents","authors":["P Deshpande, S Jahirabadkar - Smart Trends in Computing and Communications, 2022"],"snippet":"… German Wiki data is used as real data and synthetic data is a common crawl data. Synthetic data is used to increase size of data. Three settings are considered for generation of summaries: (1) Transformer model using real data for training. …","url":["https://link.springer.com/chapter/10.1007/978-981-16-4016-2_69"]}
{"year":"2021","title":"A Synthetic FACS Framework for Expanding Facial Expression Lexicons","authors":["C Butler - 2021"],"snippet":"Page 1. A Synthetic FACS Framework for Expanding Facial Expression Lexicons DISSERTATION Submitted in Partial Fulfillment of the Requirements for the Degree of DOCTOR OF PHILOSOPHY (Computer Science) at the …","url":["https://search.proquest.com/openview/ebf5a87df8275e6fc6fac0b1c0b21b44/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2021","title":"A system for proactive risk assessment of application changes in cloud operations","authors":["R Batta, L Shwartz, M Nidd, AP Azad, H Kumar - 2021 IEEE 14th International …, 2021"],"snippet":"Abstract Change is one of the biggest contributors to service outages. With more enterprises migrating their applications to cloud and using automated build and deployment the volume and rate of changes has significantly increased. Furthermore …","url":["https://www.computer.org/csdl/proceedings-article/cloud/2021/006000a112/1ymJ4TXNxUA"]}
{"year":"2021","title":"A Systematic Investigation of Commonsense Understanding in Large Language Models","authors":["XL Li, A Kuncoro, CM d'Autume, P Blunsom… - arXiv preprint arXiv …, 2021"],"snippet":"… 2019), we train our models using the cleaned version of Common Crawl corpus (C4), around 800 GB of data. Our largest model, with 32 transformer layers and 7 billion parameters, has a similar number of parameters to the open-sourced GPT-J model (Wang …","url":["https://arxiv.org/pdf/2111.00607"]}
{"year":"2021","title":"A systems-wide understanding of the human olfactory percept chemical space","authors":["J Kowalewski, B Huynh, A Ray - Chemical Senses, 2021"],"snippet":"… 2015; spaCy, 2016), and a convolutional neural network previously trained on GloVe Common Crawl (Pennington, Socher, & Manning, 2014) and OntoNotes 5. The training set is comprised of more than 1 million English …","url":["https://academic.oup.com/chemse/advance-article-abstract/doi/10.1093/chemse/bjab007/6153471"]}
{"year":"2021","title":"A Targeted Attack on Black-Box Neural Machine Translation with Parallel Data Poisoning","authors":["C Xu, J Wang, Y Tang, F Guzmán, BIP Rubinstein… - Proceedings of the Web …, 2021"],"snippet":"… 3https://commoncrawl.org/ 4We assume that these poisoned web pages are archived and to be used for parallel data extraction. This assumption is realistic as we find that the crawling services commonly used for parallel …","url":["https://dl.acm.org/doi/abs/10.1145/3442381.3450034"]}
{"year":"2021","title":"A unified approach to sentence segmentation of punctuated text in many languages","authors":["R Wicks, M Post"],"snippet":"Page 1. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3995–4007 August 1–6, 2021 …","url":["https://aclanthology.org/2021.acl-long.309.pdf"]}
{"year":"2021","title":"A word embedding-based approach to cross-lingual topic modeling","authors":["CH Chang, SY Hwang - Knowledge and Information Systems, 2021"],"snippet":"The cross-lingual topic analysis aims at extracting latent topics from corpora of different languages. Early approaches rely on high-cost multilingual reso.","url":["https://link.springer.com/article/10.1007/s10115-021-01555-7"]}
{"year":"2021","title":"ABC: Attention with Bounded-memory Control","authors":["H Peng, J Kasai, N Pappas, D Yogatama, Z Wu, L Kong… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. Under review as a conference paper at ICLR 2022 ABC: ATTENTION WITH BOUNDED-MEMORY CONTROL Hao Peng♠ Jungo Kasai♠ Nikolaos Pappas♠ Dani Yogatama♣ Zhaofeng Wu♦∗ Lingpeng Kong♦ Roy Schwartz …","url":["https://arxiv.org/pdf/2110.02488"]}
{"year":"2021","title":"Abuse is Contextual, What about NLP? The Role of Context in Abusive Language Annotation and Detection","authors":["S Menini, AP Aprosio, S Tonelli - arXiv preprint arXiv:2103.14916, 2021"],"snippet":"… In particular, all vectors are extracted starting from the pre-trained embeddings obtained from the Common Crawl corpus.5 Since SVM takes in input sentence embeddings, we convert the context and the current tweet …","url":["https://arxiv.org/pdf/2103.14916"]}
{"year":"2021","title":"Accelerated execution via eager-release of dependencies in task-based workflows","authors":["H Elshazly, F Lordan, J Ejarque, RM Badia - The International Journal of High …, 2021"],"snippet":"Task-based programming models offer a flexible way to express the unstructured parallelism patterns of nowadays complex applications. This expressive capability is required to achieve maximum possi...","url":["https://journals.sagepub.com/doi/abs/10.1177/1094342021997558"]}
{"year":"2021","title":"Accelerating Text Communication via Abbreviated Sentence Input","authors":["J Adhikary, J Berger, K Vertanen"],"snippet":"… For our out-of-domain training set, we used one billion words of web text from Common Crawl1. We only … As shown in Table 1, random sentences from Common Crawl averaged 30 words. The cross-entropy 1https://commoncrawl …","url":["https://aclanthology.org/2021.acl-long.514.pdf"]}
{"year":"2021","title":"Accurate Word Representations with Universal Visual Guidance","authors":["Z Zhang, H Yu, H Zhao, R Wang, M Utiyama - arXiv preprint arXiv:2012.15086, 2020"],"snippet":"… WMT'14 EN-DE 4.43M bilingual sentence pairs of the WMT14 dataset were used as training data, including Common Crawl, News Commentary, and Europarl v7. The newstest2013 and newstest2014 datasets were …","url":["https://arxiv.org/pdf/2012.15086"]}
{"year":"2021","title":"Acquiring and Harnessing Verb Knowledge for Multilingual Natural Language Processing","authors":["O Majewska - 2021"],"snippet":"Advances in representation learning have enabled natural language processing models to derive non-negligible linguistic information directly from text corpora in an unsupervised fashion. However, this signal is underused in downstream tasks …","url":["https://www.repository.cam.ac.uk/bitstream/handle/1810/329292/Majewska_PhDThesis_final.pdf?sequence=4"]}
{"year":"2021","title":"Active Learning for Argument Mining: A Practical Approach","authors":["N Solmsdorf, D Trautmann, H Schütze - arXiv preprint arXiv:2109.13611, 2021"],"snippet":"… easily discernible argumentative statements. The corpus contains 1,000 sentences per topic, ie, in total 8,000 instances, which were tapped from a Common Crawl snapshot and in- dexed with Elasticsearch. The time-consuming …","url":["https://arxiv.org/pdf/2109.13611"]}
{"year":"2021","title":"Adapting Neural Machine Translation for Automatic Post-Editing","authors":["A Sharma, P Gupta, A Nelakanti"],"snippet":"… reference as the output. 3.2 Pre-training on domain-specific data FAIR's WMT'19 NMT model was trained on Newscrawl and Commoncrawl datasets while the source of this year's APE data is Wikipedia. To fix the domain mismatch …","url":["https://assets.amazon.science/dc/df/5443c00541a9b6257f6110c5bb86/adapting-neural-machine-translation-for-automatic-post-editing.pdf"]}
{"year":"2021","title":"Adaptive Ranking Relevant Source Files for Bug Reports Using Genetic Algorithm","authors":["H Fujita, H Perez-Meana - 2021"],"snippet":"Abstract. Precisely locating buggy files for a given bug report is a cumbersome and time-consuming task, particularly in a large-scale project with thousands of source files and bug reports. An efficient bug localization module is desirable to improve the …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=GYxJEAAAQBAJ&oi=fnd&pg=PA430&dq=commoncrawl&ots=IRy65Bdasc&sig=WyEQAu3sL155gzwLW6pjIX4mCwQ"]}
{"year":"2021","title":"ADEPT: An Adjective-Dependent Plausibility Task","authors":["A Emami, I Porada, A Olteanu, K Suleman, A Trischler…"],"snippet":"… 4 Dataset To construct ADEPT, we scrape text samples from English Wikipedia and Common Crawl, extracting adjectival modifier-noun pairs that occur with high frequency … We extracted 10 million pairs from English …","url":["https://aclanthology.org/2021.acl-long.553.pdf"]}
{"year":"2021","title":"ADPBC: Arabic Dependency Parsing Based Corpora for Information Extraction","authors":["S Mohamed, M Hussien, HM Mousa - 2021"],"snippet":"… Sch. Econ. Res. Pap. No. WP BRP., 2018. [27] A. Panchenko, E. Ruppert, S. Faralli, SP Ponzetto, and C. Biemann, “Building a web-scale dependency-parsed corpus from common crawl,” Lr. 2018 - 11th Int. Conf. Lang. Resour. Eval., pp. 1816–1823, 2019 …","url":["http://www.mecs-press.org/ijitcs/ijitcs-v13-n1/IJITCS-V13-N1-4.pdf"]}
{"year":"2021","title":"Advances and Trends in Artificial Intelligence. From Theory to Practice: 34th International Conference on Industrial, Engineering and Other Applications of Applied …","authors":["H Fujita"],"snippet":"Page 1. Hamido Fujita Ali Selamat Jerry Chun-Wei Lin Moonis Ali (Eds.) Advances and Trends in Artificial Intelligence From Theory to Practice 34th International Conference on Industrial, Engineering and Other Applications …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=ihg5EAAAQBAJ&oi=fnd&pg=PR5&dq=commoncrawl&ots=fSoeFLOj--&sig=NR8xQHWDWjlGUhvSgV52wEaaT6Y"]}
{"year":"2021","title":"Aggressive and Offensive Language Identification in Hindi, Bangla, and English: A Comparative Study","authors":["R Kumar, B Lahiri, AK Ojha - SN Computer Science, 2021"],"snippet":"In the present paper, we carry out a comparative study between offensive and aggressive language and attempt to understand their inter-relationship. To car.","url":["https://link.springer.com/article/10.1007/s42979-020-00414-6"]}
{"year":"2021","title":"AlephBERT: A Hebrew Large Pre-Trained Language Model to Start-off your Hebrew NLP Application With","authors":["A Seker, E Bandel, D Bareket, I Brusilovsky… - arXiv preprint arXiv …, 2021"],"snippet":"… Oscar: A deduplicated Hebrew portion of the OSCAR corpus, which is “extracted from Common Crawl via language classification, filtering and cleaning” (Ortiz Suárez et al., 2020). • Twitter: Texts of Hebrew tweets collected between 2014-09-28 and 2018-03-07 …","url":["https://arxiv.org/pdf/2104.04052"]}
{"year":"2021","title":"Alignment of Language Agents","authors":["Z Kenton, T Everitt, L Weidinger, I Gabriel, V Mikulik… - arXiv preprint arXiv …, 2021","ZKTEL Weidinger, IGVMG Irving"],"snippet":"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system …","url":["https://ar5iv.labs.arxiv.org/html/2103.14659","https://arxiv.org/pdf/2103.14659"]}
{"year":"2021","title":"All Labels Are Not Created Equal: Enhancing Semi-supervision via Label Grouping and Co-training","authors":["I Nassar, S Herath, E Abbasnejad, W Buntine, G Haffari - arXiv preprint arXiv …, 2021"],"snippet":"… A detailed description of such relations and examples thereof can be found in the ConceptNet documentation8. On the other hand, GloVe and word2vec are two prominent sets of word embeddings, the former is trained on 840 …","url":["https://arxiv.org/pdf/2104.05248"]}
{"year":"2021","title":"All NLP Tasks Are Generation Tasks: A General Pretraining Framework","authors":["Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang - arXiv preprint arXiv:2103.10360, 2021"],"snippet":"Page 1. All NLP Tasks Are Generation Tasks: A General Pretraining Framework Zhengxiao Du *12 Yujie Qian * 3 Xiao Liu 1 2 Ming Ding 1 2 Jiezhong Qiu 1 2 Zhilin Yang 4 2 Jie Tang 1 2 Abstract There have been various types …","url":["https://arxiv.org/pdf/2103.10360"]}
{"year":"2021","title":"Allocating Large Vocabulary Capacity for Cross-lingual Language Model Pre-training","authors":["B Zheng, L Dong, S Huang, S Singhal, W Che, T Liu… - arXiv preprint arXiv …, 2021"],"snippet":"… are learned on the reconstructed CommonCrawl corpus (Chi et al., 2021b; Conneau et al., 2020) using SentencePiece (Kudo and Richardson, 2018) with the unigram language model (Kudo, 2018). The unigram distributions …","url":["https://arxiv.org/pdf/2109.07306"]}
{"year":"2021","title":"ALX: Large Scale Matrix Factorization on TPUs","authors":["H Mehta, S Rendle, W Krichene, L Zhang - arXiv preprint arXiv:2112.02194, 2021"],"snippet":"We present ALX, an open-source library for distributed matrix factorization using Alternating Least Squares, written in JAX. Our design allows for efficient use of the TPU architecture and scales well to matrix factorization problems of O(B) rows/columns …","url":["https://arxiv.org/pdf/2112.02194"]}
{"year":"2021","title":"AMMUS: A Survey of Transformer-based Pretrained Models in Natural Language Processing","authors":["KS Kalyan, A Rajasekharan, S Sangeetha - arXiv preprint arXiv:2108.05542, 2021"],"snippet":"… mT6 [91], XLM-E [89] CC-Aligned [108] Parallel corpus of 292 million non-English common crawl document pairs and 100 million English common crawl document pairs. XLM-E [89] Dakshina [109] Parallel corpus containing 10K sentences for 12 In- dian languages …","url":["https://arxiv.org/pdf/2108.05542"]}
{"year":"2021","title":"An Alignment-Based Approach to Semi-Supervised Bilingual Lexicon Induction with Small Parallel Corpora","authors":["K Marchisio, C Xiong, P Koehn"],"snippet":"… learning. 6 Experimental Settings Language Corpus # of words English WaCky, BNC, Wikipedia 2.8 B Italian itWac 1.6 B German SdeWaC 0.9 B Spanish News Crawl 2007-2012 386 M Finnish Common Crawl 2016 2.8 B Table …","url":["https://aclanthology.org/2021.mtsummit-research.24.pdf"]}
{"year":"2021","title":"An analysis of full-size Russian complexly NER labelled corpus of Internet user reviews on the drugs based on deep learning and language neuron nets","authors":["AG Sboeva, SG Sboevac, IA Moloshnikova…"],"snippet":"Page 1. An analysis of full-size Russian complexly NER labelled corpus of Internet user reviews on the drugs based on deep learning and language neuron nets AG Sboeva,b,, SG Sboevac, IA Moloshnikova, AV Gryaznova, RB …","url":["https://sagteam.ru/papers/med-corpus/4.pdf"]}
{"year":"2021","title":"An Effective Deep Learning Approach for Extractive Text Summarization","authors":["MT Luu, TH Le, MT Hoang"],"snippet":"Page 1. An Effective Deep Learning Approach for Extractive Text Summarization Minh-Tuan Luu PhD. Student, School of Information and Communication Technology, Hanoi University of Science and Technology, No.1 Dai Co …","url":["http://www.ijcse.com/docs/INDJCSE21-12-02-141.pdf"]}
{"year":"2021","title":"An embedding method for unseen words considering contextual information and morphological information","authors":["MS Won, YS Choi, S Kim, CW Na, JH Lee - Proceedings of the 36th Annual ACM …, 2021"],"snippet":"… Random embeddings are assigned for OOv words. Glove is implemented by using preǦtrained 300Ǧdimensional Glove embedding, which is trained on Common Crawl with 840B word tokens. Random embeddings are assigned for OOv words …","url":["https://dl.acm.org/doi/abs/10.1145/3412841.3441982"]}
{"year":"2021","title":"An empirical evaluation of text representation schemes to filter the social media stream","authors":["S Modha, P Majumder, T Mandl - Journal of Experimental & Theoretical Artificial …, 2021"],"snippet":"… Glove pre-trained model available with different embed size and trained on the common crawl, Twitter. We have used the Glove pre-trained model with a vocabulary size of 2.2 million and trained on the common crawl. fastText …","url":["https://www.tandfonline.com/doi/full/10.1080/0952813X.2021.1907792"]}
{"year":"2021","title":"An Empirical Exploration in Quality Filtering of Text Data","authors":["L Gao - arXiv preprint arXiv:2109.00698, 2021"],"snippet":"… (2020), with a Paretodistribution thresholded filtering method and a shallow CommonCrawl-WebText classfier … (2020) has been made public, we instead use the same type of fasttext (Joulin et al., 2017) classifier between unfiltered …","url":["https://arxiv.org/pdf/2109.00698"]}
{"year":"2021","title":"An Empirical Study on Task-Oriented Dialogue Translation","authors":["S Liu - ICASSP 2021-2021 IEEE International Conference on …, 2021"],"snippet":"… consistent). We valid them with SENT-BASE model on En⇒De task. data in WMT20 news domain, which consists of CommonCrawl and NewsCommentary. We conduct data selection to select similar amount of sentences …","url":["https://ieeexplore.ieee.org/abstract/document/9413521/"]}
{"year":"2021","title":"An End-to-end Point of Interest (POI) Conflation Framework","authors":["R Low, ZD Tekler, L Cheah - arXiv preprint arXiv:2109.06073, 2021"],"snippet":"… words that did not appear in the training data [63]. For this study, the fastText model was pre-trained on 2 million word vectors with subword information from commoncrawl.org. The second advantage of using the fastText library to …","url":["https://arxiv.org/pdf/2109.06073"]}
{"year":"2021","title":"An evaluation dataset for depression detection in Arabic social media","authors":["S Elimam, M Bougeussa - International Journal of Knowledge Engineering and …, 2021"],"snippet":"Studying depression in Arabic social media has been neglected compared to other languages and the traditional way of dealing with depression (face-to-face medical diagnose) is not enough as the number of people that suffer from depression in …","url":["https://www.inderscienceonline.com/doi/abs/10.1504/IJKEDM.2021.119888"]}
{"year":"2021","title":"An Explainable Multi-Modal Hierarchical Attention Model for Developing Phishing Threat Intelligence","authors":["Y Chai, Y Zhou, W Li, Y Jiang - IEEE Transactions on Dependable and Secure …, 2021"],"snippet":"Phishing website attack, as one of the most persistent forms of cyber threats, evolves and remains a major cyber threat. Various detection methods (eg, lookup systems, fraud cue-based methods) have been proposed to identify phishing websites. The …","url":["https://ieeexplore.ieee.org/abstract/document/9568704/"]}
{"year":"2021","title":"An Exploration of Alignment Concepts to Bridge the Gap between Phrase-based and Neural Machine Translation","authors":["JT Peter"],"snippet":"Page 1. An Exploration of Alignment Concepts to Bridge the Gap between Phrase-based and Neural Machine Translation Von der Fakultät für Mathematik, Informatik und Naturwissenschaften der RWTH Aachen University zur …","url":["https://www-i6.informatik.rwth-aachen.de/publications/download/1175/PeterJan-Thorsten--ExplorationofAlignmentConceptstoBridgetheGapbetweenPhrase-basedNeuralMachineTranslation--2020.pdf"]}
{"year":"2021","title":"An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers","authors":["T Ranasinghe, C Orasan, R Mitkov - arXiv preprint arXiv:2106.00143, 2021"],"snippet":"… Our architecture relies on the XLM-R transformer model (Conneau et al., 2020) to derive the representations of the input sentences. XLM-R has been trained on a large-scale multilingual dataset in 104 languages, totalling …","url":["https://arxiv.org/pdf/2106.00143"]}
{"year":"2021","title":"An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining","authors":["Z Zhang, X Song - arXiv preprint arXiv:2109.01411, 2021"],"snippet":"… The Web Data Commons3 (WDC) project extracts such structured data from the CommonCrawl4 as RDF n-quads5, and release them on … to create a very large training dataset for product entity linking using semantic markup data …","url":["https://arxiv.org/pdf/2109.01411"]}
{"year":"2021","title":"An extended analysis of the persistence of persistent identifiers of the scholarly web","authors":["M Klein, L Balakireva - International Journal on Digital Libraries, 2021"],"snippet":"… These findings were confirmed in a large-scale study by Thompson and Jian [22] based on two samples of the web taken from Common Crawl Footnote 3 datasets. The authors were motivated to quantify the use of HTTP DOIs versus URLs of …","url":["https://link.springer.com/article/10.1007/s00799-021-00315-w"]}
{"year":"2021","title":"An Intrinsic and Extrinsic Evaluation of Learned COVID-19 Concepts using Open-Source Word Embedding Sources","authors":["S Parikh, A Davoudi, S Yu, C Giraldo, E Schriver… - medRxiv"],"snippet":"… 8] and GloVe [9] on large corpora of texts including domain-independent texts (eg, internet web pages like Wikipedia and CommonCrawl; social media … Standard GloVe Embeddings Paper Vectors [9] Common Crawl Token 10 …","url":["https://www.medrxiv.org/content/medrxiv/early/2021/01/04/2020.12.29.20249005.full.pdf"]}
{"year":"2021","title":"An Investigation towards Differentially Private Sequence Tagging in a Federated Framework","authors":["A Jana, C Biemann"],"snippet":"… 2The hyperparameter settings to train those models are as follows: epochs- 10, batch size - 32, learning rate - 0.15, optimizer - Stochastic gradient descent (SGD) Common Crawl corpus) from spaCy library3, the dimension of which is 300 …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/publications/2021-janabiemann-privnlp-fed.pdf"]}
{"year":"2021","title":"An Overview on Evaluation Labs and Open Issues in Health-related Credible Information Retrieval","authors":["R Upadhyay, G Pasi, M Viviani - 2021"],"snippet":"… The 2020 Track used a dataset provided by Common Crawl, in particular related to different news collected in the first four months of 2020.4 On … 2https://trec-health-misinfo.github.io/2019.html 3https://lemurproject.org …","url":["http://52.178.216.184/paper31.pdf"]}
{"year":"2021","title":"Analysis and Evaluation of Language Models for Word Sense Disambiguation","authors":["D Loureiro, K Rezaee, MT Pilehvar… - Computational Linguistics, 2021"],"snippet":"Page 1. Analysis and Evaluation of Language Models for Word Sense Disambiguation Daniel Loureiro∗ LIAAD - INESC TEC Department of Computer Science - FCUP University of Porto, Portugal dloureiro@fc.up.pt Kiamehr …","url":["https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli_a_00405/1900170/coli_a_00405.pdf"]}
{"year":"2021","title":"Analysis of Machine Learning and Deep Learning Frameworks for Opinion Mining on Drug Reviews","authors":["F Youbi, N Settouti - The Computer Journal, 2021"],"snippet":"… More precisely, GloVe consists of collecting word co- occurrence statistics in a form of a word co-occurrence matrix, in which its developers have provided pre-embed millions of English tokens obtained from Wikipedia data and common crawl data …","url":["https://academic.oup.com/comjnl/advance-article-abstract/doi/10.1093/comjnl/bxab084/6311550"]}
{"year":"2021","title":"Analyzing Hyperonyms of Stack Overflow Posts","authors":["L Tóth, L Vidács"],"snippet":"… They applied a similar lexico-syntactic pattern-based mining on the dataset obtained from CommonCrawl [17] using a slightly different grammar for NP identification and, therefore, a slightly different set of patterns. Despite the differences …","url":["https://www.researchgate.net/profile/Laszlo-Toth-12/publication/356192289_Analyzing_Hyperonyms_of_Stack_Overflow_Posts/links/61910421d7d1af224bea68e9/Analyzing-Hyperonyms-of-Stack-Overflow-Posts.pdf"]}
{"year":"2021","title":"Analyzing Multimodal Language via Acoustic-and Visual-LSTM with Channel-aware Temporal Convolution Network","authors":["S Mai, S Xing, H Hu - IEEE/ACM Transactions on Audio, Speech, and …, 2021"],"snippet":"Page 1. 2329-9290 (c) 2021 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/9387606/"]}
{"year":"2021","title":"Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models","authors":["T He, J Liu, K Cho, M Ott, B Liu, J Glass, F Peng - … of the 16th Conference of the …, 2021"],"snippet":"… 4.1 Datasets For pretraining, we use the large-scale CCNEWS data (Bakhtin et al., 2019) which is a de-duplicated subset of the English portion of the CommonCrawl news dataset1 … We tune the 1 http://commoncrawl.org/2016/10/ news-dataset-available Page 5. 1125 …","url":["https://www.aclweb.org/anthology/2021.eacl-main.95.pdf"]}
{"year":"2021","title":"Analyzing transfer learning impact in biomedical cross-lingual named entity recognition and normalization","authors":["RM Rivera-Zavala, P Martínez - BMC Bioinformatics, 2021"],"snippet":"… The FastText-2M [52] pre-trained English word embedding model trained with subword information on Common Crawl using the FastText implementation. Finally, the PubMed and PMC [53] pre-trained English word embedding model, trained on a …","url":["https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04247-9"]}
{"year":"2021","title":"Annotation of Fine-Grained Geographical Entities in German Texts","authors":["J Moreno-Schneider, M Plakidis, G Rehm - 3rd Conference on Language, Data and …, 2021"],"snippet":"… The SpaCy models are trained on Ontonotes 5 and Common Crawl (English; en_core_web_md) and WikiNER and TIGER (German; de_core_news_md). The Stanford models are trained on the CoNLL 2003 data [18]. BERT-NER is trained on WikiNER [11] …","url":["https://drops.dagstuhl.de/opus/volltexte/2021/14547/pdf/OASIcs-LDK-2021-11.pdf"]}
{"year":"2021","title":"Answering questions about insurance supervision with a Neural Machine Translator","authors":["J Glowienke - 2021"],"snippet":"… Conneau et al. [3] pre-train a model based on the RoBERTa architecture to create crosslingual representations. The XLM-R model is pre-trained on a common crawl dataset of 100 languages using the masked multi-lingual language model ap- proach …","url":["https://dke.maastrichtuniversity.nl/jan.niehues/wp-content/uploads/2021/08/Glowienke-Master-thesis.pdf"]}
{"year":"2021","title":"Anticipating Attention: On the Predictability of News Headline Tests","authors":["N Hagar, N Diakopoulos, B DeWilde - Digital Journalism, 2021"],"snippet":"… These embeddings contain 300 dimensions and were trained on English language text from the OntoNotes 5.0 and GloVe Common Crawl corpora. For each headline, we computed the average embedding vector across all tokens. …","url":["https://www.tandfonline.com/doi/abs/10.1080/21670811.2021.1984266"]}
{"year":"2021","title":"Applying and Understanding an Advanced, Novel Deep Learning Approach: A Covid 19, Text Based, Emotions Analysis Study","authors":["J Choudrie, S Patil, K Kotecha, N Matta, I Pappas - Information Systems Frontiers, 2021"],"snippet":"The pandemic COVID 19 has altered individuals' daily lives across the globe. It has led to preventive measures such as physical distancing to be impo.","url":["https://link.springer.com/article/10.1007/s10796-021-10152-6"]}
{"year":"2021","title":"Applying Deep Learning Techniques for Sentiment Analysis to Assess Sustainable Transport","authors":["A Serna Nocedal, A Soroa Echave, R Agerri Gascón - 2021","A Serna, A Soroa, R Agerri - Sustainability, 2021"],"snippet":"… Thus, the multilingual version of BERT [25] was trained for 104 languages. More recently, XLM-RoBERTa [21] distributes a multilingual model which contains 100 languages trained on 2.5 TB of filtered Common Crawl text. To …","url":["https://addi.ehu.eus/bitstream/handle/10810/50497/sustainability-13-02397-v2.pdf?sequence=1&isAllowed=y","https://www.mdpi.com/2071-1050/13/4/2397/pdf"]}
{"year":"2021","title":"Applying Deep Learning Techniques for Sentiment Analysis to Assess Sustainable Transport. Sustainability 2021, 13, 2397","authors":["A Serna, A Soroa, R Agerri - 2021"],"snippet":"… Thus, the multilingual version of BERT [25] was trained for 104 languages. More recently, XLM-RoBERTa [21] distributes a multilingual model which contains 100 languages trained on 2.5 TB of filtered Common Crawl text. To …","url":["https://search.proquest.com/openview/b1ea0637935ea567d5fd68853527c980/1?pq-origsite=gscholar&cbl=2032327"]}
{"year":"2021","title":"AR-LSAT: Investigating Analytical Reasoning of Text","authors":["W Zhong, S Wang, D Tang, Z Xu, D Guo, J Wang, J Yin… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. AR-LSAT: Investigating Analytical Reasoning of Text Wanjun Zhong1∗, Siyuan Wang3∗, Duyu Tang2, Zenan Xu1∗, Daya Guo1∗ Jiahai Wang1, Jian Yin1, Ming Zhou4 and Nan Duan2 1 The School of Data and Computer Science, Sun Yat-sen University …","url":["https://arxiv.org/pdf/2104.06598"]}
{"year":"2021","title":"Arabic Offensive Language Detection in Social Media","authors":["F Husain - 2021"],"snippet":"Page 1. ARABIC OFFENSIVE LANGUAGE DETECTION IN SOCIAL MEDIA by Fatemah Ali Husain A Dissertation Submitted to the Graduate Faculty of George Mason University in Partial Fulfillment of The Requirements for the …","url":["https://search.proquest.com/openview/aefe47a620c621b1c7ed7f95196cf6ba/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2021","title":"AraCOVID19-SSD: Arabic COVID-19 Sentiment and Sarcasm Detection Dataset","authors":["MSH Ameur, H Aliane - arXiv preprint arXiv:2110.01948, 2021"],"snippet":"… Multilingual BERT (mBERT)6: A BERT-based model [17] pretrained on the first 104 major Wikipedia languages7. • XLM-Roberta 8: A large multi-lingual language model, trained on 2.5TB of filtered Common Crawl data [19]. 4.2.2 Bag-of-Words Models …","url":["https://arxiv.org/pdf/2110.01948"]}
{"year":"2021","title":"AraStance: A Multi-Country and Multi-Domain Dataset of Arabic Stance Detection for Fact Checking","authors":["T Alhindi, A Alabdulkarim, A Alshehri, M Abdul-Mageed… - arXiv preprint arXiv …, 2021"],"snippet":"… AraStance and Khoja. This indicates the suitability of the pretraining data of ARBERT that includes Books, Gi- gawords and Common Crawl data primarily from MSA but also a small amount of Egyptian Arabic. Since half of …","url":["https://arxiv.org/pdf/2104.13559"]}
{"year":"2021","title":"ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic","authors":["M Abdul-Mageed, AR Elmadany, EMB Nagoudi - arXiv preprint arXiv:2101.01785, 2020"],"snippet":"… mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) use a small Arabic text collection from Wikipedia (153M tokens) and CommonCrawl (2.9B … XLM-R (Conneau et al., 2020) is trained on Common Crawl data, hence …","url":["https://arxiv.org/pdf/2101.01785"]}
{"year":"2021","title":"Are Multilingual Models Effective in Code-Switching?","authors":["GI Winata, S Cahyawijaya, Z Liu, Z Lin, A Madotto… - arXiv preprint arXiv …, 2021"],"snippet":"… switching tasks. 2.2.2 XLM-RoBERTa XLM-RoBERTa (XLM-R) (Conneau et al., 2020) is a multilingual language model that is pre-trained on 100 languages using more than two terabytes of filtered CommonCrawl data. Thanks to …","url":["https://arxiv.org/pdf/2103.13309"]}
{"year":"2021","title":"Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? A Comprehensive Assessment for Catalan","authors":["J Armengol-Estapé, CP Carrino, C Rodriguez-Penagos… - arXiv preprint arXiv …, 2021"],"snippet":"… Catalan Government; (2) the Catalan Open Subtitles, a collection of translated movie subtitles (Tiedemann, 2012); (3) the non-shuffled version of the Catalan part of the OSCAR corpus (Suárez et al., 2019), a collection …","url":["https://arxiv.org/pdf/2107.07903"]}
{"year":"2021","title":"Are You Really Complaining? A Multi-task Framework for Complaint Identification, Emotion, and Sentiment Classification","authors":["A Singh, S Saha - International Conference on Document Analysis and …, 2021"],"snippet":"… For deep learning baseline (MT\\(_{\\mathrm{GloVe}}\\)) we also used pre-trained GloVe 13 [16] word embedding which is trained on Common Crawl (840 billion tokens) corpus to get the word embedding representations. 4.4 Results and Discussion …","url":["https://link.springer.com/chapter/10.1007/978-3-030-86331-9_46"]}
{"year":"2021","title":"ARMAN: Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization","authors":["A Salemi, E Kebriaei, GN Minaei, A Shakery - arXiv preprint arXiv:2109.04098, 2021"],"snippet":"… This corpus contains around 2.8M articles and 1.4B words in all of the articles. CC100 (Conneau et al., 2020; Wenzek et al., 2020) is a monolingual dataset for 100+ languages constructed from Commoncrawl snapshots. This …","url":["https://arxiv.org/pdf/2109.04098"]}
{"year":"2021","title":"As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical Translation","authors":["J Wang, C Xu, F Guzman, A El-Kishky, BIP Rubinstein… - arXiv preprint arXiv …, 2021"],"snippet":"… Our testing framework facilitates constructing test instances for new domains in the following steps: 1. Obtain a large corpus of text that contains numbers (eg,CommonCrawl); 2. Check if there is a number in the output translation; …","url":["https://arxiv.org/pdf/2107.08357"]}
{"year":"2021","title":"Aspect-based Sentiment Analysis with Graph Convolution over Syntactic Dependencies","authors":["A Zunic, P Corcoran, I Spasic","A Žunić, P Corcoran, I Spasić - Artificial Intelligence in Medicine, 2021"],"snippet":"… into dependency graphs. Individual words representing vertices in such graphs were mapped onto their embeddings, which were pretrained on web data from Common Crawl using the GloVe method [31]. Each input sentence …","url":["https://www.researchgate.net/profile/Irena-Spasic/publication/353775596_Aspect-based_Sentiment_Analysis_with_Graph_Convolution_over_Syntactic_Dependencies/links/61112fae169a1a0103ea3e67/Aspect-based-Sentiment-Analysis-with-Graph-Convolution-over-Syntactic-Dependencies.pdf","https://www.sciencedirect.com/science/article/pii/S0933365721001317"]}
{"year":"2021","title":"ASR4REAL: An extended benchmark for speech models","authors":["M Riviere, J Copet, G Synnaeve - arXiv preprint arXiv:2110.08583, 2021"],"snippet":"… even a language model trained on a dataset as big as Common Crawl does not seem to have significant positive effect which reiterates … For all of theses models we used the a 4-gram LM trained on Common Crawl with the decoding parameters …","url":["https://arxiv.org/pdf/2110.08583"]}
{"year":"2021","title":"Assessing reasoning and world knowledge of large language models using questionized counterfactual conditionals","authors":["J Frohberg, F Binder - 2021"],"snippet":"Page 1. Assessing reasoning and world knowledge of large language models using questionized counterfactual conditionals Jörg Frohberg apergo UG Leipzig, Germany j.frohberg@apergo.ai Frank Binder Institute for Applied …","url":["https://openreview.net/pdf?id=i9XYDrUJYyP"]}
{"year":"2021","title":"Assessing the Extent and Types of Hate Speech in Fringe Communities: A Case Study of Alt-Right Communities on 8chan, 4chan, and Reddit","authors":["D Rieger, AS Kümpel, M Wich, T Kiening, G Groh - Social Media+ Society, 2021"],"snippet":"… For this article, the fastText word vectors pre-trained on the English Common Crawl dataset were used because it is trained on web data and thus an appropriate basis (Mikolov, Grave, Bojanowski, Puhrsch, & Joulin, 2019). …","url":["https://journals.sagepub.com/doi/pdf/10.1177/20563051211052906"]}
{"year":"2021","title":"AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models","authors":["HT Madabushi, E Gow-Smith, C Scarton… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models Harish Tayyar Madabushi, Edward Gow-Smith, Carolina Scarton and Aline Villavicencio Department …","url":["https://arxiv.org/pdf/2109.04413"]}
{"year":"2021","title":"Attention-based model for predicting question relatedness on Stack Overflow","authors":["J Pei, Z Qin, Y Cong, J Guan - arXiv preprint arXiv:2103.10763, 2021"],"snippet":"… released by Stanford [22]. This word embeddings pre-trained in the Common Crawl corpus, which contains a large amount of data irrelevant to software engineering, may lead to ambiguous results [18]. Therefore, we hope that …","url":["https://arxiv.org/pdf/2103.10763"]}
{"year":"2021","title":"Attention: there is an inconsistency between android permissions and application metadata!","authors":["H Alecakir, B Can, S Sen - International Journal of Information Security"],"snippet":"Since mobile applications make our lives easier, there is a large number of mobile applications customized for our needs in the application markets. While.","url":["https://link.springer.com/article/10.1007/s10207-020-00536-1"]}
{"year":"2021","title":"Attentive Excitation and Aggregation for Bilingual Referring Image Segmentation","authors":["Q Zhou, T Hui, R Wang, H Hu, S Liu - ACM Transactions on Intelligent Systems and …, 2021"],"snippet":"… For English expression, we use GloVe1 pretrained on Common Crawl to embed each word into a 300-d vector. For Chinese expression, existing tools … GloVe word embeddings [34] pretrained on Common Crawl 840B …","url":["https://dl.acm.org/doi/abs/10.1145/3446345"]}
{"year":"2021","title":"Augmenting Poetry Composition with Verse by Verse","authors":["D Uthus, M Voitovich, RJ Mical - arXiv preprint arXiv:2103.17205, 2021"],"snippet":"… TextSETTR was shown to yield better results in transforming sentiment while preserving fluency (important aspects for our work). As described in the TextSETTR paper, we use the model that had been fine-tuned on English Common Crawl data …","url":["https://arxiv.org/pdf/2103.17205"]}
{"year":"2021","title":"Augmenting semantic lexicons using word embeddings and transfer learning","authors":["T Alshaabi, C Van Oort, M Fudolig, MV Arnold… - arXiv preprint arXiv …, 2021"],"snippet":"… words. We then pass the token embeddings to a 300dimensional embedding layer. We initialize the embedding layer with weights trained with subword information on Common Crawl and Wikipedia using FastText [59]. In …","url":["https://arxiv.org/pdf/2109.09010"]}
{"year":"2021","title":"AUGVIC: Exploiting BiText Vicinity for Low-Resource NMT","authors":["T Mohiuddin, MS Bari, S Joty - arXiv preprint arXiv:2106.05141, 2021"],"snippet":"… localization guide, respectively. For some languages, the amount of specific domain monolingual data is limited, where we added additional monolingual data of that language from Common Crawl. Following previous work …","url":["https://arxiv.org/pdf/2106.05141"]}
{"year":"2021","title":"Authorship Weightage Algorithm for Academic publications: A new calculation and ACES webserver for determining expertise","authors":["WL Wu, O Tan, KF Chan, NB Ong, D Gunasegaran… - Methods and Protocols, 2021"],"snippet":"… the back-end server. These word vectors were trained on Common Crawl (https://commoncrawl.org (last accessed on 28 April 2021)) using fastText [17], and are used to map the processed query to its corresponding values …","url":["https://www.mdpi.com/2409-9279/4/2/41/pdf"]}
{"year":"2021","title":"Automated Change Detection in Privacy Policies","authors":["A Adhikari - 2020"],"snippet":"Page 1. University of Denver Digital Commons @ DU Electronic Theses and Dissertations Graduate Studies 2020 Automated Change Detection in Privacy Policies Andrick Adhikari Follow this and additional works at: https://digitalcommons.du.edu/etd …","url":["https://digitalcommons.du.edu/cgi/viewcontent.cgi?article=2702&context=etd"]}
{"year":"2021","title":"Automated essay scoring: A review of the field","authors":["P Lagakis, S Demetriadis - … International Conference on Computer, Information and …, 2021"],"snippet":"… Transformer models make use of those huge datasets of existing general text data, such as Wikipedia Corpus and Common Crawl, to pretrain multilayer neural networks with context-sensitive meaning of, and relations between, words, such as …","url":["https://ieeexplore.ieee.org/abstract/document/9618476/"]}
{"year":"2021","title":"Automated Grading of Exam Responses: An Extensive Classification Benchmark","authors":["A Farazouli, Z Lee, P Papapetrou, U Fors - … Science: 24th International Conference, DS 2021 …","J Ljungman, V Lislevand, J Pavlopoulos, A Farazouli… - International Conference on …, 2021"],"snippet":"… This method proves that training BERT with alternative design choices and with more data, including the CommonCrawl News dataset, … training XLM-R on one hundred languages using CommonCrawl data2, in contrast to previous works such …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=IydHEAAAQBAJ&oi=fnd&pg=PA3&dq=commoncrawl&ots=QIe2sENq0_&sig=LQ1NnDlylvNDV4-vNPAiGJEMZd4","https://link.springer.com/chapter/10.1007/978-3-030-88942-5_1"]}
{"year":"2021","title":"Automated identification of bias inducing words in news articles using linguistic and context-oriented features","authors":["T Spinde, L Rudnitckaia, J Mitrović, F Hamborg… - Information Processing & …, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0306457321000157"]}
{"year":"2021","title":"Automated methods for Question-Answering in Icelandic","authors":["V Snæbjarnarson"],"snippet":"… The source of the data is the open internet, made accessible to those with relatively modest computing resources and disk storage through the targeted use of the Common Crawl datasets that comprise petabytes of data. Prior work has focused on the …","url":["https://vesteinn.is/thesis_150921.pdf"]}
{"year":"2021","title":"Automatic Detection of Fake","authors":["BM Bažík"],"snippet":"… For the data, they created the RealNews dataset, a large corpus of news articles from Common Crawl1. Fake News Detection Using Deep Learning Techniques [11] compared Logistic Regression (LR), Naive Bayes (NB) …","url":["https://is.muni.cz/th/hk1px/Martin_Bazik_master_thesis.pdf"]}
{"year":"2021","title":"Automatic Difficulty Classification of Arabic Sentences","authors":["N Khallaf, S Sharoff - arXiv preprint arXiv:2103.04386, 2021"],"snippet":"… corpus (Common Crawl and Wikipedia for ArabicBERT vs Common Crawl XML-R vs Wikipedia for BERT, AraBert and UCS) used to train the Arabic … The corpus will be classified on the ba- sis of how difficult the sentences are …","url":["https://arxiv.org/pdf/2103.04386"]}
{"year":"2021","title":"Automatic Fully-Contextualized Recommendation Extraction from Radiology Reports","authors":["J Steinkamp, C Chambers, D Lalevic, T Cook - Journal of Digital Imaging, 2021"],"snippet":"… We evaluated a simple long short-term memory (LSTM) architecture [12] on the task. We used a combination of custom-trained fastText vectors, trained on our institution's entire repository of radiology reports, with Global …","url":["https://link.springer.com/article/10.1007/s10278-021-00423-8"]}
{"year":"2021","title":"Automatic Generic Web Information Extraction at Scale","authors":["M Aljabary - 2021"],"snippet":"Page 1. 1 Page 2. 2 Automatic Generic Web Information Extraction at Scale Master Thesis Computer Science, Data Science and Technology University of Twente. Enschede, The Netherlands An attempt to bring some structure …","url":["http://essay.utwente.nl/86153/1/Aljabary_MA_EEMCS.pdf"]}
{"year":"2021","title":"Automatic Sexism Detection with Multilingual Transformer Models","authors":["S Mina, B Jaqueline, L Daria, S Djordje, K Armin… - arXiv preprint arXiv …, 2021"],"snippet":"… XLM-R is a multilingual model trained on 100 languages, similar to mBERT. Unlike the latter, XLM-R is not trained on Wikipedia data but on monolingual CommonCrawl data. The model shows improved cross-lingual language …","url":["https://arxiv.org/pdf/2106.04908"]}
{"year":"2021","title":"Automatic Stress Detection from Facial Videos","authors":["EM de Oca - 2021"],"snippet":"… , leading to the development of pretrained systems such as BERT(Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as Wikipedia …","url":["https://www.eduardomontesdeoca.com/s/ADSS-Project.pdf"]}
{"year":"2021","title":"Automatically Detecting Cyberbullying Comments on Online Game Forums","authors":["HHP Vo, HT Tran, ST Luu - arXiv preprint arXiv:2106.01598, 2021"],"snippet":"… on Wikipedia and Gigaword corpora. The fastText 3 is trained on Common Crawl and Wikipedia datasets using CBOW with position-weight in 300 dimensions with 5-grams features. D. Traditional machine learning models Logistic …","url":["https://arxiv.org/pdf/2106.01598"]}
{"year":"2021","title":"Autonomous Writing Futures","authors":["AH Duin, I Pedersen - Writing Futures: Collaborative, Algorithmic …, 2021"],"snippet":"… GPT-3 is 175 billion parameters. GPT-3 is trained on the Common Crawl data set, a corpus of almost a trillion words of texts scraped from the Web. “The dataset and model size are about two orders of magnitude larger than those used for GPT-2,” the authors write …","url":["https://link.springer.com/chapter/10.1007/978-3-030-70928-0_4"]}
{"year":"2021","title":"Auxiliary Bi-Level Graph Representation for Cross-Modal Image-Text Retrieval","authors":["X Zhong, Z Yang, M Ye, W Huang, J Yuan, CW Lin - 2021 IEEE International …, 2021"],"snippet":"… The scene graph features Soi and Srij are transformed by a learnable embedding layer which is initialized by GloVe [18] pre-trained on the Common-Crawl dataset, and maps Ioi and Irij into a vector of same dimension: Soi = WoIoi , Srij = WrIrij , (1) …","url":["https://ieeexplore.ieee.org/abstract/document/9428380/"]}
{"year":"2021","title":"Auxiliary Learning for Relation Extraction","authors":["S Lyu, J Cheng, X Wu, L Cui, H Chen, C Miao - IEEE Transactions on Emerging …, 2020"],"snippet":"… 7https://catalog.ldc.upenn.edu/LDC2018T24 8http://semeval2.fbk.eu/semeval2. php?location=data 9Following previous work, we choose GloVe word vectors with 300 dimensions (Common Crawl) https://nlp.stanford.edu/projects/glove …","url":["https://ieeexplore.ieee.org/abstract/document/9296307/"]}
{"year":"2021","title":"Background Knowledge in Schema Matching: Strategy vs. Data","authors":["J Portisch, M Hladik, H Paulheim - arXiv preprint arXiv:2107.00001, 2021"],"snippet":"… used. WebIsALOD is a large hypernymy graph based on the WebIsA database [37]. The latter is a dataset which consists of hypernymy relations extracted from the Common Crawl, a large set of crawled Web pages. The extraction …","url":["https://arxiv.org/pdf/2107.00001"]}
{"year":"2021","title":"Bambara Language Dataset for Sentiment Analysis","authors":["M Diallo, C Fourati, H Haddad - arXiv preprint arXiv:2108.02524, 2021"],"snippet":"… In this paper, we present the first common-crawl-based Bambara dialectal dataset dedicated for Sentiment Analysis, available freely for Natural Language Processing research purposes … Bambara V1 dataset represents …","url":["https://arxiv.org/pdf/2108.02524"]}
{"year":"2021","title":"Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits","authors":["J Kreutzer, D Vilar, A Sokolov - arXiv preprint arXiv:2110.06997, 2021"],"snippet":"Training data for machine translation (MT) is often sourced from a multitude of large corpora that are multi-faceted in nature, eg containing contents from multiple domains or different levels of quality or complexity. Naturally, these facets do not …","url":["https://arxiv.org/pdf/2110.06997"]}
{"year":"2021","title":"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese","authors":["NL Tran, DM Le, DQ Nguyen - arXiv preprint arXiv:2109.09701, 2021"],"snippet":"… rization. Here, mBART is pre-trained on a Common Crawl dataset of 25 languages, which contains 137 GB of syllablelevel Vietnamese texts. We employ the single-document summarization dataset VNDS (Nguyen et al. 2019 …","url":["https://arxiv.org/pdf/2109.09701"]}
{"year":"2021","title":"belabBERT: a Dutch RoBERTa-based language model applied to psychiatric classification","authors":["J Wouts, J de Boer, A Voppel, S Brederoo… - arXiv preprint arXiv …, 2021"],"snippet":"… 3.1.1. Pre-training For the pre-training of belabBERT we used the OSCAR corpus which consists of a set of monolingual corpora extracted from Common Crawl snapshots … belabBERT Common Crawl Dutch (non-shuffled) BytePairEncoding 95.92 ∗ …","url":["https://arxiv.org/pdf/2106.01091"]}
{"year":"2021","title":"Benchmarking Differential Privacy and Federated Learning for BERT Models","authors":["P Basu, TS Roy, R Naidu, Z Muftuoglu, S Singh… - arXiv preprint arXiv …, 2021"],"snippet":"… It uses 160 GB of text for pre-training, including 16GB of Books Corpus and English Wikipedia used in BERT. The additional data included CommonCrawl News dataset, Web text corpus and Stories from Common Crawl. For …","url":["https://arxiv.org/pdf/2106.13973"]}
{"year":"2021","title":"BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation","authors":["H Xu, B Van Durme, K Murray - arXiv preprint arXiv:2109.04588, 2021"],"snippet":"… on 145G German text data portion of OSCAR (Or- tiz Suárez et al., 2020), a huge multilingual corpus extracted from Common Crawl … Vaswani et al., 2017) masked language model trained on 100 languages, using more than …","url":["https://arxiv.org/pdf/2109.04588"]}
{"year":"2021","title":"BERT: A Review of Applications in Natural Language Processing and Understanding","authors":["MV Koroteev - arXiv preprint arXiv:2103.11943, 2021"],"snippet":"Page 1. BERT: A Review of Applications in Natural Language Processing and Understanding Koroteev MV, Financial University under the government of the Russian Federation, Moscow, Russia mvkoroteev@fa.ru Abstract: In …","url":["https://arxiv.org/pdf/2103.11943"]}
{"year":"2021","title":"Bertinho: Galician BERT Representations","authors":["D Vilares, M Garcia, C Gómez-Rodríguez - arXiv preprint arXiv:2103.13799, 2021"],"snippet":"Page 1. Bertinho: Galician BERT Representations Bertinho: Representaciones BERT para el gallego David Vilares,1 Marcos Garcia,2 Carlos Gómez-Rodr´ıguez 1 1Universidade da Coru˜na, CITIC, Galicia, Spain 2CiTIUS, Universidade …","url":["https://arxiv.org/pdf/2103.13799"]}
{"year":"2021","title":"Better Neural Machine Translation by Extracting Linguistic Information from BERT","authors":["HS Shavarani, A Sarkar - arXiv preprint arXiv:2104.02831, 2021"],"snippet":"… clack, clack.”). 9Europarl+CommonCrawl+NewsCommentary https://www.statmt. org/wmt14/translation-task.html, please note that in the later years this training set remained the same, but ParaCrawl data was added to it. We …","url":["https://arxiv.org/pdf/2104.02831"]}
{"year":"2021","title":"Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation","authors":["E Briakou, M Carpuat - arXiv preprint arXiv:2105.15087, 2021"],"snippet":"Page 1. Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation Eleftheria Briakou and Marine Carpuat Department of Computer Science University of Maryland College Park …","url":["https://arxiv.org/pdf/2105.15087"]}
{"year":"2021","title":"Beyond the English Web: Zero-Shot Cross-Lingual and Lightweight Monolingual Classification of Registers","authors":["L Repo, V Skantsi, S Rönnqvist, S Hellström… - arXiv preprint arXiv …, 2021"],"snippet":"… FreCORE and SweCORE are random samples of the 2017 CoNLL datasets (Ginter et al., 2017) originally drawn from Common Crawl … XLM-R is trained on 2.5TB of filtered Common Crawl (Wenzek et al., 2020) data comprising …","url":["https://arxiv.org/pdf/2102.07396"]}
{"year":"2021","title":"Bias Silhouette Analysis: Towards Assessing the Quality of Bias Metrics for Word Embedding Models","authors":["M Spliethöver, H Wachsmuth"],"snippet":"… Word Embedding Models. As biased and unbiased models, we use GloVe CommonCrawl [Pennington et al., 2014] trained on 840 billion English tokens and the English ConceptNet Numberbatch 19.08 [Speer et al., 2017] (referred to as NBatch below), respectively …","url":["https://www.ijcai.org/proceedings/2021/0077.pdf"]}
{"year":"2021","title":"Bidirectional Language Modeling: A Systematic Literature Review","authors":["M Shah Jahan, HU Khan, S Akbar, M Umar Farooq… - Scientific Programming, 2021"],"snippet":"Page 1. Review Article Bidirectional Language Modeling: A Systematic Literature Review Muhammad Shah Jahan ,1 Habib Ullah Khan ,2 Shahzad Akbar ,3 Muhammad Umar Farooq ,1 Sarah Gul ,4 and Anam Amjad 1 1Department …","url":["https://www.hindawi.com/journals/sp/2021/6641832/"]}
{"year":"2021","title":"Bilingual Lexical Induction for Sinhala-English using Cross Lingual Embedding Spaces","authors":["A Liyanage, S Ranathunga, S Jayasena - 2021 Moratuwa Engineering Research …, 2021"],"snippet":"… Using pre-trained fastText embeddings trained on Wikipedia and Common crawl data using two different evaluation dictionaries as a preliminary experiment to identify the performance of embeddings created from non-comparable corpora …","url":["https://ieeexplore.ieee.org/abstract/document/9525667/"]}
{"year":"2021","title":"Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment","authors":["H Shi, L Zettlemoyer, SI Wang - arXiv preprint arXiv:2101.00148"],"snippet":"Page 1. Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment Haoyue Shi ∗ TTI-Chicago freda@ttic.edu Luke Zettlemoyer University of Washington Facebook AI Research lsz@fb.com …","url":["https://arxiv.org/pdf/2101.00148"]}
{"year":"2021","title":"BitextEdit: Automatic Bitext Editing for Improved Low-Resource Machine Translation","authors":["E Briakou, SI Wang, L Zettlemoyer, M Ghazvininejad - arXiv preprint arXiv …, 2021"],"snippet":"Mined bitexts can contain imperfect translations that yield unreliable training signals for Neural Machine Translation (NMT). While filtering such pairs out is known to improve final model quality, we argue that it is suboptimal in low-resource conditions …","url":["https://arxiv.org/pdf/2111.06787"]}
{"year":"2021","title":"Blank spots, critical information needs and local journalism fund-ing","authors":["S Bisiani"],"snippet":"Abstract A global business model crisis in journalism, fuelled by loss in advertising revenue, challenges the survival of local news production. In Sweden, it has led to the closure of several newspapers across the country, and the concentration of …","url":["http://compscjournalism.org/projects/simona/projects/Master_Thesis_Simona_Bisiani.pdf"]}
{"year":"2021","title":"Book genre and author's gender recognition based on titles","authors":["A Pawłowski, E Herden, T Walkowiak - … and Text: Data, models, information and …, 2021"],"snippet":""}
{"year":"2021","title":"BOSS: Bandwidth-Optimized Search Accelerator for Storage-Class Memory","authors":["J Heo, SY Lee, S Min, Y Park, SJ Jung, TJ Ham…"],"snippet":"Page 1. BOSS: Bandwidth-Optimized Search Accelerator for Storage-Class Memory Jun Heo, Seung Yul Lee, Sunhong Min, Yeonhong Park, Sung Jun Jung, Tae Jun Ham, Jae W. Lee Seoul National University {j.heo, triomphant1 …","url":["https://conferences.computer.org/iscapub/pdfs/ISCA2021-4ghucdBnCWYB7ES2Pe4YdT/333300a279/333300a279.pdf"]}
{"year":"2021","title":"Bottom-Up Shift and Reasoning for Referring Image Segmentation","authors":["S Yang, M Xia, G Li, HY Zhou, Y Yu - Proceedings of the IEEE/CVF Conference on …, 2021"],"snippet":"Page 1. Bottom-Up Shift and Reasoning for Referring Image Segmentation Sibei Yang1∗† Meng Xia2∗ Guanbin Li2 Hong-Yu Zhou3 Yizhou Yu3,4† 1ShanghaiTech University 2Sun Yat-sen University 3The University of Hong Kong 4Deepwise AI Lab Abstract …","url":["https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Bottom-Up_Shift_and_Reasoning_for_Referring_Image_Segmentation_CVPR_2021_paper.pdf"]}
{"year":"2021","title":"bradleypallen/keras-quora-question-pairs","authors":["ONDJF Mar, AMJJA Sep, OSMTW Thu, F Sat"],"snippet":"… Model, Source of Word Embeddings, Accuracy. \"BiMPM model\" [5], GloVe Common Crawl (840B tokens, 300D), 0.88 … \"Decomposable attention\" [6], \"Quora's text corpus\", 0.86. \"LDC\" [5], GloVe Common Crawl (840B tokens, 300D), 0.86 …","url":["https://giters.com/bradleypallen/keras-quora-question-pairs?amp=1"]}
{"year":"2021","title":"Building a File Observatory for Secure Parser Development","authors":["T Allison, W Burke, C Mattmann, A Mensikova…"],"snippet":"… 3196–3200. [Online]. Available: http://www.lrec-conf.org/proceedings/lrec2012/pdf/534 Paper.pdf [10] “Common Crawl,” https://commoncrawl.org. [11] P. Wyatt, “Stressful PDF corpus grows!” https://www.pdfa.org/ stressful-pdf-corpus-grows/, November 2020.","url":["https://langsec.org/spw21/papers/Allison_LangSec21.pdf"]}
{"year":"2021","title":"Building a Question and Answer System for News Domain","authors":["S Basu, A Gaddala, P Chetan, G Tiwari, N Darapaneni… - arXiv preprint arXiv …, 2021"],"snippet":"… We have used two approaches for building the Embedding Layers for the models 3. GloVe Embedding: we used the 300 Dimension Common Crawl for the English language 4. Universal Sentence Encoder: we used the 512 …","url":["https://arxiv.org/pdf/2105.05744"]}
{"year":"2021","title":"Building Accountable Natural Language Processing Models: on Social Bias Detection and Mitigation","authors":["J Zhao - 2021"],"snippet":"Natural Language Processing (NLP) plays an important role in many applications, including resume filtering, text analysis, and information retrieval. Despite the remarkable accuracy enabled by the advances of machine learning methods, recent …","url":["https://escholarship.org/content/qt0441n1tt/qt0441n1tt.pdf"]}
{"year":"2021","title":"But how robust is RoBERTa actually?: A Benchmark of SOTA Transformer Networks for Sexual Harassment Detection on Twitter","authors":["P Basu, TS Roy, A Singhal - 2021 Fifth International Conference on I-SMAC (IoT in …, 2021"],"snippet":"Harassment, which is of sexual/physical in nature, is defined as any unwanted sexual misconduct, including the unwarranted and ill-suited promise of benefit in exchange for sexual indulgence. It also includes a span of actions from verbal …","url":["https://ieeexplore.ieee.org/abstract/document/9640861/"]}
{"year":"2021","title":"Can I Take Your Subdomain? Exploring Same-Site Attacks in the Modern Web","authors":["MSMTL Veronese, SCM Maffei"],"snippet":"Page 1. Can I Take Your Subdomain? Exploring Same-Site Attacks in the Modern Web Marco Squarcina1 Mauro Tempesta1 Lorenzo Veronese1 Stefano Calzavara2 Matteo Maffei1 1 TU Wien 2 Università Ca' Foscari Venezia & OWASP …","url":["https://minimalblue.com/data/papers/USENIX21_can_i_take_your_subdomain.pdf"]}
{"year":"2021","title":"Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color","authors":["M Abdou, A Kulmizev, D Hershcovich, S Frank… - arXiv preprint arXiv …, 2021"],"snippet":"… Word-type FastText embeddings trained on Common Crawl (Bojanowski et al., 2017) … These S contexts are either randomly sampled from common crawl (RC), or deterministically generated to allow for control over contextual variation (CC) …","url":["https://arxiv.org/pdf/2109.06129"]}
{"year":"2021","title":"Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches","authors":["NF Liu, T Lee, R Jia, P Liang - arXiv preprint arXiv:2102.01065, 2021"],"snippet":"Page 1. Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches Nelson F. Liu Tony Lee Robin Jia Percy Liang Computer Science Department, Stanford …","url":["https://arxiv.org/pdf/2102.01065"]}
{"year":"2021","title":"CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision","authors":["Z Li, X Ding, K Liao, T Liu, B Qin - arXiv preprint arXiv:2107.09852, 2021"],"snippet":"… ambiguity and precise causal patterns to extract word level causeeffect pairs from the preprocessed English Common Crawl corpus (5.14 … (2016) for creating a causal lexical knowledge base, we reproduce a variant of their …","url":["https://arxiv.org/pdf/2107.09852"]}
{"year":"2021","title":"CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training","authors":["P Huber, A Aghajanyan, B Oğuz, D Okhonko, W Yih… - arXiv preprint arXiv …, 2021"],"snippet":"… Consequently, we propose a novel QA dataset based on the Common Crawl project in this paper. Using the readily available schema.org annotation, we extract around 130 million multilingual question-answer pairs, including about 60 million …","url":["https://arxiv.org/pdf/2110.07731"]}
{"year":"2021","title":"CDA: a Cost Efficient Content-based Multilingual Web Document Aligner","authors":["T Vu, AA AI, A Moschitti - 2021"],"snippet":"… CommonCrawl Sextet Previous datasets share the same domains that are heavily biased toward French content (see Table 3). We leverage a monthly crawl from CommonCrawl, specifically … Table 4: Parallel English tokens …","url":["https://assets.amazon.science/01/69/5f786b844c08a079eda7e6437c16/cda-a-cost-efficient-content-based-multilingual-web-document-aligner.pdf"]}
{"year":"2021","title":"Censorship of Online Encyclopedias: Implications for NLP Models","authors":["E Yang, ME Roberts - arXiv preprint arXiv:2101.09294, 2021"],"snippet":"… Word embeddings are also useful because they can be pre-trained on large corpuses of text like Wikipedia or Common Crawl, and these pre-trained embeddings can then be used as an initial layer in applications that may have less training data …","url":["https://arxiv.org/pdf/2101.09294"]}
{"year":"2021","title":"Challenges for cognitive decoding using deep learning methods","authors":["AW Thomas, C Ré, RA Poldrack - arXiv preprint arXiv:2108.06896, 2021"],"snippet":"… learning in 251 the target domain. Transfer learning has been especially successful in CV and NLP, where large 252 publicly available datasets exist (eg, [72,73] and http://www.commoncrawl.org). Here, DL 253 models are first …","url":["https://arxiv.org/pdf/2108.06896"]}
{"year":"2021","title":"Changing the World by Changing the Data","authors":["A Rogers - arXiv preprint arXiv:2105.13947, 2021"],"snippet":"… The use of uncontrolled samples (like the Common-Crawl-based corpora) would have to be justified by arguing either that the above types of bias can be safely ignored, or that the benefits outweigh the risks. 2.2.3 Might not be the best approach …","url":["https://arxiv.org/pdf/2105.13947"]}
{"year":"2021","title":"Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling","authors":["I Kulikov, M Eremeev, K Cho - arXiv preprint arXiv:2112.08914, 2021"],"snippet":"… We use the subset of WMT’19 training set consisting of news commentary v12 and common crawl resulting in slightly more than 1M and 2M training sentence pairs for Ru→En and De↔En pairs, respectively. We fine-tuned single model checkpoints …","url":["https://arxiv.org/pdf/2112.08914"]}
{"year":"2021","title":"Characterizing Network Infrastructure Using the Domain Name System","authors":["P Kintis - 2020"],"snippet":"Page 1. CHARACTERIZING NETWORK INFRASTRUCTURE USING THE DOMAIN NAME SYSTEM A Dissertation Presented to The Academic Faculty By Panagiotis Kintis In Partial Fulfillment of the Requirements for the Degree …","url":["https://smartech.gatech.edu/bitstream/handle/1853/64165/KINTIS-DISSERTATION-2020.pdf"]}
{"year":"2021","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization","authors":["Y Tay, VQ Tran, S Ruder, J Gupta, HW Chung, D Bahri… - arXiv preprint arXiv …, 2021"],"snippet":"… In addition, we compare to the byte-level models from §3.1, which we pre-train on multilingual data. Setup We pre-train CHARFORMER as well as the Byte-level T5 and Byte-level T5+LASC baselines on multilingual …","url":["https://arxiv.org/pdf/2106.12672"]}
{"year":"2021","title":"ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information","authors":["Z Sun, X Li, X Sun, Y Meng, X Ao, Q He, F Wu, J Li - arXiv preprint arXiv:2106.16038, 2021"],"snippet":"… We collected our pretraining data from CommonCrawl5. After pre-processing (such as removing the data with too much … ERNIE BERT-wwm MacBERT ChineseBERT Data Source Heterogeneous Wikipedia Heterogeneous …","url":["https://arxiv.org/pdf/2106.16038"]}
{"year":"2021","title":"Claim Detection in Biomedical Twitter Posts","authors":["A Wührl, R Klinger - arXiv preprint arXiv:2104.11639, 2021"],"snippet":"… The first three models (NB, LG, BiLSTM) use 50-dimensional FastText (Bojanowski et al., 2017) embeddings trained on the Common Crawl corpus (600 billion tokens) as input6. NB. We use a (Gaussian) naive Bayes with …","url":["https://arxiv.org/pdf/2104.11639"]}
{"year":"2021","title":"Classification of Emotions Based on Text and Qualitative Variables","authors":["J Dobša, D Šebalj, D Bužić"],"snippet":"… Experiments were done both with Common Crawl GloVe pretrained vectors with the dimensionality of 300, and without pretrained vectors. Fifteen percent of learning samples were used for validation. We constructed six neural networks models: CNN …","url":["https://www.researchgate.net/profile/Jasminka-Dobsa/publication/355461190_Classification_of_Emotions_Based_on_Text_and_Qualitative_Variables/links/61716c97750da711ac647d77/Classification-of-Emotions-Based-on-Text-and-Qualitative-Variables.pdf"]}
{"year":"2021","title":"Classification of Horror Stories from Reddit","authors":["D Zhou, C Kim, S Gatiganti"],"snippet":"… We further hypothesize that performance would still increase a little if we used the larger pre-trained vectors such as Common Crawl or Twitter sets, but they come with increased download sizes (>1 GB) and increased training time …","url":["http://cs229.stanford.edu/proj2021spr/report2/82008167.pdf"]}
{"year":"2021","title":"Classification of Texts Using a Vocabulary of Synonyms","authors":["A Giliazova - 2021 14th International Conference Management of …, 2021"],"snippet":"… This is a Transformer-based masked language model trained on one hundred languages, including Russian language, using more than two terabytes of filtered CommonCrawl data. The XLM-R model significantly outperforms multilingual BERT (mBERT) …","url":["https://ieeexplore.ieee.org/abstract/document/9600131/"]}
{"year":"2021","title":"CLASSIFICATION OF TWEETS USING MULTIPLE THRESHOLDS WITH SELF-CORRECTION AND WEIGHTED CONDITIONAL","authors":["TN Ahmad - 2020"],"snippet":"Page 1. CLASSIFICATION OF TWEETS USING MULTIPLE THRESHOLDS WITH SELF-CORRECTION AND WEIGHTED CONDITIONAL PROBABILITIES A thesis submitted to The University of Manchester for the degree of Doctor of Philosophy …","url":["https://www.research.manchester.ac.uk/portal/files/188959099/FULL_TEXT.PDF"]}
{"year":"2021","title":"Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications","authors":["S Sun, A El-Kishky, V Chaudhary, J Cross, F Guzmán… - arXiv preprint arXiv …, 2021"],"snippet":"… Current state of the art QE systems (Fomicheva et al., 2020b; Ranasinghe et al., 2020a; Sun et al., 2020). are built on XLM-R (Conneau et al., 2019), a contextualized language model pre-trained on more than 2 terabytes of …","url":["https://arxiv.org/pdf/2109.08627"]}
{"year":"2021","title":"Classifying Fake and Real Neurally Generated News","authors":["A Govindaraju, J Griffith - 2021 Swedish Workshop on Data Science (SweDS), 2021"],"snippet":"… In order to train and test the model, 3 datasets have been created: One containing real news extracted from a common crawl; the second comprises a neural fake news dataset generated using language modelling techniques; the third comprises a …","url":["https://ieeexplore.ieee.org/abstract/document/9638268/"]}
{"year":"2021","title":"CLEF eHealth Evaluation Lab 2021","authors":["L Kelly, LA Alemany, N Brew-Sam, V Cotik, D Filippo…"],"snippet":"… This collection consists of Web pages acquired from Common Crawl,14 which is augmented with additional pages collected from a number of known reliable health Websites and other known unreliable health Websites [9]. The topics …","url":["https://www.researchgate.net/profile/Marco-Viviani/publication/350569762_CLEF_eHealth_Evaluation_Lab_2021/links/6073f32e92851c8a7bbea835/CLEF-eHealth-Evaluation-Lab-2021.pdf"]}
{"year":"2021","title":"Click This, Not That: Extending Web Authentication with Deception","authors":["T Barron, J So, N Nikiforakis - Proceedings of the 2021 ACM Asia Conference on …, 2021"],"snippet":"… after creation. References. 2020. Common Crawl. https://commoncrawl.org/the-data/ get-started/Google Scholar Google Scholar; 2020. Mouseflow: Session Replay, Heatmaps, Funnels, Forms & User Feedback. https://mouseflow …","url":["https://dl.acm.org/doi/abs/10.1145/3433210.3453088"]}
{"year":"2021","title":"ClimateBert: A Pretrained Language Model for Climate-Related Text","authors":["N Webersinke, M Kraus, JA Bingler, M Leippold - arXiv preprint arXiv:2110.12010, 2021"],"snippet":"… 2019), and a subset of CommonCrawl that is said to resemble the storylike style of WINOGRAD schemas (Trinh and Le, 2019). While these sources are valuable to build a model working on general language, it has been shown that domain-specific …","url":["https://arxiv.org/pdf/2110.12010"]}
{"year":"2021","title":"CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions","authors":["R Abdal, P Zhu, J Femiani, NJ Mitra, P Wonka - arXiv preprint arXiv:2112.05219, 2021"],"snippet":"… The CLIP image encoder [34] is trained on the common-crawl dataset, an internet-scale set of images that encompasses a broad range of visual concepts. However, a typical high-quality GAN would be trained on a more specific set of images, for example …","url":["https://arxiv.org/pdf/2112.05219"]}
{"year":"2021","title":"Cluster analysis of agricultural household production of self-employed","authors":["AV Plotnikov - IOP Conference Series: Earth and Environmental …, 2021"],"snippet":"… To train this model, we used a sample of Russian-language documents from the CommonCrawl dump, balanced by geography, compiled by Jonathan Dunn and Ben Adams; the corpus Size is 2.1 billion words. Page 5. AGRITECH-IV-2020 IOP Conf …","url":["https://iopscience.iop.org/article/10.1088/1755-1315/677/2/022080/pdf"]}
{"year":"2021","title":"Cluster-Based Antiphishing (CAP) Model for Smart Phones","authors":["M Faisal, S Abed - Scientific Programming"],"snippet":"… latest techniques tested on UCI datasets. 4.4.2. Dataset Taken from Mendeley. Source Phishing web page: Phish Tank, Legitimate web page source: Alexa, Common Crawl (1) Dataset Information. In this scenario, the dataset …","url":["https://www.hindawi.com/journals/sp/2021/9957323/"]}
{"year":"2021","title":"Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots","authors":["S Tan, S Joty - arXiv preprint arXiv:2103.09593, 2021"],"snippet":"… However, the latter trend is replicated for BUMBLEBEE if we remove this constraint (Table 14 in Appendix G). A possible explanation is that XLM-R and Unicoder were trained on monolingual CommonCrawl (CC) data, while …","url":["https://arxiv.org/pdf/2103.09593"]}
{"year":"2021","title":"CoDesc: A Large Code–Description Parallel Dataset","authors":["M Hasan, T Muttaqueen, A Al Ishtiaq, KS Mehrab…"],"snippet":"… 9052–9065, Online. Association for Computational Linguistics. CommonCrawl Common crawl. https:// commoncrawl.org/. Accessed: 2021-01-31. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019 …","url":["http://masumhasan.net/files/CoDesc.pdf"]}
{"year":"2021","title":"Combining Natural Language Processing and Machine Learning for Profiling and Fake News Detection","authors":["A Bondielli"],"snippet":"Page 1. PHD PROGRAM IN SMART COMPUTING DIPARTIMENTO DI INGEGNERIA DELL'INFORMAZIONE (DINFO) Combining Natural Language Processing and Machine Learning for Profiling and Fake News Detection Alessandro Bondielli …","url":["https://flore.unifi.it/bitstream/2158/1244287/1/PhDThesis_AlessandroBondielli.pdf"]}
{"year":"2021","title":"Combining Pre-trained Word Embeddings and Linguistic Features for Sequential Metaphor Identification","authors":["R Mao, C Lin, F Guerin - arXiv preprint arXiv:2104.03285, 2021"],"snippet":"… For instance, GloVe was trained on Common Crawl2, from billions of web pages (840 billion tokens); ELMo was trained on WMT 2011 News Crawl data3 (800 million tokens); BERT was trained on Wikipedia4 (2.5 billion tokens) …","url":["https://arxiv.org/pdf/2104.03285"]}
{"year":"2021","title":"Combining word embeddings as a tool for subject identification","authors":["A Hamm - Wissensaustauschworkshop\" Maschinelles Lernen VII\", 2021"],"snippet":"This talk shows ungoing work aiming at finding subject matter relations between text documents and word clouds. A number of increasingly successful semantic word embedding procedures - learning semantic relations from contextual distributions …","url":["https://elib.dlr.de/147623/1/Combining%20word%20embeddings.pdf"]}
{"year":"2021","title":"Comparative Analysis of Bengali Stop Word Detection Using Different Approaches","authors":["RJ Rupa, JF Sohana, M Rahman - … on Automation, Control and Mechatronics for …, 2021"],"snippet":"… In this paper, the pretrained Bengali FastText CBOW model is utilized to produce word vectors trained on the common crawl and Wikipedia [27] and both logistic regression and support vector machine classifiers acquire a performance score of 86%. TABLE XII …","url":["https://ieeexplore.ieee.org/abstract/document/9528279/"]}
{"year":"2021","title":"Comparative Analysis of Different Transformer Based Architectures Used in Sentiment Analysis","authors":["K Pipalia, R Bhadja, M Shukla - 2020 9th International Conference System Modeling …, 2020"],"snippet":"… Distill BERT Base:66 BookCorpus wiki BERT Distillation T5 Base:220 large:770 Colossal Clean Crawled Corpus (C4) Text Infilling XLNet Base:~110 Large:~340 BookCorpus Wiki, Giga5 ClueWeb, Common Crawl …","url":["https://ieeexplore.ieee.org/abstract/document/9337081/"]}
{"year":"2021","title":"Comparing Apples and Oranges: Human and Computer Clustered Affinity Diagrams Under the Microscope","authors":["P Borlinghaus, S Huber - 26th International Conference on Intelligent User …, 2021"],"snippet":"… training corpora WSD no OOV LSI [9] − NMF [18] − LDA [5] − GloVe [26] Wiki word2vec [22] Google News corpus doc2vec [17] 900k sentences from qualitative survey ◦ fastText [6] Common Crawl, Wiki • … FastText was trained on Common Crawl and Wikipedia corpus …","url":["https://dl.acm.org/doi/abs/10.1145/3397481.3450674"]}
{"year":"2021","title":"Comparing Contextualised Embeddings for Predicting the (Graded) Effect of Context in Word Similarity","authors":["JM Albers - 2021"],"snippet":"… As data set XLM-RoBERTa uses CommonCrawl instead of Wikipedia, which provides limited scale for low resource languages. 4 Page 5 … The CommonCrawl data set is designed to be more diverse than other data sets, which mainly use Wikipedia and books …","url":["https://dspace.library.uu.nl/bitstream/handle/1874/406113/6400507_JorisAlbers_Thesis.pdf?sequence=1"]}
{"year":"2021","title":"Comparing Encoder-Decoder Architectures for Neural Machine Translation: A Challenge Set Approach","authors":["C Doan - 2021"],"snippet":"Machine translation (MT) as a field of research has known significant advances in recent years, with the increased interest for neural machine translation (NMT). By combining deep learning with translation, researchers have been able to deliver …","url":["https://ruor.uottawa.ca/bitstream/10393/42936/1/Doan_Coraline_2021_thesis.pdf"]}
{"year":"2021","title":"Comparing general and specialized word embeddings for biomedical named entity recognition","authors":["RE Ramos-Vargas, I Román-Godínez, S Torres-Ramos - PeerJ Computer Science, 2021"],"snippet":"… 01-14 Received 2020-11-05 Academic Editor Susan Gauch Subject Areas Bioinformatics, Artificial Intelligence, Computational Linguistics Keywords Word embeddings, BioNER, BiLSTM-CRF, DrugBank, MedLine, Pyysalo …","url":["https://peerj.com/articles/cs-384/"]}
{"year":"2021","title":"Comparing the Performance of NLP Toolkits and Evaluation measures in Legal Tech","authors":["MZ Khan, J Mitrovic, JMPDM Granitzer - 2021"],"snippet":"Page 1. Lehrstuhl für Data Science Comparing the Performance of NLP Toolkits and Evaluation measures in Legal Tech Masterarbeit von Muhammad Zohaib Khan Supervised By: Prof. Dr. Jelena Mitrovic 1. Prüfer 2. Prüfer …","url":["https://www.academia.edu/download/65887417/Deep_Neural_Language_Modelling_in_Law.pdf"]}
{"year":"2021","title":"Comparing Traditional and Neural Approaches for Detecting Health-Related Misinformation","authors":["D Elsweiler - … IR Meets Multilinguality, Multimodality, and Interaction …","M Fernández-Pichel, DE Losada, JC Pichel…"],"snippet":"… Table 1 reports the main statistics of the resulting datasets. We also tested classifiers for the task of distinguishing between useful documents for non-expert end users (ie, trustworthy and readable) and non-useful …","url":["http://persoal.citius.usc.es/jcpichel/docs/2021_CLEF_MFernandezPichel.pdf","https://books.google.de/books?hl=en&lr=lang_en&id=p9FCEAAAQBAJ&oi=fnd&pg=PA78&dq=commoncrawl&ots=eNycpv3vEv&sig=v7CAPFEmV26pL2Lhj2R2t581gZ0"]}
{"year":"2021","title":"Comparison of Czech Transformers on Text Classification Tasks","authors":["J Lehečka, J Švec - arXiv preprint arXiv:2107.10042, 2021"],"snippet":"… Researchers from Facebook have published multilingual XLM-RoBERTa model [3] pre-trained on one hundred languages (including Czech), using more than two terabytes of filtered Common Crawl data … 2. 1https …","url":["https://arxiv.org/pdf/2107.10042"]}
{"year":"2021","title":"Compilation and Validation of a Large Fake News Dataset in Hungarian","authors":["M Gencsi, Z Bodó, A Szenkovits - 2021 IEEE 19th International Symposium on …, 2021"],"snippet":"… The huBERT model was trained on the Hungarian subset of the Common Crawl and a snapshot of the Hungarian Wikipedia, while the multilingual model was trained on the top 104 languages with the largest Wikipedias, among them also …","url":["https://ieeexplore.ieee.org/abstract/document/9582484/"]}
{"year":"2021","title":"Comprehensive analysis of embeddings and pre-training in NLP","authors":["JK Tripathy, SC Sethuraman, MV Cruz, A Namburu… - Computer Science Review, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S1574013721000733"]}
{"year":"2021","title":"Comprehensive Evaluation of Word Embeddings for Highly Inflectional Language","authors":["P Drozda, K Sopyla, J Lewalski - International Conference on Computational …, 2021"],"snippet":"… The obtained results showed that in terms of accuracy the Facebook fasttext model learned on the Common Crawl collection should be considered the best model under assumptions of experimental session. Keywords. Word …","url":["https://link.springer.com/chapter/10.1007/978-3-030-88113-9_48"]}
{"year":"2021","title":"Comprehensive Multi-Modal Interactions for Referring Image Segmentation","authors":["K Jain, V Gandhi - arXiv preprint arXiv:2104.10412, 2021"],"snippet":"… 576. At 448 × 448 resolution, H = W = 14 and at 576 × 576 resolution, H = W = 18. We use GLoVe embeddings [17] pre-trained on Common Crawl 840B tokens to initialize word embedding for words in the expressions. The …","url":["https://arxiv.org/pdf/2104.10412"]}
{"year":"2021","title":"Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation","authors":["M Deng, B Tan, Z Liu, EP Xing, Z Hu - arXiv preprint arXiv:2109.06379, 2021"],"snippet":"Page 1. Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation Mingkai Deng1∗, Bowen Tan1∗, Zhengzhong Liu1,2, Eric P. Xing1,2,3, Zhiting Hu4 1Carnegie Mellon University …","url":["https://arxiv.org/pdf/2109.06379"]}
{"year":"2021","title":"Computational analysis and synthesis of song lyrics","authors":["P Březinová - 2021"],"snippet":"… It uses CMUdict for phonetic transcription, analyzes CommonCrawl8 web data repository for forced rhymes, Google Books Ngrams (Weiss [2015]) for building language model, and WordNet 3.0 (Pearson et al. [2005]) for semantic relations …","url":["https://dspace.cuni.cz/bitstream/handle/20.500.11956/147665/120397406.pdf?sequence=1"]}
{"year":"2021","title":"Computational Challenges for Artificial Intelligence and Machine Learning in Environmental Research","authors":["M Werner, G Dax, M Laass - INFORMATIK 2020, 2021"],"snippet":"… This includes news streams, social media messages, human-curated knowledge such as OpenStreetMap and Wikipedia, opinionated data sources such as blog posts from certain platforms, or blind web scale data collections such as common crawl …","url":["https://dl.gi.de/bitstream/handle/20.500.12116/34809/C21-1.pdf?sequence=1&isAllowed=y"]}
{"year":"2021","title":"Computational filling of curatorial gaps in a fine arts exhibition","authors":["A Flexer"],"snippet":"… Please note that we translate all keywords from German to English for this paper. We use the German fasttext5 word em- bedding, which has been trained on about 3 million words from the Wikipediaand 19 million words …","url":["https://computationalcreativity.net/iccc21/wp-content/uploads/2021/09/ICCC_2021_paper_75reduced.pdf"]}
{"year":"2021","title":"Computational methods to understand the association between emojis and emotions","authors":["AAM Shoeb - 2021"],"snippet":"Page 1. © 2021 Abu Awal Md Shoeb ALL RIGHTS RESERVED Page 2. COMPUTATIONAL METHODS TO UNDERSTAND THE ASSOCIATION BETWEEN EMOJIS AND EMOTIONS By ABU AWAL MD SHOEB A dissertation submitted to the School of Graduate Studies …","url":["https://rucore.libraries.rutgers.edu/rutgers-lib/65975/PDF/1/"]}
{"year":"2021","title":"Computer Science Review","authors":["JK Tripathy, SC Sethuraman, MV Cruz, V Vijayakumar - 2021"],"snippet":"abstract The amount of data and computing power has drastically increased over the last decade, which leads to the development of several new fronts in the field of Natural Language Processing (NLP). In addition to that, the entanglement of …","url":["https://www.researchgate.net/profile/Mangalraj-Poobalasubramanian/publication/355132427_Comprehensive_analysis_of_embeddings_and_pre-training_in_NLP/links/6164f98e1eb5da761e836888/Comprehensive-analysis-of-embeddings-and-pre-training-in-NLP.pdf"]}
{"year":"2021","title":"CoMSum and SIBERT: A Dataset and Neural Model for Query-Based Multi-document Summarization","authors":["S Kulkarni, S Chammas, W Zhu, F Sha, E Ie - International Conference on Document …, 2021"],"snippet":"… We use the cleaned Common Crawl (CC) corpus [32] to source relevant web documents that are diverse and multi-faceted for generating Natural Questions (NQ) (long-form) answers [21]. Figure 1 illustrates the overall procedure …","url":["https://link.springer.com/chapter/10.1007/978-3-030-86331-9_6"]}
{"year":"2021","title":"Concept-Based Label Embedding via Dynamic Routing for Hierarchical Text Classification","authors":["X Wang, L Zhao, B Liu, T Chen, F Zhang, D Wang"],"snippet":"… Hyper-parameters are tuned on a validation set by grid search. We take Stanford's publicly available GloVe 300-dimensional embeddings trained on 42 billion tokens from Common Crawl (Pennington et al., 2014) as initialization for word em- beddings …","url":["https://aclanthology.org/2021.acl-long.388.pdf"]}
{"year":"2021","title":"Confused by Path: Analysis of Path Confusion Based Attacks","authors":["SA Mirheidari - 2020"],"snippet":"… 93 iii Page 10. Page 11. List of Tables 4.1 Sample Grouped Web pages. . . . . 29 4.2 Narrowing down the Common Crawl to the candidate set used in our analysis (from left to right). . . . 36 4.3 Vulnerable pages and sites in the candidate set …","url":["https://iris.unitn.it/retrieve/handle/11572/280512/382175/phd_unitn_Seyed%20Ali_Mirheidari.pdf"]}
{"year":"2021","title":"ConRPG: Paraphrase Generation using Contexts as Regularizer","authors":["Y Meng, X Ao, Q He, X Sun, Q Han, F Wu, J Li - arXiv preprint arXiv:2109.00363, 2021"],"snippet":"… We implement the above models, ie p(−→ci|c<i,c>i), p(←−ci|c<i,c>i), p(c<i|ci,c>i), p(c>i|c<i,ci) based on the SEQ2SEQ structure on a subset of CommonCrawl containing 10 billion tokens in total. We use Transformers …","url":["https://arxiv.org/pdf/2109.00363"]}
{"year":"2021","title":"Consumer Health Search at CLEF eHealth 2021","authors":["L Goeuriot, G Pasi, H Suominen, E Bassani… - CLEF 2021 Evaluation Labs …, 2021"],"snippet":"… The task generated a new representative web corpus including web pages acquired from a 2021 CommonCrawl and social media content from Twitter and Reddit, along with a new collection of 55 manually generated layperson …","url":["http://ceur-ws.org/Vol-2936/paper-62.pdf"]}
{"year":"2021","title":"Contingency of semantic generalization on episodic specificity varies across development","authors":["CT Ngo, SL Benear, H Popal, IR Olson, NS Newcombe - Current Biology, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0960982221004619"]}
{"year":"2021","title":"Contrastive finetuning of generative language models for informal premise selection","authors":["JM Han, T Xu, S Polu, A Neelakantan, A Radford"],"snippet":"… contrastive training. • GPT-3 style pretraining. The model is pretrained for 300B tokens on the same data (a mix of filtered CommonCrawl, WebText, books, and Wikipedia) as GPT-3 [2]. • WebMath pretraining. Starting from the …","url":["http://aitp-conference.org/2021/abstract/paper_21.pdf"]}
{"year":"2021","title":"Control Prefixes for Text Generation","authors":["J Clive, K Cao, M Rei - arXiv preprint arXiv:2110.08329, 2021"],"snippet":"Prompt learning methods adapt pre-trained language models to downstream applications by using a task-specific prompt together with the input. Most of the current work on prompt learning in text generation relies on a shared dataset-level …","url":["https://arxiv.org/pdf/2110.08329"]}
{"year":"2021","title":"Conversations Powered by Cross-Lingual Knowledge","authors":["W Sun, C Meng, Q Meng, Z Ren, P Ren, Z Chen… - 2021"],"snippet":"Page 1. Conversations Powered by Cross-Lingual Knowledge Weiwei Sun1* Chuan Meng1* Qi Meng2 Zhaochun Ren1† Pengjie Ren1† Zhumin Chen1 Maarten de Rijke3,4 1Shandong University, Qingdao, China 2Microsoft …","url":["https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/sun-2021-conversations.pdf"]}
{"year":"2021","title":"CopyCat: Near-Duplicates within and between the ClueWeb and the Common Crawl","authors":["M Fröbe, J Bevendorff, L Gienapp, M Völske, B Stein… - 2021"],"snippet":"The amount of near-duplicates in web crawls like the ClueWeb or Common Crawl demands from their users either to develop a preprocessing pipeline for deduplication, which is costly both computationally and in person hours, or accepting …","url":["https://webis.de/downloads/publications/papers/froebe_2021.pdf"]}
{"year":"2021","title":"Corpulyzer: A Novel Framework for Building Low Resource Language Corpora","authors":["B Tahir, MA Mehmood - IEEE Access"],"snippet":"… Leveraging dataset from Common Crawl Corpus (CCC), first, we prepare a list of seed URLs by filtering the Urdu language webpages … INDEX TERMS Common crawl, web crawling, text corpus, corpus analysis, regional languages corpora …","url":["https://ieeexplore.ieee.org/iel7/6287639/9312710/09316706.pdf"]}
{"year":"2021","title":"Corpus and Baseline Model for Domain-Specific Entity Recognition in German","authors":["S Torge, W Hahn, R Jäkel, WE Nagel - 2020 6th IEEE Congress on Information …, 2021"],"snippet":"… Deepset AI4 FastText Wikipedia 100 Deepset AI GloVe Wikipedia 300 FastText [26] FastText Wikipedia, 300 Common Crawl Kyubyong5 Word2Vec Wikipedia 300 SB Tweets s[27] FastText 50 Mio Tweets 100 SB Tweets l[27] FastText 50 Mio Tweets 300 …","url":["https://ieeexplore.ieee.org/abstract/document/9357189/"]}
{"year":"2021","title":"COVIDSenti: A Large-Scale Benchmark Twitter Data Set for COVID-19 Sentiment Analysis","authors":["U Naseem, I Razzak, M Khushi, PW Eklund, J Kim - IEEE Transactions on …, 2021"],"snippet":"… Term frequency-inverse document frequency (TF-IDF) has been used for vectorization. Similarly, for word embeddings, pretrained Word2Vec, GloVe, and fastText embeddings trained on Common Crawl and Wikipedia are used and have 300-D vectors …","url":["https://ieeexplore.ieee.org/abstract/document/9340540/"]}
{"year":"2021","title":"CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models","authors":["J Frohberg, F Binder - arXiv preprint arXiv:2112.11941, 2021"],"snippet":"We introduce the CRASS (counterfactual reasoning assessment) data set and benchmark utilizing questionized counterfactual conditionals as a novel and powerful tool to evaluate large language models. We present the data set design …","url":["https://arxiv.org/pdf/2112.11941"]}
{"year":"2021","title":"Creation, Enrichment and Application of Knowledge Graphs","authors":["S Gottschalk - 2021"],"snippet":"Page 1. CREATION, ENRICHMENT AND APPLICATION OF KNOWLEDGE GRAPHS Von der Fakultät für Elektrotechnik und Informatik der Gottfried Wilhelm Leibniz Universität Hannover zur Erlangung des Grades DOKTOR …","url":["https://www.repo.uni-hannover.de/bitstream/handle/123456789/11125/phd_thesis_gottschalk.pdf?sequence=1"]}
{"year":"2021","title":"CrisisBench: Benchmarking Crisis-related Social Media Datasets for Humanitarian Information Processing","authors":["F Alam, H Sajjad, M Imran, F Ofli - 2021"],"snippet":"… 2017). fastText: For the fastText (Joulin et al. 2017), we used pretrained embeddings trained on Common Crawl, which is re- leased by fastText for English. Transformer models: Pre-trained models have achieved …","url":["https://mimran.me/papers/CrisisBench_Benchmarking_Crisis_related_Social_Media_Datasets_ICWSM21.pdf"]}
{"year":"2021","title":"Cross-Domain Transfer of Generative Explanations Using Text-to-Text Models","authors":["KF Erliksson, A Arpteg, M Matskin, AH Payberah - International Conference on …, 2021"],"snippet":"… An analysis of BERT'S attention. In: ACL Blackbox NLP Workshop (2019)Google Scholar. 7. Common Crawl. https://www.commoncrawl.org. 8. Danilevsky, M., et al.: A survey of the state of explainable AI for natural language processing …","url":["https://link.springer.com/chapter/10.1007/978-3-030-80599-9_8"]}
{"year":"2021","title":"Cross-language Information Retrieval","authors":["P Galuščáková, DW Oard, S Nair - arXiv preprint arXiv:2111.05988, 2021"],"snippet":"Two key assumptions shape the usual view of ranked retrieval: (1) that the searcher can choose words for their query that might appear in the documents that they wish to see, and (2) that ranking retrieved documents will suffice because the searcher …","url":["https://arxiv.org/pdf/2111.05988"]}
{"year":"2021","title":"Cross-Language Learning for Entity Matching","authors":["R Peeters, C Bizer - arXiv preprint arXiv:2110.03338, 2021"],"snippet":"… We further use a German BERT model pre-trained on a di- verse set of German data ('bert-base-german-dbmdz-uncased'), including the German Wikipedia, parts of the Common Crawl, and EU Bookshop corpus, which …","url":["https://arxiv.org/pdf/2110.03338"]}
{"year":"2021","title":"Cross-lingual Capsule Network for Hate Speech Detection in Social Media","authors":["A Jiang, A Zubiaga - arXiv preprint arXiv:2108.03089, 2021"],"snippet":"… mBERT. Multilingual BERT6 (mBERT) is a variant of BERT [14] that was trained on 104 languages of Wikipedia; XLM-R. XLM-RoBERTa is a scaled cross-lingual sentence en- coder across 100 languages from Common Crawl [11]; …","url":["https://arxiv.org/pdf/2108.03089"]}
{"year":"2021","title":"Cross-lingual Fine-tuning for Abstractive Arabic Text Summarization","authors":["M Kahla, ZG Yang, A Novák - 2021"],"snippet":"… We only kept ar- ticles from the “Main/Top Stories” section, and filtered out all articles where either the main article text or the lead was too short or missing and items where the text is shorter than 4 times the 2https://www …","url":["http://nlpg.itk.ppke.hu/sites/default/files/publications/RANLP2021_Cross_lingual_fine_tuning_for_Abstractive_Arabic_Text_Summarization.pdf"]}
{"year":"2021","title":"Cross-lingual Hate Speech Detection using Transformer Models","authors":["T Tiţa, A Zubiaga - arXiv preprint arXiv:2111.00981, 2021"],"snippet":"Hate speech detection within a cross-lingual setting represents a paramount area of interest for all medium and large-scale online platforms. Failing to properly address this issue on a global scale has already led over time to morally questionable real-life …","url":["https://arxiv.org/pdf/2111.00981"]}
{"year":"2021","title":"Cross-lingual Language Model Pretraining for Retrieval","authors":["P Yu, H Fei, P Li - Proceedings of the Web Conference 2021, 2021"],"snippet":"… (MLM) and next sentence prediction (NSP) objectives [13] on the top 102 languages with the largest Wikipedia dumps. (v) XLM-R. We use the public checkpoint9 [8]. It was originally pretrained with the MLM objective on the CommonCrawl corpus in 100 languages …","url":["https://dl.acm.org/doi/abs/10.1145/3442381.3449830"]}
{"year":"2021","title":"Cross-Lingual Text Classification of Transliterated Hindi and Malayalam","authors":["J Krishnan, A Anastasopoulos, H Purohit, H Rangwala - arXiv preprint arXiv …, 2021"],"snippet":"… The improvement produced by our model is due to the fact that the pre-trained models have likely not seen that many transliterations in these languages. mBERT is trained using data from Wikipedia, while XLM-R uses Common Crawl …","url":["https://arxiv.org/pdf/2108.13620"]}
{"year":"2021","title":"Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph","authors":["N Chairatanakul, N Sriwatanasakdi… - arXiv preprint arXiv …, 2021"],"snippet":"… FastText (Bojanowski et al., 2017): a taskindependent word embedding trained on Wikipedia and Common Crawl only of a target language. • RCSLS (Joulin et al., 2018): a CLWE trained on Wikipedia. We used public FastText4 version …","url":["https://arxiv.org/pdf/2109.04400"]}
{"year":"2021","title":"Cross-lingual transfer of abstractive summarizer to less-resource language","authors":["A Žagar, M Robnik-Šikonja - Journal of Intelligent Information Systems, 2021"],"snippet":"Automatic text summarization extracts important information from texts and presents the information in the form of a summary. Abstractive summarization app.","url":["https://link.springer.com/article/10.1007/s10844-021-00663-8"]}
{"year":"2021","title":"Cross-lingual Transferring of Pre-trained Contextualized Language Models","authors":["Z Li, K Parnow, H Zhao, Z Zhang, R Wang, M Utiyama… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. ARXIV, DECEMBER 2020 1 Cross-lingual Transferring of Pre-trained Contextualized Language Models Zuchao Li, Kevin Parnow, Hai Zhao, Zhuosheng Zhang, Rui Wang, Masao Utiyama, and Eiichiro Sumita Abstract …","url":["https://arxiv.org/pdf/2107.12627"]}
{"year":"2021","title":"Cross-Lingual Word Embedding Refinement by $\\ell_ {1} $ Norm Optimisation","authors":["X Peng, C Lin, M Stevenson - arXiv preprint arXiv:2104.04916, 2021"],"snippet":"… Tibetan family). News-Embs (Artetxe et al., 2018a): embeddings trained on a multilingual News text collection, ie, Page 5. the WaCKy Crawl of {EN, DE, IT}, the Common Crawl of FI, and the WMT News Crawl of ES. News-Embs …","url":["https://arxiv.org/pdf/2104.04916"]}
{"year":"2021","title":"Cross-Lingual Word Embedding Refinement by ℓ1 Norm Optimisation Norm Optimisation","authors":["X Peng, C Lin, M Stevenson - Proceedings of the 2021 Conference of the North …, 2021"],"snippet":"… family). News-Embs (Artetxe et al., 2018a): embeddings trained on a multilingual News text collection, ie, Page 5. 2694 the WaCKy Crawl of {EN, DE, IT}, the Common Crawl of FI, and the WMT News Crawl of ES. News-Embs …","url":["https://www.aclweb.org/anthology/2021.naacl-main.214.pdf"]}
{"year":"2021","title":"Cross-lingual word embedding refinement by ℓ1 norm optimisation","authors":["X Peng, C Lin, M Stevenson - arXiv, 2021"],"snippet":"… Tibetan family). News-Embs (Artetxe et al., 2018a): embeddings trained on a multilingual News text collection, ie, Page 6. the WaCKy Crawl of {EN, DE, IT}, the Common Crawl of FI, and the WMT News Crawl of ES. News-Embs …","url":["https://eprints.whiterose.ac.uk/173333/1/2104.04916v1.pdf"]}
{"year":"2021","title":"Cross-Modal Progressive Comprehension for Referring Segmentation","authors":["S Liu, T Hui, S Huang, Y Wei, B Li, G Li - IEEE Transactions on Pattern Analysis and …, 2021"],"snippet":"Page 1. 0162-8828 (c) 2021 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/9430750/"]}
{"year":"2021","title":"CTNet: Conversational Transformer Network for Emotion Recognition","authors":["Z Lian, B Liu, J Tao - IEEE/ACM Transactions on Audio, Speech, and …, 2021"],"snippet":"… utterances. Specifically, we convert transcripts into the concatenation of vectors of the constituent words. These vectors are the public available word vectors trained on the Common Crawl and Wikipedia datasets [44]. Totally …","url":["https://ieeexplore.ieee.org/abstract/document/9316758/"]}
{"year":"2021","title":"CUNI systems for WMT21: Terminology translation Shared Task","authors":["J Jon, M Novák, JP Aires, D Variš, O Bojar - arXiv preprint arXiv:2109.09350, 2021"],"snippet":"… 3.1 Data We used all English-French corpora allowed by the organizers, aside from Paracrawl (with the exception of one model, which is marked). Namely this means Europarl v10, Common Crawl, UN Parallel Corpus …","url":["https://arxiv.org/pdf/2109.09350"]}
{"year":"2021","title":"Current Advances in Neural Networks","authors":["V Gallego, DR Insua - 2021"],"snippet":"Abstract This article reviews current advances and developments in neural networks. This requires recalling some of the earlier work in the field. We emphasize Bayesian approaches and their benefits compared to more standard maximum likelihood …","url":["https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-040220-112019"]}
{"year":"2021","title":"Data Augmentation for Speech Recognition in Maltese: A Low-Resource Perspective","authors":["C Mena, A DeMarco, C Borg, L van der Plas, A Gatt - arXiv preprint arXiv:2111.07793, 2021"],"snippet":"Developing speech technologies is a challenge for low-resource languages for which both annotated and raw speech data is sparse. Maltese is one such language. Recent years have seen an increased interest in the computational processing of …","url":["https://arxiv.org/pdf/2111.07793"]}
{"year":"2021","title":"Data augmentation impact on domain-specific text summarization","authors":["A LAIFA, C CRUZ, L Gautier"],"snippet":"… Association for Computational Linguistics, Seattle / Virtual, United States (2020) 4. Lin C., Text Summarization Branches Out, 74–81. Association for Computational Linguistics, Barcelona, Spain (2004) 5. Common Crawl Foundation, https://commoncrawl.org/","url":["https://www.researchgate.net/profile/Abdelghani-Laifa/publication/351935366_Data_augmentation_impact_on_domain-specific_text_summarization/links/60b0b492458515bfb0ac029e/Data-augmentation-impact-on-domain-specific-text-summarization.pdf"]}
{"year":"2021","title":"Data Efficient Deep Learning Models for Text Classification","authors":["R Garreta"],"snippet":"Page 1. Universidad de la República PEDECIBA Informática Master Thesis Data Efficient Deep Learning Models for Text Classification Author: Raúl Garreta Supervisors …","url":["https://www.fing.edu.uy/inco/grupos/pln/publicaciones/tesis_garreta.pdf"]}
{"year":"2021","title":"Data Filtering using Cross-Lingual Word Embeddings","authors":["C Herold, J Rosendahl, J Vanvinckenroye, H Ney - … of the 2021 Conference of the …, 2021"],"snippet":"… The De→En training data consists of the corpora Commoncrawl, Europarl, Rapid and ParaCrawl from the WMT 2019 news translation task2. We use the czeng 1.7 corpus3 from the WMT 2018 news translation task for En→Cs …","url":["https://www.aclweb.org/anthology/2021.naacl-main.15.pdf"]}
{"year":"2021","title":"Data science with Vadalog: Knowledge Graphs with machine learning and reasoning in practice","authors":["L Bellomarini, RR Fayzrakhmanov, G Gottlob… - Future Generation …, 2021"],"snippet":"… Such resources originate from data streams collected from internal systems (eg, Enterprise Resource Planning, Workflow Management, and Supply Chain Management) and external resources (eg, news, social media feeds and the Common Crawl 1 ). …","url":["https://www.sciencedirect.com/science/article/pii/S0167739X21004179"]}
{"year":"2021","title":"Dataset for Identification of Homophobia and Transophobia in Multilingual YouTube Comments","authors":["BR Chakravarthi, R Priyadharshini, R Ponnusamy… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. Abstract The increased proliferation of abusive content on social media platforms has a negative impact on online users. The dread, dislike, discomfort, or mistrust of lesbian, gay, transgender or bisexual persons is de- fined as homophobia/transphobia …","url":["https://arxiv.org/pdf/2109.00227"]}
{"year":"2021","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing","authors":["P He, J Gao, W Chen - arXiv preprint arXiv:2111.09543, 2021"],"snippet":"This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our …","url":["https://arxiv.org/pdf/2111.09543"]}
{"year":"2021","title":"Debiasing Multilingual Word Embeddings: A Case Study of Three Indian Languages","authors":["S Bansal, V Garimella, A Suhane, A Mukherjee - arXiv preprint arXiv:2107.10181, 2021"],"snippet":"… We do not plan to release non-anonymized bios-data since it is already available for download from the commoncrawl portal. Page 4 … One of the main challenges here is the absence of appropriate datasets (bios) in Indic languages on Common Crawl dumps …","url":["https://arxiv.org/pdf/2107.10181"]}
{"year":"2021","title":"Decentralised Privacy: A Distributed Ledger Approach","authors":["P Papadopoulos, N Pitropakis, WJ Buchanan - … , and Devices; Hussain, CM, Di Sia, P …, 2021"],"snippet":"Our world due to the technological progress became fast-paced and is constantly evolving, thus changing every single day. Consequently, the most valuable asset on earth is not gold or oil anymore but data. Big data companies try to take advantage …","url":["https://link.springer.com/content/pdf/10.1007/978-3-030-58675-1_58-1.pdf"]}
{"year":"2021","title":"Deciphering Speech: a Zero-Resource Approach to Cross-Lingual Transfer in ASR","authors":["O Klejch, E Wallington, P Bell - arXiv preprint arXiv:2111.06799, 2021"],"snippet":"… To prevent this issue we always used a language model trained on CommonCrawl data when deciphering and later re-decoding the training data. The CommonCrawl language models were trained on approximately 100M tokens and …","url":["https://arxiv.org/pdf/2111.06799"]}
{"year":"2021","title":"Deduplicating Training Data Makes Language Models Better","authors":["K Lee, D Ippolito, A Nystrom, C Zhang, D Eck… - arXiv preprint arXiv …, 2021"],"snippet":"… models trained on CommonCrawl in- clude GPT-3 (Brown et al., 2020) with the addition of book datasets, GROVER (Zellers et al., 2019) on a restricted subset filtered to news domains called RealNews, and T5 (Raffel et al., 2020) …","url":["https://arxiv.org/pdf/2107.06499"]}
{"year":"2021","title":"Deep artificial neural networks reveal a distributed cortical network encoding propositional sentence-level meaning","authors":["AJ Anderson, D Kiela, JR Binder, L Fernandino… - Journal of Neuroscience, 2021"],"snippet":"… represents individual words as 300-dimensional floating point vectors derived by factorizing a word co- 286 occurrence matrix (vocabulary size is 2.2 million words and co-occurrences were measured across 840 287 billion tokens …","url":["https://www.jneurosci.org/content/early/2021/03/22/JNEUROSCI.1152-20.2021.abstract"]}
{"year":"2021","title":"Deep contextualized text representation and learning for fake news detection","authors":["M Samadi, M Mousavian, S Momtazi - Information Processing & Management, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0306457321002077"]}
{"year":"2021","title":"Deep Learning and Computational Chemistry","authors":["T James, D Hristozov - Artificial Intelligence in Drug Design, 2022"],"snippet":"… For example, ImageNet contains approximately 1.2 million labeled images [99] and Common Crawl, which contributes the bulk of the training data for GPT-3, nearly one trillion words [100]. The initial training phase for AlphaGo likewise utilized a set …","url":["https://link.springer.com/protocol/10.1007/978-1-0716-1787-8_5"]}
{"year":"2021","title":"Deep Learning for Blocking in Entity Matching: A Design Space Exploration","authors":["H Li, N Tang, M Ouzzani, Y Govind, D Paulsen, G Fung…"],"snippet":"… Popular word-level embeddings such as word2vec [52] and GloVe [67] and characterlevel embeddings such as fastText [9] often come with pre-trained embeddings that are trained on a large generic external corpus such as Wikipedia, Common Crawl or PubMed …","url":["http://da.qcri.org/ntang/pubs/deepblocker.pdf"]}
{"year":"2021","title":"Deep learning for sentence clustering in essay grading support","authors":["LH Chang, I Rastas, S Pyysalo, F Ginter - arXiv preprint arXiv:2104.11556, 2021"],"snippet":"… [16] and refer readers to this paper for further details of the embeddings. These embedding were induced using the implementation of the skip-gram algorithm [22] in the word2vec software package on Finnish Common Crawl data …","url":["https://arxiv.org/pdf/2104.11556"]}
{"year":"2021","title":"Deep Learning Transformer Architecture for Named Entity Recognition on Low Resourced Languages: State of the art results","authors":["R Hanslo - arXiv preprint arXiv:2111.00830, 2021"],"snippet":"… model trained on 100 languages uses 2.5 TB of CommonCrawl (CC) data [2]. From the 100 languages used by the XLM-R multilingual masked language model, it is noted that Afrikaans (af) and isiXhosa (xh) are included in the pre-training. …","url":["https://arxiv.org/pdf/2111.00830"]}
{"year":"2021","title":"Deep Learning With Anaphora Resolution for the Detection of Tweeters With Depression: Algorithm Development and Validation Study","authors":["A Wongkoblap, MA Vadillo, V Curcin - JMIR Mental Health, 2021"],"snippet":"Background: Mental health problems are widely recognized as a major public health challenge worldwide. This concern highlights the need to develop effective tools for detecting mental health disorders in the population. Social …","url":["https://mental.jmir.org/2021/8/e19824/"]}
{"year":"2021","title":"Deep Learning-based Sentiment Analysis of Facebook Data: The Case of Turkish Users","authors":["Ö Çoban, SA Özel, A İnan - The Computer Journal, 2021"],"snippet":"Abstract. Sentiment analysis (SA) is an essential task for many domains where it is crucial to know users' public opinion about events, products, brands, politi.","url":["https://academic.oup.com/comjnl/advance-article-abstract/doi/10.1093/comjnl/bxaa172/6095851"]}
{"year":"2021","title":"DeepEva: A Deep Neural Network Architecture for Assessing Sentence Complexity in Italian and English Languages","authors":["GL Bosco, G Pilato, D Schicchi - Array, 2021","GL Boscoa, G Pilato, D Schicchic"],"snippet":"Abstract Automatic Text Complexity Evaluation (ATE) is a research field that aims at creating new methodologies to make autonomous the process of the text complexity evaluation, that is the study of the text-linguistic features (eg, lexical, syntactical …","url":["https://iris.unipa.it/retrieve/handle/10447/524419/1257448/1-s2.0-S2590005621000424-main.pdf","https://www.sciencedirect.com/science/article/pii/S2590005621000424"]}
{"year":"2021","title":"Delving into Deep Imbalanced Regression","authors":["Y Yang, K Zha, YC Chen, H Wang, D Katabi - arXiv preprint arXiv:2102.09554, 2021"],"snippet":"Page 1. Delving into Deep Imbalanced Regression Yuzhe Yang 1 Kaiwen Zha 1 Ying-Cong Chen 1 Hao Wang 2 Dina Katabi 1 Abstract Real-world data often exhibit imbalanced distributions, where certain target values …","url":["https://arxiv.org/pdf/2102.09554"]}
{"year":"2021","title":"Democratic Backsliding and Media Responses to Government Repression: Machine Learning Evidence from Tanzania","authors":["FS Adiguzel, D Romero, E Wibbels"],"snippet":"… Using the list of international, regional and national domains, we first check GDELT and the Internet Archive for available links, pull the available web pages from the Common Crawl and from the websites directly. We then initialize Scrapy …","url":["https://mlp.trinity.duke.edu/assets/Tanzania_ML4P.pdf"]}
{"year":"2021","title":"Dense Events Grounding in Video","authors":["P Bao, Q Zheng, Y Mu - 2021"],"snippet":"… 2015) as previous methods to extract C3D video features on both datasets. And we use Glove (Jeffrey Pennington and Manning 2014) word embeddings pretrained on Common Crawl to represent each word in the sentences …","url":["http://www.muyadong.com/paper/3254_PeijunB.pdf"]}
{"year":"2021","title":"Depression Detection of Twitter Posters using Deep Learning with Anaphora Resolution","authors":["A Wongkoblap, MNR District"],"snippet":"… occurrent words as vectors [26]. GloVe was trained on several textual datasets eg, Wikipedia and common crawl (a copy of web content), and supported 50, 100, 200, and 300 dimensional vectors. Anaphora resolution is another …","url":["https://kclpure.kcl.ac.uk/portal/files/150024721/Journal_V2_270121_submitted.pdf"]}
{"year":"2021","title":"DESCGEN: A Distantly Supervised Datasetfor Generating Abstractive Entity Descriptions","authors":["W Shi, M Joshi, L Zettlemoyer - arXiv preprint arXiv:2106.05365, 2021"],"snippet":"… We further parse the HTML texts of the web pages and extract contents as source documents. Real News To expand the collection of source documents, we extract entity mentions in RealNews (Zellers et al., 2019) …","url":["https://arxiv.org/pdf/2106.05365"]}
{"year":"2021","title":"Design, analysis and implementation of advanced methodologies to measure the socio-economic impact of personal data in large online services","authors":["J González Cabañas - 2021"],"snippet":"The web ecosystem is enormous, and overall it is sustained by an intangible attribute that mainly supports the majority of free services: the exploitation of personal information. Over the years, concerns on how services use personal data …","url":["https://e-archivo.uc3m.es/bitstream/handle/10016/33628/tesis_jose_gonzalez_caba%C3%B1as_2021.pdf?sequence=1"]}
{"year":"2021","title":"Detecting Anatomical and Functional Connectivity Relations in Biomedical Literature via Language Representation Models","authors":["IB Ozyurt, J Menke, A Bandrowski, M Martone - … of the Second Workshop on Scholarly …, 2021"],"snippet":"… For word embeddings we have used 300 dimensional Common Crawl (840B tokens) trained GloVe (Pennington et al., 2014) vectors. The de- pendency parse trees for the input sentences were generated via Stanford CoreNLP (Manning et al., 2014) package …","url":["https://www.aclweb.org/anthology/2021.sdp-1.4.pdf"]}
{"year":"2021","title":"Detecting Discrete Emotions in Text Using Neural Networks","authors":["SA Seyeditabari - 2020"],"snippet":"Page 1. DETECTING DISCRETE EMOTIONS IN TEXT USING NEURAL NETWORKS by Seyed Armin Seyeditabari A dissertation submitted to the faculty of The University of North Carolina at Charlotte in partial fulfillment of the …","url":["https://search.proquest.com/openview/474e6c4af557600c03e4c198c711cf47/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2021","title":"Detecting ethnicity-targeted hate speech in Russian social media texts","authors":["E Pronoza, P Panicheva, O Koltsova, P Rosso - Information Processing & …, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0306457321001606"]}
{"year":"2021","title":"Detecting formal thought disorder by deep contextualized word representations","authors":["J Sarzynska-Wawer, A Wawer, A Pawlak… - Psychiatry Research, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0165178121004315"]}
{"year":"2021","title":"Detecting Hate Speech on Social Media Using Deep Learning Techniques","authors":["DO Idris, EO Akinola, S Olalekan"],"snippet":"… A FastText pre-trained model was obtained from Facebook's FastText website which contained 2 million word vectors with 300 dimensions trained with subword information on Common Crawl (consisting of a total of 600 billion tokens) …","url":["http://journals.ui.edu.ng/index.php/uijslictr/article/download/86/68"]}
{"year":"2021","title":"Detecting Online Risks and Supportive Interaction in Instant Messenger Conversations using Czech Transformers","authors":["O Sotolář, J Plhák, M Tkaczyk, M Lebedíková… - RASLAN 2021 Recent …, 2021"],"snippet":"We present a comparison of state-of-the-art models for text classification of Online Risks and Supportive Interaction in anonymized Instant Messenger conversations held in Czech. We compare the transformer models Czert, RobeCzech, and FERNET-C5 …","url":["https://nlp.fi.muni.cz/raslan/raslan21.pdf#page=27"]}
{"year":"2021","title":"Detecting Phishing Sites--An Overview","authors":["P Kalaharsha, BM Mehtre - arXiv preprint arXiv:2103.12739, 2021"],"snippet":"… Some of the common sources of datasets are shown below in Table 1. Alexa and Common crawl contains names of the legitimate sites which are likely to be used for phishing [62][63] … 2 Common-Crawl [63] 940 million URLs 2.8 billion webpages …","url":["https://arxiv.org/pdf/2103.12739"]}
{"year":"2021","title":"Detecting the target of sarcasm is hard: Really??","authors":["P Parameswaran, A Trotman, V Liesaputra, D Eyers - Information Processing & …, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0306457321000972"]}
{"year":"2021","title":"Detection and Morphological Analysis of Novel Russian Loanwords","authors":["Y Spektor - 2021"],"snippet":"… is compiled. The first dataset is the CC-100 Russian dataset (Conneau et al., 2020, Wenzek et al., 2020), which includes Common Crawl2 data from January 2018 to February 2019. A second … 3. 2 https://commoncrawl.org/ Page 16. 6 …","url":["https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=5665&context=gc_etds"]}
{"year":"2021","title":"Detection of phishing attacks using probabilistic neural network with a novel training algorithm for reduced Gaussian kernels and optimal smoothing parameter …","authors":["S Priya, S Selvakumar - International Journal of Ad Hoc and Ubiquitous …, 2021"],"snippet":"Page 1. Int. J. Ad Hoc and Ubiquitous Computing, Vol. 36, No. 2, 2021 67 Detection of phishing attacks using probabilistic neural network with a novel training algorithm for reduced Gaussian kernels and optimal …","url":["https://www.inderscienceonline.com/doi/abs/10.1504/IJAHUC.2021.113381"]}
{"year":"2021","title":"Detection of Phishing Websites from URLs by using Classification Techniques on WEKA","authors":["B Geyik, K Erensoy, E Kocyigit - 2021 6th International Conference on Inventive …, 2021"],"snippet":"… We get the dataset from [21]. In this dataset, legitimate sites are collected from common-crawl and Alexa while phishing sites are collected from Phishtank. After some pre-processing steps 122,055 values 85,220 with …","url":["https://ieeexplore.ieee.org/abstract/document/9358642/"]}
{"year":"2021","title":"Development and Evaluation of Novel Ophthalmology Domain-Specific Neural Word Embeddings to Predict Visual Prognosis","authors":["S Wang, B Tseng, T Hernandez-Boussard - International Journal of Medical …, 2021"],"snippet":"… entity recognition, sentence classification, relation extraction, and other general natural language processing tasks,[8] and general domain GloVe (Global Vectors for Word Representation), pre-trained on large corpora of general …","url":["https://www.sciencedirect.com/science/article/pii/S1386505621000903"]}
{"year":"2021","title":"Development of a language model and opinion extraction for text analysis of online platforms","authors":["A Qudar, M Md - 2021"],"snippet":"Page 1. Lakehead University Knowledge Commons,http://knowledgecommons. lakeheadu.ca Electronic Theses and Dissertations Electronic Theses and Dissertations from 2009 2021 Development of a language model and opinion …","url":["https://knowledgecommons.lakeheadu.ca/bitstream/handle/2453/4822/QudarM2021m-1a.pdf?sequence=1&isAllowed=y"]}
{"year":"2021","title":"DFKI SLT at GermEval 2021: Multilingual Pre-training and Data Augmentation for the Classification of Toxicity in Social Media Comments","authors":["R Calizzano, M Ostendorff, G Rehm"],"snippet":"… It was trained on the Common Crawl corpus in 100 languages using masked language modeling. We choose XLM-RoBERTa instead of Multilingual BERT5 because XLM-RoBERTa outperforms Multilingual BERT on a variety of cross-lingual benchmarks …","url":["https://netlibrary.aau.at/obvukloa/periodical/titleinfo/6435231/full.pdf"]}
{"year":"2021","title":"Dictionary-based Debiasing of Pre-trained Word Embeddings","authors":["M Kaneko, D Bollegala - arXiv preprint arXiv:2101.09525, 2021"],"snippet":"… 3M words learned from Google News corpus (Mikolov et al., 2013a)), GloVe4 (300dimensional embeddings for ca. 2.1M words learned from the Common Crawl (Pennington et al., 2014)), and fastText5 (300-dimensional embeddings for ca …","url":["https://arxiv.org/pdf/2101.09525"]}
{"year":"2021","title":"Different Keystrokes for Different Folks: Visualizing Crowdworker Querying Behavior","authors":["R Benham, J Mackenzie, JS Culpepper, A Moffat - 2021"],"snippet":"… In this paper, we present the Common Crawl News Query Ex- plorer, 1 which enables information retrieval practitioners to ob- serve and compare human query formulation a posteriori … Common Crawl News Query Explorer …","url":["https://jmmackenzie.io/pdf/bm+21-chiir.pdf"]}
{"year":"2021","title":"Difficulty-as-improvement: The courage to keep going in the face of life's difficulties","authors":["V Yan, D Oyserman, G Kiper, M Atari - 2021"],"snippet":"… Our computational linguistic analyses of the “Common Crawl” corpus suggest that when people talk about difficulty they also talk about importance, impossibility, and improvement (Study 1). Studies 2-14 (total N = 2,378) use our brief difficulty-as-improvement …","url":["https://psyarxiv.com/f7epj/download?format=pdf"]}
{"year":"2021","title":"Digital humanities and web archives: Possible new paths for combining datasets","authors":["N Brügger - International Journal of Digital Humanities, 2021"],"snippet":"… Only a limited number of web archives are open to the wider public, most notably the US based Internet Archive, but also the Portuguese, the Croatian, the Australian, and the Icelandic web archives, and open collections such …","url":["https://link.springer.com/article/10.1007/s42803-021-00038-z"]}
{"year":"2021","title":"Discovering prerequisite relations from educational documents through word embeddings","authors":["F Gasparetti - Future Generation Computer Systems, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0167739X21003290"]}
{"year":"2021","title":"Discovering Representation Sprachbund For Multilingual Pre-Training","authors":["Y Fan, Y Liang, A Muzio, H Hassan, H Li, M Zhou… - arXiv preprint arXiv …, 2021"],"snippet":"… We use Wikipedia1 corpora (100 languages are included, total size 101GB) and a clean version of Common Crawl (CC)2 (89 languages are included, total size 2500GB) following Liang et al. (2020). 108 languages are included in our multilingual corpora …","url":["https://arxiv.org/pdf/2109.00271"]}
{"year":"2021","title":"Discovering X Degrees of Keyword Separation in a Fine Arts Collection","authors":["A Flexer"],"snippet":"… Please note that we translate all keywords from German to English for this paper. We use the German fasttext word embedding, which has been trained on about 3 million words from the Wikipediaand 19 million words from …","url":["https://www.researchgate.net/profile/Arthur-Flexer/publication/350721506_Discovering_X_Degrees_of_Keyword_Separation_in_a_Fine_Arts_Collection/links/606e9f15a6fdcc5f778cad52/Discovering-X-Degrees-of-Keyword-Separation-in-a-Fine-Arts-Collection.pdf"]}
{"year":"2021","title":"Discrete and Soft Prompting for Multilingual Models","authors":["M Zhao, H Schütze - arXiv preprint arXiv:2109.03630, 2021"],"snippet":"… Setup. We conduct all experiments using the pretrained XLM-RoBERTa-base model (Conneau et al., 2020) containing 270M parameters trained on 2.5 TB CommonCrawl data in 100 languages. We use PyTorch (Paszke et al., 2019) and the Hugprompt …","url":["https://arxiv.org/pdf/2109.03630"]}
{"year":"2021","title":"Discrete Cosine Transform as Universal Sentence Encoder","authors":["N Almarwani, M Diab - arXiv preprint arXiv:2106.00934, 2021"],"snippet":"… Specifically, we used a sample of 1 million parallel sentences from WMT'13 common-crawl data; this subset is the same one used in (Aldarmaki and Diab, 2019).4 To assess efficacy of the DCT models for the cross …","url":["https://arxiv.org/pdf/2106.00934"]}
{"year":"2021","title":"Discriminative Reranking for Neural Machine Translation","authors":["A Lee, M Auli, MA Ranzato"],"snippet":"… We follow the steps in Ng et al. (2019) for data preprocessing, including sentence deduplication, language identification filtering on all bitext and monolingual data (Joulin et al., 2017) and in- domain filtering (Moore …","url":["https://aclanthology.org/2021.acl-long.563.pdf"]}
{"year":"2021","title":"Dissociating Semantic and Phonemic Search Strategies in the Phonemic Verbal Fluency Task in early Dementia","authors":["H Lindsay, P Mueller, N Linz, R Zeghari, MM Mina… - Proceedings of the Seventh …, 2021"],"snippet":"… To obtain semantic word embeddings, the pretrained French fastText model is used. This model is trained on Common Crawl and Wikipedia corpora using the continuous bag of words (CBOW) algorithm with a negative sampling loss function …","url":["https://www.aclweb.org/anthology/2021.clpsych-1.4.pdf"]}
{"year":"2021","title":"DistilBERT-based Argumentation Retrieval for Answering Comparative Questions","authors":["A Alhamzeh, M Bouhaouel, E Egyed-Zsigmond… - Working Notes of CLEF, 2021"],"snippet":"… doi:10.1007/ 978-3-030-22948-1\\_5. [16] J. Bevendorff, B. Stein, M. Hagen, M. Potthast, Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl, in: L. Azzopardi, A. Hanbury, G. Pasi, B. Piwowarski (Eds.), Advances in Information Retrieval …","url":["http://ceur-ws.org/Vol-2936/paper-209.pdf"]}
{"year":"2021","title":"DISTRIBUTIONAL INVESTIGATION OF SOME FREQUENT TURKISH DERIVATIONAL AFFIXES FOR EXPLORING THEIR SEMANTICS","authors":["GN Özdemir - 2021"],"snippet":"Page 1. DISTRIBUTIONAL INVESTIGATION OF SOME FREQUENT TURKISH DERIVATIONAL AFFIXES FOR EXPLORING THEIR SEMANTICS A THESIS SUBMITTED TO THE GRADUATE SCHOOL OF INFORMATICS OF THE …","url":["https://open.metu.edu.tr/bitstream/handle/11511/91631/gizemnur_ozdemir_tez.pdf"]}
{"year":"2021","title":"Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP","authors":["T Bansal, K Gunasekaran, T Wang, T Munkhdalai… - arXiv preprint arXiv …, 2021"],"snippet":"… This performance drops even further (≈ 10%; r2c4) when the tasks are sampled from a different domain (common crawl) than the training domain … Figure 2 further shows that, for the Cluster tasks, constructing tasks from more diverse domain such …","url":["https://arxiv.org/pdf/2111.01322"]}
{"year":"2021","title":"DLJUST at SemEval-2021 Task 7: Hahackathon: Linking Humor and Offense","authors":["H Al-Omari, R Duwairi"],"snippet":"… Training the model on larger data set significantly improves the model performance using BookCurpus, English Wikipedia, CC-news with 63 million English news articles, OpenWebText, and Stories; which is a subset of CommonCrawl data …","url":["https://aclanthology.org/2021.semeval-1.155.pdf"]}
{"year":"2021","title":"Do Images really do the Talking? Analysing the significance of Images in Tamil Troll meme classification","authors":["SU Hegde, A Hande, R Priyadharshini, S Thavareesan… - arXiv preprint arXiv …, 2021"],"snippet":"… To address data imbalance during pretraining, the low-resource data are up- sampled while the high-resourceful data are downsampled to smoothen the data. The model is pretrained from scratch using the Wikipedia2, Common …","url":["https://arxiv.org/pdf/2108.03886"]}
{"year":"2021","title":"Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing","authors":["RH Maudslay, R Cotterell - Proceedings of the 2021 Conference of the North …, 2021"],"snippet":"Page 1. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 124–131 June 6–11, 2021. ©2021 Association for Computational Linguistics 124 …","url":["https://www.aclweb.org/anthology/2021.naacl-main.11.pdf"]}
{"year":"2021","title":"DOCmT5: Document-Level Pretraining of Multilingual Language Models","authors":["CH Lee, A Siddhant, V Ratnakar, M Johnson - arXiv preprint arXiv:2112.08709, 2021"],"snippet":"In this paper, we introduce DOCmT5, a multilingual sequence-to-sequence language model pre-trained with large scale parallel documents. While previous approaches have focused on leveraging sentence-level parallel data, we try to build …","url":["https://arxiv.org/pdf/2112.08709"]}
{"year":"2021","title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus","authors":["J Dodge, M Sap, A Marasovic, W Agnew, G Ilharco…"],"snippet":"… In addition to C4.EN, we host the “uncleaned” version (C4.EN.NOCLEAN), which is the snapshot of Common Crawl identified as … 5https://github.com/ allenai/c4- documentation/discussions 6https://commoncrawl.org/, where …","url":["https://homes.cs.washington.edu/~msap/pdfs/dodge2021documentingC4.pdf"]}
{"year":"2021","title":"Documenting the English Colossal Clean Crawled Corpus","authors":["J Dodge, M Sap, A Marasovic, W Agnew, G Ilharco… - arXiv preprint arXiv …, 2021"],"snippet":"… been established. In this work we provide the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin …","url":["https://arxiv.org/pdf/2104.08758"]}
{"year":"2021","title":"Does Robustness Improve Fairness? Approaching Fairness with Word Substitution Robustness Methods for Text Classification","authors":["Y Pruksachatkun, S Krishna, J Dhamala, R Gupta… - arXiv preprint arXiv …, 2021"],"snippet":"… We take 97,320 examples from the public leaderboard as the test set. • Bias in Bios (De-Arteaga et al., 2019) 3 is a dataset for occupation classification derived from Common Crawl corpus. It consists of 178,619 train and 91,917 test examples …","url":["https://arxiv.org/pdf/2106.10826"]}
{"year":"2021","title":"Does semantics aid syntax? An empirical study on named entity recognition and classification","authors":["X Zhong, E Cambria, A Hussain - Neural Computing and Applications, 2021"],"snippet":"Many researchers jointly model multiple linguistic tasks (eg, joint modeling of named entity recognition and named entity classification and joint modeli.","url":["https://link.springer.com/article/10.1007/s00521-021-05949-0"]}
{"year":"2021","title":"Domain-specific Taxonomy Enrichment based on Meta-Embeddings","authors":["M Tikhomirov, NV Loukachevitch - 2021"],"snippet":"In this paper we study the use of meta-embeddings approaches, which combine several source embeddings, for the taxonomy class prediction of new terms. We test the proposed approach in the informationsecurity domain in the task of enriching the …","url":["http://ceur-ws.org/Vol-3036/paper23.pdf"]}
{"year":"2021","title":"Don't Stop at the Top: Using Certificate Transparency Logs to Extend Domain Lists for Web Security Studies","authors":["F Marquardt, C Schmidt - 2020 IEEE 45th Conference on Local Computer …, 2020"],"snippet":"… They propose a system that collects massive amounts of records from various data sources, including data from top lists, blacklists, the Common Crawl project and DNS zone files. From these data sources, they were able to extract 199,110,841 unique domain names …","url":["https://ieeexplore.ieee.org/abstract/document/9314793/"]}
{"year":"2021","title":"DRo: A data-scarce mechanism to revolutionize the performance of DL-based Security Systems","authors":["M Sewak, SK Sahay, H Rathore - 2021 IEEE 46th Conference on Local Computer …, 2021"],"snippet":"… For pre-training a Transformer based TL language models like the OpenAI's GPT-3 [7] or the Google's BERT [5], billions to even trillions of (web) common-crawl documents could be easily and inexpensively made available …","url":["https://ieeexplore.ieee.org/abstract/document/9524929/"]}
{"year":"2021","title":"DS-TOD: Efficient Domain Specialization for Task Oriented Dialog","authors":["CC Hung, A Lauscher, SP Ponzetto, G Glavaš - arXiv preprint arXiv:2110.08395, 2021"],"snippet":"Recent work has shown that self-supervised dialog-specific pretraining on large conversational datasets yields substantial gains over traditional language modeling (LM) pretraining in downstream task-oriented dialog (TOD). These approaches, however …","url":["https://arxiv.org/pdf/2110.08395"]}
{"year":"2021","title":"DSGPT: Domain-Specific Generative Pre-Training of Transformers for Text Generation in E-commerce Title and Review Summarization","authors":["X Zhang, Y Jiang, Y Shang, Z Cheng, C Zhang, X Fan… - Proceedings of the 44th …, 2021"],"snippet":"… powerful. Pre-training a language model from a general corpus such as Wikipedia or the Common Crawl requires tremendous time and resource commitment, and can be wasteful if the downstream tasks are limited in variety …","url":["https://dl.acm.org/doi/pdf/10.1145/3404835.3463037"]}
{"year":"2021","title":"Dual-Objective Fine-Tuning of BERT for Entity Matching","authors":["R Peeters, C Bizer"],"snippet":"… WDC LSPC datasets: We use the training and test sets from the WDC Product Data Corpus for Large-scale Product Matching (WDC LSPC)1 [27] for the evaluation. The datasets were built by ex- tracting product offers from the Common Crawl …","url":["https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_Research/Web-based_Systems/pub/Peeters-Bizer-Dual-Objective-Fine-Tuning-VLDB2021-preprint.pdf"]}
{"year":"2021","title":"Dual-State Capsule Networks for Text Classification","authors":["P Demotte, S Ranathunga - arXiv preprint arXiv:2109.04762, 2021"],"snippet":"Page 1. Dual-State Capsule Networks for Text Classification Piyumal Demotte, Surangika Ranathunga Department of Computer Science and Engineering, University of Moratuwa Katubedda 10400, Sri Lanka {piyumalanthony.16,surangika}@cse.mrt.ac.lk Abstract …","url":["https://arxiv.org/pdf/2109.04762"]}
{"year":"2021","title":"DuRecDial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation","authors":["Z Liu, H Wang, ZY Niu, H Wu, W Che - arXiv preprint arXiv:2109.08877, 2021"],"snippet":"Page 1. DuRecDial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation Zeming Liu1∗, Haifeng Wang2, Zheng-Yu Niu2, Hua Wu2, Wanxiang Che1† 1Research Center for Social Computing and Information …","url":["https://arxiv.org/pdf/2109.08877"]}
{"year":"2021","title":"Dynamic Knowledge Capitalization in Big Data Context: Monte Carlo Tree Search Based Reinforcement Learning","authors":["B Hirchoua, B Ouhbi, B Frikh - The International Conference on Artificial Intelligence …, 2021"],"snippet":"… 1. Fig. 1. Sub-trees examples alongside their local rewards. In order to demonstrate the effectiveness and efficiency of the proposed approach, a Common Crawl big data set is used for evaluation. Texts in this collection …","url":["https://link.springer.com/chapter/10.1007/978-3-030-76346-6_64"]}
{"year":"2021","title":"Dynamic Tuning and Weighting of Meta-learning for NMT Domain Adaptation","authors":["Z Song, Z Ma, K Qi, G Liu - International Conference on Artificial Neural Networks, 2021"],"snippet":"… 4 Experiments. 4.1 Datasets. Experiments are conducted on English-German parallel datasets. We use 6.4M sentence pairs to get a pre-trained model. The training data are sampled from CommonCrawl, Europarl, NewsCommentary, Wikititles and Rapid2019 …","url":["https://link.springer.com/chapter/10.1007/978-3-030-86383-8_46"]}
{"year":"2021","title":"Easy-to-use combination of POS and BERT model for domain-specific and misspelled terms","authors":["A Benamar, M Bothua, C Grouin, A Vilnat"],"snippet":"… – Oscar [19] is a set of monolingual corpora extracted from Common Crawl. It was selected using a classification model for each language following the approach of [6] based on FastText [12]. The classifier was previously pretrained on Wikipedia …","url":["http://ceur-ws.org/Vol-3015/paper132.pdf"]}
{"year":"2021","title":"Echo chambers revisited: The (overwhelming) sharing of in-group politicians, pundits and media on Twitter","authors":["M Wojcieszak, A Casas, X Yu, J Nagler, JA Tucker - 2021"],"snippet":"Page 1. DRAFT Echo chambers revisited: The (overwhelming) sharing of in-group politicians, pundits and media on Twitter Magdalena Wojcieszak a,b , Andreu Casas c , Xudong Yu a , Jonathan Nagler d , and Joshua A. Tucker d …","url":["https://osf.io/xwc79/download?format=pdf"]}
{"year":"2021","title":"ECKPN: Explicit Class Knowledge Propagation Network for Transductive Few-shot Learning","authors":["C Chen, X Yang, C Xu, X Huang, Z Ma - Proceedings of the IEEE/CVF Conference on …, 2021"],"snippet":"… Specifically, we first leverage the GloVe (pretrained on a large text corpora with self-supervised constraint) [33] to obtain the d1-dimensional semantic embeddings of class labels. The Common Crawl version of the GloVe is …","url":["https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_ECKPN_Explicit_Class_Knowledge_Propagation_Network_for_Transductive_Few-Shot_Learning_CVPR_2021_paper.pdf"]}
{"year":"2021","title":"EEGdenoiseNet: A benchmark dataset for end-to-end deep learning solutions of EEG denoising","authors":["H Zhang, M Zhao, C Wei, D Mantini, Z Li, Q Liu - Journal of Neural Engineering, 2021"],"snippet":"Page 1. Journal of Neural Engineering ACCEPTED MANUSCRIPT • OPEN ACCESS EEGdenoiseNet: A benchmark dataset for end-to-end deep learning solutions of EEG denoising To cite this article before publication: Haoming Zhang et al 2021 J. Neural Eng …","url":["https://iopscience.iop.org/article/10.1088/1741-2552/ac2bf8/pdf"]}
{"year":"2021","title":"Effective Seed-Guided Topic Labeling for Dataless Hierarchical Short Text Classification","authors":["Y Yang, H Wang, J Zhu, W Shi"],"snippet":"… We set the number of iterations to 8 as our model achieves competitive performance since then. For word embeddings, we employ the widely used GloVe Common Crawl6 [21], which contains 840B tokens, 2.2M vocab and 300d vectors …","url":["https://link.springer.com/content/pdf/10.1007/978-3-030-74296-6_21.pdf"]}
{"year":"2021","title":"Efficiency and Scalability of Large Search Architectures","authors":["M Siedlaczek - 2021"],"snippet":"Page 1. Page 2. 4/21/2021 04/25/2021 Page 3. iii Micro lm or other copies of this dissertation are obtainable from UMI Dissertation Publishing ProQuest CSA 789 E. Eisenhower Parkway PO Box 1346 Ann Arbor, MI 48106-1346 Page 4. iv V …","url":["https://search.proquest.com/openview/eac3a8ad7dcc3c4ed49f8beb9d39dfce/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2021","title":"Efficient Large Scale Language Modeling with Mixtures of Experts","authors":["M Artetxe, S Bhosale, N Goyal, T Mihaylov, M Ott… - arXiv preprint arXiv …, 2021"],"snippet":"Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models …","url":["https://arxiv.org/pdf/2112.10684"]}
{"year":"2021","title":"Efficient Passage Retrieval with Hashing for Open-domain Question Answering","authors":["I Yamada, A Asai, H Hajishirzi - arXiv preprint arXiv:2106.00882, 2021"],"snippet":"… the index of a common knowledge source (eg, Wikipedia) requires dozens of gigabytes.1 A reduction in the index size is critical for real-world QA that requires large knowledge sources such as scientific databases (eg, PubMed) …","url":["https://arxiv.org/pdf/2106.00882"]}
{"year":"2021","title":"Emerging trends: A gentle introduction to fine-tuning","authors":["KW Church, Z Chen, Y Ma - Natural Language Engineering, 2021"],"snippet":"The previous Emerging Trends article (Church et al., 2021. Natural Language Engineering 27(5), 631–645.) introduced deep nets to poets. Poets is an imperfect metaphor, intended as a gesture toward inclusion. The future for deep nets will …","url":["https://www.cambridge.org/core/services/aop-cambridge-core/content/view/C31D429D0928351D6A6692F8ECD1E7ED/S1351324921000322a.pdf/emerging_trends_a_gentle_introduction_to_finetuning.pdf"]}
{"year":"2021","title":"Emotionally informed hate speech detection: a multi-target perspective","authors":["P Chiril, EW Pamungkas, F Benamara, V Moriceau… - Cognitive Computation, 2021"],"snippet":"Hate Speech and harassment are widespread in online communication, due to users' freedom and anonymity and the lack of regulation provided by social me.","url":["https://link.springer.com/article/10.1007/s12559-021-09862-5"]}
{"year":"2021","title":"Empirical Best Practices On Using Product-Specific Schema. org","authors":["M Kejriwal, RK Selvam, CC Ni, N Torzec - Proceedings of the AAAI Conference on …, 2021"],"snippet":"… Keywords: E-commerce, Schema.org, Best Practices, Web Data Commons, Common Crawl, Markup, Big Data Abstract. Schema.org has experienced high growth in recent years. Structured descriptions of products embedded …","url":["https://ojs.aaai.org/index.php/AAAI/article/view/17816"]}
{"year":"2021","title":"Empirical Regularization for Synthetic Sentence Pairs in Unsupervised Neural Machine Translation","authors":["X Ai, B Fang - 2021"],"snippet":"Page 1. Empirical Regularization for Synthetic Sentence Pairs in Unsupervised Neural Machine Translation Xi Ai, Bin Fang Chongqing University barid.x.ai@gmail.com, fb@cqu.edu.cn Abstract UNMT tackles …","url":["https://www.aaai.org/AAAI21Papers/AAAI-520.AiXi.pdf"]}
{"year":"2021","title":"End-to-End Multimodal Clinical Depression Recognition using Deep Neural Networks: A comparative Analysis","authors":["M Muzammel, H Salam, A Othmani - Computer Methods and Programs in …, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0169260721005071"]}
{"year":"2021","title":"Energy-saving Cross-layer Optimization of Big Data Transfer Based on Historical Log Analysis","authors":["L Rodolph, MD Nine, L Di Tacchio, T Kosar - arXiv preprint arXiv:2104.01192, 2021"],"snippet":"… These include: (1) the small file size dataset consisting of 20,000 HTML files derived from the common crawl project [2]; (2) the medium file size dataset consisting of 5,000 image files derived from Flickr [4]; (3) the large …","url":["https://arxiv.org/pdf/2104.01192"]}
{"year":"2021","title":"Engineering Knowledge Graph from Patent Database","authors":["L Siddharth, L Blessing, KL Wood, J Luo - arXiv preprint arXiv:2106.06739, 2021"],"snippet":"… pdf Page 25. embedding models that were trained on Google News, Wikipedia, gigaword, and Common Crawl as the original data sources. The entries in Table 2 that mention these models are the resultant semantic networks …","url":["https://arxiv.org/pdf/2106.06739"]}
{"year":"2021","title":"Engineering Practical Lempel-Ziv Tries","authors":["D Arroyuelo, R Cánovas, J Fischer, D Köppl, M Löbel…"],"snippet":"Page 1. Engineering Practical Lempel-Ziv Tries ∗ Diego Arroyuelo1,2 Rodrigo Cánovas3 Johannes Fischer4 Dominik Köppl5 Marvin Löbel4 Gonzalo Navarro 1,6 Rajeev Raman7 1 Millennium Institute for Foundational Research on Data (IMFD), Chile …","url":["https://users.dcc.uchile.cl/~gnavarro/ps/jea21.pdf"]}
{"year":"2021","title":"ENHANCED REPRESENTATIONS AND EFFICIENT ANALYSIS OF SYNTACTIC DEPENDENCIES WITHIN AND BEYOND TREE STRUCTURES","authors":["T Shi - 2021"],"snippet":"Page 1. ENHANCED REPRESENTATIONS AND EFFICIENT ANALYSIS OF SYNTACTIC DEPENDENCIES WITHIN AND BEYOND TREE STRUCTURES A Dissertation Presented to the Faculty of the Graduate School of Cornell University …","url":["http://www.cs.cornell.edu/~tianze/papers/thesis.pdf"]}
{"year":"2021","title":"Enhanced sentiment extraction architecture for social media content analysis using capsule networks","authors":["P Demotte, K Wijegunarathna, D Meedeniya, I Perera - Multimedia Tools and …, 2021"],"snippet":"Recent research has produced efficient algorithms based on deep learning for text-based analytics. Such architectures could be readily applied to text-base.","url":["https://link.springer.com/article/10.1007/s11042-021-11471-1"]}
{"year":"2021","title":"Enhancing Context Through Contrast","authors":["K Ambilduke, A Shetye, D Bagade, R Bhagwatkar…"],"snippet":"Neural machine translation benefits from semantically rich representations. Considerable progress in learning such representations has been achieved by language modelling and mutual information maximization objectives using …","url":["https://preregister.science/papers_21neurips/18_paper.pdf"]}
{"year":"2021","title":"Enhancing Cross-lingual Semantic Annotations using Deep Network Sentence Embeddings","authors":["YC Lin, P Hoffmann, E Rahm - 2021"],"snippet":"… RoBERTa. XLM- R uses only MLM as training objective as RoBERTa (ie no TLM) but with multilingual corpus. It is trained on even larger dataset (the CommonCrawl corpus (Wenzek et al., 2019), 2.5 TB) and in 100 languages. XLM …","url":["https://dbs.uni-leipzig.de/file/HEALTHINF21_YCLIN.pdf"]}
{"year":"2021","title":"Enhancing Viewing Experience of Generated Visual Storylines for Promotional Videos","authors":["C Liu, H Yu, Z Shen, I Dixon, Y Yu, Z Gao, P Wang… - 2021 IEEE International …, 2021"],"snippet":"… ResNet50. Both models are trained on the DeepFashion dataset [21] until convergence2. Word embeddings are obtained from the pre-trained model on the 840B Common Crawl dataset. λi is set to 0.5. α, β, αc and βc are set to 0.4 …","url":["https://ieeexplore.ieee.org/abstract/document/9428292/"]}
{"year":"2021","title":"Ensembling of Distilled Models from Multi-task Teachers for Constrained Resource Language Pairs","authors":["A Hendy, EA Gad, M Abdelghaffar, JS ElMosalami… - arXiv preprint arXiv …, 2021"],"snippet":"… all the available monolingual data, eg NewsCrawl + CommonCrawl + Extended CommonCrawl for Hausa, and Extended CommonCrawl for both Xhosa and Zulu. For relatively … In addition to the NewsCrawl monolingual subset, we add a …","url":["https://arxiv.org/pdf/2111.13284"]}
{"year":"2021","title":"Enterprise Data Integration: On Extracting Data from HTML Tables","authors":["JC Roldán Salvador - 2020"],"snippet":"… layer is used. The authors learnt the network using 3 567 tables from 200 web sites, and evaluated the results on 60 678 ta- bles from 300 web sites; the documents were selected from the April 2016 Common Crawl. They also …","url":["https://idus.us.es/bitstream/handle/11441/105486/1/Rold%C3%A1n%20Salvador%2C%20Carlos%20tesis.pdf?sequence=1"]}
{"year":"2021","title":"ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation","authors":["Y Sun, S Wang, S Feng, S Ding, C Pang, J Shang, J Liu… - arXiv preprint arXiv …, 2021"],"snippet":"… However, these large-scale pre-trained language models with hundreds of billions of parameters are trained on plain texts. For example, the 175-billion-parameter GPT-3 is trained on a corpus with 570GB filtered texts from Common Crawl …","url":["https://arxiv.org/pdf/2107.02137"]}
{"year":"2021","title":"ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora","authors":["X Ouyang, S Wang, C Pang, Y Sun, H Tian, H Wu… - arXiv preprint arXiv …, 2020"],"snippet":"… XLM-R (Conneau et al., 2019) studies the effects of models when trained on a large scale corpus. It used 2.5T data extracted from Common Crawl (Wenzek et al., 2019) that involves 100 languages for MMLM training. Results …","url":["https://arxiv.org/pdf/2012.15674"]}
{"year":"2021","title":"Error Analysis of using BART for Multi-Document Summarization: A Study for English and German Language","authors":["T Johner, A Jana, C Biemann"],"snippet":"… For instance, single-document summarization of text in German language was done by Parida and Motlicek (2019) who utilized transformer models for abstractive summarization on two datasets — SwissText 20191 and …","url":["https://www.researchgate.net/profile/Abhik-Jana/publication/351548608_Error_Analysis_of_using_BART_for_Multi-Document_Summarization_A_Study_for_English_and_German_Language/links/609cfb0c299bf1259ed0452e/Error-Analysis-of-using-BART-for-Multi-Document-Summarization-A-Study-for-English-and-German-Language.pdf"]}
{"year":"2021","title":"Error identification for machine translation with metric embedding and attention","authors":["R Rubino, A Fujita, B Marie - Proceedings of the 2nd Workshop on Evaluation and …, 2021"],"snippet":"Abstract Quality Estimation (QE) for Machine Translation has been shown to reach relatively high accuracy in predicting sentence-level scores, relying on pretrained contextual embeddings and human-produced quality scores. However, the lack of …","url":["https://aclanthology.org/2021.eval4nlp-1.15.pdf"]}
{"year":"2021","title":"ESPnet-ST IWSLT 2021 Offline Speech Translation System","authors":["H Inaguma, B Yan, S Dalmia, P Gu, J Shi, K Duh… - arXiv preprint arXiv …, 2021"],"snippet":"… Must-C - 0.68M Must-C v2 0.74M ST-TED (cleaned) 0.40M Europarl 1.82M Commoncrawl 2.39M Paracrawl 34.37M NewsCommentary 0.37M WikiTitles 1.38M RAPID 1.63M WikiMatrix 1.57M Table 1: Corpus statistics data was …","url":["https://arxiv.org/pdf/2107.00636"]}
{"year":"2021","title":"Est-ce que vous compute? Code-switching, cultural identity, and AI","authors":["A Falbo, T LaCroix - arXiv preprint arXiv:2112.08256, 2021"],"snippet":"… of the Common Crawl dataset, resulting in approximately 14 billion tokens (though they do not provide details of how the Common Crawl … is predominantly English: around 45% of HTML pages in the Common Crawl dataset have English as their …","url":["https://arxiv.org/pdf/2112.08256"]}
{"year":"2021","title":"Establishing Trustworthiness Through Algorithmic Approaches to Qualitative Research","authors":["H Nguyen, J Ahn, A Belgrave, J Lee, L Cawelti, HE Kim… - International Conference on …, 2021","HE Kim, Y Prado, R Santagata, A Villavicencio - … 2020, Malibu, CA, USA, February 1-3 …, 2021"],"snippet":"… The model contains 300 dimensional word vectors that were trained on a vocabulary of 2 million words from web page data (Common Crawl dataset; GloVe,[21]). We then worked to cluster words together using the word embedding developed with spaCy …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=k90XEAAAQBAJ&oi=fnd&pg=PA47&dq=commoncrawl&ots=6DvGd4cN1o&sig=FHX30JOd7Xl7m8NWbE-QfeuJbks","https://link.springer.com/chapter/10.1007/978-3-030-67788-6_4"]}
{"year":"2021","title":"Estimating the Effects of Text Genre, Image Resolution and Algorithmic Complexity needed for Sinhala Optical Character Recognition","authors":["I Anuradha, C Liyanage, R Weerasinghe - International Journal on Advances in ICT …, 2021"],"snippet":"… 2) 5million+ sentences in Sinhala common crawler: In 2019, Guzman [16] presented two monolingual corpora for Sinhala. Those were a combination of 155k+ sentences of filtered Sinhala Wikipedia and 5178k+ sentences of Sinhala common crawl …","url":["https://icter.sljol.info/articles/10.4038/icter.v14i3.7231/galley/5596/download/"]}
{"year":"2021","title":"Estimation of Imageability Ratings of English Words Using Neural Networks","authors":["VV Bochkarev, AV Savinkov, AV Shevlyakova - Mexican International Conference on …, 2021"],"snippet":"… (a set of CommonCrawl vectors). The values of the correlation coefficients obtained in [6] are shown in the last row of the table. We chose the best values from those given in [6] that were obtained also using the fastText vectors trained on the …","url":["https://link.springer.com/chapter/10.1007/978-3-030-89820-5_5"]}
{"year":"2021","title":"Evaluating and Explaining Natural Language Generation with GenX","authors":["K Duskin, S Sharma, JY Yun, E Saldanha, D Arendt - … on Data Science with Human in …, 2021"],"snippet":"… While we demonstrated utility on datasets with tens of thousands of text examples, the nearest neighbor approach used would become intractable on massive text corpora such as CommonCrawl 7. This limits GenX to …","url":["https://www.aclweb.org/anthology/2021.dash-1.12.pdf"]}
{"year":"2021","title":"Evaluating Contextualized Language Models for Hungarian","authors":["J Ács, D Lévai, D Nemeskey, A Kornai"],"snippet":"… It was trained on Webcorpus 2.0 (Nemeskey, 2020), 9- billion-token corpus compiled from the Hungarian subset of Common Crawl3. Its string identifier in Huggingface Transformers is … XLMRoBERTa was trained on 2TB of …","url":["https://hlt.bme.hu/media/pdf/huBERT_eval.pdf"]}
{"year":"2021","title":"Evaluating Document Coherence Modelling","authors":["A Shen, M Mistica, B Salehi, H Li, T Baldwin, J Qi - arXiv preprint arXiv:2103.10133, 2021"],"snippet":"Page 1. arXiv:2103.10133v1 [cs.CL] 18 Mar 2021 Evaluating Document Coherence Modelling Aili Shen ♣ , Meladel Mistica ♣ , Bahar Salehi ♣ , Hang Li ♢ , Timothy Baldwin ♣ , Jianzhong Qi ♣ ♣ The University of Melbourne …","url":["https://arxiv.org/pdf/2103.10133"]}
{"year":"2021","title":"Evaluating Off-the-Shelf Machine Listening and Natural Language Models for Automated Audio Captioning","authors":["B Weck, X Favory, K Drossos, X Serra - arXiv preprint arXiv:2110.07410, 2021"],"snippet":"… We use the publicly available model trained with subword information on the Common Crawl corpus, which contains 600B tokens and is significantly larger than the corpora used for the Glove and word2vec model [38]. We employ BERT as our …","url":["https://arxiv.org/pdf/2110.07410"]}
{"year":"2021","title":"Evaluating Sequence-to-Sequence Modelling for Dialogue State Tracking","authors":["M Tuli, S Agrawal"],"snippet":"… 3.2.1 Word embeddings. We use GloVe [21] pretrained word embeddings, as done in [9]. We use the GloVe-840B-300D embeddings: 300-dimensional embeddings trained on 840B tokens from Common Crawl. 3.2.2 Models …","url":["https://www.mathieutuli.com/docs/nmt_for_dst.pdf"]}
{"year":"2021","title":"Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer","authors":["E Briakou, S Agrawal, J Tetreault, M Carpuat - arXiv preprint arXiv:2110.10668, 2021"],"snippet":"While the field of style transfer (ST) has been growing rapidly, it has been hampered by a lack of standardized practices for automatic evaluation. In this paper, we evaluate leading ST automatic metrics on the oft-researched task of formality style …","url":["https://arxiv.org/pdf/2110.10668"]}
{"year":"2021","title":"Evaluating the Text-to-SQL Capabilities of Large Language Models","authors":["MVAEX TS","N Rajkumar, R Li, D Bahdanau - arXiv preprint arXiv:2204.00498, 2022"],"snippet":"We perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting …","url":["https://arxiv.org/pdf/2204.00498","https://openreview.net/pdf?id=lYli-bAuK54"]}
{"year":"2021","title":"Evaluating the textbugger NLP Attack on","authors":["S Eicher"],"snippet":"… 1 Introduction The ubiquity of deep learning models which learn from their user-base, like social recommenders or CAPTCHA[4], or other relatively unsupervised datasets, like the Common Crawl, in practice has given rise …","url":["http://cs229.stanford.edu/proj2021spr/report2/82526938.pdf"]}
{"year":"2021","title":"Evaluating Word Embeddings with Categorical Modularity","authors":["S Casacuberta, K Halevy, DE Blasi - arXiv preprint arXiv:2106.00877, 2021"],"snippet":"… these embeddings. FastText. Monolingual embeddings for 157 languages trained on Common Crawl and Wikipedia that use CBOW with position-weights and character n-grams (Bojanowski et al., 2017). MUSE. Cross-lingual …","url":["https://arxiv.org/pdf/2106.00877"]}
{"year":"2021","title":"Evaluation and Interpretation of Word Embeddings","authors":["J Kuchár"],"snippet":"… 2.3.1 Facebook's Wikipedia and Common Crawl FastText Word Embeddings for 157 languages Produced by Grave et al. [8], the Facebook's Wikipedia and Common Crawl fastText word embeddings may be the most-used and …","url":["https://is.muni.cz/th/eycxc/Evaluation_and_interpretation_of_word_embeddings_Archive.pdf"]}
{"year":"2021","title":"Evaluation of contextual embeddings on less-resourced languages","authors":["M Ulčar, A Žagar, CS Armendariz, A Repar, S Pollak… - arXiv preprint arXiv …, 2021"],"snippet":"… The ELMoForManyLangs project (EFML) [Che et al., 2018] trained ELMo models for several languages but used relatively small datasets of 20 million words randomly sampled from the raw text released by the CONLL 2017 …","url":["https://arxiv.org/pdf/2107.10614"]}
{"year":"2021","title":"Evaluation of Neural Network Transformer Models for Named-Entity Recognition on Low-Resourced Languages","authors":["R Hanslo - 2021"],"snippet":"… LANGUAGE MODEL ARCHITECTURE XLM-Roberta (XLM-R) is a transformer-based multilingual masked language model [2]. This language model trained on 100 languages uses 2.5 TB of CommonCrawl (CC) data …","url":["https://annals-csis.org/proceedings/2021/pliks/7.pdf"]}
{"year":"2021","title":"Evaluation of Sentence Representations in Semantic Text Similarity Tasks","authors":["N Balzar Ekenbäck - 2021"],"snippet":"… 5. Transformer based encoder, using RoBERTa. For the baseline representation, word embeddings trained on common crawl data was used.[30] A few different approaches were used to construct the sentence vector representation …","url":["https://www.diva-portal.org/smash/get/diva2:1535961/FULLTEXT01.pdf"]}
{"year":"2021","title":"Evaluation of Split-and-Rephrase Output of the Knowledge Extraction Tool in the Intelligent Tutoring System","authors":["A Grubišić, B Žitko, A Gašpar, D Vasić, A Dodaj - Expert Systems with Applications, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0957417421012562"]}
{"year":"2021","title":"Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction","authors":["M Yarmohammadi, S Wu, M Marone, H Xu, S Ebner… - arXiv preprint arXiv …, 2021"],"snippet":"… We focus on the coarsest “Abstract” level, where the goal is to identify events and their agents and patients. The documents come from the news-specific portion of Common Crawl … We use the same Common Crawl corpus as XLM-R for pretraining …","url":["https://arxiv.org/pdf/2109.06798"]}
{"year":"2021","title":"Evolution of diversity and dominance of companies in online activity","authors":["PX McCarthy, X Gong, S Eghbal, DS Falster, MA Rizoiu - Plos one, 2021"],"snippet":"… We compile the linkage record over time of online websites towards domains using Common Crawl (https://commoncrawl.org/). Common Crawl is the largest open index of the web and which has been shown to represent over …","url":["https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0249993"]}
{"year":"2021","title":"Evolution of Gaussian Process kernels for machine translation post-editing effort estimation","authors":["I Roman, R Santana, A Mendiburu, JA Lozano - Annals of Mathematics and Artificial …, 2021"],"snippet":"Page 1. Annals of Mathematics and Artificial Intelligence https://doi.org/10.1007/s10472021-09751-5 Evolution of Gaussian Process kernels for machine translation post-editing effort estimation Ibai Roman1 ·Roberto Santana1 ·Alexander Mendiburu1 ·Jose A. Lozano1 …","url":["https://link.springer.com/article/10.1007/s10472-021-09751-5"]}
{"year":"2021","title":"Examination Committee.","authors":["S Colcher - 2021"],"snippet":"… evaluates the model on a German Wiki data 9 using data extracted from Common Crawl 10 as synthetic summaries … it delivers a good value on aiding the overall performance assessment. 9https://www.swisstext.org/ 10http://commoncrawl.org/ Page 18 …","url":["https://www.maxwell.vrac.puc-rio.br/53550/53550.PDF"]}
{"year":"2021","title":"Examining Media Bias in the 2020 Democratic Primary","authors":["D Färber - 2021"],"snippet":"Page 1. Examining Media Bias in the 2020 Democratic Primary by Extraction of Entity-Relationships from News Articles Master Thesis submitted by Daniel Färber at University of Konstanz Faculty of Politics, Law and Economics …","url":["https://www.researchgate.net/profile/Daniel-Faerber/publication/354219152_Examining_Media_Bias_in_the_2020_Democratic_Primary_by_Extraction_of_Entity-Relationships_from_News_Articles/links/612cd83f2b40ec7d8bd2405b/Examining-Media-Bias-in-the-2020-Democratic-Primary-by-Extraction-of-Entity-Relationships-from-News-Articles.pdf"]}
{"year":"2021","title":"Examining the Effects of Preprocessing on the Detection of Offensive Language in German Tweets","authors":["S Reimann, D Dakota"],"snippet":"… dings. We ultimately settled on on the default 300 dimensional fastText German embeddings (Grave et al., 2018), trained on the German CommonCrawl and Wikpedia, as they yielded the most stable performance. We also …","url":["https://konvens2021.phil.hhu.de/wp-content/uploads/2021/09/2021.KONVENS-1.14.pdf"]}
{"year":"2021","title":"Executive function & semantic memory impairments in Alzheimer's disease—investigating the decline of executive function and semantic memory in Alzheimer's …","authors":["JM Tröger - 2021"],"snippet":"Alzheimer’s Disease (AD) has a huge impact on an ever-aging society in highly developed industrialized countries such as the EU member states: according to the World Alzheimer’s Association the number one risk factor for AD is age. AD patients …","url":["https://publikationen.sulb.uni-saarland.de/bitstream/20.500.11880/31994/1/Troeger-Executive_Function_vs._Semantic_Memory_in_AD-v3.4.pdf"]}
{"year":"2021","title":"Experience and prediction: a metric of hardness for a novel litmus test","authors":["N Isaak, L Michael - Journal of Logic and Computation, 2021"],"snippet":"Abstract. In the past decade, the Winograd schema challenge (WSC) has become a central aspect of the research community as a novel litmus test. Consequently, th.","url":["https://academic.oup.com/logcom/advance-article-abstract/doi/10.1093/logcom/exab005/6129485"]}
{"year":"2021","title":"Experience Report: Deep Learning-based System Log Analysis for Anomaly Detection","authors":["Z Chen, J Liu, W Gu, Y Su, MR Lyu - arXiv preprint arXiv:2107.05908, 2021"],"snippet":"Page 1. Experience Report: Deep Learning-based System Log Analysis for Anomaly Detection Zhuangbin Chen, Jinyang Liu, Wenwei Gu The Chinese University of Hong Kong Hong Kong, China {zbchen,jyliu,wwgu}@cse.cuhk.edu.hk …","url":["https://arxiv.org/pdf/2107.05908"]}
{"year":"2021","title":"Experimental Evaluation of Deep Learning models for Marathi Text Classification","authors":["A Kulkarni, M Mandhane, M Likhitkar, G Kshirsagar… - arXiv preprint arXiv …, 2021"],"snippet":"… It was created by processing January-December 2018 Commoncrawl snapshots … OSCAR: Open Super-large Crawled AL- MAnaCH coRpus (OSCAR) is obtained by filtering and language classifying the Common Crawl corpus (Suárezetal., 2019) …","url":["https://arxiv.org/pdf/2101.04899"]}
{"year":"2021","title":"Experiments with adversarial attacks on text genres","authors":["M Lepekhin, S Sharoff - arXiv preprint arXiv:2107.02246, 2021"],"snippet":"… It has the same architecture, but uses bigger and more genre diverse corpora and an updated pre-training procedure. In addition, XLM-RoBERTa is a multilingual model trained on Common Crawl data in comparison to multilingual BERT only trained on Wikipedia …","url":["https://arxiv.org/pdf/2107.02246"]}
{"year":"2021","title":"ExpertRank: A Multi-level Coarse-grained Expert-based Listwise Ranking Loss","authors":["Z Chen, C Eickhoff - arXiv preprint arXiv:2107.13752, 2021"],"snippet":"… For KNRM, ConvKNRM and MatchPyramid, GloVe word embeddings with 300 dimensions [25], trained on the Common Crawl dataset2, are used for vocabulary initialization … 1https://www.tensor ow.org/ 2https://commoncrawl …","url":["https://arxiv.org/pdf/2107.13752"]}
{"year":"2021","title":"Explainable Prediction of Text Complexity: The Missing Preliminaries for Text Simplification","authors":["C Gârbacea, M Guo, S Carton, Q Mei"],"snippet":"… all positions without data corruption. We use the 12 layer XLNeT base pre-trained model on the English Wikipedia, the Books corpus (similar to BERT), Giga5, ClueWeb 2012-B, and Common Crawl. 4.1.2 Evaluation Metric We …","url":["https://aclanthology.org/2021.acl-long.88.pdf"]}
{"year":"2021","title":"Exploiting Linguistic Information from Nepali Transcripts for Early Detection of Alzheimer's Disease using Natural Language Processing and Machine Learning …","authors":["S Adhikari, S Thapa, U Naseem, P Singh, H Huo… - International Journal of …, 2021"],"snippet":"Alzheimer's disease (AD) is considered as progressing brain disease, which can be slowed down with the early detection and proper treatment by identifying the early symptoms. Language change serves as an early sign that a patient's cognitive …","url":["https://www.sciencedirect.com/science/article/pii/S1071581921001798"]}
{"year":"2021","title":"Exploiting semantic similarity models to automate transfer credit assessment in academic mobility","authors":["D Chandrasekaran - 2021"],"snippet":"Page 1. Exploiting semantic similarity models to automate transfer credit assessment in academic mobility by Dhivya Chandrasekaran A thesis submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the …","url":["https://lurepository.lakeheadu.ca/bitstream/handle/2453/4779/ChandrasekaranD2021m-1a.pdf?sequence=1&isAllowed=y"]}
{"year":"2021","title":"Exploring Conditional Text Generation for Aspect-Based Sentiment Analysis","authors":["SUS Chebolu, F Dernoncourt, N Lipka, T Solorio - arXiv preprint arXiv:2110.02334, 2021"],"snippet":"… (2019). The encoder utilizes a denoising goal similar to BERT, while the decoder tries to recreate the original sequence (autoencoder) token by token using the preceding (uncorrupted) tokens and the encoder output. T5 uses common crawl web extracted text …","url":["https://arxiv.org/pdf/2110.02334"]}
{"year":"2021","title":"Exploring deep learning methods for recognizing rare diseases and their clinical manifestations from texts","authors":["I Segura-Bedmar, D Camino-Perdonas… - arXiv preprint arXiv …, 2021"],"snippet":"… The model contains word embeddings of di- mension 300 for 3 million words. • GloVe [54], a pre-trained word embedding model trained using Common Crawl, an open repository of web crawl data. The model contains …","url":["https://arxiv.org/pdf/2109.00343"]}
{"year":"2021","title":"Exploring intrinsic information content models for addressing the issues of traditional semantic measures to evaluate verb similarity","authors":["MKS Prasad, P Sharma - Computer Speech & Language, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0885230821000838"]}
{"year":"2021","title":"Exploring Monolingual Data for Neural Machine Translation with Knowledge Distillation","authors":["AF Aji, K Heafield - arXiv preprint arXiv:2012.15455, 2020"],"snippet":"… Out- of-domain back-translation data (CommonCrawl– mixed, Open Subtitle–conversation) damages the performance, however … Though, CommonCrawl or Opensub1 conversational monolingual data is still better than not us- ing any monolingual data at all …","url":["https://arxiv.org/pdf/2012.15455"]}
{"year":"2021","title":"Exploring Pre-Trained Transformers and Bilingual Transfer Learning for Arabic Coreference Resolution","authors":["B Min - Proceedings of the Fourth Workshop on Computational …, 2021"],"snippet":"Abstract In this paper, we develop bilingual transfer learning approaches to improve Arabic coreference resolution by leveraging additional English annotation via bilingual or multilingual pre-trained transformers. We show that bilingual transfer …","url":["https://aclanthology.org/2021.crac-1.10.pdf"]}
{"year":"2021","title":"Exploring the Possibilities of Applying Transfer Learning Methods for Natural Language Processing in Software Development","authors":["W Ding"],"snippet":"Page 1. DEPARTMENT OF INFORMATICS TECHNISCHE UNIVERSITÄT MÜNCHEN Master's Thesis in Informatics: Data Engineering and Analytics Exploring the Possibilities of Applying Transfer Learning Methods for Natural …","url":["https://wwwmatthes.in.tum.de/file/1d56vodcuu5ou/Sebis-Public-Website/-/Master-Thesis-Wei-Ding/Masterthesis_WeiDing.pdf"]}
{"year":"2021","title":"Exploring the Predictive Power of News and Neural Machine Learning Models for Economic Forecasting","authors":["V Bitetta, I Bordino, A Ferretti, F Gullo, G Ponti… - Mining Data for Financial …"],"snippet":"… rule-based procedure that builds on the linguistic features of the spaCy Python library 5 . The NLP pipeline relies on the en_core_web_lg model of spaCy 6 , an English multi-task Convolutional Neural Network trained on OntoNotes …","url":["https://europepmc.org/article/pmc/pmc7808169"]}
{"year":"2021","title":"Exploring Transformers in Emotion Recognition: a comparison of BERT, DistillBERT, RoBERTa, XLNet and ELECTRA","authors":["D Cortiz - arXiv preprint arXiv:2104.02041, 2021"],"snippet":"… tokens but in random order. XLNet was trained with over 130 GB of textual data. In addition to BERT's two datasets, they included three more corpora: Giga5, ClueWeb, and Common Crawl. XLNet outperformed BERT on 20 …","url":["https://arxiv.org/pdf/2104.02041"]}
{"year":"2021","title":"Exploring Unsupervised Pretraining Objectives for Machine Translation","authors":["C Baziotis, I Titov, A Birch, B Haddow - arXiv preprint arXiv:2106.05634, 2021"],"snippet":"… (2019). The (pretraining) monolingual data contain 5M sentences from Common Crawl and Wikipedia per language, while the parallel data are approximately 600K sentences from the Bible, Open Subtitles, GNOME/KDE/Ubuntu, and Paracrawl …","url":["https://arxiv.org/pdf/2106.05634"]}
{"year":"2021","title":"Exploring Zero-shot Cross-lingual Aspect-based Sentiment Analysis using Pre-trained Multilingual Language Models","authors":["KTK Phan, DN Hao, D Van Thin, NLT Nguyen - 2021 International Conference on …, 2021"],"snippet":"… The mBERT model is trained on the Wikipedia data of 104 languages, while the XLM-R model is trained on the CommonCrawl data in 100 … In detail, the XLM-R model is trained on the combination of Common Crawl with Wikipedia data, while …","url":["https://ieeexplore.ieee.org/abstract/document/9585242/"]}
{"year":"2021","title":"Exploring Zero-Shot Emotion Recognition in Speech Using Semantic-Embedding Prototypes","authors":["X Xu, J Deng, N Cummins, Z Zhang, L Zhao… - IEEE Transactions on …, 2021"],"snippet":"Page 1. 1520-9210 (c) 2021 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/9449953/"]}
{"year":"2021","title":"Exposure of occupations to technologies of the fourth industrial revolution","authors":["B Meindl, MR Frank, J Mendonça - arXiv preprint arXiv:2110.13317, 2021"],"snippet":"… The common crawl corpus comprises texts from various sources available on the web, and therefore the embeddings represent various aspects of language. However, the structure and words used in patent data are more technical than standard …","url":["https://arxiv.org/pdf/2110.13317"]}
{"year":"2021","title":"EXPRESS: Attribute Sentiment Scoring with Online Text Reviews: Accounting for Language Structure and Missing Attributes","authors":["I Chakraborty, M Kim, K Sudhir - Journal of Marketing Research, 2021"],"snippet":"Page 1. Peer Review Version Attribute Sentiment Scoring with Online Text Reviews: Accounting for Language Structure and Missing Attributes Journal: Journal of Marketing Research Manuscript ID JMR.19.0192.R3 Manuscript …","url":["https://journals.sagepub.com/doi/abs/10.1177/00222437211052500"]}
{"year":"2021","title":"Extending Neural Temporal Tagging Systems with External Knowledge","authors":["M Pömsl"],"snippet":"Page 1. Extending Neural Temporal Tagging Systems with External Knowledge Martin Pömsl Dr. phil. Tobias Thelen Institute of Cognitive Science, Osnabrück M. Sc. Luzian Hahn Fraunhofer Institute for Integrated Circuits, Erlangen Osnabrück University …","url":["https://mpoemsl.github.io/documents/bachelor_thesis_mpoemsl.pdf"]}
{"year":"2021","title":"Extracting ESG data from business documents","authors":["J Van Der Elst, S Nijssen"],"snippet":"… Common Crawl (Want to use our data? – Common Crawl 2021) for example is an open repository of web crawl data consisting of web page data, metadata extracts and text extracts. It is obtained by scraping web pages and ignoring HTML markup …","url":["https://dial.uclouvain.be/downloader/downloader.php?pid=thesis%3A30732&datastream=PDF_01&cover=cover-mem"]}
{"year":"2021","title":"Extraction and Analysis of Stories: Natural Language Processing for Software Engineering.","authors":["H Guo - 2021"],"snippet":"Page 1. ABSTRACT GUO, HUI. Extraction and Analysis of Stories: Natural Language Processing for Software Engineering. (Under the direction of Dr. Munindar P. Singh.) Modern software systems are deployed in sociotechnical …","url":["https://repository.lib.ncsu.edu/bitstream/handle/1840.20/38875/etd.pdf?sequence=1"]}
{"year":"2021","title":"Extremely low-resource neural machine translation for Asian languages","authors":["R Rubino, B Marie, R Dabre, A Fujita, M Utiyama… - Machine Translation, 2021"],"snippet":"This paper presents a set of effective approaches to handle extremely low-resource language pairs for self-attention based neural machine translation (NMT).","url":["https://link.springer.com/article/10.1007/s10590-020-09258-6"]}
{"year":"2021","title":"Facebook AI WMT21 News Translation Task Submission","authors":["C Tran, S Bhosale, J Cross, P Koehn, S Edunov, A Fan - arXiv preprint arXiv …, 2021"],"snippet":"… For each language, we train an n-gram language model (Heafield, 2011) on all available news-domain data (Newscrawl) and a n-gram language model on a similarly sized sample from general-domain data (Commoncrawl) …","url":["https://arxiv.org/pdf/2108.03265"]}
{"year":"2021","title":"Fact-Checking of Claims from the English Wikipedia Using Evidence in the Wild","authors":["A Sathe - 2021"],"snippet":"… architectures. These advancements enable taking advantage of vast amounts of unsupervised text data obtained using the internet, via sources such as Wikipedia and Common Crawl. Using these large corpora, researchers have demonstrated …","url":["https://scholarship.richmond.edu/cgi/viewcontent.cgi?article=2562&context=honors-theses"]}
{"year":"2021","title":"Factorising Meaning and Form for Intent-Preserving Paraphrasing","authors":["T Hosking, M Lapata - arXiv preprint arXiv:2105.15053, 2021"],"snippet":"… English. Although this system was trained on high volumes of data (including Common Crawl), the training data contains relatively few questions, and we would not expect it to perform well in the domain under consideration …","url":["https://arxiv.org/pdf/2105.15053"]}
{"year":"2021","title":"Fast and Compact Set Intersection through Recursive Universe Partitioning","authors":["GE Pibiri"],"snippet":"… CCNews is a dataset of news freely available from CommonCrawl; precisely, it consists of the news appeared from 09/01/16 to 30/03/18. Identifiers were assigned to documents according to the lexicographic order of their URLs …","url":["http://pages.di.unipi.it/pibiri/papers/DCC21.pdf"]}
{"year":"2021","title":"Fast Extraction of Word Embedding from Q-contexts","authors":["J Kong, W Li, Z Liu, B Liao, J Qiu, CY Hsieh, Y Cai… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. Fast Extraction of Word Embedding from Q-contexts Junsheng Kong ∗† School of Software Engineering, South China University of Technology China sescut_kongjunsheng@mail.scut.edu.cn Weizhao Li ∗† School …","url":["https://arxiv.org/pdf/2109.07084"]}
{"year":"2021","title":"FAST: A carefully sampled and cognitively motivated dataset for distributional semantic evaluation","authors":["S Evert, G Lapesa"],"snippet":"… Embeddings: word2vec (Mikolov et al., 2013) trained on 100G tokens of Google News data,10 GloVe (Pennington et al., 2014) trained on 6G to- kens of wikipedia + newspapers or 42G tokens of Common Crawl (CC) Web …","url":["https://files.de-1.osf.io/v1/resources/cd8ar/providers/osfstorage/613cdcae079a0a027601be5b?action=download&direct&version=1"]}
{"year":"2021","title":"Faster Re-translation Using Non-Autoregressive Model For Simultaneous Neural Machine Translation","authors":["H Han, S Indurthi, MA Zaidi, NK Lakumarapu, B Lee… - arXiv preprint arXiv …, 2020"],"snippet":"… We choose the best system based on the MuST-C dev set and report the results on the MuST-C tst-COMMON test set. The WMT19 dataset further consists of Europarl v9, ParaCrawl v3, Common Crawl, News Commentary …","url":["https://arxiv.org/pdf/2012.14681"]}
{"year":"2021","title":"fastText-based methods for Emotion Identification in Russian Internet Discourse","authors":["A Babii, M Kazyulina, A Malafeev - 13th ACM Web Science Conference 2021, 2021"],"snippet":"… 0.8. The only exception is the model based on the GRU architecture and pre-trained on the Common Crawl. That … 0.81. The lowest performance was shown by the CNN model pre-trained on the Common Crawl embeddings. We …","url":["https://dl.acm.org/doi/abs/10.1145/3447535.3462499"]}
{"year":"2021","title":"FEEL-IT: Emotion and Sentiment Classification for the Italian Language","authors":["F Bianchi, D Nozza, D Hovy - Proceedings of the Eleventh Workshop on …, 2021"],"snippet":"… In this paper, we use the Italian BERT model UmBERTo trained on Commoncrawl ITA.4 As the first experimental condition, we fine-tune the UmBERTo model for the task of emotion classification with the considered training data (UmBERTo-FT) …","url":["https://www.aclweb.org/anthology/2021.wassa-1.8.pdf"]}
{"year":"2021","title":"Felix Hamborg, Philipp Meschenmoser, Moritz Schubotz","authors":["P Scharpf, B Gipp - Wahrheit und Fake im postfaktisch-digitalen Zeitalter"],"snippet":"… These filter criteria are passed to news-please, which subsequently searches for relevant news articles in the Common Crawl News … 2 News-please currently accesses the raw archive provided by the Common Crawl project. To speed up the …","url":["https://link.springer.com/content/pdf/10.1007/978-3-658-32957-0.pdf#page=164"]}
{"year":"2021","title":"Feminist Curation of Text for Data-centric AI","authors":["M Bartl, S Leavy"],"snippet":"Abstract Language models are becoming increasingly central to artificial intelligence through their use in online search, recommendation engines and language generation technologies. However, concepts of gender can be deeply embedded in …","url":["https://datacentricai.org/papers/79_CameraReady_DCAI_Workshop_NeurIPS_2021_final.pdf"]}
{"year":"2021","title":"Few-shot Learning with Multilingual Language Models","authors":["XV Lin, T Mihaylov, M Artetxe, T Wang, S Chen… - arXiv preprint arXiv …, 2021"],"snippet":"… Our models are trained on a static multilingual corpus extracted from CommonCrawl, with English text comprising 32.6% of the total number … As such we also note the potential difference in genres between CommonCrawl and the genres used in GPT-3 …","url":["https://arxiv.org/pdf/2112.10668"]}
{"year":"2021","title":"FH-SWF SG at GermEval 2021: Using Transformer-Based Language Models to Identify Toxic, Engaging, & Fact-Claiming Comments","authors":["C Gawron, S Schmidt - arXiv preprint arXiv:2109.02966, 2021"],"snippet":"… Philipp Reissel and Philip May have published both a German ELECTRA model (Reissel and May, 2020) and a “German colossal, cleaned Common Crawl corpus” (GC4) (Reissel and May, 2021) with about 540 GB of German …","url":["https://arxiv.org/pdf/2109.02966"]}
{"year":"2021","title":"Findings of the WMT 2021 Biomedical Translation Shared Task: Summaries of Animal Experiments as New Test Set","authors":["L Yeganova, D Wiemann, M Neves, F Vezzani, A Siu… - Sixth Conference on …, 2021"],"snippet":"In the sixth edition of the WMT Biomedical Task, we addressed a total of eight language pairs, namely English/German, English/French, English/Spanish, English/Portuguese, English/Chinese, English/Russian, English/Italian, and English/Basque. Further, our …","url":["https://hal.archives-ouvertes.fr/hal-03435096/document"]}
{"year":"2021","title":"Fine and Coarse Granular Argument Classification before Clustering","authors":["L Dumani, T Wiesenfeldt, R Schenkel - Proceedings of the 30th ACM International …, 2021"],"snippet":"… This dataset consists of eight topics each with 1,000 usergenerated sentences originating from the Common Crawl archive.A key difference to our dataset, besides presumably different appearing frames as well as topics being used as queries, is …","url":["https://dl.acm.org/doi/abs/10.1145/3459637.3482431"]}
{"year":"2021","title":"Fine-tuning Distributional Semantic Models for Closely-Related Languages","authors":["K Bhatia, D Aggarwal, A Vaidya - Proceedings of the Eighth Workshop on NLP for …, 2021"],"snippet":"… We further compare our fine-tuned models with two pre-trained word embedding models, namely fastText (FT-WC), trained on Common Crawl and Wikipedia (Grave et al., 2018) and IndicFastText (I- FT) (Kakwani et al., 2020) …","url":["https://www.aclweb.org/anthology/2021.vardial-1.7.pdf"]}
{"year":"2021","title":"FinMatcher at FinSim-2: Hypernym Detection in the Financial Services Domain using Knowledge Graphs","authors":["J Portisch, M Hladik, H Paulheim - arXiv preprint arXiv:2103.01576, 2021"],"snippet":"… DBpedia [10] – but instead on the whole Web: The dataset consists of hypernymy relations extracted from the Common Crawl5, a … 3see https://www.wikidata.org/wiki/Q1881843 4see https://query.wikidata.org …","url":["https://arxiv.org/pdf/2103.01576"]}
{"year":"2021","title":"Focused Attention Improves Document-Grounded Generation","authors":["S Prabhumoye, K Hashimoto, Y Zhou, AW Black… - arXiv preprint arXiv …, 2021"],"snippet":"… 4.1 Wikipedia Update Generation This task involves generating an update for Wikipedia context given a news article (Prabhumoye et al., 2019). The dataset was collected by parsing Wikipedia articles and Common Crawl for news articles …","url":["https://arxiv.org/pdf/2104.12714"]}
{"year":"2021","title":"FoodChem: A food-chemical relation extraction model","authors":["G Cenikj, BK Seljak, T Eftimov - arXiv preprint arXiv:2110.02019, 2021"],"snippet":"… Apart from the datasets used for pre-training the original BERT model, 3 additional sources are involved in the pre-training of RoBERTa: the OpenWebText corpus [32], the Stories subset from the Common Crawl dataset [33] and the CommonCrawl News dataset [34] …","url":["https://arxiv.org/pdf/2110.02019"]}
{"year":"2021","title":"Four dimensions characterize attributions from faces using a representative set of English trait words","authors":["C Lin, U Keles, R Adolphs - Nature Communications, 2021"],"snippet":"People readily (but often inaccurately) attribute traits to others based on faces. While the details of attributions depend on the language available to describe social traits, psychological theories argue that two or three dimensions (such …","url":["https://www.nature.com/articles/s41467-021-25500-y"]}
{"year":"2021","title":"From Argument Search to Argumentative Dialogue: A Topic-independent Approach to Argument Acquisition for Dialogue Systems","authors":["N Rach, C Schindler, I Feustel, J Daxenberger…"],"snippet":"… query. The engine utilizes a web crawl from the year 2016 based on CommonCrawl 2 to retrieve relevant documents and subsequently classify sentences in the documents as either pro, con or no argument (Stab et al., 2018) …","url":["https://sigdial.org/sites/default/files/workshops/conference22/Proceedings/pdf/2021.sigdial-1.39.pdf"]}
{"year":"2021","title":"From OpenAPI Fragments to API Pattern Primitives and Design Smells","authors":["S Serbout, C Pautasso, U Zdun, O Zimmermann - 2021"],"snippet":"In the past few years, the OpenAPI Specification (OAS) has emerged as a standard description language for accurately modeling Web APIs. Today, thousands of OpenAPI descriptions can be found by mining open source repositories. In this paper …","url":["http://eprints.cs.univie.ac.at/7145/1/main.pdf"]}
{"year":"2021","title":"gaBERT--an Irish Language Model","authors":["J Barry, J Wagner, L Cassidy, A Cowap, T Lynn… - arXiv preprint arXiv …, 2021"],"snippet":"… texts, and official documents. OSCAR The unshuffled Irish portion of the OS- CAR Corpus (Ortiz Suárez et al., 2019) which is a collection of documents from CommonCrawl and sorted by language ID. ParaCrawl The Irish portion …","url":["https://arxiv.org/pdf/2107.12930"]}
{"year":"2021","title":"Gaze-based Multimodal Meaning Recovery for Noisy/Complex Environments","authors":["Ö Alaçam, G Malhotra, E Ruppert, C Biemann - Proceedings of the 2021 International …, 2021"],"snippet":"… In addition, we also utilized xlm-roberta-base masked language model, which is a large multi-lingual language model, trained on 2.5 TB of filtered CommonCrawl data [8]. Unlike multilingual BERT and RoBERTa, XLM-RoBERTa can infer the language of …","url":["https://dl.acm.org/doi/abs/10.1145/3462244.3481002"]}
{"year":"2021","title":"Gender Bias in English and German Children's Literature: A Computational Analysis Using Word Embeddings","authors":["D Geißler - 2021"],"snippet":"Page 1. Gender Bias in English and German Children's Literature: A Computational Analysis Using Word Embeddings MASTER THESIS Dominique Geißler Master Interaction Technology Faculty of Electrical Engineering, Mathematics …","url":["http://essay.utwente.nl/88570/1/Geissler_MA_EEMCS.pdf"]}
{"year":"2021","title":"Gender Stereotypes in Natural Language: Word Embeddings Show Robust Consistency Across Child and Adult Language Corpora of More Than 65 Million Words","authors":["TES Charlesworth, V Yang, TC Mann, B Kurdi… - Psychological Science, 2020"],"snippet":"… In this case, lemmatization is particularly helpful because the target corpora are smaller than other natural-language corpora, such as the Common Crawl corpus (which we use for validation as described below) with more than 600 billion word tokens (Mikolov, Grave …","url":["https://journals.sagepub.com/doi/abs/10.1177/0956797620963619"]}
{"year":"2021","title":"Generalization of cyberbullying traces","authors":["MA Larochelle - 2020"],"snippet":"Page 1. © Marc-André Larochelle, 2020 Generalization of Cyberbullying Traces Mémoire Marc-André Larochelle Maîtrise en informatique - avec mémoire Maître ès sciences (M. Sc.) Québec, Canada Page 2. Generalization of Cyberbullying Traces Mémoire …","url":["https://corpus.ulaval.ca/jspui/bitstream/20.500.11794/67899/1/36823.pdf"]}
{"year":"2021","title":"Generalized zero-shot object recognition withoutclass-attribute relations","authors":["M Er - 2021"],"snippet":"Page 1. GENERALIZED ZERO-SHOT OBJECT RECOGNITION WITHOUT CLASS-ATTRIBUTE RELATIONS A THESIS SUBMITTED TO THE GRADUATE SCHOOL OF NATURAL AND APPLIED SCIENCES OF MIDDLE EAST …","url":["https://open.metu.edu.tr/bitstream/handle/11511/89793/12626239.pdf"]}
{"year":"2021","title":"Generating Sense Inventories for Ambiguous Arabic Words","authors":["M Alian, A Awajan"],"snippet":"… 4.3. Fasttext Pre-Trained Embeddings Arabic Fasttext embeddings are provided by Grave et al. [12]. These embeddings result from training on Wikipedia and Common Crawl corpus. They use an extension of the Fasttext model with subword information …","url":["https://www.researchgate.net/profile/Marwah-Alian/publication/351762403_Generating_Sense_Inventories_for_Ambiguous_Arabic_Words/links/60b9d6f192851cb13d7442ab/Generating-Sense-Inventories-for-Ambiguous-Arabic-Words.pdf"]}
{"year":"2021","title":"Generative Adversarial Networks based on Mixed-Attentions for Citation Intent Classification in Scientific Publications","authors":["YS Wang, CY Chen, LH Lee - Proceedings of the 33rd Conference on Computational …, 2021"],"snippet":"Abstract We propose the mixed-attention-based Generative Adversarial Network (named maGAN), and apply it for citation intent classification in scientific publication. We select domain-specific training data, propose a mixed-attention mechanism, and …","url":["https://aclanthology.org/2021.rocling-1.36.pdf"]}
{"year":"2021","title":"Generative Chatbot Framework for Cybergrooming Prevention","authors":["P Wang - 2021"],"snippet":"Cybergrooming refers to the crime of establishing personal close relationships with potential victims, commonly teens, for the purpose of sexual exploitation or abuse via online social media platforms. Cybergrooming has been recognized as a serious …","url":["https://vtechworks.lib.vt.edu/bitstream/handle/10919/107136/Wang_P_T_2021.pdf?sequence=1&isAllowed=y"]}
{"year":"2021","title":"Generative Question Answering in a Low-Resource Setting","authors":["L Isotalo"],"snippet":"… While the monolingual T5 was pre-trained on the C4 data set which consists of 750GB of English text from Common Crawl, mT5 has been pre-trained on the data likewise collected from Common Crawl but in over 100 languages …","url":["https://dke.maastrichtuniversity.nl/jan.niehues/wp-content/uploads/2021/07/Isotalo-Thesis.pdf"]}
{"year":"2021","title":"Generic Key Value Extractions from Emails","authors":["R Gupta - International Conference on Big Data Analytics, 2020"],"snippet":"… sentences. Authors of [14] describe a machine learning-based model trained on common-crawl data with multi-task optimizations to represent each keyword phrase using a 500-dimension vector of real numbers. Cosine similarity …","url":["https://link.springer.com/chapter/10.1007/978-3-030-66665-1_13"]}
{"year":"2021","title":"GenSumm: A Joint Framework for Multi-task Tweet Classification and Summarization using Sentiment Analysis and Generative Modelling","authors":["D Bansal, R Grover, N Saini, S Saha - IEEE Transactions on Affective Computing, 2021"],"snippet":"Social media platforms like Twitter act as a medium for communication among people, government agencies, NGOs, and other relief providing agencies in widespread humanitarian havoc during a disaster outbreak when other …","url":["https://ieeexplore.ieee.org/abstract/document/9629222/"]}
{"year":"2021","title":"Geoscience Language Processing for Exploration","authors":["H Denli, HA Chughtai, B Hughes, R Gistri, P Xu - Abu Dhabi International Petroleum …, 2021"],"snippet":"Deep learning has recently been providing step-change capabilities, particularly using transformer models, for natural language processing applications such as question answering, query-based summarization, and language translation for …","url":["https://onepetro.org/SPEADIP/proceedings-abstract/21ADIP/3-21ADIP/D031S102R003/474196"]}
{"year":"2021","title":"GeoVectors: A Linked Open Corpus of OpenStreetMap Embeddings on World Scale","authors":["N Tempelmeier, S Gottschalk, E Demidova - arXiv preprint arXiv:2108.13092, 2021"],"snippet":"… them. Training: Pre-trained word vectors are available at the fastText website6. As most of the OSM keys are in English, we chose the 300dimensional English word vectors trained on the Common Crawl, and Wikipedia [10] …","url":["https://arxiv.org/pdf/2108.13092"]}
{"year":"2021","title":"German Speech Recognition System using DeepSpeech","authors":["J Xu, K Matta, S Islam, A Nürnberger - … of the 4th International Conference on Natural …, 2020"],"snippet":"… In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 4774–4778. [5] Common crawl 2020. Language model. http://commoncrawl org/. [6] DeepSpeech Deutsch 2020. German dataset of Mozilla Common voice …","url":["https://dl.acm.org/doi/abs/10.1145/3443279.3443313"]}
{"year":"2021","title":"GFST: Gender-Filtered Self-Training for More Accurate Gender in Translation","authors":["PK Choubey, A Currey, P Mathur, G Dinu…"],"snippet":"Page 1. GFST: Gender-Filtered Self-Training for More Accurate Gender in Translation Prafulla Kumar Choubey∗† Texas A&M University prafulla.choubey@tamu.edu Anna Currey∗, Prashant Mathur, Georgiana Dinu Amazon …","url":["https://assets.amazon.science/db/c9/8f55d40b417ebc2266c4fb81d90f/gfst-gender-filtered-self-training-for-more-accurate-gender-in-translation.pdf"]}
{"year":"2021","title":"GitTables: A Large-Scale Corpus of Relational Tables","authors":["M Hulsebos, Ç Demiralp, P Groth - 2021","MHS Computing, ÇDS Computing"],"snippet":"… 2 Related work Web initiatives such as Common Crawl, Wikipedia, and Open Data have been cost-effective resources for curating unstructured and structured data at scale [26, 3, 27] … We use the character-level n-gram FastText …","url":["https://ar5iv.labs.arxiv.org/pdf/2106.07258.pdf","https://openreview.net/pdf?id=yYQuqGcxFvb"]}
{"year":"2021","title":"Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer","authors":["R Powalski, Ł Borchmann, D Jurkiewicz, T Dwojak… - arXiv preprint arXiv …, 2021"],"snippet":"… Library ∗ , and PDF files from Common Crawl. Then, a T5-like masked language model pretraining objective is used, but in a salient span masking scheme, ie, named entities are preferred rather than random tokens [47,15] …","url":["https://arxiv.org/pdf/2102.09550"]}
{"year":"2021","title":"Golos: Russian Dataset for Speech Research","authors":["N Karpov, A Denisenko, F Minkin - arXiv preprint arXiv:2106.10161, 2021"],"snippet":"… language. Additionally, we built a 3-gram language model on the open Common Crawl dataset and merged it with the train set transcriptions. Using … 8. 7https:// commoncrawl.org 8https://github.com/sberdevices/golos Page 5. 8 …","url":["https://arxiv.org/pdf/2106.10161"]}
{"year":"2021","title":"Governments and the Net: Defense, Control, and Trust in the Fifth Domain","authors":["L Kawerau - 2021"],"snippet":"… The by far largest data source is the Common Crawl archive. Every four weeks, Common Crawl generates a new dataset that contains content and metadata of billions of webpages based on following the links on a large sample of websites …","url":["http://kops.uni-konstanz.de/bitstream/handle/123456789/55972/Kawerau_2-176y1g4fs6pvh0.pdf?sequence=5&isAllowed=y"]}
{"year":"2021","title":"GPT Perdetry Test: Generating new meanings for new words","authors":["N Malkin, S Lanka, P Goel, S Rao, N Jojic - Proceedings of the 2021 Conference of …, 2021"],"snippet":"… Most of the words were known by none of them. 36 of the 74 words do not appear in the top 2 million words of the Common Crawl corpus, according to the GloVe embedding matrix (Pennington et al., 2014), and the median rank of the other words is 656565 …","url":["https://www.aclweb.org/anthology/2021.naacl-main.439.pdf"]}
{"year":"2021","title":"GPT3-to-plan: Extracting plans from text using GPT-3","authors":["A Olmo, S Sreedharan, S Kambhampati - arXiv preprint arXiv:2106.07131, 2021"],"snippet":"… 2020) is the latest version of the GPT models developed by OpenAI1. A 175 billion parameter autoregressive language model with 96 layers trained on a 560GB+ web corpora (Common Crawl2 and WebText2 (Gokaslan …","url":["https://arxiv.org/pdf/2106.07131"]}
{"year":"2021","title":"Grammar Index By Induced Suffix Sorting","authors":["H Bannai, M Takeda - String Processing and Information Retrieval: 28th …","T Akagi, D Köppl, Y Nakashima, S Inenaga, H Bannai… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. arXiv:2105.13744v1 [cs.DS] 28 May 2021 Grammar Index By Induced Suffix Sorting Tooru Akagi Dominik Köppl Yuto Nakashima Shunsuke Inenaga Hideo Bannai Masayuki Takeda May 31, 2021 Abstract …","url":["https://arxiv.org/pdf/2105.13744","https://books.google.de/books?hl=en&lr=lang_en&id=gEdFEAAAQBAJ&oi=fnd&pg=PA85&dq=commoncrawl&ots=0CKdrg1l4k&sig=X0BiUyMEx7vsbj3O2GcRTMn6Cms"]}
{"year":"2021","title":"Graph-based KB and Text Fusion Interaction Network for Open Domain Question Answering","authors":["Y Ding, Y Rao, F Yang - 2021 International Joint Conference on Neural …, 2021"],"snippet":"… C. Implementation Details Through our experiments, the 300-dimension GloVe embedding trained on the Common Crawl corpus is used to initialize the word embedding. The hidden dimension of LSTM and the dimension of entity embedding are both 100 …","url":["https://ieeexplore.ieee.org/abstract/document/9534439/"]}
{"year":"2021","title":"Grounding Linguistic Commands to Navigable Regions","authors":["N Rufus, K Jain, UKR Nair, V Gandhi, KM Krishna - 2021"],"snippet":"… Input images are resized to 448 × 448 spatial resolution. We use 300d GloVe embeddings pre-trained on Common Crawl 840B tokens [22]. The maximum length of commands is set to T = 40 and for both visual and linguistic features, channel dimension C = 512 …","url":["https://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b947b8b2836d208.49524f5332315f313135365f46492e706466.pdf"]}
{"year":"2021","title":"H2oloo at TREC 2020: When all you got is a hammer... Deep Learning, Health Misinformation, and Precision Medicine","authors":["R Pradeep, X Ma, X Zhang, H Cui, R Xu, R Nogueira… - Corpus"],"snippet":"… ranking. We adopt all the default settings. We index all the news articles found in the CommonCrawl News crawl from January 1st, 2020 to April 30th, 2020. At inference time, we retrieve the top-k0 (=1000) documents per query …","url":["https://trec.nist.gov/pubs/trec29/papers/h2oloo.DL.HM.PM.pdf"]}
{"year":"2021","title":"HAHA at EmoEvalEs 2021: Sentiment Analysis in Spanish Tweets with Cross-lingual Model","authors":["K Li - Proceedings of the Iberian Languages Evaluation …, 2021"],"snippet":"… It is a large multi-lingual language model, trained on 2.5TB of filtered Common Crawl data … Because the XLM-RoBERTa model increases the number of languages and the number of training data sets, it uses more …","url":["http://ceur-ws.org/Vol-2943/emoeval_paper5.pdf"]}
{"year":"2021","title":"Harmonized System Code Classification Using Transfer Learning with Pre-Trained Weights","authors":["K Pain - 2021"],"snippet":"Page 1. HARMONIZED SYSTEM CODE CLASSIFICATION USING TRANSFER LEARNING WITH PRE-TRAINED WEIGHTS by Koustav Pain (Tukai) Submitted in partial fulfillment of the requirements for the degree of Master of Computer Science at …","url":["https://dalspace.library.dal.ca/bitstream/handle/10222/80672/KoustavPain2021.pdf?sequence=1&isAllowed=y"]}
{"year":"2021","title":"Hate Speech Detection based on Sentiment Knowledge Sharing","authors":["X Zhou, Y Yang, X Fan, G Ren, Y Song, Y Diao, L Yang…"],"snippet":"… To compare with previous works, We report results of DV using the standard Accuracy and weighted F1. In our experiments, for the input layer, all word vectors are initialized by Glove Common Crawl Embeddings (840B Token), and the dimension is 300 …","url":["https://aclanthology.org/2021.acl-long.556.pdf"]}
{"year":"2021","title":"HDRS: Hindi Dialogue Restaurant Search Corpus for Dialogue State Tracking in Task-Oriented Environment","authors":["S Malviya, R Mishra, SK Barnwal, US Tiwary - IEEE/ACM Transactions on Audio …, 2021"],"snippet":"Page 1. 2329-9290 (c) 2021 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/9376978/"]}
{"year":"2021","title":"HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish","authors":["R Mroczkowski, P Rybak, A Wróblewska, I Gawlik - … of the 8th Workshop on Balto …, 2021"],"snippet":"… CCNet (Wenzek et al., 2020) is a clean monolingual corpus extracted from Common Crawl6 dataset of crawled websites … The first one did not use pretrained embeddings while the latter utilized fastText (Bo- janowski …","url":["https://www.aclweb.org/anthology/2021.bsnlp-1.1.pdf"]}
{"year":"2021","title":"Hierarchical Ranking for Answer Selection","authors":["H Gao, M Hu, R Cheng, T Gao - arXiv preprint arXiv:2102.00677, 2021"],"snippet":"Page 1. Hierarchical Ranking for Answer Selection Hang Gao, Mengting Hu, Renhong Cheng, Tiegang Gao Nankai University knimet@mail.nankai. edu.cn Abstract Answer selection is a task to choose the positive answers …","url":["https://arxiv.org/pdf/2102.00677"]}
{"year":"2021","title":"HIGH PERFORMANCE DOCUMENT STORE IMPLEMENTATION IN RUST","authors":["I Aggarwal - 2021"],"snippet":"Databases are a core part of any application which requires persistence of data. The performance of applications involving the use of database systems is directly proportional to how fast their database read-write operations are. The aim of this …","url":["https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=2044&context=etd_projects"]}
{"year":"2021","title":"HISTOGRAM OF TASK UNCERTAINTY","authors":["QL SCHEDULER, MT LEARNING"],"snippet":"… 2014) word embedding (300d version on 840B Common Crawl data), and Xavier initialization for the parameters. The mini-batch size is set to 16, including samples of the same task. Other than these, we follow the training procedure for (Søgaard …","url":["https://openreview.net/pdf?id=sHUFhv03qX_"]}
{"year":"2021","title":"HmBlogs: A big general Persian corpus","authors":["HM Khansari, M Shamsfard - arXiv preprint arXiv:2111.02362, 2021"],"snippet":"… The main part of PRT is obtained from the Persian section of the Common Crawl project (Common Crawl, nd). Common Crawl is a web crawl project that crawls and collects resources available on the web in any language, including some in Persian …","url":["https://arxiv.org/pdf/2111.02362"]}
{"year":"2021","title":"HONEST: Measuring hurtful sentence completion in language models","authors":["D Nozza, F Bianchi, D Hovy - Proceedings of the 2021 Conference of the North …, 2021"],"snippet":"Page 1. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2398–2406 June 6–11, 2021. ©2021 Association for Computational Linguistics 2398 …","url":["https://www.aclweb.org/anthology/2021.naacl-main.191.pdf"]}
{"year":"2021","title":"Hope Speech detection in under-resourced Kannada language","authors":["A Hande, R Priyadharshini, A Sampath, KP Thamburaj… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. Advances in Computational Intelligence manuscript No. (will be inserted by the editor) Hope Speech detection in under-resourced Kannada language Adeep Hande · Ruba Priyadharshini · Anbukkarasi Sampath · Kingston …","url":["https://arxiv.org/pdf/2108.04616"]}
{"year":"2021","title":"How do blind people know that blue is cold? Distributional semantics encode color-adjective associations","authors":["J van Paridon, Q Liu, G Lupyan - PsyArXiv. February, 2021"],"snippet":"… trained on. Using the projection method, we computed color associations in embeddings trained on four different text corpora: The Common Crawl and Wikipedia (Grave, Bojanowski, Gupta, Joulin, & Page 2. Mikolov, 2018 …","url":["https://psyarxiv.com/vyxpq/download/?format=pdf"]}
{"year":"2021","title":"How Do Simple Transformations of Text and Image Features Impact Cosine-Based Semantic Match?","authors":["G Collell, MF Moens - ECIR (1), 2021"],"snippet":"… 1. Word-level features: – GloVe6 [40]: We use 300-d vectors pre-trained on the Common Crawl corpus with 840B tokens and a 2.2M-word vocabulary. – word2vec (w2v) [37]: We use the skip-gram 300-d embeddings trained on Wikipedia …","url":["https://www.researchgate.net/profile/Guillem-Collell/publication/350427435_How_Do_Simple_Transformations_of_Text_and_Image_Features_Impact_Cosine-Based_Semantic_Match/links/60851cdd907dcf667bc0b6a8/How-Do-Simple-Transformations-of-Text-and-Image-Features-Impact-Cosine-Based-Semantic-Match.pdf"]}
{"year":"2021","title":"How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task","authors":["R Aralikatte, HRM Bello, M de Lhoneux, D Hershcovich…"],"snippet":"… translation task. It is pre-trained on publicly available corpora from Wikipedia and Common Crawl. It … tion task. This model is pre-trained on mC4, a multilingual version of the Common Crawl consisting of text from 101 languages. It …","url":["https://aclanthology.org/2021.wat-1.24.pdf"]}
{"year":"2021","title":"How good BERT based models are in sentiment analysis of Croatian tweets: comparison of four multilingual BERTs","authors":["M Ptiček"],"snippet":"… It is pre-trained on texts on 100 different languages and more than two terabytes of data filtered from CommonCrawl data. As stated by the authors “XLM-RoBERTa (XLM-R) outperforms mBERT on cross-lingual classification by up to 23% accuracy on low-resource …","url":["http://archive.ceciis.foi.hr/app/public/conferences/2021/Proceedings/IS/IS1.pdf"]}
{"year":"2021","title":"How Great is the Great Firewall? Measuring China's DNS Censorship","authors":["NP Hoang, AA Niaki, J Dalek, J Knockel, P Lin… - arXiv preprint arXiv …, 2021"],"snippet":"… nsarchive.gwu.edu, cs.colorado.edu) are censored but their SLDs (eg, mit.edu, gwu.edu, colorado.edu) are not. We complement our test list by including domains from the Citizen Lab test lists (CLTL) [13], the Tranco list [66], and the Common Crawl project [14] …","url":["https://arxiv.org/pdf/2106.02167"]}
{"year":"2021","title":"How much is a cow like a meow? A novel database of human judgements of","authors":["K Wegner-Clemens, GL Malcolm, S Shomstein - Psychological Science"],"snippet":"… This model was trained on a total of 650 billion words including Wikipedia from June 2017, two news corpuses (statmt.org news, UMBC news), and corpuses derived from a wide range of websites (Gigagword, Common Crawl). The words …","url":["https://psyarxiv.com/7h82c/download/?format=pdf"]}
{"year":"2021","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages","authors":["A Ebrahimi, K Kann - arXiv preprint arXiv:2106.02124, 2021"],"snippet":"… 2018) who present embeddings for 157 languages trained on additional data from Common Crawl … the larger vocabulary of 250k subwords created using SentencePiece tokenization (Kudo and Richardson, 2018) and the …","url":["https://arxiv.org/pdf/2106.02124"]}
{"year":"2021","title":"How Viable is Password Cracking in Digital Forensic Investigation? Analyzing the Guessability of over 3.9 Billion Real-World Accounts","authors":["A Kanta, S Coray, I Coisel, M Scanlon - Forensic Science International: Digital …, 2021"],"snippet":"… GloVe [34] was used to automate this process with its Common Crawl 42B 300d, a pre-trained model in English for GloVe15 … This is because they do not have a representation in the Common Crawl and therefore cannot be compared to the categories …","url":["https://www.forensicsandsecurity.com/papers/PasswordCracking3BillionAccounts.pdf"]}
{"year":"2021","title":"How Vulnerable Are Automatic Fake News Detection Methods to Adversarial Attacks?","authors":["C Koenders, J Filla, N Schneider, V Woloszyn - arXiv preprint arXiv:2107.07970, 2021"],"snippet":"Page 1. How Vulnerable Are Automatic Fake News Detection Methods to Adversarial Attacks? Camille Koenders Technische Universität Berlin camille.koenders@gmail.com Johannes Filla Technische Universität Berlin johannesfilla@gmx.de …","url":["https://arxiv.org/pdf/2107.07970"]}
{"year":"2021","title":"HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow Articles","authors":["O Boni, G Feigenblat, G Lev, M Shmueli-Scheuer… - arXiv preprint arXiv …, 2021"],"snippet":"… a corpus. They es- tablish this approach by extracting answers that are Wikipedia paragraphs from Google Natural Questions (Kwiatkowski et al., 2019) and documents from Common Crawl (Raffel et al., 2020) corpus. In some …","url":["https://arxiv.org/pdf/2110.03179"]}
{"year":"2021","title":"HTLM: Hyper-Text Pre-Training and Prompting of Language Models","authors":["A Aghajanyan, D Okhonko, M Lewis, M Joshi, H Xu… - arXiv preprint arXiv …, 2021"],"snippet":"… Hyper-text, such as the HTML found in the Common Crawl1, has a number of advantages for pretraining over plain text … Our HyperText Language Model (HTLM) is trained on 23TB of simplified HTML which we automatically extract …","url":["https://arxiv.org/pdf/2107.06955"]}
{"year":"2021","title":"HULTIG-C: NLP Corpus and Services in the Cloud","authors":["S Pais, J Cordeiro, M Jamil - 2021"],"snippet":"… [11] presented a Multilingual Web Corpus in over 50 languages with over 10 billion tokens licensed under the Creative Commons license. The texts that made up the corpus were extracted from Common Crawl, with around 2 billion crawled URLs …","url":["https://www.researchsquare.com/article/rs-696114/latest.pdf"]}
{"year":"2021","title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter","authors":["F Alam, U Qazi, M Imran, F Ofli - arXiv preprint arXiv:2104.03090, 2021"],"snippet":"Page 1. HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks Firoj Alam, Umair Qazi, Muhammad Imran, Ferda Ofli Qatar Computing Research Institute, HBKU, Qatar {fialam,uqazi,mimran,fofli}@hbku.edu.qa …","url":["https://arxiv.org/pdf/2104.03090"]}
{"year":"2021","title":"Human and Transformer-Based Prosodic Phrasing in Two Speech Genres","authors":["J Volín, M Řezáčková, J Matouřek - International Conference on Speech and …, 2021"],"snippet":"… We worked with the pre-trained Text-to-Text TransferTransformer (T5) model [18] and used the ability of weights pre-training from a huge amount of unlabeled text in the language [4, 18] – for this task we used Czech …","url":["https://link.springer.com/chapter/10.1007/978-3-030-87802-3_68"]}
{"year":"2021","title":"Human-Centered Financial Summarization","authors":["T PASSALI - 2021"],"snippet":"Page 1. ARISTOTLE UNIVERSITY OF THESSALONIKI MASTER'S THESIS Human-Centered Financial Summarization Author: Tatiana PASSALI Supervisor: Dr. Grigorios TSOUMAKAS A thesis submitted in fulfillment of …","url":["http://ikee.lib.auth.gr/record/329076/files/GRI-2021-30128.pdf"]}
{"year":"2021","title":"Hybrid approach to detecting symptoms of depression in social media entries","authors":["A Wołk, K Chlasta, P Holas - arXiv preprint arXiv:2106.10485, 2021"],"snippet":"… 2004). To conduct the experiments, we trained our model on the data from the Common Crawl (Bevendorff et al. 2018) project. As the CommonCrawl corpus is very polluted, an algorithmic division into sentences, data de-duplication, and cleaning was necessary …","url":["https://arxiv.org/pdf/2106.10485"]}
{"year":"2021","title":"I Wish I Would Have Loved This One, But I Didn't--A Multilingual Dataset for Counterfactual Detection in Product Reviews","authors":["J O'Neill, P Rozenshtein, R Kiryo, M Kubota… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. arXiv:2104.06893v1 [cs.CL] 14 Apr 2021 I Wish I Would Have Loved This One, But I Didn't – A Multilingual Dataset for Counterfactual Detection in Product Reviews James O'Neill‡,∗∗Polina Rozenshtein†,∗ Ryuichi …","url":["https://arxiv.org/pdf/2104.06893"]}
{"year":"2021","title":"IBM MNLP IE at CASE 2021 task 1: Multi-granular and multilingual event detection on protest news","authors":["P Awasthy, J Ni, K Barker, R Florian - Proceedings of the 4th Workshop on …, 2021"],"snippet":"… Hence we decided to use XLM-R as the text encoder. We use HuggingFace's pytorch im- plementation of transformers (Wolf et al., 2019). XLM-R was pre-trained with unlabeled Wikipedia text and the CommonCrawl Corpus of 100 languages …","url":["https://aclanthology.org/2021.case-1.18.pdf"]}
{"year":"2021","title":"iCompass at NLP4IF-2021–Fighting the COVID-19 Infodemic","authors":["W Henia, O Rjab, H Haddad, C Fourati - Proceedings of the Fourth Workshop on NLP …, 2021"],"snippet":"… Page 3. 117 3.4 Arabic ALBERT Arabic ALBERT2 by (KUIS-AI-Lab) models were pretrained on 4.4 Billion words: Arabic version of OSCAR (unshuffled version of the corpus) filtered from Common Crawl and Recent dump of Arabic Wikipedia …","url":["https://www.aclweb.org/anthology/2021.nlp4if-1.17.pdf"]}
{"year":"2021","title":"Identification of Biased Terms in News Articles by Comparison of Outlet-specific Word Embeddings","authors":["T Spinde, L Rudnitckaia, F Hamborg"],"snippet":"… We scraped articles from both news outlets, published in the last decade, from 2010 to 2020, from Common Crawl [4]. For preprocessing, we use Genism simple preprocessing and generate n-grams. 3.4 Linear mapping between vector spaces …","url":["https://www.researchgate.net/profile/Timo_Spinde2/publication/348303058_Identification_of_Biased_Terms_in_News_Articles_by_Comparison_of_Outlet-specific_Word_Embeddings/links/5ff70425a6fdccdcb837e519/Identification-of-Biased-Terms-in-News-Articles-by-Comparison-of-Outlet-specific-Word-Embeddings.pdf"]}
{"year":"2021","title":"Identification of Toxic, Engaging, and Fact-Claiming Comments","authors":["J Risch, A Stoll, L Wilms, M Wiegand - 2021"],"snippet":"Page 1. Proceedings of the GermEval 2021 Workshop on the Identification of Toxic, Engaging, and Fact-Claiming Comments 17th Conference on Natural Language Processing KONVENS 2021 Heinrich Heine University Düsseldorf …","url":["https://netlibrary.aau.at/obvukloa/content/titleinfo/6435190/full.pdf"]}
{"year":"2021","title":"Identifying communicative functions in discourse with content types","authors":["T Caselli, R Sprugnoli, G Moretti - Language Resources and Evaluation, 2021"],"snippet":"Texts are not monolithic entities but rather coherent collections of micro illocutionary acts which help to convey a unitary message of content and purpose.","url":["https://link.springer.com/article/10.1007/s10579-021-09554-4"]}
{"year":"2021","title":"Idiomatic Expression Identification using Semantic Compatibility","authors":["Z Zeng, S Bhat - arXiv preprint arXiv:2110.10064, 2021"],"snippet":"Idiomatic expressions are an integral part of natural language and constantly being added to a language. Owing to their non-compositionality and their ability to take on a figurative or literal meaning depending on the sentential context, they have been a …","url":["https://arxiv.org/pdf/2110.10064"]}
{"year":"2021","title":"IIITT@ DravidianLangTech-EACL2021: Transfer Learning for Offensive Language Detection in Dravidian Languages","authors":["K Yasaswini, K Puranik, A Hande, R Priyadharshini… - Proceedings of the First …, 2021"],"snippet":"… 4.3 XLM-RoBERTa XLM-RoBERTa (Conneau et al., 2020) is a large multi-lingual language model, trained on 2.5TB of cleaned CommonCrawl data in 100 languages. It can be recognized as a union of XLM (Lample …","url":["https://www.aclweb.org/anthology/2021.dravidianlangtech-1.25.pdf"]}
{"year":"2021","title":"IIITT@ LT-EDI-EACL2021-Hope Speech Detection: There is always hope in Transformers","authors":["K Puranik, A Hande, R Priyadharshini, S Thavareesan… - arXiv preprint arXiv …, 2021"],"snippet":"… It's trained on over 2TB of filtered CommonCrawl data in 100 different languages. It was an update to the XLM-100 model (Lample and Conneau, 2019) but with in- creased training data … Wikipedia, Common Crawl7, PMINDIA8 and Dakshina9 datasets …","url":["https://arxiv.org/pdf/2104.09066"]}
{"year":"2021","title":"Image Emotion Recognition Using Visual and Semantic Features Reflecting Emotional and Similar Objects","authors":["T YAMAMOTO, S TAKEUCHI, A NAKAZAWA - IEICE TRANSACTIONS on Information …, 2021"],"snippet":"… GloVe has been used for sentiment analysis [40]; therefore, we use GloVe as the method for obtaining the word vector. It has a 300-dimensional word embedding weight pre-trained by Common Crawl (840 billion tokens), which is publicly available [41] …","url":["https://www.jstage.jst.go.jp/article/transinf/E104.D/10/E104.D_2020EDP7218/_pdf"]}
{"year":"2021","title":"Impacts of Homophone Normalization on Semantic Models for Amharic","authors":["TD Belay, E Kombolcha, AA Ayele, G Gelaye…"],"snippet":"… embedding representation. The only available pretrained static word embedding for Amharic text is the fastText model, which is trained from Wikipedia and web data scraped in the common crawl project [19]. This embedding …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/publications/2021-belayetal-ict4da-amharicnorm.pdf"]}
{"year":"2021","title":"Implicit social cognition: A brief (and gentle) introduction","authors":["B Kurdi, MR Banaji - PsyArXiv. January, 2021"],"snippet":"… individual-level implicit attitudes toward ten social categories indexed using the IAT are closely aligned with societal-level semantic associations derived from the vast Common Crawl corpus via so- called word embeddings …","url":["https://psyarxiv.com/a4pjy/download/?format=pdf"]}
{"year":"2021","title":"Implicitly Abusive Comparisons–A New Dataset and Linguistic Analysis","authors":["M Wiegand, M Geulig, J Ruppenhofer - Proceedings of the 16th Conference of the …, 2021"],"snippet":"… A comparison was represented by the embedding vector of the word in that comparison whose similarity was highest to the centroid. As embeddings we chose the fastText emeddings (Joulin et al., 2017) induced on Common Crawl.8 Emotions (EMO) …","url":["https://www.aclweb.org/anthology/2021.eacl-main.27.pdf"]}
{"year":"2021","title":"Improved Automated Classification of Sentences in Data Science Exercises","authors":["AM Angelone, A Galassi, P Vittorini - … in Methodologies and intelligent Systems for …, 2021"],"snippet":"… In our setting, fastText embeddings are vectors of 300 dimensions, BERT embeddings are vectors of 768 dimensions. As for fastText, we adopted the precomputed Italian language model 1 , trained on Common Crawl and Wikipedia …","url":["https://link.springer.com/chapter/10.1007/978-3-030-86618-1_2"]}
{"year":"2021","title":"Improved Word Representations Via Summed Target and Context Embeddings","authors":["N Fulda, N Robinson"],"snippet":"… The word2vec and word2vec-PLUS scores here were trained on the corpus of scraped ar- ticles since it resembles the Common Crawl data used to train GloVe [9] more closely than the other, more specialized corpora we explored …","url":["https://dragn.ai/wp-content/uploads/2021/01/sami2021_39_Nancy-Fulda.pdf"]}
{"year":"2021","title":"Improving Hierarchical Product Classification using Domain-specific Language Modelling","authors":["A Brinkmann, C Bizer - 2021"],"snippet":"… Afterwards, we investigate whether it is possible to improve the performance of the transformer models by performing additional self-supervised pre-training using different corpora of product offers, which were extracted from the Common Crawl …","url":["https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_Research/Web-based_Systems/pub/Brinkmann-Bizer-Improving_Hierarchical_Product_Classification_using_domain_specific_laguage_modelling-PKG4Ecommerce2021.pdf"]}
{"year":"2021","title":"Improving Large-scale Language Models and Resources for Filipino","authors":["JCB Cruz, C Cheng - arXiv preprint arXiv:2111.06053, 2021"],"snippet":"… 2019) is a massive dataset obtained from language identification and filtering of the Common Crawl dataset. We use the deduplicated version of the Filipino (Tagalog) portion of OSCAR and add it to our pretraining corpus. …","url":["https://arxiv.org/pdf/2111.06053"]}
{"year":"2021","title":"Improving Multilingual Models for the Swedish Language: Exploring CrossLingual Transferability and Stereotypical Biases","authors":["S Katsarou - 2021"],"snippet":"… To acquire the natural text needed for pretraining purposes, the authors used common crawl [21]. Common crawl is a non commercial … To justify the choice of Common Crawl the authors say that it is a publicly available repository, allowing any …","url":["https://www.diva-portal.org/smash/get/diva2:1618310/FULLTEXT01.pdf"]}
{"year":"2021","title":"Improving question answering for event-focused questions in temporal collections of news articles","authors":["J Wang, A Jatowt, M Färber, M Yoshikawa - Information Retrieval Journal"],"snippet":"Temporal collections of news articles (or news archives) contain numerous accurate and time-aligned articles, which offer immense value to our society, hel.","url":["https://link.springer.com/article/10.1007/s10791-020-09387-9"]}
{"year":"2021","title":"Improving Sentiment Classification for Arabic Short Text Using Deep Learning Approaches","authors":["A Alwehaibi - 2021"],"snippet":"Page 1. Improving Sentiment Classification for Arabic Short Text Using Deep Learning Approaches Ali Alwehaibi North Carolina A&T State University A dissertation submitted to the graduate faculty in partial fulfillment of the requirements for the degree of …","url":["https://search.proquest.com/openview/615ff8051266a66aec4a923d0faafa78/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2021","title":"Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation","authors":["A Chronopoulou, D Stojanovski, A Fraser - arXiv preprint arXiv:2103.10531, 2021"],"snippet":"… We use 3K randomly sampled sentences of SETIMES (Tiedemann, 2012) as validation/test sets. We also use 68M En sentences from NewsCrawl. For Sq and Mk we use all the CommonCrawl corpora from Ortiz Suárez et …","url":["https://arxiv.org/pdf/2103.10531"]}
{"year":"2021","title":"in the Language Technology (LT) group","authors":["T Fischer, C Biemann, SM Yimam"],"snippet":"… The model is trained on the colossal and cleaned version of common crawl (C4) as well as HugeNews which consists of 1.5B news articles from the web. Unsurprisingly, this method achieves state-of-the-art results on many summarization datasets …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2021-ma-timfischer.pdf"]}
{"year":"2021","title":"indicnlp@ kgp at DravidianLangTech-EACL2021: Offensive Language Identification in Dravidian Languages","authors":["K Kedia, A Nandy - arXiv preprint arXiv:2102.07150, 2021"],"snippet":"… use the bert-base-multilingual-cased model, mBERT trained on cased text in 104 languages from large Wikipedia articles, and the xlm-roberta-base model, XLM-R (Conneau et al., 2020) trained on 100 languages, using …","url":["https://arxiv.org/pdf/2102.07150"]}
{"year":"2021","title":"IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation","authors":["S Cahyawijaya, GI Winata, B Wilie, K Vincentio, X Li… - arXiv preprint arXiv …, 2021"],"snippet":"… Wiki Javanese1 6,015,961 231,571 53.2 MB formal Wikipedia CC-100 Sundanese 13,761,754 433,086 107.6 MB mixed Common Crawl CC-100 Javanese 20,560,458 690,517 161.9 MB mixed Common Crawl TOTAL 3,626,283,931 276,838,931 23.79 GB …","url":["https://arxiv.org/pdf/2104.08200"]}
{"year":"2021","title":"Information Extraction from Semi-Structured Websites","authors":["C Lockard - 2021"],"snippet":"… Note the log scale of the x-axis. . . . . 42 3.4 Extraction precision vs. number of extractions on the CommonCrawl dataset at various confidence thresholds; the orange bar indicates a 0.75 threshold yielding 1.25millionextractionsat90%precision. . . . . 47 …","url":["https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/47422/Lockard_washington_0250E_22994.pdf?sequence=1&isAllowed=y"]}
{"year":"2021","title":"Information Retrieval Model for Social Media Applications","authors":["S Bhandari - 2021"],"snippet":"… 2019 IEEE International Conference on Healthcare Informatics (ICHI), pp. 1–3, 2019. [17] S. Nagel, “Cc-news.” https://commoncrawl.org/2016/10/news-datasetavailable/, 2016. [18] M. Heidari and JH Jones, “Using bert to extract …","url":["http://129.174.21.13/bitstream/handle/1920/11956/bhandari_social-media-ml.pdf?sequence=1&isAllowed=y"]}
{"year":"2021","title":"Information retrieval versus deep learning approaches for generating traceability links in bilingual projects","authors":["J Lin, Y Liu, J Cleland-Huang - Empirical Software Engineering, 2022"],"snippet":"Abstract Software traceability links are established between diverse artifacts of the software development process in order to support tasks such as compliance analysis, safety assurance, and requirements validation. However, practice has shown that it …","url":["https://link.springer.com/article/10.1007/s10664-021-10050-0"]}
{"year":"2021","title":"INFOTEC-LaBD at PAN@ CLEF21: Profiling Hate Speech Spreaders on Twitter through Emotion-based Representations","authors":["H Cabrera, S Miranda-Jiménez, ES Tellez - CLEF, 2021"],"snippet":"… 26] for mapping Tweets to semantic space. In particular, we use the model pre-trained on 600 billion tokens on Common Crawl for the English task and the one from [27] to tackle the Spanish task. We use the NRC Lexicon [12 …","url":["http://ceur-ws.org/Vol-2936/paper-159.pdf"]}
{"year":"2021","title":"Infusing Future Information into Monotonic Attention Through Language Models","authors":["MA Zaidi, S Indurthi, B Lee, NK Lakumarapu, S Kim - arXiv preprint arXiv:2109.03121, 2021"],"snippet":"Page 1. Infusing Future Information into Monotonic Attention Through Language Models Mohd Abbas Zaidi∗, Sathish Indurthi∗†, Beomseok Lee Nikhil Kumar Lakumarapu, Sangha Kim NLP Lab, Samsung Research, Seoul, South …","url":["https://arxiv.org/pdf/2109.03121"]}
{"year":"2021","title":"Input Augmentation Improves Constrained Beam Search for Neural Machine Translation: NTT at WAT 2021","authors":["K Chousa, M Morishita - arXiv preprint arXiv:2106.05450, 2021"],"snippet":"… We also used CommonCrawl provided by the WMT 2020 news shared task (Barrault et al., 2020) as additional monolingual data. For CommonCrawl data, we chose the ten million English and Japanese sentences that are similar …","url":["https://arxiv.org/pdf/2106.05450"]}
{"year":"2021","title":"Integrated Training for Sequence-to-Sequence Models Using Non-Autoregressive Transformer","authors":["E Tokarchuk, J Rosendahl, W Wang, P Petrushkov…"],"snippet":"… Figure 2: Different variants of the encoder-decoder model integration through the connection interface. 5.1 Data Training data for French→German includes Eu- roparl corpus version 7 (Koehn, 2005), CommonCrawl2 corpus and the newstest2008-2010 …","url":["https://www-i6.informatik.rwth-aachen.de/publications/download/1194/TokarchukEvgeniiaRosendahlJanWangWeiyuePetrushkovPavelLancewickiTomerKhadiviShahramNeyHermann--IntegratedTrainingforSequence-to-SequenceModelsUsingNon-AutoregressiveTransformer--2021.pdf"]}
{"year":"2021","title":"Intelligent Arabic-Based Healthcare Assistant","authors":["T Wael, A Hesham, M Youssef, O Adel, H Hesham… - 2021 3rd Novel Intelligent …, 2021"],"snippet":"… It was pre-trained on ∼8.2 billion words from the Arabic version of OSCAR (unshuffled version of the corpus) - filtered from Common Crawl [6], the recent dump of Arabic Wikipedia, and other Arabic resources, which sum up to ∼95GB of text. The corpus …","url":["https://ieeexplore.ieee.org/abstract/document/9600526/"]}
{"year":"2021","title":"Intelligent Document Validation","authors":["E de Souza Pais - 2021"],"snippet":"Abstract Processes in organizations over the past few years have been increasingly automated. In order to make them more efficient and practical. However, one area in which manual work is still common is document analysis. In this area, due to the …","url":["https://estudogeral.uc.pt/retrieve/217107/Eduardo%20de%20Sousa%20Pais.pdf"]}
{"year":"2021","title":"Intelligent Methods for Accurately Detecting Phishing Websites","authors":["M Al-kasassbeh, ABUZ AL MAHA"],"snippet":"… A. Data set The data set used in this paper is offered by [19] is composed of 5000 phishing websites and 5000 legitimate websites. Phishing websites are collected from PhishTank and OpenPhish, while legitimate websites are collected from Alexa and Common Crawl …","url":["https://www.academia.edu/download/65538093/200207223.pdf"]}
{"year":"2021","title":"Interactive Clustering of Cooking Recipe Instructions: Towards the Automatic Detection of Events Involving Kitchen Devices","authors":["F Ventirozos, M Jacobo-Romero, S Clinch… - 2021 IEEE 15th …, 2021"],"snippet":"… Based on these synonym sets, we enriched our list by adding similar words based on GloVe word embedding vectors [17], which were pre-trained on the Common Crawl corpus1 … 1https://commoncrawl.org/ 342 Page 3. Figure …","url":["https://ieeexplore.ieee.org/abstract/document/9364446/"]}
{"year":"2021","title":"Interactive query expansion for professional search applications","authors":["T Russell-Rose, P Gooch, U Kruschwitz - arXiv preprint arXiv:2106.13528, 2021"],"snippet":"… The DBpedia data set describes 4.58 million entities, out of which 4.22 million are classified in a consistent ontology. 2. WebISA (Seitner et al., 2016) is a publicly available database containing hypernymy relations extracted from the CommonCrawl web corpus …","url":["https://arxiv.org/pdf/2106.13528"]}
{"year":"2021","title":"Internet-Augmented Dialogue Generation","authors":["M Komeili, K Shuster, J Weston - arXiv preprint arXiv:2107.07566, 2021"],"snippet":"… First, we store and utilize the Common Crawl dump of the internet from Wenzek et al … DPR+FAISS-based models We trained DPR+FAISS-based models using either the WoW or WizInt training datasets, and using either …","url":["https://arxiv.org/pdf/2107.07566"]}
{"year":"2021","title":"Interpretability in Word Sense Disambiguation using Tsetlin Machine","authors":["RK Yadav, L Jiao, OC Granmo, M Goodwin - 2021"],"snippet":"… FastText-Base (FTX- B), FastText-CommonCrawl (FTX-C), 1 neural network (NN) BERT base, and our proposed TM. FTX- B is a fast text linear classifier without pre-trained embeddings and FTX-C is a fast text linear classifier with …","url":["https://www.scitepress.org/Papers/2021/103821/103821.pdf"]}
{"year":"2021","title":"Interpretable bias mitigation for textual data: Reducing gender bias in patient notes while maintaining classification performance","authors":["JR Minot, N Cheney, M Maier, DC Elbers, CM Danforth… - arXiv preprint arXiv …, 2021"],"snippet":"… and resulting word embeddings. For instance, general purpose language models are often trained on Wikipedia and the Common Crawl collection of web pages (eg, BERT [27], RoBERTa [28]). Training language models on …","url":["https://arxiv.org/pdf/2103.05841"]}
{"year":"2021","title":"Interpreting and Controlling Linguistic Features in Neural Networks' Representations","authors":["T Limisiewicz"],"snippet":"Abstract In recent years, Neural Networks proved their usefulness in computing and processing vector representations to solve various NLP tasks. The key objective of the thesis is to provide a better explanation of the neural network’s representation of …","url":["https://ufal.mff.cuni.cz/~zabokrtsky/pgs/thesis_proposal/tomasz-limisiewicz-proposal.pdf"]}
{"year":"2021","title":"Introducing A large Tunisian Arabizi Dialectal Dataset for Sentiment Analysis","authors":["C Fourati, H Haddad, A Messaoudi, M BenHajhmida… - Proceedings of the Sixth …, 2021"],"snippet":"… In this pa- per, we introduce a large common-crawl-based Tunisian Arabizi dialectal dataset dedicated for Sentiment Analysis … Hence, our contributions can be summarized as: the first re- lease of a Tunizi large-scale Common-Crawl-based Page 3. 228 …","url":["https://www.aclweb.org/anthology/2021.wanlp-1.25.pdf"]}
{"year":"2021","title":"Introducing huBERT","authors":["ND Márk"],"snippet":"… Abstract This paper introduces the huBERT family of models. The flagship is the eponymous BERT Base model trained on the new Hungarian Webcorpus 2.0, a 9-billion-token corpus of Web text collected from the Common Crawl …","url":["https://hlt.bme.hu/media/pdf/huBERT.pdf"]}
{"year":"2021","title":"Introduction: digital humanities and the use of web archives","authors":["K Teszelszky - 2021"],"snippet":"… Also, an ever-increasing amount of data is selected, archived, and presented worldwide in web collections by organizations like the Internet Archive, Common Crawl, national libraries, and other academic research and digital heritage …","url":["https://link.springer.com/article/10.1007/s42803-021-00040-5"]}
{"year":"2021","title":"Investigating Depression Semantics on Reddit","authors":["S Agarwal, P Singh, J Shah, N Sanjeev - International Conference on Neural …, 2021"],"snippet":"Major depression is a challenging issue affecting individuals and those of the people around them. This paper investigates the Reddit comments for the automated identification of comments being indicative of depressive behaviour. We …","url":["https://link.springer.com/chapter/10.1007/978-3-030-92310-5_75"]}
{"year":"2021","title":"Investigating Hostile Post Detection in Hindi","authors":["V Bhatnagar, P Kumar, P Bhattacharyya - Neurocomputing, 2021"],"snippet":"… Wiki + CC : Pretrained Embeddings trained on Wikipedia and Common Crawl Wiki : Pretrained Embeddings trained on Wikipedia TEAP : … Wikipedia pretrained word vectors 25 and Wikipedia + Common Crawl pretrained word vectors 26 for …","url":["https://www.sciencedirect.com/science/article/pii/S0925231221018099"]}
{"year":"2021","title":"Investigating Monolingual and Multilingual BERTModels for Vietnamese Aspect Category Detection","authors":["D Van Thin, LS Le, VX Hoang, NLT Nguyen - arXiv preprint arXiv:2103.09519, 2021"],"snippet":"… Besides, there are many variants of BERT models which are published for Vietnamese language and multi-lingual model as cross-lingual language model (XLM-R) [10] or multilingual BERT [9]. These models are pre-trained …","url":["https://arxiv.org/pdf/2103.09519"]}
{"year":"2021","title":"Investigation of different decoding algorithms for the automatic generation of alternative language patterns in German to English translation","authors":["M Zanon"],"snippet":"… Crawled Corpus (C4) to pretrain the model on. They base this data set on the text data from April 2019 of the Common Crawl archive, which provides web extracted text by scraping about 20TB of text from HTML files each month …","url":["https://users.informatik.haw-hamburg.de/~schumann/BachelorArbeitMelinaZanon.pdf"]}
{"year":"2021","title":"Is Domain Adaptation Worth Your Investment? Comparing BERT and FinBERT on Financial Tasks","authors":["B Peng, E Chersoni, YY Hsu, CR Huang - Proceedings of the Third Workshop on …, 2021"],"snippet":"… The first-generation Transformers were mainly trained on general corpora, such as Wikipedia or Common Crawl. However, considering domain adaptations, many researchers have later injected domain-specific knowledge in such architectures …","url":["https://aclanthology.org/2021.econlp-1.5.pdf"]}
{"year":"2021","title":"Is the Number of Trainable Parameters All That Actually Matters?","authors":["A Chatelain, A Djeghri, D Hesslow, J Launay, I Poli - arXiv preprint arXiv:2109.11928, 2021"],"snippet":"… Architecture details for the four model sizes considered are available in the supplementary. The model is trained on French data obtained using a Common Crawl dump filtered by CCNet [23]. We use byte-level byte-pair encoding, with a vocabulary of 50,262 tokens …","url":["https://arxiv.org/pdf/2109.11928"]}
{"year":"2021","title":"isiZulu Word Embeddings","authors":["S Dlamini, E Jembere, A Pillay, B van Niekerk - 2021 Conference on Information …, 2021"],"snippet":"… TABLE 1: THE MODELS TO BE COMPARED ALONG WITH DETAILS OF THE CORPORA THEY WERE TRAINED ON. Model Corpus Corpus Size Vocabulary Word2Vec Google News 6 billion 1 million GloVe Common …","url":["https://ieeexplore.ieee.org/abstract/document/9395011/"]}
{"year":"2021","title":"JABER: Junior Arabic BERt","authors":["A Ghaddar, Y Wu, A Rashid, K Bibi, M Rezagholizadeh… - arXiv preprint arXiv …, 2021"],"snippet":"… 2020) is a 12 layer BERT model trained on 95GB of common crawl, news, and Wikipedia Arabic data. AraBERT (Antoun et al.… • Common Crawl (CC) This data was downloaded from 10 shards of monthly Common Crawl 3 covering March to …","url":["https://arxiv.org/pdf/2112.04329"]}
{"year":"2021","title":"Jigsaw: Large Language Models meet Program Synthesis","authors":["N Jain, S Vaidyanath, A Iyer, N Natarajan… - arXiv preprint arXiv …, 2021"],"snippet":"Large pre-trained language models such as GPT-3, Codex, and Google's language model are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and …","url":["https://arxiv.org/pdf/2112.02969"]}
{"year":"2021","title":"JMIR Mental Health","authors":["A Wongkoblap, M Vadillo, V Curcin, C Rauschenberg… - Mental Health"],"snippet":"… occurrent words as vectors [25]. GloVe was trained on several textual data sets, such as Wikipedia and common crawl (a copy of web content), and supported 50D, 100D, 200D, and 300D vectors. Anaphora resolution is another …","url":["https://mental.jmir.org/2021/8/PDF"]}
{"year":"2021","title":"Joint Open Knowledge Base Canonicalization and Linking","authors":["Y Liu, W Shen, Y Wang, J Wang, Z Yang, X Yuan - Proceedings of the 2021 …, 2021"],"snippet":"… 1 − Simemb (si,sj ) if xij = 0 For instance, the score ofSimemb (“Barack Obama”, “President Obama”) is 0.873 using fastText [17] embeddings trained on Common Crawl via MindSpore Framework1, which indicates these two NPs are likely to refer to the same entity …","url":["https://dl.acm.org/doi/abs/10.1145/3448016.3452776"]}
{"year":"2021","title":"Joint source–target encoding with pervasive attention","authors":["M Elbayad, L Besacier, J Verbeek - Machine Translation, 2021"],"snippet":"The pervasive attention model is a sequence-to-sequence model that addresses the issue of source–target interaction in encoder–decoder models by jointly encoding the two sequences with a two-dimensional convolutional neural network. We …","url":["https://link.springer.com/article/10.1007/s10590-021-09289-7"]}
{"year":"2021","title":"Juris2vec: Building Word Embeddings from Philippine Jurisprudence","authors":["E Peramo, C Cheng, M Cordel - … International Conference on Artificial Intelligence in …, 2021"],"snippet":"… utilized a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and …","url":["https://ieeexplore.ieee.org/abstract/document/9415251/"]}
{"year":"2021","title":"Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts","authors":["A Baheti, M Sap, A Ritter, M Riedl - arXiv preprint arXiv:2108.11830, 2021"],"snippet":"… For example, OpenAI's GPT-3 (Brown et al., 2020) is a 175 billion parameter neural network, trained on a 570GB subset CommonCrawl that is capable of engaging in impressive open-domain di- alogues with a user, when …","url":["https://arxiv.org/pdf/2108.11830"]}
{"year":"2021","title":"Just What do You Think You're Doing, Dave?'A Checklist for Responsible Data Use in NLP","authors":["A Rogers, T Baldwin, K Leins - arXiv preprint arXiv:2109.06598, 2021"],"snippet":"Page 1. 'Just What do You Think You're Doing, Dave?' ∗ A Checklist for Responsible Data Use in NLP Anna Rogers Center for Social Data Science University of Copenhagen Denmark arogers@sodas.ku.dk Timothy Baldwin …","url":["https://arxiv.org/pdf/2109.06598"]}
{"year":"2021","title":"KAN: Knowledge-Augmented Networks for Few-Shot Learning","authors":["Z Zhu, X Lin - ICASSP 2021-2021 IEEE International Conference on …, 2021"],"snippet":"… 3.3. Hyperparameters For the knowledge graph, to represent the initial semantic features of nodes, we use the GloVe model [23] which is the Common Crawl version trained on 840B tokens. The dimension of the semantic features is 300 …","url":["https://ieeexplore.ieee.org/abstract/document/9413612/"]}
{"year":"2021","title":"KazNERD: Kazakh Named Entity Recognition Dataset","authors":["R Yeshpanov, Y Khassanov, HA Varol - arXiv preprint arXiv:2111.13419, 2021"],"snippet":"We present the development of a dataset for Kazakh named entity recognition. The dataset was built as there is a clear need for publicly available annotated corpora in Kazakh, as well as annotation guidelines containing straightforward--but rigorous--rules …","url":["https://arxiv.org/pdf/2111.13419"]}
{"year":"2021","title":"KI2TE: Knowledge-Infused InterpreTable Embeddings for COVID-19 Misinformation Detection","authors":["W Shiao, EE Papalexakis"],"snippet":"… After cleaning, we were left with around 20K fake news articles. 3.4.3 Real News Dataset. We used the list from the BS Detector Chrome extension3 to pick the reputable sites. We then collected ar- ticles from all of the matching …","url":["https://www.cs.ucr.edu/~epapalex/papers/Knod2021_paper_7.pdf"]}
{"year":"2021","title":"KIT's IWSLT 2021 Offline Speech Translation System","authors":["TN Nguyen, TS Nguyen, C Huber, M Awiszus… - Proceedings of the 18th …, 2021"],"snippet":"… On the other hand, Page 2. 126 Table 2: Text Training Data Dataset Sentences TED Talks (TED) 220K Europarl (EPPS) 2.2MK CommonCrawl 2.1M Rapid 1.21M ParaCrawl 25.1M OpenSubtitles 12.6M WikiTitle 423K Back-translated News 26M …","url":["https://aclanthology.org/2021.iwslt-1.13.pdf"]}
{"year":"2021","title":"Knowledge Graphs for Analyzing and Searching Legal Data","authors":["K Sabrina, P Axel - 2021"],"snippet":"Page 1. Titel der Dissertation: Knowledge Graphs for Analyzing and Searching Legal Data Dissertation zur Erlangung des akademischen Grades eines Doktors der Sozialund Wirtschaftswissenschaften an der Wirtschaftsuniversität Wien eingereicht bei …","url":["https://aic.ai.wu.ac.at/~polleres/supervised_theses/Erwin_Filtz_PhD_2021.pdf"]}
{"year":"2021","title":"Knowledge-driven Answer Generation for Conversational Search","authors":["M Leite, R Ferreira, D Semedo, J Magalhães - arXiv preprint arXiv:2104.06892, 2021"],"snippet":"… This model is pre-trained on the large C4 cor- 5 Page 6. pus, which was derived from Common Crawl1. A masked language modelling objective is used, where the model is trained to predict corrupted randomly sampled tokens of varying sizes. 5. Evaluation 5.1 …","url":["https://arxiv.org/pdf/2104.06892"]}
{"year":"2021","title":"Knowledge-enhanced neural grammar Induction","authors":["B Li - 2021"],"snippet":"… become the dominant approach. Thanks to the Transformer's superior capability of parallelism, PLMs can be efficiently trained on enormous raw text (eg, common crawl of the internet) containing billions of tokens. Surprisingly, although PLMs contain no …","url":["https://era.ed.ac.uk/bitstream/handle/1842/37896/BowenLi_2021.pdf?sequence=1"]}
{"year":"2021","title":"Label Imputation for Homograph Disambiguation: Theoretical and Practical Approaches","authors":["JM Seale - 2021"],"snippet":"Page 1. City University of New York (CUNY) CUNY Academic Works Dissertations, Theses, and Capstone Projects CUNY Graduate Center 9-2021 Label Imputation for Homograph Disambiguation: Theoretical and Practical Approaches Jennifer M. Seale …","url":["https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=5591&context=gc_etds"]}
{"year":"2021","title":"Label-Descriptive Patterns and their Application to Characterizing Classification Errors","authors":["M Hedderich, J Fischer, D Klakow, J Vreeken - arXiv preprint arXiv:2110.09599, 2021"],"snippet":"State-of-the-art deep learning methods achieve human-like performance on many tasks, but make errors nevertheless. Characterizing these errors in easily interpretable terms gives insight into whether a model is prone to making systematic …","url":["https://arxiv.org/pdf/2110.09599"]}
{"year":"2021","title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs","authors":["C Schuhmann, R Vencu, R Beaumont, R Kaczmarczyk… - arXiv preprint arXiv …, 2021"],"snippet":"… To create image-text pairs, we parse through WAT files from Common Crawl and parse out all HTML IMG tags containing an alt-text attribute. We download the raw images from the parsed URLs with asynchronous requests using Trio and Asks libraries. …","url":["https://arxiv.org/pdf/2111.02114"]}
{"year":"2021","title":"LAMBERT: Layout-Aware Language Modeling for Information Extraction","authors":["Ł Garncarek, R Powalski, T Stanisławek, B Topolski…"],"snippet":"… tuning, respectively. The training was performed on a collection of PDFs extracted from Common Crawl containing a variety of documents (we randomly selected up to 10 documents from any single domain). The documents …","url":["https://www.researchgate.net/profile/Lukasz-Garncarek/publication/339374440_LAMBERT_Layout-Aware_language_Modeling_for_information_extraction/links/605c2fdda6fdccbfea04b14a/LAMBERT-Layout-Aware-language-Modeling-for-information-extraction.pdf"]}
{"year":"2021","title":"Language Embeddings for Typology and Cross-lingual Transfer Learning","authors":["D Yu, T He, K Sagae - arXiv preprint arXiv:2106.02082, 2021"],"snippet":"… lang_emb represents language embeddings from our proposed denoising autoencoder trained with language specific MUSE embeddings, using CommonCrawl text. Eng. lang_emb represents language embeddings trained …","url":["https://arxiv.org/pdf/2106.02082"]}
{"year":"2021","title":"Language Models are not Models of Language","authors":["C Veres - arXiv preprint arXiv:2112.07055, 2021"],"snippet":"… Also mentioned is the Common Crawl web content where \"... difficulty of cleaning and filtering the Common Crawl data ...\" which has low … Common Crawl data also contains duplicate lines which often signals highly repetitive \"boilerplate text\", which …","url":["https://arxiv.org/pdf/2112.07055"]}
{"year":"2021","title":"Language Models have a Moral Dimension","authors":["P Schramowski, C Turan, N Andersen, C Rothkopf… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. Language Models have a Moral Dimension Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin Rothkopf & Kristian Kersting Abstract Artificial writing is permeating our lives due to recent advances in large …","url":["https://arxiv.org/pdf/2103.11790"]}
{"year":"2021","title":"Language Representation Models: An Overview","authors":["T Schomacker, M Tropmann-Frick - Entropy, 2021"],"snippet":"… Since C4 is based on the common crawl project, it not only contains book-like or article-like texts but a much wider variety of English texts. Despite its colossal size [11], p. 7, ensured that, by their used techniques, C4 only consists of reasonably clean …","url":["https://www.mdpi.com/1099-4300/23/11/1422/pdf"]}
{"year":"2021","title":"Language representations for computational argumentation","authors":["A Lauscher - 2021"],"snippet":"… CA Computational Argumentation. – , , , CBOW Continuous Bag of Words. – , , , CC Citation Contexts. CC- CommonCrawl- . , CCC Citation Context Identi cation. – CL Computational Linguistics. , , , , CLWE Cross-lingual Word Embe","url":["https://madoc.bib.uni-mannheim.de/60201/1/dissertation_lauscher.pdf"]}
{"year":"2021","title":"Language Transfer for Identifying Diagnostic Paragraphs in Clinical","authors":["L Di Liello, O Uryupina, A Moschitti - 2021"],"snippet":"This paper aims at uncovering the structure of clinical documents, in particular, identifying paragraphs describing “diagnosis” or “procedures”. We present transformer-based architectures for approaching this task in a monolingual setting (English) …","url":["http://ceur-ws.org/Vol-3033/paper56.pdf"]}
{"year":"2021","title":"LaoPLM: Pre-trained Language Models for Lao","authors":["N Lin, Y Fu, Z Yang, S Jiang - arXiv preprint arXiv:2110.05896, 2021"],"snippet":"… On the one hand, we utilize all the Lao data from the OSCAR corpus9, a humongous multilingual corpus whose texts all come from the Common Crawl corpus10. Suárez et al. [23] proposes the goclassy architecture to perform language …","url":["https://arxiv.org/pdf/2110.05896"]}
{"year":"2021","title":"Large Margin Training Improves Language Models for ASR","authors":["J Wang, J Huang, KW Church - ICASSP 2021-2021 IEEE International Conference on …, 2021"],"snippet":"… on enWiki+bookcorpus. We thus adopt the published model and skip the pretraining phase. On the other hand, GPT, as a realization of transformer decoder model, is pretrained on a larger corpus, common crawl. For a fair …","url":["https://ieeexplore.ieee.org/abstract/document/9414724/"]}
{"year":"2021","title":"Large-Scale Contextualised Language Modelling for Norwegian","authors":["A Kutuzov, J Barnes, E Velldal, L Øvrelid, S Oepen - arXiv preprint arXiv:2104.06546, 2021"],"snippet":"… models for Bokmål and Nynorsk). However, these models were trained on very modestly sized corpora of 20 million words for each language (randomly sampled from Wikipedia dumps and Common Crawl data). In a parallel effort …","url":["https://arxiv.org/pdf/2104.06546"]}
{"year":"2021","title":"Large-Scale Self-and Semi-Supervised Learning for Speech Translation","authors":["C Wang, A Wu, J Pino, A Baevski, M Auli, A Conneau - arXiv preprint arXiv …, 2021"],"snippet":"… We also make use of monolingual text data in the target language to further improve translation quality. Specifically, we train a language model (LM) on part of the CommonCrawl data3 that is in similar domains as CoVoST …","url":["https://arxiv.org/pdf/2104.06678"]}
{"year":"2021","title":"LARO: Language Agnostic Sentence Representations from finetuned RoBERTa⋆","authors":["AS Salvado - Language"],"snippet":"… The training resulted in the model approximating the meaning and syntax of all languages in the training data. The dataset which XLM-Roberta was trained on consists of 2.5TB CommonCrawl Data. The architecture of …","url":["https://users.informatik.haw-hamburg.de/~ubicomp/projekte/master2021-proj/soblechero.pdf"]}
{"year":"2021","title":"LaTr: Layout-Aware Transformer for Scene-Text VQA","authors":["AF Biten, R Litman, Y Xie, S Appalaraju, R Manmatha - arXiv preprint arXiv …, 2021"],"snippet":"We propose a novel multimodal architecture for Scene Text Visual Question Answering (STVQA), named Layout-Aware Transformer (LaTr). The task of STVQA requires models to reason over different modalities. Thus, we first investigate the …","url":["https://arxiv.org/pdf/2112.12494"]}
{"year":"2021","title":"LATS: Low-Resource Abstractive Text Summarization","authors":["CB van Yperen, F Frasincar, K Gruber - 2021"],"snippet":"… texts from 350 million Web pages. The dataset is based on the Common Crawl Web archive which is publicly available and contains scraped HTML files from which the markup has been removed. The HTML files have been …","url":["https://thesis.eur.nl/pub/56900/MSc_Thesis_27042021_CvanYperen.pdf"]}
{"year":"2021","title":"Learned Construction Grammars Converge Across Registers Given Increased Exposure","authors":["J Dunn, HT Madabushi - arXiv preprint arXiv:2110.05663, 2021"],"snippet":"This paper measures the impact of increased exposure on whether learned construction grammars converge onto shared representations when trained on data from different registers. Register influences the frequency of constructions, with …","url":["https://arxiv.org/pdf/2110.05663"]}
{"year":"2021","title":"Learning adaptive representations for entity recognition in the biomedical domain","authors":["I Lauriola, F Aiolli, A Lavelli, F Rinaldi - Journal of Biomedical Semantics, 2021"],"snippet":"Named Entity Recognition is a common task in Natural Language Processing applications, whose purpose is to recognize named entities in textual documents. Several systems exist to solve this task in the biomedical domain …","url":["https://jbiomedsem.biomedcentral.com/articles/10.1186/s13326-021-00238-0"]}
{"year":"2021","title":"Learning Mathematical Properties of Integers","authors":["M Ryskina, K Knight - arXiv preprint arXiv:2109.07230, 2021"],"snippet":"… Ours OEIS 14M 133,004 GloVe–840B–300D Common Crawl 840B 64,729 … (2019), we choose three embedding models pre-trained on English text corpora: • GloVe–840B–300D: 300-dimensional GloVe vectors trained …","url":["https://arxiv.org/pdf/2109.07230"]}
{"year":"2021","title":"Learning policy scheduling for text augmentation","authors":["S Li, X Ao, F Pan, Q He - Neural Networks, 2021"],"snippet":"Abstract When training deep learning models, data augmentation is an important technique to improve the performance and alleviate overfitting. In natural language processing (NLP), existing augmentation methods often use fixed strategies …","url":["https://www.sciencedirect.com/science/article/pii/S0893608021003907"]}
{"year":"2021","title":"Learning Sense-Specific Static Embeddings using Contextualised Word Embeddings as a Proxy","authors":["D Bollegala, Y Zhou - Proceedings of the 35th Pacific Asia Conference on …, 2021","Y Zhou, D Bollegala - arXiv preprint arXiv:2110.02204, 2021"],"snippet":"… In our experiments, we use the pretained GloVe3 embeddings (Common Crawl with 840B tokens and 2.2M vocabulary) as the static word embeddings g(u) with p = 300. We use pretrained BERT (large-bert-cased4) as the contextualised embedding …","url":["https://aclanthology.org/2021.paclic-1.2.pdf","https://arxiv.org/pdf/2110.02204"]}
{"year":"2021","title":"Learning Sentiment Sentence Representation with Multiview Attention Model","authors":["Y Zhang, J Wang, X Zhang - Information Sciences, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0020025521005041"]}
{"year":"2021","title":"Learning Structured Neural Semantic Parsers","authors":["P Yin - 2021"],"snippet":"Page 1. Learning Structured Neural Semantic Parsers Pengcheng Yin CMU LTI 21 014 August 16, 2021 Language Technologies Institute School of Computer Science Carnegie Mellon University 5000 Forbes Ave., Pittsburgh, PA 15123 www.lti.cs.cmu.edu …","url":["https://www.lti.cs.cmu.edu/sites/default/files/yin%2C%20pengcheng%20-%20Thesis.pdf"]}
{"year":"2021","title":"Learning to Lemmatize in the Word Representation Space","authors":["J Lagus, A Klami - NoDaLiDa 2021, 2021"],"snippet":"… Data We validate the approach on Finnish language, using pretrained embeddings provided by the fastText library (Bojanowski et al., 2016). The embeddings were trained on Common Crawl and Wikipedia corpora and have dimensionality of d= 300 …","url":["https://ep.liu.se/ecp/178/ecp2021178.pdf#page=265"]}
{"year":"2021","title":"Learning to rank for Consumer Health Search","authors":["H Yang, X Liu, B Zheng, G Yang - CLEF 2021 Evaluation Labs and Workshop: Online …, 2021"],"snippet":"… Totally, the collection consists of over 5 million medical webpages from selected domains acquired from the CommonCrawl and other resources [4]. 3.3. Topics Totally 55 topics are used in the CLEF 2021 CHS task, and they are based on realistic search scenarios …","url":["http://ceur-ws.org/Vol-2936/paper-71.pdf"]}
{"year":"2021","title":"Learning To Retrieve Prompts for In-Context Learning","authors":["O Rubin, J Herzig, J Berant - arXiv preprint arXiv:2112.08633, 2021"],"snippet":"In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its …","url":["https://arxiv.org/pdf/2112.08633"]}
{"year":"2021","title":"Leveraging Longitudinal Data for Personalized Prediction and Word Representations","authors":["C Welch - 2021"],"snippet":"Page 1. Leveraging Longitudinal Data for Personalized Prediction and Word Representations by Charles Welch A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer …","url":["https://deepblue.lib.umich.edu/bitstream/handle/2027.42/167971/cfwelch_1.pdf?sequence=1"]}
{"year":"2021","title":"Leveraging Transformers for Hate Speech Detection in Conversational Code-Mixed Tweets","authors":["ZM Farooqi, S Ghosh, RR Shah - arXiv preprint arXiv:2112.09986, 2021"],"snippet":"… • XLM-Roberta (XLM-R) [42] is a transformer-based masked language model pretrained on Common Crawl data having about 100 languages. It was proposed by Facebook and happened to be one of the best-performing transformer models for …","url":["https://arxiv.org/pdf/2112.09986"]}
{"year":"2021","title":"Lexicon Development for COVID-19-related Concepts Using Open-source Word Embedding Sources: An Intrinsic and Extrinsic Evaluation","authors":["S Parikh, A Davoudi, S Yu, C Giraldo, E Schriver… - JMIR Medical Informatics, 2021"],"snippet":"Background: Scientists are developing new computational methods and prediction models to better clinically understand COVID-19 prevalence, treatment efficacy, and patient outcomes. These efforts could be improved by leveraging …","url":["https://medinform.jmir.org/2021/2/e21679"]}
{"year":"2021","title":"Light gradient boosting machine-based phishing webpage detection model using phisher website features of mimic URLs","authors":["E Oram, PB Dash, B Naik, J Nayak, S Vimal, SK Nataraj - Pattern Recognition Letters, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0167865521003445"]}
{"year":"2021","title":"LILE: Look In-Depth before Looking Elsewhere--A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives","authors":["D Maleki, H Tizhoosh - 2021"],"snippet":"The volume of available data has grown dramatically in recent years in many applications. Furthermore, the age of networks that used multiple modalities separately has practically ended. Therefore, enabling bidirectional cross-modality …","url":["https://openreview.net/pdf?id=ZmsElUaQ0Xm"]}
{"year":"2021","title":"Limitations of Knowledge Distillation for Zero-shot Transfer Learning","authors":["S Soltan, AA AI, H Khan, W Hamza"],"snippet":"… However, they focused on distillation during pretraining and evaluate their model only on XNLI from XTREME benchmarks. Since they used common crawl data for pretraining and a different architecture, their models are not directly comparable to ours. 3 Method …","url":["https://assets.amazon.science/3b/46/17df4724436d850c46fcb41b1569/limitations-of-knowledge-distillation-for-zero-shot-transfer-learning.pdf"]}
{"year":"2021","title":"LIMSI@ MLIA","authors":["SA Rauf, F Yvon - 2020"],"snippet":"… In addition, we used the Cochrane bilingual parallel corpus [4]3 and the Taus Corona Crisis corpus.4. For out-of-domain corpus, we used the parallel corpora provided by the WMT14 campaign, which included Gigafr-en …","url":["http://eval.covid19-mlia.eu/meetings/round1/report/20210114-limsi.pdf"]}
{"year":"2021","title":"Lingua Custodia's participation at the WMT 2021 Machine Translation using Terminologies shared task","authors":["M Ailem, J Liu, R Qader - arXiv preprint arXiv:2111.02120, 2021"],"snippet":"This paper describes Lingua Custodia's submission to the WMT21 shared task on machine translation using terminologies. We consider three directions, namely English to French, Russian, and Chinese. We rely on a Transformer-based …","url":["https://arxiv.org/pdf/2111.02120"]}
{"year":"2021","title":"Linking place records using multi-view encoders","authors":["V Cousseau, L Barbosa - Neural Computing and Applications, 2021"],"snippet":"… For address embeddings, we utilized case-sensitive embeddings trained with the FastText model [4] on top of a Wikipedia and Common Crawl corpus [22]. The reasoning behind the different embedding sources for names and …","url":["https://link.springer.com/article/10.1007/s00521-021-05932-9"]}
{"year":"2021","title":"LIORI at SemEval-2021 Task 2: Span Prediction and Binary Classification approaches to Word-in-Context Disambiguation","authors":["A Davletov, N Arefyev, D Gordeev, A Rey - Proceedings of the Fifteenth Workshop on …, 2021"],"snippet":"… tokens. XLM-R was trained on a cleaned two-terabyte CommonCrawl Corpus in 100 languages. A new promising approach to language task modelling is treating any natural language task as a question answering problem …","url":["https://aclanthology.org/2021.semeval-1.103.pdf"]}
{"year":"2021","title":"LISN@ WMT 2021","authors":["J Xu, SA Rauf, MQ Pham, F Yvon - 6th Conference on Statistical Machine Translation, 2021"],"snippet":"This paper describes LISN's submissions to two shared tasks at WMT'21. For the biomedical translation task, we have developed resource-heavy systems for the English-French language pair, using both out-ofdomain and in-domain corpora. The …","url":["https://hal.archives-ouvertes.fr/hal-03430610/document"]}
{"year":"2021","title":"LMMS Reloaded: Transformer-based Sense Embeddings for Disambiguation and Beyond","authors":["D Loureiro, AM Jorge, J Camacho-Collados - arXiv preprint arXiv:2105.12449, 2021"],"snippet":"Page 1. LMMS Reloaded: Transformer-based Sense Embeddings for Disambiguation and Beyond Daniel Loureiroa,∗, Alıpio Mário Jorgea, Jose Camacho-Colladosb aLIAAD - INESC TEC, Dept. of Computer Science, FCUP …","url":["https://arxiv.org/pdf/2105.12449"]}
{"year":"2021","title":"LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking","authors":["H Jiang, S Gurajada, Q Lu, S Neelam, L Popa, P Sen… - arXiv preprint arXiv …, 2021"],"snippet":"… We also employ a suite of pretrained or custom trained neural language models to compute the similarity of mi and eij. Pre-trained Embedding Models. These include SpaCy's semantic similarity6 function that uses Glove …","url":["https://arxiv.org/pdf/2106.09795"]}
{"year":"2021","title":"Log Parsing and Template Extraction Using Neueral Sequence-To-Sequence Models","authors":["K Pawlikowski - 2021"],"snippet":"Abstract System log messages are produced by software systems as their code executes. The contents of these log messages vary across systems and are defined by the software developers of the system. Log messages can be captured and …","url":["https://search.proquest.com/openview/6e178e5359e69ed4aa85193bc726d8ac/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2021","title":"Looking for Semantic Similarity: What a Vector Space Model of Semantics Can Tell Us About Attention in Real-world Scenes","authors":["TR Hayes, JM Henderson - 2021"],"snippet":"… ing the semantic vectors from Word2vec (Mikolov, Chen, Corrado, & Dean, 2013) and GloVe 1.2 (Pennington, Socher, & Manning, 2014) that learn how words are associated with each other from large text corpora (ie, Google …","url":["https://osf.io/wsyz9/download?format=pdf"]}
{"year":"2021","title":"Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models","authors":["R Wolfe, A Caliskan - arXiv preprint arXiv:2110.00672, 2021"],"snippet":"Page 1. Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models Robert Wolfe University of Washington rwolfe3@uw.edu Aylin Caliskan University of Washington aylin@uw.edu Abstract We use …","url":["https://arxiv.org/pdf/2110.00672"]}
{"year":"2021","title":"LSI_UNED at CLEF eHealth2021: Exploring the effects of transfer learning in negation detection and entity recognition in clinical texts","authors":["H Fabregat, A Duque, L Araujo, J Martinez-Romo - CLEF eHealth, 2021"],"snippet":"… Word embeddings: Two different pre-trained word embeddings from different sources are used for text representation. On the one hand, we use general domain Spanish 100dimensional word embeddings in …","url":["http://ceur-ws.org/Vol-2936/paper-65.pdf"]}
{"year":"2021","title":"M6: A Chinese Multimodal Pretrainer","authors":["J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang… - arXiv preprint arXiv …, 2021"],"snippet":"… Xu et al. [45] released a 100GB corpus named CLUECorpus2020, which is re- trieved from the multilingual Common Crawl dataset … Forum discussion Plain-text - 8.7 39.0 223.1 - 18.0 Common Crawl Plain-text - 40.3 108.7 370.7 - 83.3 …","url":["https://arxiv.org/pdf/2103.00823"]}
{"year":"2021","title":"M6: Multi-Modality-to-Multi-Modality Multitask Mega-transformer for Unified Pretraining","authors":["J Lin, R Men, A Yang, C Zhou, Y Zhang, P Wang… - 2021"],"snippet":"… Xu et al. [45] released a 100𝐺𝐵 corpus named CLUECorpus2020, which is retrieved from the multilingual Common Crawl dataset … Forum discussion Plain-text - 8.7 39.0 223.1 - 18.0 Common Crawl Plain-text - 40.3 108.7 370.7 - 83.3 …","url":["https://keg.cs.tsinghua.edu.cn/jietang/publications/KDD21-Lin-et-al-M6.pdf"]}
{"year":"2021","title":"Machine Habitus: Toward a Sociology of Algorithms","authors":["M Airoldi - 2021"]}
{"year":"2021","title":"Machine Learning and Economic Models to Enable Risk-Informed Condition Based Maintenance of a Nuclear Plant Asset","authors":["V Agarwal, A Manjunatha, JA Smith, AV Gribok… - 2021"],"snippet":"Page 1. INL/EXT-21-61984 Revision 0 Industry Funding Opportunity Announcement Award Machine Learning and Economic Models to Enable Risk-Informed Condition Based Maintenance of a Nuclear Plant Asset March …","url":["https://www.osti.gov/servlets/purl/1770866"]}
{"year":"2021","title":"Machine Learning and Security Studies","authors":["S Dorsey - 2020"],"snippet":"Page 1. Machine Learning and Security Studies by Spencer Dorsey Department of Political Science Duke University Date: Approved: Scott de Marchi, Advisor Peter Feaver, Advisor Laia Balcells Erik Wibbels Dissertation submitted …","url":["https://dukespace.lib.duke.edu/dspace/bitstream/handle/10161/22158/Dorsey_duke_0066D_15915.pdf?sequence=1"]}
{"year":"2021","title":"Machine learning for financial transaction classification across companies using character-level word embeddings of text fields","authors":["RK Jørgensen, C Igel - Intelligent Systems in Accounting, Finance and …"],"snippet":"Abstract An important initial step in accounting is mapping financial transfers to the corresponding accounts. We devised machine-learning-based systems that automate this process. They use word em...","url":["https://onlinelibrary.wiley.com/doi/full/10.1002/isaf.1500"]}
{"year":"2021","title":"Machine Learning Meets Natural Language Processing--The story so far","authors":["NI Galanis, P Vafiadis, KG Mirzaev, GA Papakostas - arXiv preprint arXiv:2104.10213, 2021"],"snippet":"… in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4938–4947, 2020. 42. “Common Crawl”, Accessed: Mar. 24, 2021.[Online]. Available: https://commoncrawl.org/ 43. “Kaggle”, Accessed: Mar. 24, 2021.[Online] …","url":["https://arxiv.org/pdf/2104.10213"]}
{"year":"2021","title":"Machine Learning with Applications","authors":["N Kanakaris, N Giarelis, I Siachos, N Karacapilidis","S Casola, I Lauriola, A Lavelli"],"snippet":"Pre-trained transformers have rapidly become very popular in the Natural Language Processing (NLP) community, surpassing the previous state of the art in a wide variety of tasks. While their effectiveness is indisputable, these methods are …","url":["https://www.researchgate.net/profile/Nikos-Kanakaris/publication/356403657_Making_personnel_selection_smarter_through_word_embeddings_A_graph-based_approach/links/61a4ac35d7d1af224b2be56e/Making-personnel-selection-smarter-through-word-embeddings-A-graph-based-approach.pdf","https://www.researchgate.net/profile/Silvia-Casola/publication/361083726_Pre-trained_transformers_an_empirical_comparison/links/62a3370f55273755ebe1d59b/Pre-trained-transformers-an-empirical-comparison.pdf"]}
{"year":"2021","title":"Machine Translation Customization via Automatic Training Data Selection from the Web","authors":["T Vu, A Moschitti"],"snippet":"… 5 Fig. 1: Customization process of MT Systems Corpus Sent. (MM) News Commentary v13 0.3 Rapid (press releases) 1.3 Common Crawl 1.9 Europarl v7 2.4 ParaCrawl (Zipporah) 40.6 ParaCrawl (BiCleaner) 27.7 …","url":["https://assets.amazon.science/ed/9f/3bc994bc4be7bc53449bda397d83/machine-translation-customization-via-automatic-training-data-selection-from-the-web.pdf"]}
{"year":"2021","title":"Majority Voting with Bidirectional Pre-translation Improves Bitext Retrieval","authors":["A Jones, DT Wijaya - arXiv preprint arXiv:2103.06369, 2021"],"snippet":"Page 1. Majority Voting with Bidirectional Pre-translation Improves Bitext Retrieval Alex Jones Dartmouth College alexander.g.jones.23@dartmouth.edu Derry Tanti Wijaya Boston University wijaya@bu.edu Abstract Obtaining …","url":["https://arxiv.org/pdf/2103.06369"]}
{"year":"2021","title":"Making personnel selection smarter through word embeddings: A graph-based approach","authors":["N Kanakaris, N Giarelis, I Siachos, N Karacapilidis - Machine Learning with …, 2021"],"snippet":"… GoogleNews using Word2Vec and Common Crawl using GloVe are some of the most favored public pre-trained word embeddings. For an in-depth analysis as far as the differences and use-cases of various pre-trained word embeddings are …","url":["https://www.sciencedirect.com/science/article/pii/S2666827021001079"]}
{"year":"2021","title":"Maps Search Misspelling Detection Leveraging Domain-Augmented Contextual Representations","authors":["Y Li - arXiv preprint arXiv:2108.06842, 2021"],"snippet":"… The second two models are trained with 42B Glove (Common Crawl) vectors, with embedding dimension 300, hidden state di- mension 50 and the same vocabulary. Our motivation of using a Page 4. KDD WIT '21, Aug …","url":["https://arxiv.org/pdf/2108.06842"]}
{"year":"2021","title":"Marked Attribute Bias in Natural Language Inference","authors":["H Dawkins"],"snippet":"… Otherwise, we apply the method to the 2Taken as the GloVe embeddings trained on the Common Crawl corpus for 840B tokens; available at https://nlp.stanford.edu/projects/glove/. Results were not found to vary …","url":["https://aclanthology.org/2021.findings-acl.369.pdf"]}
{"year":"2021","title":"MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding","authors":["J Li, Y Xu, L Cui, F Wei - arXiv preprint arXiv:2110.08518, 2021"],"snippet":"… Equipped with the existing webpage datasets Common Crawl (CC)3, we pre-train MarkupLM with large-scale unlabeled HTML data and evaluate the pre-trained models on web-based structural reading comprehension and information extraction tasks. …","url":["https://arxiv.org/pdf/2110.08518"]}
{"year":"2021","title":"MasakhaNER: Named Entity Recognition for African Languages","authors":["DI Adelani, J Abbott, G Neubig, D D'souza, J Kreutzer… - arXiv preprint arXiv …, 2021"],"snippet":"… XLMR XLMR (Conneau et al., 2020) was trained on 100 languages including Amharic, Hausa, and Swahili. The major differences be tween XLMR and mBERT are (1) XLMR was trained on Common Crawl while mBERT was …","url":["https://arxiv.org/pdf/2103.11811"]}
{"year":"2021","title":"Masked Language Model Entity Matching for Cultural Heritage Data","authors":["D Piché, A Zouaq, M Gagnon, L Font - 2021"],"snippet":"… CamemBERT [10] is trained on the French portion of the OSCAR corpus, a pre-filtered version of Common Crawl, while FlauBERT [8] trains on a combination of French sets, including WMT19, OPUS, Wikimedia and Project Gutenberg. 3 Data overview …","url":["http://ceur-ws.org/Vol-2949/paper5.pdf"]}
{"year":"2021","title":"Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques","authors":["S Yildirim, M Asgari-Chenaghlu - 2021"],"snippet":"Page 1. Mastering Transformers Build state-of-the-art models from scratch with advanced natural language processing techniques 0001) il -0 1 0 -0 -O 0 - -0 1 -0 - -0 1 1. -0 -0 -0 -0 -0 -0 1 1 0 010 1 0 1 0 1 01100110 0001101010 …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=A9Y6EAAAQBAJ&oi=fnd&pg=PP1&dq=commoncrawl&ots=vd7ymgpejf&sig=Vl9kWAVZADIBu0pKOCgd-uJ7Xw8"]}
{"year":"2021","title":"Matches Made in Heaven: Toolkit and Large-Scale Datasets for Supervised Query Reformulation","authors":["N Arabzadeh, A Bigdeli, S Seyedsalehi, M Zihayat…"],"snippet":"Page 1. Matches Made in Heaven: Toolkit and Large-Scale Datasets for Supervised Query Reformulation Negar Arabzadeh University of Waterloo narabzad@uwaterloo. ca Amin Bigdeli Ryerson University abigdeli@ryerson.ca …","url":["http://ls3.rnet.ryerson.ca/wiki/images/a/ab/Matches-made-in-heaven.pdf"]}
{"year":"2021","title":"MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model","authors":["RK Jørgensen, M Hartmann, X Dai, D Elliott - arXiv preprint arXiv:2109.06605, 2021"],"snippet":"Page 1. MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model Rasmus Kær Jørgensen1,2 and Mareike Hartmann3∗ and Xiang Dai1 and Desmond Elliott1 1University of Copenhagen, Denmark 2PricewaterhouseCoopers …","url":["https://arxiv.org/pdf/2109.06605"]}
{"year":"2021","title":"Measuring associational thinking through word embeddings","authors":["C Periñán-Pascual - Artificial Intelligence Review, 2021"],"snippet":"The development of a model to quantify semantic similarity and relatedness between words has been the major focus of many studies in various fields, eg p.","url":["https://link.springer.com/article/10.1007/s10462-021-10056-6"]}
{"year":"2021","title":"Measuring Gender Bias in Word Embeddings for Russian Language","authors":["AS Pestova"],"snippet":"… The effect size is calculated as Cohen's d [6]: mean𝑥𝑥∈𝑋𝑋 𝑐𝑐(𝑥𝑥𝑎 𝐴𝐴𝑎 𝐵𝐵) - mean𝑦𝑦∈𝑌𝑌 𝑐𝑐(𝑦𝑦𝑎 𝐴𝐴𝑎 𝐵𝐵) std - dev𝑤𝑤∈𝑋𝑋∪𝑌𝑌 𝑐𝑐(𝑤𝑤𝑎 𝐴𝐴𝑎 𝐵𝐵) (4) 3https://tatianashavrina.github.io/taiga\\_site …","url":["http://www.dialog-21.ru/media/5322/pestovaas047.pdf"]}
{"year":"2021","title":"Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset","authors":["HR Kirk, Y Jun, P Rauba, G Wachtel, R Li, X Bai… - arXiv preprint arXiv …, 2021"],"snippet":"… F T-SNE Text Embeddings The meme-level embeddings are calculated by (i) extracting a 300-dimensional embedding for each word in the meme, using fastText embeddings trained on Wikipedia and Common Crawl; (ii) …","url":["https://arxiv.org/pdf/2107.04313"]}
{"year":"2021","title":"Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese","authors":["Z Zhang, H Zhang, K Chen, Y Guo, J Hua, Y Wang… - arXiv preprint arXiv …, 2021"],"snippet":"Although pre-trained models (PLMs) have achieved remarkable improvements in a wide range of NLP tasks, they are expensive in terms of time and resources. This calls for the study of training more efficient models with less computation but still …","url":["https://arxiv.org/pdf/2110.06696"]}
{"year":"2021","title":"MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning","authors":["M Xia, G Zheng, S Mukherjee, M Shokouhi, G Neubig… - arXiv preprint arXiv …, 2021"],"snippet":"… The Hausdorff distance between the source and target representations drops from 0.57 to 0.20 with F1 score improvement from 74.07 to 78.15. Wikipedia and XLM-R (Conneau et al., 2020) is pre-trained on 100 languages with CommonCrawl Corpora …","url":["https://arxiv.org/pdf/2104.07908"]}
{"year":"2021","title":"Metric Learning in Multilingual Sentence Similarity Measurement for Document Alignment","authors":["C Rajitha, L Piyarathne, D Sachintha, S Ranathunga - arXiv preprint arXiv …, 2021"],"snippet":"… applied to the training data. XLM-R is trained with common crawl data from 100 languages, and has shown to outperform mBERT on multiple Natural Language Processing tasks. 3 Metric Learning Unsupervised metrics such as …","url":["https://arxiv.org/pdf/2108.09495"]}
{"year":"2021","title":"MFAQ: a Multilingual FAQ Dataset","authors":["M De Bruyn, E Lotfi, J Buhmann, W Daelemans - arXiv preprint arXiv:2109.12870, 2021"],"snippet":"… provides an open repository of the web.5 Common Crawl's complete web archive consists of petabytes of data collected over 10 years of web crawling (Ortiz Suárez et al., 2020). The repository is organized in monthly bucket …","url":["https://arxiv.org/pdf/2109.12870"]}
{"year":"2021","title":"Mind the Gap: Assessing Temporal Generalization in Neural Language Models","authors":["A Lazaridou, A Kuncoro, E Gribovskaya, D Agrawal… - Advances in Neural …, 2021"],"snippet":"Abstract Our world is open-ended, non-stationary, and constantly evolving; thus what we talk about and how we talk about it change over time. This inherent dynamic nature of language contrasts with the current static language modelling …","url":["https://proceedings.neurips.cc/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf"]}
{"year":"2021","title":"Minimum Bayes Risk Decoding with Neural Metrics of Translation Quality","authors":["M Freitag, D Grangier, Q Tan, B Liang - arXiv preprint arXiv:2111.09388, 2021"],"snippet":"This work applies Minimum Bayes Risk (MBR) decoding to optimize diverse automated metrics of translation quality. Automatic metrics in machine translation have made tremendous progress recently. In particular, neural metrics, fine-tuned on …","url":["https://arxiv.org/pdf/2111.09388"]}
{"year":"2021","title":"Mining emotion-aware sequential rules at user-level from micro-blogs","authors":["MP Skenduli, M Biba, C Loglisci, M Ceci, D Malerba - Journal of Intelligent Information …, 2021"],"snippet":"… the fine-grained emotion analysis task. Based on the obtained results, we decide to use multi-lingual word embeddings, pre-trained on Common Crawl and Wikipedia corpora using fastText. These models were trained using …","url":["https://link.springer.com/article/10.1007/s10844-021-00647-8"]}
{"year":"2021","title":"Misinformation Retrieval","authors":["S Rizvi - 2021"],"snippet":"… 34 4.1 Excerpt from a correct document from the Common Crawl Collection with fact used to validate tweets in blue. . . . . 43 viii Page 9 … for a given topic. The corpus used for both tasks was the common crawl corpus consisting of news articles online …","url":["https://uwspace.uwaterloo.ca/bitstream/handle/10012/17609/Rizvi_Saira.pdf?sequence=3&isAllowed=y"]}
{"year":"2021","title":"Mitigating the Position Bias of Transformer Models in Passage Re-Ranking","authors":["S Hofstätter, A Lipani, S Althammer, M Zlabinger… - arXiv preprint arXiv:2101.06980"],"snippet":"… For the Transformer layers in TK we evaluate 2 layers each with 16 attention heads with size 32 and a feed-forward dimension of 100. For kernel-pooling we set the number 3 github.com/microsoft/BlingFire 4 42B CommonCrawl …","url":["https://arxiv.org/pdf/2101.06980"]}
{"year":"2021","title":"MKPM: Multi keyword-pair matching for natural language sentences","authors":["X Lu, Y Deng, T Sun, Y Gao, J Feng, X Sun, R Sutcliffe - Applied Intelligence, 2021"],"snippet":"Sentence matching is widely used in various natural language tasks, such as natural language inference, paraphrase identification and question answering. F.","url":["https://link.springer.com/article/10.1007/s10489-021-02306-5"]}
{"year":"2021","title":"MLLP-VRAIN Spanish ASR Systems for the Albayzin-RTVE 2020 Speech-To-Text Challenge","authors":["J Jorge, A Giménez, P Baquero-Arnal…"],"snippet":"… El Periódico [28] 2677 46637 291 Common Crawl [29] 1719 41792 486 Internal: parliamentary data 1361 35170 126 … [27] “Eldiario.es,” https://www.eldiario.es/. [28] “ElPeriodico.com,” https://www.elperiodico …","url":["https://www.isca-speech.org/archive/IberSPEECH_2021/pdfs/25.pdf"]}
{"year":"2021","title":"mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models","authors":["R Ri, I Yamada, Y Tsuruoka - arXiv preprint arXiv:2110.08151, 2021"],"snippet":"… 2019) trained with CommonCrawl data from 100 languages, and the parameters in common (eg, the weights of the transformer encoder and the word embeddings) are initialized using the checkpoint from the Transformers library.The size of the …","url":["https://arxiv.org/pdf/2110.08151"]}
{"year":"2021","title":"Mobile Testing Framework Exploiting Machine Learning and NLP","authors":["L ARDITO, M MORISIO, R COPPOLA - 2021"],"snippet":"… the NLP step. Bert-Base, Multilingual Cased for BERT feature vector, GloVe Common Crawl 300 dimensional model for Glove, and lastly Fasttext 300 dimensional pre-trained model for Fasttext vector was used. On the android …","url":["https://webthesis.biblio.polito.it/19106/1/tesi.pdf"]}
{"year":"2021","title":"Model Averaging in Distributed Machine Learning: A Case Study with Apache Spark","authors":["Y Guo, Z Zhang, J Jiang, W Wu, C Zhang, B Cui, J Li"],"snippet":"Page 1. Noname manuscript No. (will be inserted by the editor) Model Averaging in Distributed Machine Learning: A Case Study with Apache Spark Yunyan Guo · Zhipeng Zhang · Jiawei Jiang · Wentao Wu · Ce Zhang · Bin …","url":["http://pages.cs.wisc.edu/~wentaowu/papers/vldbj-model-avg.pdf"]}
{"year":"2021","title":"Modeling Endorsement for Multi-Document Abstractive Summarization","authors":["L Lebanoff, B Wang, Z Feng, F Liu - arXiv preprint arXiv:2110.07844, 2021"],"snippet":"A crucial difference between singleand multi-document summarization is how salient content manifests itself in the document(s). While such content may appear at the beginning of a single document, essential information is frequently reiterated in a …","url":["https://arxiv.org/pdf/2110.07844"]}
{"year":"2021","title":"Modeling Global and Local Interactions for Online Conversation Recommendation","authors":["X Zeng, J Li, L Wang, KF Wong - ACM Transactions on Information Systems (TOIS), 2021"],"snippet":"… For the parameters with respect to local interactions, we initialize the word embedding layer with 200-dimensional Glove embedding [66], where the Twitter version is used for our Twitter dataset, and the Common Crawl version is applied on …","url":["https://dl.acm.org/doi/full/10.1145/3473970"]}
{"year":"2021","title":"Modern-Day Shakespeare: Training Set Experiments with a Generative Pre-Trained Transformer-Best Paper","authors":["R Sheverack - 2021"],"snippet":"Page 1. 1 Modern-Day Shakespeare: Training Set Experiments with a Generative Pre-Trained Transformer Applied Project Final Report By Roksolana Maria Sheverack Spring 2021 A paper submitted in partial …","url":["http://archive.nyu.edu/bitstream/2451/62802/2/R.Sheverack%20-%20Final%20Project%20Report%20-%20Modern-Day%20Shakespeare%3ATraining%20Set%20Experiments%20with%20a%20Generative%20Pre-Trained%20Transformer.pdf"]}
{"year":"2021","title":"Monitoring COVID-19 pandemic through the lens of social media using natural language processing and machine learning","authors":["Y Liu, C Whitfield, T Zhang, A Hauser, T Reynolds… - Health Information Science …, 2021"],"snippet":"… The custom NER model that we trained was based on spaCy's multi-task, OntoNotes-trained Convolutional Neural Network which uses GloVe vectors that were trained using Common Crawl [37] corpus. Topic modeling. Topic …","url":["https://link.springer.com/article/10.1007/s13755-021-00158-4"]}
{"year":"2021","title":"Monolingual and Cross-Lingual Acceptability Judgments with the Italian CoLA corpus","authors":["D Trotta, R Guarasci, E Leonardelli, S Tonelli - arXiv preprint arXiv:2109.12053, 2021"],"snippet":"… Each sentence is represented as a sequence of word embeddings, obtained with the Italian model of FastText (Grave et al., 2018) trained on Common Crawl and Wikipedia with size 300.4 The network is implemented with Keras …","url":["https://arxiv.org/pdf/2109.12053"]}
{"year":"2021","title":"Moral rights","authors":["NB Eve"],"snippet":"… offer other services, including: CC Search, which makes openly licensed material easier to discover and use. As it makes use of the Common Crawl dataset, creators do not have to apply to be included (“About CC Search”) …","url":["http://sharedspaces.usask.ca/ipandcopyright"]}
{"year":"2021","title":"More Data and New Tools. Advances in Parsing the Index Thomisticus Treebank","authors":["F Gamba, M Passarotti, P Ruffolo - Proceedings http://ceur-ws. org ISSN, 2021"],"snippet":"Abstract This paper investigates the recent advances in parsing the Index Thomisticus Treebank, which encompasses Medieval Latin texts by Thomas Aquinas. The research focuses on two types of variables. On the one hand, it examines the …","url":["http://ceur-ws.org/Vol-2989/long_paper20.pdf"]}
{"year":"2021","title":"More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering","authors":["Y Bai, DZ Wang - arXiv preprint arXiv:2109.12264, 2021"],"snippet":"… Long-form QA ELI5[16] 2019 EN Web. Common Crawl Reddit Reddit Free-form ROUG-1,2,L MASH-QA[17] 2020 EN Health WebMD QA section QA section Span of words F1, EM NLQuAD[18] 2021 EN News BBC …","url":["https://arxiv.org/pdf/2109.12264"]}
{"year":"2021","title":"Morph Call: Probing Morphosyntactic Content of Multilingual Transformers","authors":["V Mikhailov, O Serikov, E Artemova - arXiv preprint arXiv:2104.12847, 2021"],"snippet":"… languages. D-BERT (Sanh et al., 2019) or DistilBERT is a 6-layer distilled version of M-BERT model. XLM-R (Conneau et al., 2019) was pre-trained over filtered CommonCrawl data in 100 languages (Wenzek et al., 2019). MiniLM …","url":["https://arxiv.org/pdf/2104.12847"]}
{"year":"2021","title":"Morphological Skip-Gram: Replacing FastText characters n-gram with morphological knowledge","authors":["TD Bispo, FAO Santos, HT Macedo, C Zanchettin - Inteligencia Artificial, 2021"],"snippet":"Page 1. Inteligencia Artificial, 24(67), 1-17 doi: 10.4114/intartif.vol24iss67pp1-17 INTELIGENCIA ARTIFICIAL http://journal.iberamia.org/ Morphological Skip-Gram: Replacing FastText characters n-gram with morphological knowledge …","url":["https://www.journal.iberamia.org/index.php/intartif/article/download/588/131"]}
{"year":"2021","title":"Multi-granular Legal Topic Classification on Greek Legislation","authors":["C Papaloukas, I Chalkidis, K Athinaios, DA Pantazi… - arXiv preprint arXiv …, 2021"],"snippet":"… learning rate. Based on this study, Conneau et al. (2020) proposed the XLMRoBERTa model, which supports 100 different languages (Greek included) and is trained on 2.5TB of filtered Common Crawl data. GREEK-BERT …","url":["https://arxiv.org/pdf/2109.15298"]}
{"year":"2021","title":"Multi-Granularity Contrasting for Cross-Lingual Pre-Training","authors":["S Li, P Yang, F Luo, J Xie"],"snippet":"… Following Conneau et al. (2019), we re- construct Common-Crawl Corpus to obtain monolingual training datasets while the bilingual data is obtained from the OPUS website 5. We also conduct up/down-sample for all pre-training data with a smoothing parameter …","url":["https://aclanthology.org/2021.findings-acl.149.pdf"]}
{"year":"2021","title":"Multi-level Embeddings for Processing Arabic Social Media Contents","authors":["L Moudjari, F Benamara, K Akli-Astouati - Computer Speech & Language, 2021"],"url":["https://www.sciencedirect.com/science/article/pii/S0885230821000474"]}
{"year":"2021","title":"Multi-Level Sentiment Analysis of Product Reviews Based on Grammar Rules","authors":["HD Nguyen, T Le, K Tran, ST Luu, SN Hoang, HT Phan - New Trends in Intelligent …, 2021"],"snippet":"… BERT is a transfer learning method which is trained on large language corpus such as Wikipedia and Common Crawl in different languages, then applied to specific tasks by fine tuning the model [7]. The appearance of BERT makes …","url":["https://ebooks.iospress.nl/volumearticle/57466"]}
{"year":"2021","title":"Multi-task Learning Using a Combination of Contextualised and Static Word Embeddings for Arabic Sarcasm Detection and Sentiment Analysis","authors":["AI Alharbi, M Lee - Proceedings of the Sixth Arabic Natural Language …, 2021"],"snippet":"… al., 2017). The Ara2Vec word embeddings is a well-known open-source tool made up of six Arabic models. Data was obtained from Wikipedia, Twitter and Common Crawl (webpage crawl data) to train Ara2Vec. Al- though it …","url":["https://www.aclweb.org/anthology/2021.wanlp-1.39.pdf"]}
{"year":"2021","title":"MultiEmo: Multilingual, Multilevel, Multidomain Sentiment Analysis Corpus of Consumer Reviews","authors":["K Kanclerz"],"snippet":"… The first model, Unsupervised Cross-lingual Representation Learning at Scale (XLM-RoBERTa), is a large multillingual language model, trained on 2.5TB of filtered CommonCrawl data, using self-supervised training techniques to achieve state-of-the-art performance …","url":["https://www.iccs-meeting.org/archive/iccs2021/papers/127430291.pdf"]}
{"year":"2021","title":"MultiEURLEX--A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer","authors":["I Chalkidis, M Fergadiotis, I Androutsopoulos - arXiv preprint arXiv:2109.00904, 2021"],"snippet":"… It is pretrained on Common Crawl with a vocabulary of 250k sub-words shared across languages … (2021) released a multilingual variant of T5 (Raffel et al., 2020), an encoder-decoder TRANSFORMER-based model …","url":["https://arxiv.org/pdf/2109.00904"]}
{"year":"2021","title":"Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification","authors":["S Rönnqvist, V Skantsi, M Oinonen, V Laippala - NoDaLiDa 2021, 2021"],"snippet":"… The English CORE is based on unrestricted search queries of extremely frequent n-grams, while the other datasets are randomly sampled from the 2017 CoNLL Shared Task datasets, originally drawn from Common Crawl (Ginter et al., 2017) …","url":["https://ep.liu.se/ecp/178/ecp2021178.pdf#page=173"]}
{"year":"2021","title":"Multilingual Autoregressive Entity Linking","authors":["N De Cao, L Wu, K Popat, M Artetxe, N Goyal… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. Multilingual Autoregressive Entity Linking Nicola De Cao1,2, Ledell Wu1, Kashyap Popat1, Mikel Artetxe1, Naman Goyal1, Mikhail Plekhanov1, Luke Zettlemoyer1,3, Nicola Cancedda1, Sebastian Riedel1,4, Fabio Petroni1 …","url":["https://arxiv.org/pdf/2103.12528"]}
{"year":"2021","title":"Multilingual Counter Narrative Type Classification","authors":["YL Chung, M Guerini, R Agerri - arXiv preprint arXiv:2109.13664, 2021"],"snippet":"… The XLM-R language model (Conneau et al., 2020), pre-trained on CommonCrawl for 100 languages, reports strong performance on several cross-lingual downstream tasks such as natural language inference or …","url":["https://arxiv.org/pdf/2109.13664"]}
{"year":"2021","title":"Multilingual discussion summarisation in social networks","authors":["TN Andreevic - 2021"],"snippet":"Page 1. Санкт–Петербургский государственный университет Тарасов Никита Андреевич Выпускная квалификационная работа Суммаризация мультиязычных дискуссий в социальных сетях Уровень образования: магистратура …","url":["https://dspace.spbu.ru/bitstream/11701/32265/1/thesis.pdf"]}
{"year":"2021","title":"Multilingual ELMo and the Effects of Corpus Sampling","authors":["V Ravishankar, A Kutuzov, L Øvrelid, E Velldal - NoDaLiDa 2021, 2021"],"snippet":"… 3.2 Sampling Our initial starting point for collecting the language model training corpora were the CoNLL 2017 Wikipedia/Common Crawl dumps released as part of the shared task on Universal Dependencies parsing …","url":["https://ep.liu.se/ecp/178/ecp2021178.pdf#page=395"]}
{"year":"2021","title":"Multilingual Epidemic Event Extraction","authors":["S Mutuvi, E Boros, A Doucet, G Lejeune, A Jatowt… - International Conference on …, 2021"],"snippet":"Abstract In this paper, we focus on epidemic event extraction in multilingual and low-resource settings. The task of extracting epidemic events is defined as the detection of disease names and locations in a document. We experiment with a multilingual …","url":["https://link.springer.com/chapter/10.1007/978-3-030-91669-5_12"]}
{"year":"2021","title":"Multilingual Learning for Mild Cognitive Impairment Screening from a Clinical Speech Task","authors":["H Lindsay, P Müller, I Kröger, J Tröger, N Linz, A König… - 2021"],"snippet":"… al., 2020a). As semantic vectors are learned from large amounts of text corpora (usually Wikipedia and OSCAR common crawl), embedding quality heavily depends on the quantity of the available training data. While French …","url":["https://www.researchgate.net/profile/Hali-Lindsay-2/publication/354604884_Multilingual_Learning_for_Mild_Cognitive_Impairment_Screening_from_a_Clinical_Speech_Task/links/61420ec62db97e68051fbf26/Multilingual-Learning-for-Mild-Cognitive-Impairment-Screening-from-a-Clinical-Speech-Task.pdf"]}
{"year":"2021","title":"Multilingual Named Entity Recognition through Data and Model Transfer","authors":["S Palma-Suominen - 2021"],"snippet":"Page 1. UNIVERSITY OF HELSINKI FACULTY OF ARTS DEPARTMENT OF DIGITAL HUMANITIES Master's Thesis Multilingual Named Entity Recognition through Data and Model Transfer Saara Palma-Suominen 014291850 …","url":["https://helda.helsinki.fi/bitstream/handle/10138/330451/palma-suominen_saara_tutkielma_2021.pdf?sequence=2"]}
{"year":"2021","title":"Multilingual Neural Semantic Parsing for Low-Resourced Languages","authors":["M Xia, E Monti - arXiv preprint arXiv:2106.03469, 2021"],"snippet":"… It is trained for 100 languages on the CommonCrawl corpus, which is several orders of magnitude larger than the Wikipedia dump, especially for low-resourced languages. We use the public XLM-R Base model3 (12-layer, 768-hidden, 12 heads) in our experiment …","url":["https://arxiv.org/pdf/2106.03469"]}
{"year":"2021","title":"Multilingual Sentiment Analysis and Toxicity Detection for Text Messages in Russian","authors":["D Bogoradnikova, O Makhnytkina, A Matveev… - 2021 29th Conference of …, 2021"],"snippet":"… 179M pa- rameters, trained on cased texts in the top 104 languages with the largest Wikipedias), and xlm-roberta (270M parameters with 12-layers, 768-hidden-state, 3072 feed-forward hiddenstate, 8-heads, trained on 2.5 …","url":["https://ieeexplore.ieee.org/abstract/document/9435584/"]}
{"year":"2021","title":"Multilingual Transfer Learning for Code-Switched Language and Speech Neural Modeling","authors":["GI Winata - arXiv preprint arXiv:2104.06268, 2021"],"snippet":"Page 1. Multilingual Transfer Learning for Code-Switched Language and Speech Neural Modeling by Genta Indra Winata A Thesis Submitted to The Hong Kong University of Science and Technology in Partial Fulfillment of the Requirements for …","url":["https://arxiv.org/pdf/2104.06268"]}
{"year":"2021","title":"Multilingual Translation from Denoising Pre-Training","authors":["Y Tang, C Tran, X Li, PJ Chen, N Goyal, V Chaudhary…"],"snippet":"… al., 2019; Brown et al., 2020). Monolingual data is far more prevalent for low resource languages, particularly in resources such as Wikipedia or Commoncrawl, a version of the web. Recent work has explored monolingual denoising …","url":["https://aclanthology.org/2021.findings-acl.304.pdf"]}
{"year":"2021","title":"Multilingual Zero-Shot and Few-Shot Causality Detection","authors":["SM Reimann - 2021"],"snippet":"… BPE vocabulary with 100k subwords. Moreover, regarding training data, they build a CommonCrawl corpus for 100 languages, which is notably larger than the Wikipedia corpus used for mBERT. For pretraining XLM-R, they …","url":["https://www.diva-portal.org/smash/get/diva2:1569716/FULLTEXT01.pdf"]}
{"year":"2021","title":"Multimodal and Multilingual Embeddings for Large-Scale Speech Mining","authors":["PA Duquenne, H Gong, H Schwenk - Advances in Neural Information Processing …, 2021"],"snippet":"… Using a similarity metric in that multimodal embedding space, we perform mining of audio in German, French, Spanish and English from Librivox against billions of sentences from Common Crawl. This yielded more than twenty thousand hours of …","url":["https://proceedings.neurips.cc/paper/2021/file/8466f9ace6a9acbe71f75762ffc890f1-Paper.pdf"]}
{"year":"2021","title":"Multimodal datasets: misogyny, pornography, and malignant stereotypes","authors":["A Birhane, VU Prabhu, E Kahembwe - arXiv preprint arXiv:2110.01963, 2021"],"snippet":"… project to train a GDPR-Article(9)-Sensitive-URL classifier which revealed that, of the 1 Billion URLs they audited in the Common Crawl project, 155 million URLs fell into the sensitive category. The Realtoxicityprompts work …","url":["https://arxiv.org/pdf/2110.01963"]}
{"year":"2021","title":"Multimodal Emotion Recognition Using Transfer Learning on Audio and Text Data","authors":["JJ Deng, CHC Leung, Y Li - … Conference on Computational Science and Its …, 2021"],"snippet":"… As for text representation, we adopt fine-tuned Text-To-Text Transfer Transformer (T5) [23] model trained by a common crawl (C4) dataset to extract text embeddings. Operations of transfer learning have greatly accelerated model training and applied in specific domains …","url":["https://link.springer.com/chapter/10.1007/978-3-030-86970-0_39"]}
{"year":"2021","title":"Multimodal MRI cerebral correlates of verbal fluency switching and its impairment in women with depression","authors":["L Domain, M Guillery, N Linz, A König, JM Batail… - NeuroImage: Clinical, 2021"],"snippet":"Background The search of biomarkers in the field of depression requires easy implementable tests that are biologically rooted. Qualitative analysis of verbal fluency tests (VFT) are good candidates, but its cerebral correlates are unknown …","url":["https://www.sciencedirect.com/science/article/pii/S2213158221003545"]}
{"year":"2021","title":"Multimodal or Text? Retrieval or BERT? Benchmarking Classifiers for the Shared Task on Hateful Memes","authors":["V Kougia, J Pavlopoulos"],"snippet":"… for English pre-trained on Common Crawl (Grave et al., 2018) (MNN:base). 2 The second variant em- ploys three BERT models, each fine-tuned on one of our tasks (see subsection 3.2), from which we extracted the CLS tokens …","url":["https://aclanthology.org/2021.woah-1.24.pdf"]}
{"year":"2021","title":"Multiplicative Position-aware Transformer Models for Language Understanding","authors":["Z Huang, D Liang, P Xu, B Xiang - arXiv preprint arXiv:2109.12788, 2021"],"snippet":"Page 1. Multiplicative Position-aware Transformer Models for Language Understanding Zhiheng Huang AWS AI zhiheng@amazon.com Davis Liang AWS AI liadavis@amazon.com Peng Xu AWS AI pengx@amazon.com …","url":["https://arxiv.org/pdf/2109.12788"]}
{"year":"2021","title":"Multitask Learning for Complaint Identification and Sentiment Analysis","authors":["A Singh, S Saha, M Hasanuzzaman, K Dey - Cognitive Computation, 2021"],"snippet":"In today's competitive business world, customer service is often at the heart of businesses that can help strengthen their brands. Resolution of cust.","url":["https://link.springer.com/article/10.1007/s12559-021-09844-7"]}
{"year":"2021","title":"MuRIL: Multilingual Representations for Indian Languages","authors":["S Khanuja, D Bansal, S Mehtani, S Khosla, A Dey… - arXiv preprint arXiv …, 2021"],"snippet":"… Monolingual Data: We collect monolingual data for the 17 languages mentioned above from the Common Crawl OSCAR corpus1 and Wikipedia2 … Wikipedia Common Crawl Wikipedia Common Crawl PMINDIA …","url":["https://arxiv.org/pdf/2103.10730"]}
{"year":"2021","title":"My House, My Rules: Learning Tidying Preferences with Graph Neural Networks","authors":["I Kapelyukh, E Johns - arXiv preprint arXiv:2111.03112, 2021"],"snippet":"… We load word embeddings from a FastText model [20], pre-trained on the Common Crawl dataset of over 600 billion tokens. Names of objects are provided to NeatNet either by the simulator or by an object detection system if deployed …","url":["https://arxiv.org/pdf/2111.03112"]}
{"year":"2021","title":"Nahuatl Neural Machine Translation Using Attention Based Architectures: A Comparative Analysis for RNNs and Transformers as a Mobile Application Service","authors":["SK Bello García, E Sánchez Lucero, E Bonilla Huerta… - … International Conference on …, 2021"],"snippet":"… Trained with 175 billions parameters and more than a trillion words from Common Crawl’s [27] and Wikipedia, making GPT-3 the largest language model at the time. The architecture is composed by 96 stacked encoder-decoder layers, 96 multi-head …","url":["https://link.springer.com/chapter/10.1007/978-3-030-89820-5_10"]}
{"year":"2021","title":"Named Entity Recognition and Classification on Historical Documents: A Survey","authors":["M Ehrmann, A Hamdi, EL Pontes, M Romanello… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. Named Entity Recognition and Classification on Historical Documents: A Survey MAUD EHRMANN, Ecole Polytechnique Fédérale de Lausanne AHMED HAMDI, University of La Rochelle ELVYS LINHARES PONTES …","url":["https://arxiv.org/pdf/2109.11406"]}
{"year":"2021","title":"Named entity recognition and linking augmented with large-scale structured data","authors":["P Rychlikowski, A Lancucki, A Kaczmarek, B Najdecki… - Proceedings of the 8th …, 2021"],"snippet":"… possible. This line of reasoning applied especially to word-level embeddings. Ideally we wanted our systems to have, for every language, em- beddings trained on Wikipedia, Common Crawl1, and a collection of news articles. We …","url":["https://www.aclweb.org/anthology/2021.bsnlp-1.14.pdf"]}
{"year":"2021","title":"Named Entity Recognition and Normalization in Biomedical Literature: A Practical Case in SARS-CoV-2 Literature","authors":["ÁA Casero - 2021"],"snippet":"Page 1. Universidad Politécnica de Madrid Escuela Técnica Superior de Ingenieros Informáticos Máster Universitario en Inteligencia Artificial Trabajo Fin de Máster Named Entity Recognition and Normalization in Biomedical Literature: A …","url":["http://oa.upm.es/67933/1/TFM_ALVARO_ALONSO_CASERO.pdf"]}
{"year":"2021","title":"Naming unrelated words predicts creativity","authors":["JA Olson, J Nahas, D Chmoulevitch, SJ Cropper… - Proceedings of the National …, 2021"],"snippet":"Skip to main content. Main menu. Home; Articles: Current; Special Feature Articles - Most Recent; Special Features; Colloquia; Collected Articles; PNAS Classics; List of Issues. Front Matter: Front Matter Portal; Journal Club. News: For …","url":["https://www.pnas.org/content/pnas/118/25/e2022340118.full.pdf"]}
{"year":"2021","title":"Natural and artificial intelligence; natural and artificial language","authors":["D Santos - 2021"],"snippet":"… desinteligência, Inteligência, inteligência-do-cinema, inteligibilidade, inteligXancia, deligência, inteligencia, inteligência (0.85) 300 columns, obtained on 2 billion words, common crawl: fasttext, words: inteligencia (0.77), Inteligência, ainteligência …","url":["https://www.linguateca.pt/Diana/download/SantosSLATE2021.pdf"]}
{"year":"2021","title":"Natural language inference for Malayalam language using language agnostic sentence representation","authors":["S Renjit, S Idicula - PeerJ Computer Science, 2021"],"snippet":"… It is available in many languages, including Malayalam. fastText provided pretrained word vector representations trained on Common Crawl and Wikipedia data in 294 languages, including Malayalam, which we utilized to create sentence representations …","url":["https://peerj.com/articles/cs-508/"]}
{"year":"2021","title":"Negation typology and general representation models for cross-lingual zero-shot negation scope resolution in Russian, French, and Spanish.","authors":["A Shaitarova, F Rinaldi - Proceedings of the 2021 Conference of the North …, 2021"],"snippet":"… We build on this work and compare mBERT with a new multilingual general purpose representation model, XLM-R. Unlike mBERT, XLM-R was pre-trained on more than two terabytes of filtered data collected by CommonCrawl …","url":["https://www.aclweb.org/anthology/2021.naacl-srw.3.pdf"]}
{"year":"2021","title":"Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding","authors":["Z Wang, L Wang, T Wu, T Li, G Wu - arXiv preprint arXiv:2109.04872, 2021"],"snippet":"Page 1. Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding Zhenzhi Wang, Limin Wang*, Tao Wu, Tianhao Li, Gangshan Wu State Key Laboratory for Novel Software Technology, Nanjing University …","url":["https://arxiv.org/pdf/2109.04872"]}
{"year":"2021","title":"Network embeddings from distributional thesauri for improving static word representations","authors":["A Jana, S Haldar, P Goyal - Expert Systems with Applications, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0957417421012276"]}
{"year":"2021","title":"Neural approaches to discourse coherence: modeling, evaluation and application","authors":["Y Farag - 2021"],"snippet":"Page 1. Neural approaches to discourse coherence: modeling, evaluation and application Youmna Farag Murray Edwards College This dissertation is submitted in September 2020 for the degree of Doctor of Philosophy Page 2. Page 3. Declaration …","url":["https://www.repository.cam.ac.uk/bitstream/handle/1810/322567/thesis.pdf?sequence=3"]}
{"year":"2021","title":"Neural Machine Translation: A Review of Methods, Resources, and Tools","authors":["Z Tan, S Wang, Z Yang, G Chen, X Huang, M Sun… - arXiv preprint arXiv …, 2020"],"snippet":"Page 1. Neural Machine Translation: A Review of Methods, Resources, and Tools Zhixing Tana,c,d, Shuo Wanga,c,d, Zonghan Yanga,c,d, Gang Chena,c,d, Xuancheng Huanga,c,d, Maosong Suna,c,d,e, Yang Liua,b,c,d,e,∗ …","url":["https://arxiv.org/pdf/2012.15515"]}
{"year":"2021","title":"Neural Methods for Answer Passage Retrieval over Sparse Collections","authors":["D Cohen - 2021"],"snippet":"Page 1. University of Massachusetts Amherst ScholarWorks@UMass Amherst Doctoral Dissertations Dissertations and Theses April 2021 Neural Methods for Answer Passage Retrieval over Sparse Collections Daniel Cohen …","url":["https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=3157&context=dissertations_2"]}
{"year":"2021","title":"Neural Representation Learning onTables","authors":["H Sun"],"snippet":"Page 1. From Tables to Knowledge (KDD'21): Jay Pujara, Pedro Szekely, Huan Sun, Muhao Chen Huan Sun The Ohio State University From Tables to Knowledge: Recent Advances in Table Understanding (Part III) Neural …","url":["https://pdfs.semanticscholar.org/62f0/dc599074928447c2582a136e08151255f79c.pdf"]}
{"year":"2021","title":"Neural Search: Learning Query and Product Representations in Fashion E-commerce","authors":["L Kumar, S Sarkar - arXiv preprint arXiv:2107.08291, 2021"],"snippet":"Page 1. Neural Search: Learning Query and Product Representations in Fashion E-commerce Sagnik Sarkar* sagnik.sarkar@myntra.com Myntra Designs Pvt. Ltd. India Lakshya Kumar* lakshya.kumar@myntra.com Myntra Designs Pvt. Ltd. India …","url":["https://arxiv.org/pdf/2107.08291"]}
{"year":"2021","title":"Neural Sequential Modeling and Applications","authors":["G Lai - 2021"],"snippet":"Page 1. Neural Sequential Modeling and Applications Guokun Lai CMU-LTI-21-010 Language Technology Institute School of Computer Science Carnegie Mellon University 5000 Forbes Ave., Pittsburgh, PA 15213 www.lti.cs.cmu.edu …","url":["https://www.lti.cs.cmu.edu/sites/default/files/lai%2C%20guokun%20-%20Thesis.pdf"]}
{"year":"2021","title":"Neural Supervised Domain Adaptation by Augmenting Pre-trained Models with Random Units","authors":["S Meftah, N Semmar, Y Tamaazousti, H Essafi, F Sadat - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. Neural Supervised Domain Adaptation by Augmenting Pre-trained Models with Random Units Sara Meftah∗, Nasredine Semmar∗, Youssef Tamaazousti∗, Hassane Essafi∗, Fatiha Sadat+ ∗CEA-List, Université Paris …","url":["https://arxiv.org/pdf/2106.04935"]}
{"year":"2021","title":"Neural Text Generation with Artificial Negative Examples to Address Repeating and Dropping Errors","authors":["K Shirai, K Hashimoto, A Eriguchi, T Ninomiya, S Mori - Journal of Natural Language …, 2021"],"snippet":"… Both Japanese and English sentences were preprocessed as recommended in WAT'15.5 For the De-En task, we used the datasets provided by WMT'166 as well as all parallel corpora (Europarl v7 …","url":["https://www.jstage.jst.go.jp/article/jnlp/28/3/28_751/_pdf"]}
{"year":"2021","title":"Neural Text Generation with Artificial Negative Examples","authors":["K Shirai, K Hashimoto, A Eriguchi, T Ninomiya, S Mori - arXiv preprint arXiv …, 2020"],"snippet":"… For the De-En task, we used the datasets provided by WMT'166 and used all parallel corpora (Europarl v7, Common Crawl corpus, and News Commentary v11) for training, NewsTest2013 for development, and NewsTest2014 for testing …","url":["https://arxiv.org/pdf/2012.14124"]}
{"year":"2021","title":"Neural Transfer Learning for Domain Adaptation in Natural Language Processing","authors":["S Meftah - 2021"],"snippet":"Page 1. Neural Transfer Learning for Domain Adaptation in Natural Language Processing Apprentissage par Transfert Neuronal pour l'Adaptation aux Domaines en Traitement Automatique de la Langue Thèse de doctorat de l'université Paris-Saclay …","url":["https://www.theses.fr/2021UPASG021.pdf"]}
{"year":"2021","title":"Neural Transfer Learning with Transformers for Social Science Text Analysis","authors":["S Wankmüller - arXiv preprint arXiv:2102.02111, 2021"],"snippet":"Page 1. Neural Transfer Learning with Transformers for Social Science Text Analysis Sandra Wankmüller Ludwig-Maximilians-University Munich sandra.wankmueller@ gsi.lmu.de Abstract. During the last years, there have been …","url":["https://arxiv.org/pdf/2102.02111"]}
{"year":"2021","title":"NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons Learned","authors":["S Min, J Boyd-Graber, C Alberti, D Chen, E Choi… - arXiv preprint arXiv …, 2021"],"snippet":"… T5-XL+SSM T5 [13] is a text-to-text Transformer language model utilizing an encoder-decoder architecture that was pre-trained using a “span corruption” objective on a cleaned and de-duplicated subset of Common Crawl. Roberts et al …","url":["https://arxiv.org/pdf/2101.00133"]}
{"year":"2021","title":"New Synonyms Extraction Model Based on a Novel Terms Weighting Scheme","authors":["AH Ababneh, J Lu, Q Xu"],"snippet":"Page 1. 171 JIOS, VOL. 45. NO. 1 (2021), PP. 171-221 JIOS, VOL. 45, NO. 1 (2021) SUBMITTED 08/20; ACCEPTED 11/20 New Synonyms Extraction Model Based on a Novel Terms Weighting Scheme Ahmad Hussein Ababneh ahmad.ababneh@aum.edu.jo …","url":["https://jios.foi.hr/index.php/jios/article/view/1526/904"]}
{"year":"2021","title":"News Headline Summarization","authors":["A Kulkarni, A Shivananda, A Kulkarni - Natural Language Processing Projects, 2022"],"snippet":"… This transformer model is trained on the Colossal Clean Common Crawl (C4) data set. The architecture is integrated to devise text2text model. And hence T5 can also be used for many text-to-text-based applications. …","url":["https://link.springer.com/chapter/10.1007/978-1-4842-7386-9_10"]}
{"year":"2021","title":"NICT-2 Translation System at WAT-2021: Applying a Pretrained Multilingual Encoder-Decoder Model to Low-resource Language Pairs","authors":["K Imamura, E Sumita - Malay (Ms)"],"snippet":"… model can learn multiple languages. The published pretrained mBART model2 consists of a 12-layer encoder and decoder with a model dimension of 1,024 on 16 heads. This model was trained on 25 languages in the Common Crawl corpus (Wenzek et al., 2019) …","url":["https://aclanthology.org/2021.wat-1.8.pdf"]}
{"year":"2021","title":"NLI Data Sanity Check: Assessing the Effect of Data Corruption on Model Performance","authors":["A Talman, M Apidianaki, S Chatzikyriakidis… - arXiv preprint arXiv …, 2021"],"snippet":"… 4The contexts for R1 and R2 consist of sentences re- trieved from Wikipedia. In R3 the contexts are retrieved from Wikipedia, News (Common Crawl), fiction, The Children's Book Test (CBT), formal spoken text and …","url":["https://arxiv.org/pdf/2104.04751"]}
{"year":"2021","title":"NLM at TREC 2020 Health Misinformation and Deep Learning Tracks","authors":["Y Mrabet, M Sarrouti, AB Abacha, S Gayen, T Goodwin…"],"snippet":"… Figure 1 presents an overview of our data pipeline, approaches and workflow. 1 Common Crawl News: https://github.com/commoncrawl/news-crawl 2 https://github.com/optimaize/ language-detector Page 2. 2 Authors Suppressed Due to Excessive Length Fig …","url":["https://trec.nist.gov/pubs/trec29/papers/NLM.HM.DL.pdf"]}
{"year":"2021","title":"No News is Good News: A Critique of the One Billion Word Benchmark","authors":["H Ngo, JGM Araújo, J Hui, N Frosst - arXiv preprint arXiv:2110.12609, 2021"],"snippet":"… We train models solely on Common Crawl web scrapes partitioned by year, and demonstrate that they perform worse on this task over time due to distributional shift. Analysis of this corpus reveals that it contains several examples of harmful text, as …","url":["https://arxiv.org/pdf/2110.12609"]}
{"year":"2021","title":"Node. js based Document Store for Web Crawling","authors":["D Bui - 2021"],"snippet":"… and internet archiving such as the Common Crawl Project. While the Internet Archive focuses on preservation, Common Crawl focuses on making web crawled data publicly accessible for scientific analysis and studies [3]. Common Crawl …","url":["https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=2039&context=etd_projects"]}
{"year":"2021","title":"NormFormer: Improved Transformer Pretraining with Extra Normalization","authors":["S Shleifer, J Weston, M Ott - arXiv preprint arXiv:2110.09456, 2021"],"snippet":"During pretraining, the Pre-LayerNorm transformer suffers from a gradient magnitude mismatch: gradients at early layers are much larger than at later layers. These issues can be alleviated by our proposed NormFormer architecture, which …","url":["https://arxiv.org/pdf/2110.09456"]}
{"year":"2021","title":"Not a Mirror, but an Engine: Digital Methods for Contextual Analysis of “Social Big Data”","authors":["JA Schwarz - Digital Human Sciences"],"snippet":"… In SNA, for example, the set of social relations is always at least two-dimensional 16 and is usually represented in the form of a node table (listing all the nodes 14 There are, eg, open science projects like …","url":["https://stockholmuniversitypress.se/site/books/10.16993/bbk/download/7869/#page=34"]}
{"year":"2021","title":"Not Far Away, Not So Close: Sample Efficient Nearest Neighbour Data Augmentation via MiniMax","authors":["E Kamalloo, M Rezagholizadeh, P Passban, A Ghodsi - arXiv preprint arXiv …, 2021"],"snippet":"… The aim of this stage is to find interpretable unannotated augmented samples that are semantically close to training data. For this purpose, we use one of the sentence repositories from SentAugment (Du et al., 2021), comprising …","url":["https://arxiv.org/pdf/2105.13608"]}
{"year":"2021","title":"Novelty Detection in Textual Data using Document Encoders and Self Attention.","authors":["A Parkhi - 2021"],"snippet":"… two-layer neural network. We try to minimize the negative log-likelihood of the next word for an entire corpus. GloVe vectors are trained on the common crawl dataset, consisting of 840B tokens and 2.2M different tokens. We use glove vectors with a spacy …","url":["https://repository.lib.ncsu.edu/bitstream/handle/1840.20/38854/etd.pdf?sequence=1&isAllowed=y"]}
{"year":"2021","title":"NPVec1: Word Embeddings for Nepali-Construction and Evaluation","authors":["P Koirala, NB Niraula"],"snippet":"… Common Crawl3 is a non-profit organization which collects data through web crawling and makes it publicly available. 3.1.3 Nepali Wikipedia Corpus We obtained Nepali Wikipedia corpus from Kaggle (Gaurav, 2020) …","url":["https://aclanthology.org/2021.repl4nlp-1.18.pdf"]}
{"year":"2021","title":"NUIG at TIAD 2021: Cross-lingual Word Embeddings for Translation Inference","authors":["S Banerjee, JP McCrae"],"snippet":"… concepts WordNet [13] graph analysis and cross-lingual word embeddings monolingual corpora of Common Crawl and Wikipedia [8] graph analysis relying on paths, synonyms, similarities and cardinality in the translation graph …","url":["https://sinaahmadi.github.io/docs/articles/ahmadi2021TIAD.pdf"]}
{"year":"2021","title":"NukeLM: Pre-Trained and Fine-Tuned Language Models for the Nuclear and Energy Domains","authors":["L Burke, K Pazdernik, D Fortin, B Wilson, R Goychayev… - arXiv preprint arXiv …, 2021"],"snippet":"… which tokens to predict in each batch rather than deciding offline, before training; it uses much larger batch sizes; it uses byte-level tokenization instead of character-level; and finally, it considers much more pre-training data …","url":["https://arxiv.org/pdf/2105.12192"]}
{"year":"2021","title":"Obtaining Better Static Word Embeddings Using Contextual Embedding Models","authors":["P Gupta, M Jaggi - arXiv preprint arXiv:2106.04302, 2021"],"snippet":"… We also evaluate some pre-trained 300 dimensional GLOVE (Pennington et al., 2014) and FASTTEXT (Bojanowski et al., 2016) models in Table 3. The GLOVE model was trained on Common-Crawl corpus of 840 Billion …","url":["https://arxiv.org/pdf/2106.04302"]}
{"year":"2021","title":"of black-box classifiers'","authors":["K Sinha - 2021"],"snippet":"… The embeddings used came from the 840b token Common Crawl pre-trained 300 dimensional GloVe vectors, limited to the vocabulary present in the SST dataset4. Ultimately, this model achieves 84% accuracy on the bi- nary SST classification task …","url":["https://www.researchgate.net/profile/Mario-Holubar/publication/352058956_Re_Replication_Study_of_'Generative_causal_explanations_of_black-box_classifiers'/links/60b77888a6fdcc476be1c863/Re-Replication-Study-of-Generative-causal-explanations-of-black-box-classifiers.pdf"]}
{"year":"2021","title":"Offensive Language Detection in Turkish Tweets with Bert Models","authors":["A Özberk, İ Çiçekli - 2021 6th International Conference on Computer …, 2021"],"snippet":"… XLM-RoBERTa that is a transformer-based cross-lingual model is pre-trained using a multilingual masNed language model objective on 2.5 TB of CommonCrawl data in 100 languages [15]. Our offensive language detection models use BERT …","url":["https://ieeexplore.ieee.org/abstract/document/9559000/"]}
{"year":"2021","title":"Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling","authors":["A Hande, K Puranik, K Yasaswini, R Priyadharshini… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling Adeep Handea,, Karthik Puranika,, Konthala Yasaswinia, Ruba Priyadharshinib, Sajeetha Thavareesanc …","url":["https://arxiv.org/pdf/2108.12177"]}
{"year":"2021","title":"Offensive, aggressive, and hate speech analysis: From data-centric to human-centered approach","authors":["J Kocoń, A Figas, M Gruza, D Puchalska… - Information Processing & …, 2021"],"url":["https://www.sciencedirect.com/science/article/pii/S0306457321001333"]}
{"year":"2021","title":"Omission of Information: Identifying Political Slant via an Analysis of Co-occurring Entities","authors":["J Ehrhardt, T Spinde, A Vardasbi, F Hamborg - 2021"],"snippet":"… 3 https://commoncrawl.org/ Page 6. Omission of Information: Identifying Political Slant via an Analysis of … 85 We scraped a total of 76.221 articles from nine different news outlets equally distributed over the three slant …","url":["https://epub.uni-regensburg.de/44939/1/isi_ehrhardt_et_al.pdf"]}
{"year":"2021","title":"OmniNet: Omnidirectional Representations from Transformers","authors":["Y Tay, M Dehghani, V Aribandi, J Gupta, P Pham, Z Qin… - arXiv preprint arXiv …, 2021"],"snippet":"… We run experiments on large-scale autoregressive (unidirectional) language modeling. We use two large-scale datasets, language modeling one billion (LM1B) (Chelba et al., 2013) and the Colossal Cleaned Common …","url":["https://arxiv.org/pdf/2103.01075"]}
{"year":"2021","title":"On a Utilitarian Approach to Privacy Preserving Text Generation","authors":["Z Xu, A Aggarwal, O Feyisetan, N Teissier - arXiv preprint arXiv:2104.11838, 2021"],"snippet":"… Due to the high dimensional nature of textual tasks and very large vocabulary sizes (eg 2.2M words for GLOVE common crawl (Pennington et al., 2014)), this can lead to adding a lot of noise for achieving the desired privacy …","url":["https://arxiv.org/pdf/2104.11838"]}
{"year":"2021","title":"On Anytime Learning at Macroscale","authors":["L Caccia, J Xu, M Ott, MA Ranzato, L Denoyer - arXiv preprint arXiv:2106.09563, 2021"],"snippet":"… For instance, every once in a while new data from common crawl is gathered to train even larger language models, or every few weeks new images and associated labels are collected to train better visual object recognizers …","url":["https://arxiv.org/pdf/2106.09563"]}
{"year":"2021","title":"On the dangers of stochastic parrots: Can language models be too big","authors":["EM Bender, T Gebru, A McMillan-Major, S Shmitchell - Proceedings of FAccT, 2021"],"snippet":"… show that models like GPT-3 trained with at least 570GB of data derived mostly from Common Crawl16 can generate sentences with high toxicity … 16https://commoncrawl.org/the-data/ 17GPT-3's training data is not openly …","url":["http://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf"]}
{"year":"2021","title":"On the Language Coverage Bias for Neural Machine Translation","authors":["S Wang, Z Tu, Z Tan, S Shi, M Sun, Y Liu - arXiv preprint arXiv:2106.03297, 2021"],"snippet":"Page 1. On the Language Coverage Bias for Neural Machine Translation Shuo Wang1 Zhaopeng Tu2 Zhixing Tan1 Shuming Shi2 Maosong Sun1,3 Yang Liu1,3,4 1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua …","url":["https://arxiv.org/pdf/2106.03297"]}
{"year":"2021","title":"On the Learning Dynamics of Semi-Supervised Training for ASR}}","authors":["E Wallington, B Kershenbaum, O Klejch, P Bell - Proc. Interspeech 2021, 2021"],"snippet":"… Following initial analysis, we use these same varying-quality models to investigate iSST. 2.1. Language Models The LM training data consists of the Tagalog CommonCrawl data-set1 and four smaller Tagalog sets provided …","url":["https://www.isca-speech.org/archive/pdfs/interspeech_2021/wallington21_interspeech.pdf"]}
{"year":"2021","title":"On the Multilingual Capabilities of Very Large-Scale English Language Models","authors":["J Armengol-Estapé, OG Bonet, M Melero - arXiv preprint arXiv:2108.13349, 2021"],"snippet":"… models (Nozza et al., 2020). Even the multilingual ones2 such as mBERT (Devlin et al., 2018) or XLM-R (Conneau et al., 2019) employ large multilingual datasets based on Wikipedia or CommonCrawl. A very recent work trained …","url":["https://arxiv.org/pdf/2108.13349"]}
{"year":"2021","title":"On the Robustness of Neural Network: Attacks and Defenses","authors":["M Cheng - 2021"],"snippet":"Page 1. UCLA UCLA Electronic Theses and Dissertations Title On the Robustness of Neural Network: Attacks and Defenses Permalink https://escholarship.org/uc/item/3k2780bg Author Cheng, Minhao Publication …","url":["https://escholarship.org/content/qt3k2780bg/qt3k2780bg.pdf"]}
{"year":"2021","title":"On the Usability of Transformers-based models for a French Question-Answering task","authors":["O Cattan, C Servan, S Rosset - Recent Advances in Natural Language Processing …, 2021"],"snippet":"… Transformer-based language models such as BERT (Devlin et al., 2019) are pre-trained on largescale data collections sourced from Wikipedia or Common Crawl (CC) with one or multiple training objectives (masked …","url":["https://hal.archives-ouvertes.fr/hal-03336060/document"]}
{"year":"2021","title":"On the validity of pre-trained transformers for natural language processing in the software engineering domain","authors":["J von der Mosel, A Trautsch, S Herbold - arXiv preprint arXiv:2109.04738, 2021"],"snippet":"… Therefore, most of the available models are pre-trained by large companies on vast amounts of general domain data such as the entire English Wikipedia or the Common Crawl News dataset [5]. While these models achieve …","url":["https://arxiv.org/pdf/2109.04738"]}
{"year":"2021","title":"On-Line Retrieval of Health Information Based on Language Complexity, Information Customization and Information Quality","authors":["M Alfano - … and Communication Technologies for Ageing Well and …, 2021"],"snippet":"… initiative. The Web Data Commons (WDC)[30] contain all Microformat, Microdata and RDFa (Resource Description Framework in Attributes) data extracted from the open repository of Web crawl data named Common Crawl (CC) …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=9kQgEAAAQBAJ&oi=fnd&pg=PA1&dq=commoncrawl&ots=V3u-4Vufj1&sig=xQGsuQdnwOKvDO0cKw5-XZNUvjM"]}
{"year":"2021","title":"One to rule them all: Towards Joint Indic Language Hate Speech Detection","authors":["M Bhatia, TS Bhotia, A Agarwal, P Ramesh, S Gupta… - arXiv preprint arXiv …, 2021"],"snippet":"… A summary for each model is as follows: • XLM-RoBERTa: The pre-training of XLM-RoBERTa is based on 100 languages, using around 2.5TB of preprocessed CommonCrawl dataset to train cross-language …","url":["https://arxiv.org/pdf/2109.13711"]}
{"year":"2021","title":"Open Dataset Archive","authors":["T Weber"],"snippet":"… data: to name an example, while large corpora of tabular data from Web tables have been made available via CommonCrawl [5], the … Common Crawl The Common Crawl Foundation is a also a non-profit organization which …","url":["https://aic.ai.wu.ac.at/~polleres/supervised_theses/Thomas_Weber_BSc_2020.pdf"]}
{"year":"2021","title":"Open-Domain Conversational Search Assistant with Transformers","authors":["R Ferreira, M Leite, D Semedo, J Magalhaes - arXiv preprint arXiv:2101.08197, 2021"],"snippet":"… Page 6. 6 R. Ferreira et al. – Text-to-Text Transfer Transformer (T5) [26] is a text-to-text model based on the encoder-decoder Transformer architecture, pre-trained on the large C4 corpus, which was derived from Common …","url":["https://arxiv.org/pdf/2101.08197"]}
{"year":"2021","title":"Opening up Military Innovation: Causal Effects of 'Bottom-Up'Reforms to US Defense Research","authors":["ST Howell, J Rathje, J Van Reenen, J Wong - Available at SSRN 3825034, 2021"],"snippet":"Page 1. Opening up Military Innovation: Causal Effects of 'Bottom-Up' Reforms to US Defense Research Sabrina T. Howell, Jason Rathje, John Van Reenen and Jun Wong ∗ April 8, 2021 Abstract When investing in research …","url":["https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3825034"]}
{"year":"2021","title":"OpenViDial: A Large-Scale, Open-Domain Dialogue Dataset with Visual Contexts","authors":["Y Meng, S Wang, Q Han, X Sun, F Wu, R Yan, J Li - arXiv preprint arXiv:2012.15015, 2020"],"snippet":"… and the open-sourced OCR model suffices to fulfill our need. With text-free images in hand, we pair them with texts. Texts are randomly selected from the CommonCrawl English corpus, then added to the images. Texts in images …","url":["https://arxiv.org/pdf/2012.15015"]}
{"year":"2021","title":"Operationalizing a National Digital Library: The Case for a Norwegian Transformer Model","authors":["PE Kummervold, J De la Rosa, F Wetjen, SA Brygfjeld - arXiv preprint arXiv …, 2021"],"snippet":"… For resource-rich languages such as Chinese, En- glish, French, and Spanish, collections of texts from open sources such as Wikipedia (2021a), variations of Common Crawl data (2021), and other open-source corpora such as the BooksCorpus (Zhu et al., 2015) are …","url":["https://arxiv.org/pdf/2104.09617"]}
{"year":"2021","title":"Opinion Classification via Word and Emoji Embedding Models with LSTM","authors":["I Rabbimov, S Kobilov, I Mporas - International Conference on Speech and Computer, 2021"],"snippet":"… Twitter. 1,661. 300. The UzbekFastText1 word vector is the fastText model trained with crawled texts published on Wikipedia using Common Crawl [36]. UzbekFastText2 was presented in [37] and it is a 300-dimensional Uzbek …","url":["https://www.researchgate.net/profile/Ilyos-Rabbimov/publication/354758888_Opinion_Classification_via_Word_and_Emoji_Embedding_Models_with_LSTM/links/614e7a91154b3227a8a8b8b8/Opinion-Classification-via-Word-and-Emoji-Embedding-Models-with-LSTM.pdf"]}
{"year":"2021","title":"Out-of-vocabulary but not meaningless: Evidence for semantic-priming effects in pseudoword processing","authors":["D Gatti, M Marelli, L Rinaldi - 2021"],"snippet":"… The model was trained on Common Crawl (around 630 billion words) and Wikipedia (around 9 billion words) using the Continuous Bag of … The model was trained on Common Crawl and Italian Wikipedia (around 11 billion words) using …","url":["https://psyarxiv.com/y2fm3/download?format=pdf"]}
{"year":"2021","title":"Overcoming Data Challenges in Machine Translation","authors":["H Khayrallah - 2021"],"snippet":"Page 1. OVERCOMING DATA CHALLENGES IN MACHINE TRANSLATION by Huda Khayrallah A dissertation submitted to The Johns Hopkins University in conformity with the requirements for the degree of Doctor of Philosophy. Baltimore, Maryland July, 2021 …","url":["https://jscholarship.library.jhu.edu/bitstream/handle/1774.2/64381/KHAYRALLAH-DISSERTATION-2021.pdf?sequence=1"]}
{"year":"2021","title":"Overlapping Community Detection in Social Networks","authors":["A Panchal - 2021"],"snippet":"Page 1. San Jose State University SJSU ScholarWorks Master's Projects Master's Theses and Graduate Research Spring 5-25-2021 Overlapping Community Detection in Social Networks Akshar Panchal Follow this and additional …","url":["https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=2011&context=etd_projects"]}
{"year":"2021","title":"Overview of Touché 2021: Argument Retrieval","authors":["A Bondarenko, L Gienapp, M Fröbe, M Beloucif… - 2021"],"snippet":"… Other argument retrieval systems such as ArgumenText [3] and TARGER [4] take advantage of the large web document collection Common Crawl, but their ability to reliably retrieve arguments to support sides in a decision process is limited …","url":["https://webis.de/downloads/publications/papers/bondarenko_2021b.pdf"]}
{"year":"2021","title":"PAGnol: An Extra-Large French Generative Model","authors":["J Launay, EL Tommasone, B Pannier, F Boniface… - arXiv preprint arXiv …, 2021"],"snippet":"… Having been pre-trained on Common Crawl-based corpora, our models are certainly not immune from toxic content generation. Gehman et al. (2020) explored two main ways to filter toxic outputs, one being based on the pursuing the pretraining on less …","url":["https://arxiv.org/pdf/2110.08554"]}
{"year":"2021","title":"PALI at SemEval-2021 Task 2: Fine-Tune XLM-RoBERTa for Word in Context Disambiguation","authors":["S Xie, J Ma, H Yang, L Jiang, Y Mo, J Shen - arXiv preprint arXiv:2104.10375, 2021"],"snippet":"… 2017). Especially, the XLM-RoBERTa (Conneau et al., 2020) model is a newly released large cross-lingual language model based on RoBERTa and is trained on 2.5TB filtered CommonCrawl data in 100 languages. Different …","url":["https://arxiv.org/pdf/2104.10375"]}
{"year":"2021","title":"PAN: Prototype-based Adaptive Network for Robust Cross-modal Retrieval","authors":["Z Zeng, S Wang, N Xu, W Mao - Proceedings of the 44th International ACM SIGIR …, 2021"],"snippet":"Page 1. PAN: Prototype-based Adaptive Network for Robust Cross-modal Retrieval Zhixiong Zeng, Shuai Wang, Nan Xu, Wenji Mao∗ SKL-MCCS, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China …","url":["https://dl.acm.org/doi/abs/10.1145/3404835.3462867"]}
{"year":"2021","title":"PanGu-$\\alpha $: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation","authors":["W Zeng, X Ren, T Su, H Wang, Y Liao, Z Wang, X Jiang… - arXiv preprint arXiv …, 2021"],"snippet":"… To build Chinese corpus with comprehensive coverage, we collect a large amount of data from a wide range of resources, including Common Crawl, e-Books, encyclopedias, news, and so on … Common Crawl 176.2B 62.81 …","url":["https://arxiv.org/pdf/2104.12369"]}
{"year":"2021","title":"PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining","authors":["M Reid, M Artetxe - arXiv preprint arXiv:2108.01887, 2021"],"snippet":"Page 1. PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining Machel Reid The University of Tokyo machelreid@weblab.tu-tokyo.ac.jp Mikel Artetxe Facebook AI Research artetxe@fb.com Abstract …","url":["https://arxiv.org/pdf/2108.01887"]}
{"year":"2021","title":"Parallel feature weight decay algorithms for fast development of machine translation models","authors":["E Biçici - Machine Translation, 2021"],"snippet":"Parallel feature weight decay algorithms, parfwd, are engineered for languageand task-adaptive instance selection to build distinct machine translation (.","url":["https://link.springer.com/article/10.1007/s10590-021-09275-z"]}
{"year":"2021","title":"Parallelizing Legendre Memory Unit Training","authors":["N Chilkuri, C Eliasmith - arXiv preprint arXiv:2102.11417, 2021"],"snippet":"… After showing that the training of this simplified variant can be 1https:// commoncrawl.org … Therefore, we construct parameter efficient models that em- ploy just the DN layer, with d = 1 and θ = maxlen. We use 300D Glove …","url":["https://arxiv.org/pdf/2102.11417"]}
{"year":"2021","title":"Paraphrase Generation as Unsupervised Machine Translation","authors":["C Fan, Y Tian, Y Meng, N Peng, X Sun, F Wu, J Li - arXiv preprint arXiv:2109.02950, 2021"],"snippet":"… LDA and K-means algorithms are performed on part of the the CommonCrawl corpus containing 10 billion English tokens … An overview of deriving the UMT-Multi model is shown in Figure 1. Up to now, UMT-Multi is purely based on unlabeled commoncrawl corpus …","url":["https://arxiv.org/pdf/2109.02950"]}
{"year":"2021","title":"Pathogens Are Linked to Human Moral Systems Across Time and Space","authors":["M Atari, NK Reimer, J Graham, J Hoover, B Kennedy… - 2021"],"snippet":"Infectious diseases have been an impending threat to the survival of individuals and groups throughout our evolutionary history. As a result, humans have developed psychological pathogen-avoidance mechanisms and groups have developed …","url":["https://psyarxiv.com/tnyh9/download?format=pdf"]}
{"year":"2021","title":"Pegasus@ Dravidian-CodeMix-HASOC2021: Analyzing Social Media Content for Detection of Offensive Text","authors":["PK Jada, K Yasaswini, K Puranik, A Sampath… - arXiv preprint arXiv …, 2021"],"snippet":"… One of the reason for the unparalleled performance is that it was trained on a mammoth 2.5 TB of CommonCrawl data [25]. It was trained with MLM loss as it’s objective on 100 different languages, and it shares similar training routine as the …","url":["https://arxiv.org/pdf/2111.09836"]}
{"year":"2021","title":"PENGEMBANGAN APLIKASI INFORMATION GATHERING MENGGUNAKAN METODE HYBRID SCAN BERBASIS GRAPHICAL USER INTERFACE","authors":["M Riasetiawan, A Wisnuaji, D Hariyadi, T Febrianto - Cyber Security dan Forensik …, 2021"],"snippet":"… F-Secure Riddler, Cert Spotter, Hacker Target, Certificate Search, DN Dumpster, AlienFault, Rapid DNS, URL Scan, Threat Crowd, Wayback Machine, dan Common Crawl … q=\"+self.url, 1 2 \"https://riddler.io/search …","url":["http://202.0.92.5/saintek/cybersecurity/article/download/2449/1994"]}
{"year":"2021","title":"Penn-Helsinki Parsed Corpus of Early Modern English: First Parsing Results and Analysis","authors":["S Kulick, N Ryant, B Santorini - arXiv preprint arXiv:2112.08532, 2021"],"snippet":"We present the first parsing results on the Penn-Helsinki Parsed Corpus of Early Modern English (PPCEME), a 1.9 million word treebank that is an important resource for research in syntactic change. We describe key features of PPCEME that …","url":["https://arxiv.org/pdf/2112.08532"]}
{"year":"2021","title":"Personalized sentiment classification of customer reviews via an interactive attributes attention model","authors":["Y Zhang, J Wang, X Zhang - Knowledge-Based Systems, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0950705121003981"]}
{"year":"2021","title":"Phishing Detection Methods: A Review","authors":["M Alanezi - 2021"],"snippet":"Abstract The web has turned into a principal part of our conventional social and financial activities. The web isn't significant for singular clients just yet additionally for associations, since associations that offer web-based exchanging can accomplish …","url":["https://techniumscience.com/index.php/technium/article/view/4973/1723"]}
{"year":"2021","title":"PhoMT: A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation","authors":["L Doan, LT Nguyen, NL Tran, T Hoang, DQ Nguyen - arXiv preprint arXiv:2110.12199, 2021"],"snippet":"We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs, which is 2.9M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We conduct experiments comparing strong …","url":["https://arxiv.org/pdf/2110.12199"]}
{"year":"2021","title":"Planning with Entity Chains for Abstractive Summarization","authors":["S Narayan, Y Zhao, J Maynez, G Simoes, R McDonald - arXiv preprint arXiv …, 2021"],"snippet":"… Pre-training Datasets. Following Zhang et al. (2019), our model pretraining also relied on two large web corpus which were processed to look like plain text: (i) C4 (Raffel et al., 2019) is composed of 350M Web-pages …","url":["https://arxiv.org/pdf/2104.07606"]}
{"year":"2021","title":"Planning with Learned Entity Prompts for Abstractive Summarization","authors":["S Narayan, Y Zhao, J Maynez, G Simões, V Nikolaev… - Transactions of the …, 2021"],"snippet":"We introduce a simple but flexible mechanism to learn an intermediate plan to ground the generation of abstractive summaries. Specifically, we prepend (or prompt) target summaries with entity chains—ordered sequences of entities mentioned in the …","url":["https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00438/108867"]}
{"year":"2021","title":"Polish Web resources described in the\" Polish World\" directory (1997). Characteristics of domains and their conservation state","authors":["M Wilkowski - Archiwa-Kancelarie-Zbiory, 2020"],"snippet":"… Sep. 2008, p. 6. https://hal-bnf.archives-ouvertes.fr/hal-01098538 (accessed 19.09.2020). 18 Common Crawl, https://commoncrawl.org/ (accessed 12.09.2020). 19 HTTP Archive, https://httparchive.org/ (accessed 12.09.2020). Page 8. 126 Marcin Wilkowski …","url":["https://apcz.umk.pl/czasopisma/index.php/AKZ/article/download/AKZ.2020.005/28213"]}
{"year":"2021","title":"Political Attacks in 280 Characters or Less: A New Tool for the Automated Classification of Campaign Negativity on Social Media","authors":["V Petkevic, A Nai - American Politics Research, 2021"],"snippet":"Negativity in election campaign matters. To what extent can the content of social media posts provide a reliable indicator of candidates' campaign negativity? We introduce and critically assess an automated classification procedure that we trained …","url":["https://journals.sagepub.com/doi/pdf/10.1177/1532673X211055676"]}
{"year":"2021","title":"Political Polarization in Online News Consumption","authors":["K Garimella, T Smith, R Weiss, R West - arXiv preprint arXiv:2104.06481, 2021"],"snippet":"… The Common Crawl project1 regularly crawls the Web and releases datasets of the discovered webpages on a regular schedule, which are generally considered the most extensive publicly available sources …","url":["https://arxiv.org/pdf/2104.06481"]}
{"year":"2021","title":"PoolRank: Max/Min Pooling-based Ranking Loss for Listwise Learning & Ranking Balance","authors":["Z Chen, C Eickhoff - arXiv preprint arXiv:2108.03586, 2021"],"snippet":"… Our GloVe word embeddings are trained on the Common Crawl dataset 2. We use an Adam optimizer [13] with 0.0001 learning rate to train all models … Most queries 1https://www.tensor ow.org/ 2https://commoncrawl …","url":["https://arxiv.org/pdf/2108.03586"]}
{"year":"2021","title":"Postal address extraction from the web: a comprehensive survey","authors":["M Kayed, S Dakrory, AA Ali - Artificial Intelligence Review, 2021"],"snippet":"The Web is a source of information for Location-Based Service (LBS) applications. These applications lack postal addresses for the user's Point of In.","url":["https://link.springer.com/article/10.1007/s10462-021-09983-1"]}
{"year":"2021","title":"Practical Wavelet Tree Construction","authors":["P Dinklage, J Ellert, J Fischer, F Kurpicz, M Löbel - Journal of Experimental …, 2021","VIN LÖBEL - 2021"],"snippet":"… We denote the resulting file by RuWB. Suffix Array of Common Crawl. We construct the suffix array [35] of CommonCrawl, which is a permutation of the positive integers smaller than the length of CommonCrawl. We store it as …","url":["https://dl.acm.org/doi/pdf/10.1145/3457197","https://www.kurpicz.org/assets/pdfs/jea_2021.pdf"]}
{"year":"2021","title":"Pre-trained Language Models for Tagalog with Multi-source Data","authors":["S Jiang, Y Fu, X Lin, N Lin - CCF International Conference on Natural Language …, 2021"],"snippet":"… [19] to process, filter and classify Common Crawl, which is a non-profit organization that produces and maintains an open, freely available repository of crawled data from the web. The filtering step used to create OSCAR involves …","url":["https://link.springer.com/chapter/10.1007/978-3-030-88480-2_17"]}
{"year":"2021","title":"Pre-trained Language Models in Biomedical Domain: A Survey from Multiscale Perspective","authors":["B Wang, Q Xie, J Pei, P Tiwari, Z Li - arXiv preprint arXiv:2110.05006, 2021"],"snippet":"Pre-trained language models have been the de facto paradigm for most natural language processing (NLP) tasks. In the biomedical domain, which also benefits from NLP techniques, various pre-trained language models were proposed by …","url":["https://arxiv.org/pdf/2110.05006"]}
{"year":"2021","title":"Pre-trained Models and Evaluation Data for the Myanmar Language","authors":["S Jiang, X Huang, X Cai, N Lin - International Conference on Neural Information …, 2021"],"snippet":"… On the one hand, we utilized all the Myanmar data from the OSCAR corpus 3 , a humongous multilingual corpus whose texts all come from the Common Crawl corpus 4 . In addition, articles on CC-100 [16] were also used as part of our corpus …","url":["https://link.springer.com/chapter/10.1007/978-3-030-92310-5_52"]}
{"year":"2021","title":"Pre-Trained Transformer-Based Language Models for Sundanese","authors":["W Wongso, H Lucky, D Suhartono - 2021"],"snippet":"… Data The dataset used during pre-training consists of Sundanese subsets of multilingual common-crawl corpora of OSCAR [18], CC-100 [19, 20], and C4 [21]. They are of the size 141KB, 15MB, and 464MB, respectively. Seeing …","url":["https://www.researchsquare.com/article/rs-907893/latest.pdf"]}
{"year":"2021","title":"Pre-trained Word Embeddings for Malayalam Language: A Review","authors":["KR Rahmath, PCR Raj, PC Rafeeque - … on Artificial Intelligence and Smart Systems …, 2021"],"snippet":"… pre-trained on 17 In- dian languages, and their transliterated counterparts including Malayalam [8]. The corpora used for pre-training process are Wikipedia, Common Crawl, PMINDIA and … This dataset has been augmented …","url":["https://ieeexplore.ieee.org/abstract/document/9396042/"]}
{"year":"2021","title":"Pre-training Methods in Information Retrieval","authors":["Y Fan, X Xie, Y Cai, J Chen, X Ma, X Li, R Zhang, J Guo… - arXiv preprint arXiv …, 2021"],"snippet":"The core of information retrieval (IR) is to identify relevant information from large-scale resources and return it as a ranked list to respond to user's information need. Recently, the resurgence of deep learning has greatly advanced this field and leads …","url":["https://arxiv.org/pdf/2111.13853"]}
{"year":"2021","title":"Predicting cross-linguistic adjective order with information gain","authors":["W Dyer, R Futrell, Z Liu, G Scontras - arXiv preprint arXiv:2012.15263, 2020"],"snippet":"… 4.1 Data Our study relies on two types of source data, both extracted from the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal De- pendencies (Ginter et al., 2017; Zeman et al., 2017) a set of …","url":["https://arxiv.org/pdf/2012.15263"]}
{"year":"2021","title":"Predicting Depression and Suicide Ideation in the Canadian Population Using Social Media Data","authors":["R Skaik - 2021"],"snippet":"Page 1. Predicting Depression and Suicide Ideation in the Canadian Population Using Social Media Data by Ruba Skaik Thesis submitted to the University of Ottawa in partial Fulfillment of the requirements for the Ph.D. degree in Computer Science …","url":["https://ruor.uottawa.ca/bitstream/10393/42346/7/Skaik_Ruba_2021_thesis.pdf"]}
{"year":"2021","title":"Predicting leadership perception with large-scale natural language data","authors":["S Bhatia, CY Olivola, N Bhatia, A Ameen - The Leadership Quarterly, 2021"],"snippet":"","url":["https://www.sciencedirect.com/science/article/pii/S1048984321000400"]}
{"year":"2021","title":"Predicting Next Dialogue Action in Emotionally Loaded Conversation","authors":["D Deksne, R Skadiņš - Proceedings of the Future Technologies Conference, 2021"],"snippet":"… the Latvian Wikipedia data 2 and trained with our internal monolingual corpora, as well as several BERT models: multilingual BERT [26], BERT-LV [27], Language-agnostic BERT Sentence Embedding (LaBSE) [28], and the multilingual XLM-R model trained …","url":["https://link.springer.com/chapter/10.1007/978-3-030-89906-6_19"]}
{"year":"2021","title":"Predicting the Performance of Multilingual NLP Models","authors":["A Srinivasan, S Sitaram, T Ganu, S Dandapat, K Bali… - arXiv preprint arXiv …, 2021"],"snippet":"… The pretraining procedure makes use of only unlabelled data (raw text), which is easy to obtain for a large number of languages from various sources including CommonCrawl1 and Wikipedia dumps2. These models can then be used to solve a …","url":["https://arxiv.org/pdf/2110.08875"]}
{"year":"2021","title":"Preprint from: https://www. gipp. com/pub","authors":["T Spinde, L Rudnitckaia, K Sinha, F Hamborg"],"snippet":"… We scraped articles from both news outlets, published in the last decade, from 2010 to 2020, from Common Crawl [4]. For preprocessing, we use Genism simple preprocessing and generate n-grams. 3.4 Linear mapping between vector spaces …","url":["https://www.gipp.com/wp-content/papercite-data/pdf/spinde2021.pdf"]}
{"year":"2021","title":"PREPRINT–ARTICLE IN PRESS: Kennedy, B., Atari, M., Mostafazadeh Davani, A., Hoover, J., Omrani, A., Graham, J., & Dehghani, M.(in press). Moral Concerns are …","authors":["B Kennedy"],"snippet":"… of posts, GloVe word embedding vectors (Pennington et al., 2014), which were trained on text from the Common Crawl4, were mapped to the individual words in the posts and subsequently averaged element-wise (ie, across the dimensions of the embedding). The …","url":["https://files.osf.io/v1/resources/uqmty/providers/osfstorage/5eb44939a2199500cd5d4b7c?format=pdf&action=download&direct&version=2"]}
{"year":"2021","title":"Prescriptions in NLP with Transformer Models","authors":["OEC Zaragoza"],"snippet":"… is optimized BERT. XLMR is trained with a filtered version of CommonCrawl data with 100 languages and it uses a subword tokenization, SentencePiece [20] with a unigram language model [19]. mT5 Multilingual TexttoText …","url":["https://payberah.github.io/files/download/students/omar_contreras_master_thesis.pdf"]}
{"year":"2021","title":"Pretrained models and evaluation data for the Khmer language","authors":["S Jiang, S Fu, N Lin, Y Fu - Tsinghua Science and Technology, 2021"],"snippet":"… On the one hand, we use all the central Khmer data from the OSCAR corpus , a large multilingual corpus whose texts come from the Common Crawl corpus . Although Common Crawl comprises scraped data from the Internet and covers a …","url":["https://ieeexplore.ieee.org/iel7/5971803/9645434/09645441.pdf"]}
{"year":"2021","title":"Prevalence in News Media of Two Competing Hypotheses about COVID-19 Origins","authors":["D Rozado - Social Sciences, 2021"],"snippet":"… 2. Methods. The textual content of news and opinion articles from the outlets listed in Figure 1 is available in the outlets online domains and/or public cache repositories such as Google cache, The Internet Wayback Machine …","url":["https://www.mdpi.com/2076-0760/10/9/320/htm"]}
{"year":"2021","title":"Prevalence of Prejudice-Denoting Words in News Media Discourse: A Chronological Analysis","authors":["D Rozado, M Al-Gharbi, J Halberstadt - Social Science Computer Review, 2021"],"snippet":"This work analyzes the prevalence of words denoting prejudice in 27 million news and opinion articles written between 1970 and 2019 and published in 47 of the most popular news media outlets in the...","url":["https://journals.sagepub.com/doi/abs/10.1177/08944393211031452"]}
{"year":"2021","title":"Preventing author profiling through zero-shot multilingual back-translation","authors":["DI Adelani, M Zhang, X Shen, A Davody, T Kleinbauer… - arXiv preprint arXiv …, 2021"],"snippet":"… The languages chosen are German (DE), Spanish (ES), French (FR), Japanese (JA), Russian (RU), and Chinese (ZH) based on the large amount of resources they have on OPUS (Tiedemann, 2012) and Common Crawl corpora 2 …","url":["https://arxiv.org/pdf/2109.09133"]}
{"year":"2021","title":"Price Determinants of Airbnb Apartments","authors":["A Piispanen - 2021"],"snippet":"Page 1. Antti Piispanen Price Determinants of Airbnb Apartments An Approach with Deep Language Representations Master of Science Thesis Faculty of Information Technology and Communication Sciences Examiner …","url":["https://trepo.tuni.fi/bitstream/handle/10024/130318/PiispanenAntti.pdf?sequence=2"]}
{"year":"2021","title":"Prioritizing Original News on Facebook","authors":["X Ni, S Bu, I Markov - arXiv preprint arXiv:2102.08465, 2021"],"snippet":"… There are other open-source crawlers that serve the same purpose. Common Crawl3 is a well-maintained open repository of Web crawl data that can be accessed and analyzed by anyone. We limit news articles in the graph to …","url":["https://arxiv.org/pdf/2102.08465"]}
{"year":"2021","title":"Privacy and AI Ethics–Understanding the convergences and tensions for the responsible development of machine learning","authors":["I Cofone, O Abimana, B Bonan, E Grand-Pierre, A Qarri"],"snippet":"Context. The democratization of mobile systems and the development of information technologies have been accompanied by a massive increase in the amount and the diversity of data collected about individuals, often referred to as Big Data …","url":["https://sebastiengambs.openum.ca/files/sites/82/2021/11/OPC_final.pdf"]}
{"year":"2021","title":"Privacy-Preserving Learning of Human Activity Predictors in Smart Environments","authors":["S Zehtabian, S Khodadadeh, L Bölöni, D Turgut - arXiv preprint arXiv:2101.06564"],"snippet":"… However, many deep learning algorithms work best under big data regimes, where the number of data samples are counted from the tens of thousands (eg the MNIST dataset) to 500 billion (the Common Crawl dataset used to train the GPT-3 model) …","url":["https://arxiv.org/pdf/2101.06564"]}
{"year":"2021","title":"PrivaSeer: A Privacy Policy Search Engine","authors":["S Wilson"],"snippet":"… The URLs in the Common Crawl archive were first filtered based on a selection criteria that took advantage of the fact that most privacy policy URLs either have the word 'privacy' or the words 'data' and 'protection' in them … 6 https://commoncrawl.org …","url":["https://link.springer.com/content/pdf/10.1007/978-3-030-74296-6_22.pdf"]}
{"year":"2021","title":"Probabilistic Text Entry—Case Study 3","authors":["K Vertanen - Intelligent Computing for Interactive System Design …, 2021"],"snippet":"Page 1. 7Probabilistic Text Entry—Case Study 3 Keith Vertanen The reading and writing of text is an activity that has long dominated our interac tions with computers. Early user interfaces often displayed only text and relied on input obtained via a desktop keyboard …","url":["https://dl.acm.org/doi/abs/10.1145/3447404.3447420"]}
{"year":"2021","title":"Probing Multilingual Language Models for Discourse","authors":["M Kurfalı, R Östling - arXiv preprint arXiv:2106.04832, 2021"],"snippet":"… the XLM-tlm. Instead, it is a RoBERTa model (Liu et al., 2019), which is an optimized version of BERT, trained on 2.5 TB of cleaned CommonCrawl data covering 100 languages (Conneau et al., 2020). There are two released …","url":["https://arxiv.org/pdf/2106.04832"]}
{"year":"2021","title":"Proceedings of the First Workshop on Weakly Supervised Learning (WeaSuL)","authors":["MA Hedderich, B Roth, K Kann, B Plank, A Ratner… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. WeaSuL 2021 Proceedings of the First Workshop on Weakly Supervised Learning (WeaSuL) May 7, 2021 Co-located with ICLR (Online) arXiv:2107.03690 v1 [cs.LG] 8 Jul 2021 Page 2. Introduction Welcome to WeaSuL …","url":["https://arxiv.org/pdf/2107.03690"]}
{"year":"2021","title":"Processing large-scale Internet topology data to model Autonomous System Networks","authors":["K Bakhshaliyev - 2020"],"snippet":"… such interrelated data. The number of Facebook users grew from 500 million in 2010 to 1.5 billion in 2014. The Common Crawl web corpora [29] covered for 2012 contains 3.5 billion web pages and 128 billion hyperlinks between …","url":["https://scholarworks.unr.edu/bitstream/handle/11714/7672/Bakhshaliyev_unr_0139D_13330.pdf?sequence=1&isAllowed=y"]}
{"year":"2021","title":"Proof Artifact Co-training for Theorem Proving with Language Models","authors":["JM Han, J Rute, Y Wu, EW Ayers, S Polu - arXiv preprint arXiv:2102.06203, 2021"],"snippet":"Page 1. Proof Artifact Co-training for Theorem Proving with Language Models Jesse Michael Han 1 Jason Rute 2 Yuhuai Wu 3 Edward W. Ayers 4 Stanislas Polu 5 Abstract Labeled data for imitation learning of theorem proving …","url":["https://arxiv.org/pdf/2102.06203"]}
{"year":"2021","title":"Propaganda in Press: Challenges of Automatic Detection","authors":["D Pušelj, T Škalec - Text Analysis and Retrieval 2020 Course Project …"],"snippet":"… trained on cased Wikipedia text. RoBERTa has 125M parameters with 12 layers, 768 hidden states, 3072 feed-forward hidden states and 8 heads and is trained on CommonCrawl data in 100 languages. The XLNet is a Large …","url":["https://www.fer.unizg.hr/_download/repository/TAR-2020-ProjectReports.pdf#page=61"]}
{"year":"2021","title":"ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation","authors":["W Qi, Y Gong, Y Yan, C Xu, B Yao, B Zhou, B Cheng… - arXiv preprint arXiv …, 2021"],"snippet":"… models with downstream task finetuning scripts, including ProphetNet-En pre-trained with 160GB English raw text, ProphetNet-Zh pre-trained with 160GB Chinese raw text, ProphetNet-Multi with 101GB Wiki-100 corpus and …","url":["https://arxiv.org/pdf/2104.08006"]}
{"year":"2021","title":"Proposed Use of a Conversational Agent for Patient Empowerment","authors":["M Alfano, J Kellett, B Lenzitti, M Helfert - 2021"],"snippet":"… Depression 1 https://schema.org/ 2 https://health-lifesci.schema.org/ 3 http:// webdatacommons.org/ 4 http://commoncrawl.org o Dermatitis o Tuberculosis 4.2 Creation of Repository of Schema.org and Health-lifesci Structured Data …","url":["https://www.scitepress.org/Papers/2021/104144/104144.pdf"]}
{"year":"2021","title":"Protecting Anonymous Speech: A Generative Adversarial Network Methodology for Removing Stylistic Indicators in Text","authors":["R Balakrishnan, S Sloan, A Aswani - arXiv preprint arXiv:2110.09495, 2021"],"snippet":"With Internet users constantly leaving a trail of text, whether through blogs, emails, or social media posts, the ability to write and protest anonymously is being eroded because artificial intelligence, when given a sample of previous work, can match text …","url":["https://arxiv.org/pdf/2110.09495"]}
{"year":"2021","title":"Psychographic traits identification based on political ideology: An author analysis study on Spanish politicians' tweets posted in 2020","authors":["JA García-Díaz, R Colomo-Palacios, R Valencia-García - Future Generation …, 2021"],"snippet":"… The pre-trained model of Spanish have been trained from Common Crawl and Wikipedia but adjusted with the PoliCorpus 2020 during training. SE assemble a single vector of dimension 300 for a sequence of the individual word embeddings …","url":["https://www.sciencedirect.com/science/article/pii/S0167739X21004921"]}
{"year":"2021","title":"Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning","authors":["J Wang, C Xu, F Guzman, A El-Kishky, Y Tang… - arXiv preprint arXiv …, 2021"],"snippet":"… While this type of attack might appear unrealistic, the nature of the largest collections of monolingual data like Common Crawl (Buck et al., 2014; Wenzek et al., 2020; El-Kishky et al., 2020) (which contains blogs and other …","url":["https://arxiv.org/pdf/2107.05243"]}
{"year":"2021","title":"QMUL-SDS at EXIST: Leveraging Pre-trained Semantics and Lexical Features for Multilingual Sexism Detection in Social Networks","authors":["A Jiang, A Zubiaga - 2021"],"snippet":"… RoBERTa model (XLM-R) [7] has extended the way of pre-training MLM by scaling the amount of data by two orders of magnitude (from Wikipedia to Common Crawl) and training … FastText + TextCNN: we use the Fasttext …","url":["http://www.zubiaga.org/publications/files/jiang2021exist.pdf"]}
{"year":"2021","title":"Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets","authors":["I Caswell, J Kreutzer, L Wang, A Wahab, D van Esch… - arXiv preprint arXiv …, 2021"],"snippet":"… CC: CommonCrawl; TED-6: da, cr, sl, sk, lt, et; TED-45: 45-language subset of (Qi … et al., 2019, 2020) is a set of monolingual corpora extracted from Common Crawl snapshots, specifically from the plain text WET format distributed by Common Crawl which removes …","url":["https://arxiv.org/pdf/2103.12028"]}
{"year":"2021","title":"QuALITY: Question Answering with Long Input Texts, Yes!","authors":["RY Pang, A Parrish, N Joshi, N Nangia, J Phang… - arXiv preprint arXiv …, 2021"],"snippet":"To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current …","url":["https://arxiv.org/pdf/2112.08608"]}
{"year":"2021","title":"Quantifying context with and without statistical language models","authors":["CL Jacobs, B Hall"],"snippet":"… Others, such as the Common Crawl (Smith et al., 2013) are a hodge-podge grabbed from across the entire internet, which may include forum posts, blogs, tweets, or computer code, in addition to news articles, fan fiction, and so on …","url":["https://osf.io/mpqut/download"]}
{"year":"2021","title":"Quantum-inspired Hierarchical Attention Mechanism for Question Answering","authors":["P Guo - 2021 IEEE 33rd International Conference on Tools with …, 2021"],"snippet":"… We initialize word embeddings with 300-dimensional Glove word vectors pretrained from the 840B Common Crawl corpus. The LSTM hidden size is set to 100 over WikiQA and TrecQA. AdamOptimizer is used for updating parameters with …","url":["https://ieeexplore.ieee.org/abstract/document/9643371/"]}
{"year":"2021","title":"Query2Prod2Vec Grounded Word Embeddings for eCommerce","authors":["F Bianchi, J Tagliabue, B Yu - arXiv preprint arXiv:2104.02061, 2021"],"snippet":"Page 1. Query2Prod2Vec Grounded Word Embeddings for eCommerce Federico Bianchi Bocconi University Milano, Italy f.bianchi@unibocconi.it Jacopo Tagliabue∗ Coveo Labs New York, USA jtagliabue@coveo.com …","url":["https://arxiv.org/pdf/2104.02061"]}
{"year":"2021","title":"Question Classification for the Travel Domain using Deep Contextualized Word Embedding Models","authors":["C Weerakoon, S Ranathunga - 2021 Moratuwa Engineering Research Conference …, 2021"],"snippet":"… XLNet [5] has pre-trained data over 129 GB collected from BooksCorpus, Wikipedia, CommonCrawl, Giga5, and ClueWeb 2012-B. XLNet at its core uses the permutation technique to predict tokens in random order contrary to masked model of BERT …","url":["https://ieeexplore.ieee.org/abstract/document/9525789/"]}
{"year":"2021","title":"Re-entry Prediction for Online Conversations via Self-Supervised Learning","authors":["L Wang, X Zeng, H Hu, KF Wong, D Jiang - arXiv preprint arXiv:2109.02020, 2021"],"snippet":"… For the parameters in the main model, we first initialize the embedding layer with 200-dimensional Glove embedding (Pennington et al., 2014), whose Twitter version is used for the Twitter dataset and Common Crawl version …","url":["https://arxiv.org/pdf/2109.02020"]}
{"year":"2021","title":"Re-Evaluating GermEval17 Using German Pre-Trained Language Models","authors":["M Aßenmacher, A Corvonato, C Heumann - arXiv preprint arXiv:2102.12330, 2021"],"snippet":"… DBMDZ combine Wikipedia, EU Bookshop (Skadin š et al., 2014), Open Subtitles (Lison and Tiedemann, 2016), CommonCrawl (Ortiz Suárez et al., 2019), ParaCrawl (Espl`a-Gomis et al., 2019) and News Crawl (Haddow …","url":["https://arxiv.org/pdf/2102.12330"]}
{"year":"2021","title":"Read & Improve: A Novel Reading Tutoring System","authors":["R Watson, E Kochmar - EDM 2021: Educational Data Mining, 2021"],"snippet":"… To source news content, R&I monitors both RSS Feeds from news websites and the publicly available Common Crawl News (CC-NEWS) Dataset.3 A fully automated Indexing Pipeline (RIIP, herein) processes article URLs …","url":["https://purehost.bath.ac.uk/ws/files/219929943/EDM_2021.pdf"]}
{"year":"2021","title":"Reading Key Figures from Annual Reports","authors":["S Nordin Hällgren - 2021"],"snippet":"Page 1. Reading Numbers from Annual Reports Using OCR and NLP to Extract Key Figures from Documents Master's thesis in Engineering Mathematics and Computational Science Sara Nordin Hällgren DEPARTMENT OF MATHEMATICAL SCIENCES …","url":["https://odr.chalmers.se/bitstream/20.500.12380/302433/1/Master_thesis_Sara_Nordin_H%C3%A4llgren_210604.pdf"]}
{"year":"2021","title":"Recent advances of low-resource neural machine translation","authors":["R Haque, CH Liu, A Way - Machine Translation, 2021"],"snippet":"… As for the low-resource pairs, monolingual data was provided by the Common Crawl (Buck et al. ) and English monolingual data is blog … 12.Buck C, Heafield K, van Ooyen B (2014) N-gram counts and language models from the common crawl. In …","url":["https://link.springer.com/article/10.1007/s10590-021-09281-1"]}
{"year":"2021","title":"Recent Automatic Post Editing Research","authors":["H Moon, C Park, S Eo, J Seo, H Lim - Journal of Digital Convergence, 2021"],"snippet":"… 진행되었다. 사전학습에는 100개 언어에 대한 Common Crawl[21] 단일 언어 말 뭉치가 이용된다 … 도출하였다. 사전학습에는 25개 언어에 대한 Common Crawl 단 일 언어데이터를 활용하였고, BART[24]에서 제안된 자 기 지도 학습 방법론을 차용하였다 …","url":["https://www.koreascience.or.kr/article/JAKO202123157277846.pdf"]}
{"year":"2021","title":"Recent Research on Phishing Detection Through Machine Learning Algorithm","authors":["A Selamat, O Krejcar - … Conference on Industrial, Engineering and Other …, 2021"],"snippet":"… Phishing URLs, websites or emails are generally collected from PhishTank, Phishing Corpus, OpenPhish, SpamAssassin, etc. [12, 23, 31], whereas legitimate ones are selected from Alexa, Common Crawl, Ham, etc. [16, 24, 28, 31, 35]. Table 4 …","url":["https://link.springer.com/chapter/10.1007/978-3-030-79457-6_42"]}
{"year":"2021","title":"Recognizing and Splitting Conditional Sentences for Automation of Business Processes Management","authors":["NPA Vo, I Manotas, O Popescu, A Cerniauskas… - arXiv preprint arXiv …, 2021"],"snippet":"… length. 4.3 XLM-R-based Model XLM-RoBERTa (XLM-R) is a state of the art multilingual masked language model trained on 2.5 TB of newly created clean CommonCrawl data in 100 languages (Conneau et al., 2019). It obtains …","url":["https://arxiv.org/pdf/2104.00660"]}
{"year":"2021","title":"Reconstructing Implicit Knowledge with Language Models","authors":["M Becker, S Liang, A Frank - Proceedings of Deep Learning Inside Out (DeeLIO) …, 2021"],"snippet":"… GPT-2 is pre-trained on web pages from CommonCrawl, XLNet on CommonCrawl+ ClueWeb (Callan et al., 2009), and BART on the CNN/DM summarization dataset (Hermann et al., 2015). 3.3 Fine-tuning LMs Task-adapted Datasets for LM Fine-tuning …","url":["https://www.aclweb.org/anthology/2021.deelio-1.2.pdf"]}
{"year":"2021","title":"Reducing Human Labor Cost in Deep Learning for Natural Language Processing","authors":["H Jiang - 2021"],"snippet":"… cause it can be trained in a completely unsupervised manner with huge amount of unlabeled data, which are extremely cheap to fetch from internet nowadays. For example, the wellknown “Common Crawl Project”2 is …","url":["https://smartech.gatech.edu/bitstream/handle/1853/64783/JIANG-DISSERTATION-2021.pdf?sequence=1"]}
{"year":"2021","title":"Reference-based Weak Supervision for Answer Sentence Selection using Web Data","authors":["V Krishnamurthy, T Vu, A Moschitti - arXiv preprint arXiv:2104.08943, 2021"],"snippet":"… Work was conducted while the author was an intern at Amazon Alexa AI. more than 100MM Web documents from Common Crawl's data. Given a question-reference pair, the question is used as query to retrieve a set of relevant documents from the index …","url":["https://arxiv.org/pdf/2104.08943"]}
{"year":"2021","title":"Refined Commonsense Knowledge from Large-Scale Web Contents","authors":["TP Nguyen, S Razniewski, J Romero, G Weikum - arXiv preprint arXiv:2112.04596, 2021"],"snippet":"Commonsense knowledge (CSK) about concepts and their properties is useful for AI applications. Prior works like ConceptNet, COMET and others compiled large CSK collections, but are restricted in their expressiveness to subject-predicate-object (SPO) …","url":["https://arxiv.org/pdf/2112.04596"]}
{"year":"2021","title":"Relational Header Discovery using Similarity Search in a Table Corpus","authors":["H Harmouch, T Papenbrock, F Naumann - 2021 IEEE 37th International Conference …, 2021"],"snippet":"… Because web tables contain many distinct values (eg, there are „185M unique values in web tables extracted from the Web Common Crawl [13]), such indexes would be huge and, hence, easily exhaust available memory capacities …","url":["https://ieeexplore.ieee.org/abstract/document/9458838/"]}
{"year":"2021","title":"Reliability Prediction for Health-related Content: A Replicability Study","authors":["M Fernández-Pichel, D Losada, JC Pichel, D Elsweiler - 2021"],"snippet":"… [31] to generate the assessment pool, instead of using a fixed-depth pooling strategy. After the pool was formed, human assessors from 12 http://commoncrawl.org Page 9. Reliability Prediction for Health-related Content 9 Schwarz et al …","url":["https://tec.citius.usc.es/ir/pdf/ECIR_2021_replicability.pdf"]}
{"year":"2021","title":"Representations of Language Varieties Are Reliable Given Corpus Similarity Measures","authors":["J Dunn - arXiv preprint arXiv:2104.01294, 2021"],"snippet":"… The experiments draw on existing geographic corpora from the web (ultimately derived from the Common Crawl data) and from tweets (Dunn and Adams … discussed in Section 4. Data from tweets (TW) is shown on the …","url":["https://arxiv.org/pdf/2104.01294"]}
{"year":"2021","title":"Reproducibility Study: On Heavy-User Bias in A/B Testing","authors":["M Hofmaier, J Schneider, A Wagne"],"snippet":"… they obtained whole Wikipedia to compare it against itself. For the without-analysis they took a 10%-sample of the Common Crawl, which is an open repository for web crawl data. This results in 4.2 million articles consisting of …","url":["https://scholar.archive.org/work/i4ksgrqfh5ekva44jfx6oc433u/access/wayback/https://zenodo.org/record/4459284/files/Reproducibility%20Study%20On%20Heavy-User%20Bias%20in%20AB%20Testing.pdf"]}
{"year":"2021","title":"Research on Recent Quality Estimation","authors":["S Eo, C Park, H Moon, J Seo, H Lim - Journal of the Korea Convergence Society, 2021"],"snippet":"… XLM-R에서는 CommonCrawl[30] 데이터 중 100개 언어에 해당하는 CC100을, mBART에서 25개 언어에 해당하는 CC25를 사전 학습에 활용하였다 … 100개의 언어를 포함하는 2TB 이상의 CommonCrawl 데이터에 …","url":["https://www.koreascience.or.kr/article/JAKO202123162176745.pdf"]}
{"year":"2021","title":"Residual Energy-Based Models for Text","authors":["A Bakhtin, Y Deng, S Gross, M Ott, MA Ranzato… - Journal of Machine …, 2021"],"snippet":"… (2015); Kiros et al. (2015), which consists of fiction books in 16 different genres, totaling about half a billion words. • CCNews: We collect a de-duplicated subset of the English portion of the CommonCrawl news …","url":["https://www.jmlr.org/papers/volume22/20-326/20-326.pdf"]}
{"year":"2021","title":"Retaining Named Entities for Headline Generation","authors":["B Singh, A Marathe, AA Rizvi, AR Joshi - Inventive Computation and Information …"],"snippet":"… These vectors are Page 246. 226 B. Singh et al. trained using word co-occurrence statistics on the Common Crawl corpus. The word embeddings of more than 94% of the words of the dataset's vocabulary are present in the glove. 840B. 300d …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=XhomEAAAQBAJ&oi=fnd&pg=PA221&dq=commoncrawl&ots=EStSRi0ndc&sig=9S5PclK2A3dTQ9RjYbxYB4YNjLI"]}
{"year":"2021","title":"RetroGAN: A Cyclic Post-Specialization System for Improving Out-of-Knowledge and Rare Word Representations","authors":["P Colon-Hernandez, Y Xin, H Lieberman, C Havasi…"],"snippet":"… In our tests we use the English Common Crawl FastText with sub-word information (FT-CC) and Numberbatch 19.08 (NB) to see how performance would be affected by using embeddings that were already retrofitted with a large KB …","url":["https://aclanthology.org/2021.findings-acl.183.pdf"]}
{"year":"2021","title":"Revisiting Additive Compositionality: AND, OR and NOT Operations with Word Embeddings","authors":["M Naito, S Yokoi, G Kim, H Shimodaira - arXiv preprint arXiv:2105.08585, 2021"],"snippet":"Page 1. Revisiting Additive Compositionality: AND, OR and NOT Operations with Word Embeddings Masahiro Naito1,3 Sho Yokoi 2,3 Geewook Kim4 Hidetoshi Shimodaira 1,3 1Kyoto University 2Tohoku University 3RIKEN 4 Naver …","url":["https://arxiv.org/pdf/2105.08585"]}
{"year":"2021","title":"Revisiting Document Representations for Large-Scale Zero-Shot Learning","authors":["J Kil, WL Chao - arXiv preprint arXiv:2104.10355, 2021"],"snippet":"… al., 2009) that has more than 20,000 classes, ex- isting ZSL algorithms (Frome et al., 2013; Norouzi et al., 2013) mostly resort to word vectors of classes names (Mikolov et al., 2013; Pennington et al., 2014) that are automatically …","url":["https://arxiv.org/pdf/2104.10355"]}
{"year":"2021","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer","authors":["I Turc, K Lee, J Eisenstein, MW Chang, K Toutanova - arXiv preprint arXiv …, 2021"],"snippet":"… with impressive crosslingual transfer capabilities. mT5 Multilingual T5 (mT5) (Xue et al., 2020) is an encoder-decoder model that was jointly trained on 101 languages from Common Crawl. We use its mT5-Base variant, whose …","url":["https://arxiv.org/pdf/2106.16171"]}
{"year":"2021","title":"Revisiting Tri-training of Dependency Parsers","authors":["J Wagner, J Foster - arXiv preprint arXiv:2109.08122, 2021"],"snippet":"… 3.4 Unlabelled Data To match the training data of the word embeddings (Section 3.5), we use the Wikipedia and Common Crawl data of the CoNLL 2017 Shared Task in UD Parsing (Ginter et al., 2017; Zeman et al., 2017) as unlabelled data in tri-training …","url":["https://arxiv.org/pdf/2109.08122"]}
{"year":"2021","title":"Rewarding Open-Domain Conversational Agents Using Memory Networks","authors":["S Majumdar - 2021"],"snippet":"… To collect such pairs of sentences, we use the Common Crawl Monolingual datasets [5]. Hence the final rescaled reward that we use for our experiments is … url: https://commoncrawl.org/. [6] Jacob Devlin et al. “Bert: Pre-training of deep …","url":["https://search.proquest.com/openview/381df303e754ecee58f50ac4b9c84b17/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2021","title":"Rissanen Data Analysis: Examining Dataset Characteristics via Description Length","authors":["E Perez, D Kiela, K Cho - arXiv preprint arXiv:2103.03872, 2021"],"snippet":"Page 1. Rissanen Data Analysis: Examining Dataset Characteristics via Description Length Ethan Perez1 Douwe Kiela2 Kyunghyun Cho1 3 1New York University, 2Facebook AI Research 3CIFAR Fellow in Learning in Machines & Brains perez@nyu.edu Abstract …","url":["https://arxiv.org/pdf/2103.03872"]}
{"year":"2021","title":"Robustness of Machine Translation for Low-Resource Languages","authors":["O Aarnikoivu"],"snippet":"Abstract It is becoming increasingly common for researchers and practitioners to rely on methods within the field of Neural Machine Translation (NMT) that require the use of an extensive amount of auxiliary data. This is especially true for low-resource …","url":["https://project-archive.inf.ed.ac.uk/msc/20215190/msc_proj.pdf"]}
{"year":"2021","title":"Robustness Testing of Language Understanding in Dialog Systems","authors":["J Liu, R Takanobu, J Wen, D Wan, W Nie, H Li, C Li… - arXiv preprint arXiv …, 2020"],"snippet":"… system. We directly use the released models in the repository as DeepSpeech2's configuration, where the speech model is trained on Baidu Internal English Dataset, and the language model is trained on CommonCrawl Data …","url":["https://arxiv.org/pdf/2012.15262"]}
{"year":"2021","title":"RoGPT2: Romanian GPT2 for Text Generation","authors":["MA Niculescu, S Ruseti, M Dascalu - 2021 IEEE 33rd International Conference on …, 2021"],"snippet":"… First, OSCAR [21] has the highest percentage of the total training corpus, is a multi-language corpus, each language being extracted via language classification, filtered, and cleaned from Common Crawl. Second, the latest version of the Wikipedia Dumps for …","url":["https://ieeexplore.ieee.org/abstract/document/9643330/"]}
{"year":"2021","title":"Role of Language Relatedness in Multilingual Fine-tuning of Language Models: A Case Study in Indo-Aryan Languages","authors":["TI Dhamecha, R Murthy V, S Bharadwaj… - arXiv preprint arXiv …, 2021"],"snippet":"… 4.1 Data To train the language models, we obtained text data from various sources including: Wikipedia Dump6, WMT Common Crawl 7, WMT News CommonCrawl8, Urdu Charles University Corpus (Bojar et al., 2014; Jawaid …","url":["https://arxiv.org/pdf/2109.10534"]}
{"year":"2021","title":"ROMANIAN SPEECH RECOGNITION EXPERIMENTS FROM THE ROBIN PROJECT","authors":["AM AVRAM, V PĂIȘ, DAN TUFIȘ - ISSN 1843-911X"],"snippet":"… model could benefit in terms of accuracy. 4 http://corolaws. racai. ro/ corola_sound_search/index. php 5 http://www. racai. ro/p/reterom/ 6 https:// commoncrawl. org/ Page 116. ANDREI-MARIUS AVRAM, VASILE PĂIȘ, DAN TUFIȘ …","url":["https://profs.info.uaic.ro/~consilr/wp-content/uploads/2021/02/volum-ConsILR-v-4-final-revizuit.pdf#page=111"]}
{"year":"2021","title":"RuSentEval: Linguistic Source, Encoder Force!","authors":["V Mikhailov, E Taktasheva, E Sigdel, E Artemova - arXiv preprint arXiv:2103.00573, 2021"],"snippet":"… XLM-R (Conneau et al., 2020a) is trained on 'dy- namic' MLM task, over filtered CommonCrawl data in 100 languages (Wenzek et al., 2020). MiniLM (Wang et al., 2020) is a distilled transformer of BERT architecture, but uses the XLMRoBERTa tokenizer …","url":["https://arxiv.org/pdf/2103.00573"]}
{"year":"2021","title":"S&P500 INDEX DIRECTION PREDICTION USING TEXTUAL TWEETS AND THEIR CORRESPONDING SENTIMENT","authors":["P Mohammadalizadeh, M Jafari - 2021"],"snippet":"… The model structure is a standard sort of encoder-decoder transformer. T5 model structure is shown in Fig. 3. T5 uses common crawl web extracted text and the authors applied a simple heuristic filtering. T5 removes any …","url":["http://journal.research.fanap.com/article_135403_775c4f187c40181e2720e9fc5fc5eaec.pdf"]}
{"year":"2021","title":"S-NLP at SemEval-2021 Task 5: An Analysis of Dual Networks for Sequence Tagging","authors":["VA Nguyen, TM Nguyen, HQ Dao, QH Pham"],"snippet":"… We discard subword part, take only word vector part of a FastText model (pre-trained on Common Crawl dataset) for word representation and utilize an external English Byte Pair Embedding for out-of-vocabulary functionality …","url":["https://aclanthology.org/2021.semeval-1.120.pdf"]}
{"year":"2021","title":"SAFFRON: tranSfer leArning For Food-disease RelatiOn extractioN","authors":["G Cenikj, T Eftimov, BK Seljak - Proceedings of the 20th Workshop on Biomedical …, 2021"],"snippet":"… Wikipedia, which are used for the pretraining of BERT, RoBERTa is trained on data from 3 additional sources: the CommonCrawl News dataset (Nagel, 2016), the OpenWebText corpus (Gokaslan and Cohen, 2019) and Stories …","url":["https://www.aclweb.org/anthology/2021.bionlp-1.4.pdf"]}
{"year":"2021","title":"Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages","authors":["G Ramesh, S Doddapaneni, A Bheemaraj… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages Gowtham Ramesh1∗ Sumanth Doddapaneni1∗ Aravinth Bheemaraj2,5 Mayank Jobanputra3 …","url":["https://arxiv.org/pdf/2104.05596"]}
{"year":"2021","title":"SAU'S Submission for CCMT 2021 Quality Estimation Task","authors":["Y Li, N Ye, D Cai"],"snippet":"… XLM-R is a multi-language pre-training model proposed by Facebook, which uses 2.5TB CommonCrawl to filter data, and masked language model pre-trained on text in 100 languages, which obtains state-of-the-art performance on cross-lingual …","url":["http://sc.cipsc.org.cn/mt/conference/2021/papers/T21-2012.pdf"]}
{"year":"2021","title":"SAUCE: Truncated Sparse Document Signature Bit-Vectors for Fast Web-Scale Corpus Expansion","authors":["M Wahed, D Gruhl, A Alba, AL Gentile, P Ristoski… - arXiv preprint arXiv …, 2021"],"snippet":"… This experiment facilitates evaluation on a limited seed corpus scenario. The document collection is a snapshot (approximately 200 million documents) of the Common Crawl web archive1, which is regularly updated and consists …","url":["https://arxiv.org/pdf/2108.11948"]}
{"year":"2021","title":"Say What? Collaborative Pop Lyric Generation Using Multitask Transfer Learning","authors":["N Ram, T Gummadi, R Bhethanabotla, RJ Savery… - Proceedings of the 9th …, 2021"],"snippet":"… The authors opted for an encoder-decoder transformer based design, trained on a variant of the Common Crawl corpus named the Colossal Common Crawl Corpus (or C4 for short). The T5 shows a remarkable ability to tackle many different tasks with …","url":["https://dl.acm.org/doi/abs/10.1145/3472307.3484175"]}
{"year":"2021","title":"Scalable Graph Convolutional Variational Autoencoders","authors":["D Unyi, B Gyires-Tóth - 2021 IEEE 15th International Symposium on Applied …, 2021"],"snippet":"… In Reddit, nodes are Reddit posts, and if the same user commented on two posts, a link is drawn between them; node features are GloVe CommonCrawl word vectors [31] based on the average embedding of the …","url":["https://ieeexplore.ieee.org/abstract/document/9465579/"]}
{"year":"2021","title":"SCALE EFFICIENTLY: INSIGHTS FROM PRE-TRAINING AND FINE-TUNING TRANSFORMERS","authors":["FT TRANSFORMERS","Y Tay, M Dehghani, J Rao, W Fedus, S Abnar… - arXiv preprint arXiv …, 2021"],"snippet":"Abstract: There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both …","url":["https://arxiv.org/pdf/2109.10686","https://openreview.net/pdf?id=f2OYVDyfIB"]}
{"year":"2021","title":"Scaling Laws for Transfer","authors":["D Hernandez, J Kaplan, T Henighan, S McCandlish - arXiv preprint arXiv:2102.01293, 2021"],"snippet":"… Pre-trained text models were trained on a mix of WebText2 described in [KMH+20], Common Crawl5 [RSR+20], English Wikipedia, and publicly available Internet Books … 5https://commoncrawl.org/the-data/ 6https://www.gharchive.org/ 6 Page 7. 3 Results …","url":["https://arxiv.org/pdf/2102.01293"]}
{"year":"2021","title":"Scarecrow: A Framework for Scrutinizing Machine Text","authors":["Y Dou, M Forbes, R Koncel-Kedziorski, NA Smith… - arXiv preprint arXiv …, 2021"],"snippet":"… GPT-3 DaVinci (Brown et al., 2020) The 175B parameter variant of GPT-3, which is trained on a version of the Common Crawl web scrape with additional filtering and deduplicating. These model choices allow us to study several …","url":["https://arxiv.org/pdf/2107.01294"]}
{"year":"2021","title":"SCOPA: Soft Code-Switching and Pairwise Alignment for Zero-Shot Cross-lingual Transfer","authors":["D Lee, J Lee, G Lee, B Chun, S Hwang - Proceedings of the 30th ACM International …, 2021"],"snippet":"… To enhance such transfer, XLM-R [4] is pre-trained on 100 languages with CommonCrawl Corpora, supervised by translational objectives, to transfer from resource-rich languages (eg, English and Chinese) to resource-poor languages …","url":["https://dl.acm.org/doi/abs/10.1145/3459637.3482176"]}
{"year":"2021","title":"Screening Gender Transfer in Neural Machine Translation","authors":["G Wisniewski, L Zhu, N Bailler, F Yvon - … of the Fourth BlackboxNLP Workshop on …, 2021"],"snippet":"… It includes the Europarl, NewsCommentary and CommonCrawl corpora, and altogether contains 4,813,682 sentences and nearly 141 million French running words. All the corpora were tokenized and segmented into sub-lexical units using …","url":["https://aclanthology.org/2021.blackboxnlp-1.24.pdf"]}
{"year":"2021","title":"Screening hardware and volume factors in distributed machine learning algorithms on spark","authors":["JB Rodrigues, GC Vasconcelos, PRM Maciel - Computing, 2021"],"snippet":"This paper presents an approach to investigate distributed machine learning workloads on Spark. The work analyzes hardware and volume data factors regardin.","url":["https://link.springer.com/article/10.1007/s00607-021-00965-3"]}
{"year":"2021","title":"Selecting Parallel In-domain Sentences for Neural Machine Translation Using Monolingual Texts","authors":["JPR Sharami, D Shterionov, P Spronck - arXiv preprint arXiv:2112.06096, 2021"],"snippet":"Continuously-growing data volumes lead to larger generic models. Specific use-cases are usually left out, since generic models tend to perform poorly in domain-specific cases. Our work addresses this gap with a method for selecting in-domain data from …","url":["https://arxiv.org/pdf/2112.06096"]}
{"year":"2021","title":"Selecting the best data filtering method for NMT training","authors":["F Bane, A Zaretskaya - Proceedings of Machine Translation Summit XVIII …, 2021"],"snippet":"… Rule Based Classifier for Translation Memory Cleaning. Online. Smith, JR, Saint-Amand, H., Plamada, M., Koehn, P., Callison-Burch, C., and Lopez, A. (2013). Dirt cheap web-scale parallel text from the Common Crawl. In …","url":["https://aclanthology.org/2021.mtsummit-up.9.pdf"]}
{"year":"2021","title":"Self-Attention Network for Text Representation Learning","authors":["T Shen - 2021"],"snippet":"Page 1. Self-Attention Network for Text Representation Learning by Tao SHEN Thesis submitted in fulfilment of the requirements for the degree of Doctor of Philosophy under the supervision of Dist. Prof. Chengqi ZHANG, Prof. Guodong LONG and Dr. Jing JIANG …","url":["https://opus.lib.uts.edu.au/bitstream/10453/149266/2/02Whole.pdf"]}
{"year":"2021","title":"Self-Contextualized Attention for Abusive Language Identification","authors":["H Jarquín-Vásquez, HJ Escalante, M Montes - … of the Ninth International Workshop on …, 2021"],"snippet":"… For word representation we used pre-trained fastText embeddings (Mikolov et al., 2018), trained with subword information on Common Crawl, which have been recognized as useful for this task ac- cording to the study presented in (Corazza et al., 2020) …","url":["https://www.aclweb.org/anthology/2021.socialnlp-1.9.pdf"]}
{"year":"2021","title":"Self-Supervised Representation Learning: Introduction, Advances and Challenges","authors":["L Ericsson, H Gouk, CC Loy, TM Hospedales - arXiv preprint arXiv:2110.09327, 2021"],"snippet":"Self-supervised representation learning methods aim to provide powerful deep feature learning without the requirement of large annotated datasets, thus alleviating the annotation bottleneck that is one of the main barriers to practical deployment of …","url":["https://arxiv.org/pdf/2110.09327"]}
{"year":"2021","title":"Self-Training Pre-Trained Language Models for Zero-and Few-Shot Multi-Dialectal Arabic Sequence Labeling","authors":["M Khalifa, M Abdul-Mageed, K Shaalan - arXiv preprint arXiv:2101.04758, 2021"],"snippet":"… XLM-R also uses Common Crawl for training, which is more likely to have dialectal data than the Arabic Wikipedia (used in mBERT), making it more suited to our work … mBERT, XLM-R was trained on much larger data …","url":["https://arxiv.org/pdf/2101.04758"]}
{"year":"2021","title":"Self-training vs Pre-trained Embeddings for Automatic Essay Scoring","authors":["X Zhou, L Yang, X Fan, G Ren, Y Yang, H Lin - China Conference on Information …, 2021"],"snippet":"… In the training process, we use both the pre-trained embeddings and the randomly initialized self-trained embeddings, where the pre-trained word vector comes from Glove Common Crawl (6B token) and the dimension is …","url":["https://link.springer.com/chapter/10.1007/978-3-030-88189-4_12"]}
{"year":"2021","title":"Self–other moral bias: Evidence from implicit measures and the Word-Embedding Association Test","authors":["MH Li, PW Li, LL Rao - Personality and Individual Differences, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0191886921004864"]}
{"year":"2021","title":"Semantic Detection of Targeted Attacks Using DOC2VEC Embedding","authors":["MS El-Rahmany, EH Mohamed, MH Haggag"],"snippet":"… This model was trained using Common Crawl and Wikipedia. CBOW was used to train these models. In this study, we used dimension 300, character n-grams of length 5, a window of size 5, and 10 negatives. …","url":["https://softcom2020.fesb.unist.hr/pdfs/v17n4_2021-0113_rahmany.pdf"]}
{"year":"2021","title":"Semantic maps and metrics for science Semantic maps and metrics for science using deep transformer encoders","authors":["B Chambers, J Evans - arXiv preprint arXiv:2104.05928, 2021"],"snippet":"Page 1. Semantic maps and metrics for science using deep transformer encoders Brendan Chambers QuillBot Knowledge Lab at the University of Chicago James Evans Department of Sociology and Knowledge Lab, University of Chicago Santa Fe Institute …","url":["https://arxiv.org/pdf/2104.05928"]}
{"year":"2021","title":"Semantic Networks for Engineering Design: State of the Art and Future Directions","authors":["J Han, S Sarica, F Shi, J Luo - Journal of Mechanical Design, 2021"],"snippet":"Page 1. Journal of Mechanical Design 1 Semantic Networks for Engineering Design: State of the Art and Future Directions Ji Han1 School of Engineering University of Liverpool, the United Kingdom e-mail: ji.han@liverpool.ac.uk …","url":["https://asmedigitalcollection.asme.org/mechanicaldesign/article-abstract/doi/10.1115/1.4052148/1115821"]}
{"year":"2021","title":"Semantic Search of Mobile Applications Using Word Embeddings","authors":["J Coelho, A Neto, M Tavares, C Coutinho, R Ribeiro… - 10th Symposium on …, 2021"],"snippet":"… This way, character-level embeddings are considered, instead of word-level representations. In preliminary tests, we compared a CBOW [12] fastText model (FT1), pre-trained on English Common Crawl considering word 5-grams …","url":["https://drops.dagstuhl.de/opus/volltexte/2021/14429/pdf/OASIcs-SLATE-2021-12.pdf"]}
{"year":"2021","title":"Semantic Similarity Based Evaluation for Abstractive News Summarization","authors":["FB Fikri, K Oflazer, B Yanıkoglu - GEM 2021, 2021"],"snippet":"… et al., 2019). mT5 Multilingual T5 (mT5)(Xue et al., 2020) is a variant of T5 model (Raffel et al., 2020) that was pre-trained for 101 languages including Turkish on a new Common Crawl-based dataset. For Turkish summarization …","url":["https://aclanthology.org/2021.gem-1.pdf#page=36"]}
{"year":"2021","title":"Semantic-WER: A Unified Metric for the Evaluation of ASR Transcript for End Usability","authors":["S Roy - arXiv preprint arXiv:2106.02016, 2021"],"snippet":"… A 4-gram language model trained on common crawl dataset is used for decoding 6 7[28] … 4https://github.com/pytorch/fairseq/tree/master/examples/ wav2vec 5https://github.com/facebookresearch/wav2letter/tree/master/recipes …","url":["https://arxiv.org/pdf/2106.02016"]}
{"year":"2021","title":"Semantically Meaningful Sentence Embeddings","authors":["R Deuja - 2021"],"snippet":"Text embedding is an approach used in Natural Language Processing (NLP) to represent words, phrases, sentences, and documents. It is the process of obtaining numeric representations of text to feed into machine learning models as vectors (arrays …","url":["https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1247&context=computerscidiss"]}
{"year":"2021","title":"SemFace: Pre-training Encoder and Decoder with a Semantic Interface for Neural Machine Translation","authors":["S Ren, L Zhou, S Liu, F Wei, M Zhou, S Ma"],"snippet":"… Table 1: The datasets used in our experiments. Lang: language; Mono: monolingual; Para: parallel; #Sent: number of sentences in the monolingual corpus; NC: NewsCrawl; CC: CommonCrawl; BE: BigEst Estonian corpus; Wiki: Wiki dumps …","url":["https://aclanthology.org/2021.acl-long.348.pdf"]}
{"year":"2021","title":"Semi-automatic Construction of Sight Words Dictionary for Filipino Text Readability","authors":["JM Imperial - Knowledge Management and Acquisition for Intelligent …"],"snippet":"… The Filipino word embedding model has been pre-trained from the articles of Wikipedia and the publicly-available webscraped data of the Common Crawl project using the Continuous Bag-of-Words (CBOW) model [2]. Word …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=oTofEAAAQBAJ&oi=fnd&pg=PA168&dq=commoncrawl&ots=a3xsZgmDzY&sig=HvrePT8d-vC6XeT31rVJoZx_cyI"]}
{"year":"2021","title":"Semi-supervised log-based anomaly detection via probabilistic label estimation","authors":["L Yang, J Chen, Z Wang, W Wang, J Jiang, X Dong… - 2021 IEEE/ACM 43rd …, 2021"],"snippet":"… Then, PLELog adopts the pre-trained word vectors based on Common Crawl Corpus using the FastText algorithm [46] (which can effectively capture the intrinsic relationship among words in natural language), to extract the …","url":["https://ieeexplore.ieee.org/abstract/document/9401970/"]}
{"year":"2021","title":"Sentence encoding for Dialogue Act classification","authors":["N Duran, S Battle, J Smith - Natural Language Engineering, 2021"],"snippet":"In this study, we investigate the process of generating single-sentence representations for the purpose of Dialogue Act (DA) classification, including several aspects of text pre-processing and input representation which are often overlooked …","url":["https://www.cambridge.org/core/journals/natural-language-engineering/article/sentence-encoding-for-dialogue-act-classification/2EF3DC8E57D1019960D18FDE685B1EBA"]}
{"year":"2021","title":"Sentence simplification with ruGPT3","authors":["AA Shatilov, AI Rey"],"snippet":"… One of the latest works [12] uses an unsupervised approach to automatically create training corpora for simplification in multiple languages from raw Common Crawl web data and train simplification systems in any language with a controllable generation mechanism …","url":["http://www.dialog-21.ru/media/5281/shatilovaaplusreyai142.pdf"]}
{"year":"2021","title":"Sentiment Analysis of Code-Mixed Roman Urdu-English Social Media Text using Deep Learning Approaches","authors":["A Younas, R Nasim, S Ali, G Wang, F Qi - 2020 IEEE 23rd International Conference on …"],"snippet":"… Similarly, authors have developed XLM-RoBERTa (XLM-R) - a multilingual language model trained on 2.5 terabyte of filtered CommonCrawl data using BERT-large architecture [9]. It provides strong advantage over mBERT model …","url":["https://ieeexplore.ieee.org/abstract/document/9345887/"]}
{"year":"2021","title":"Sentiment analysis of Korean movie reviews using XLM-R","authors":["NR Shin, TH Kim, DY Yun, SJ Moon, C Hwang - International Journal of Advanced …, 2021"],"snippet":"… Among them, the Facebook AI team unveiled the XLM-R (XLM-RoBERTa), an upgraded XLM model. XLM-R solved the data limitation and the curse of multilinguality by training XLM with 2TB or more refined CC (CommonCrawl), not Wikipedia data …","url":["https://www.koreascience.or.kr/article/JAKO202119559811514.pdf"]}
{"year":"2021","title":"Sentiment Analysis of Latin Poetry: First Experiments on the Odes of Horace","authors":["R Sprugnoli, F Mambrini, M Passarotti, G Moretti - Eighth Italian Conference on …, 2021"],"snippet":"In this paper we present a set of annotated data and the results of a number of unsupervised experiments for the analysis of sentiment in Latin poetry. More specifically, we describe a small gold standard made of eight poems by Horace, in …","url":["http://ceur-ws.org/Vol-3033/paper25.pdf"]}
{"year":"2021","title":"Sentiment Analysis of Sinhala News Comments","authors":["S Ranathunga, IU Liyanage - Transactions on Asian and Low-Resource Language …, 2021"],"snippet":"… 2009], and the Sinhala Common Crawl dataset1 are examples of un-annotated Sinhala corpora. Common crawl is the largest among these, with about 110 million uncleaned tokens … Both SinMin and Common Crawl contain a lot of noise …","url":["https://dl.acm.org/doi/abs/10.1145/3445035"]}
{"year":"2021","title":"Sentiment Analysis of Users' Reviews on COVID-19 Contact Tracing Apps with a Benchmark Dataset","authors":["K Ahmad, F Alam, J Qadir, B Qolomany, I Khan, T Khan… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. 1 Sentiment Analysis of Users' Reviews on COVID-19 Contact Tracing Apps with a Benchmark Dataset Kashif Ahmad∗, Firoj Alam†, Junaid Qadir‡, Basheer Qolomany§, Imran Khan¶, Talhat Khan¶, Muhammad Suleman …","url":["https://arxiv.org/pdf/2103.01196"]}
{"year":"2021","title":"Sentiment Analysis Using XLM-R Transformer and Zero-shot Transfer Learning on Resource-poor Indian Language","authors":["A Kumar, VHC Albuquerque - Transactions on Asian and Low-Resource Language …, 2021"],"snippet":"… It is a scaled cross-lingual sentence encoder trained on 2.5 Tb of data across 100 languages data filtered from CommonCrawl texts. Compared to the original version, the biggest update of XLM-Roberta is a significant …","url":["https://dl.acm.org/doi/abs/10.1145/3461764"]}
{"year":"2021","title":"Sentiment Polarity Classification of Corporate Review Data with a Bidirectional Long-Short Term Memory (biLSTM) Neural Network Architecture","authors":["RE Loke, O Kachaniuk - 2020"],"snippet":"… Therefore, it was decided to proceed with fastText embeddings. Accordingly, pre-trained vectors for Dutch language were added to the biLSTM model as one of the layers. Moreover, these vectors were trained on Common Crawl and Wikipedia using fastText …","url":["https://pure.hva.nl/ws/files/17012226/DATA_2020_75.pdf"]}
{"year":"2021","title":"Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning","authors":["S Lee, HB Lee, J Lee, SJ Hwang - arXiv preprint arXiv:2110.02600, 2021"],"snippet":"Page 1. Preprint SEQUENTIAL REPTILE: INTER-TASK GRADIENT ALIGNMENT FOR MULTILINGUAL LEARNING Seanie Lee1∗, Hae Beom Lee1∗, Juho Lee1,2, Sung Ju Hwang1,2 KAIST1, AITRICS2, South Korea …","url":["https://arxiv.org/pdf/2110.02600"]}
{"year":"2021","title":"SG-MLP: Switch Gated Multi-Layer Perceptron Model for Natural Language Understanding","authors":["G Son, S Kim, SJ Joo, W Cho, JE Nah - … of the Korea Information Processing Society …, 2021"],"snippet":"… SG-MLP Small 은 약 16GB 에 해당하는 Book Corpus 와 Wiki 데이터로 학습 되었으며 이외의 모델은 Common Crawl’s Web Crawl Corpus 를 한 차례 정제한 Allen·C4·En[8] 데이터를 학습에 활용하였으며 이는 약 305GB 정도의 분량이다. 보유한 하드웨어 …","url":["https://www.koreascience.or.kr/article/CFKO202133649066979.pdf"]}
{"year":"2021","title":"Shall I Work with Them? A Knowledge Graph-Based Approach for Predicting Future Research Collaborations","authors":["N Kanakaris, N Giarelis, I Siachos, N Karacapilidis - Entropy, 2021"],"snippet":"We consider the prediction of future research collaborations as a link prediction problem applied on a scientific knowledge graph. To the best of our knowledge, this is the first work on the prediction of future research collaborations that …","url":["https://www.mdpi.com/1099-4300/23/6/664/pdf"]}
{"year":"2021","title":"Shallow Discourse Parsing for German","authors":["P Bourgonje"],"snippet":"Page 1. Shallow Discourse Parsing for German Peter Bourgonje Dissertation eingereicht bei der Humanwissenschaftlichen Fakultät der Universität Potsdam 2021 Page 2. Datum der Einreichung: 22-07-2020 Wissenschaftlicher Betreuer und Gutachter: Prof …","url":["https://publishup.uni-potsdam.de/files/50663/bourgonje_diss.pdf"]}
{"year":"2021","title":"Sharp Learning Bounds for Contrastive Unsupervised Representation Learning","authors":["H Bao, Y Nagano, K Nozawa - arXiv preprint arXiv:2110.02501, 2021"],"snippet":"Page 1. Sharp Learning Bounds for Contrastive Unsupervised Representation Learning *Han Bao, Yoshihiro Nagano, Kento Nozawa The University of Tokyo & RIKEN AIP {tsutsumi@ms., nagano@, nozawa@ms.}ku-tokyo.ac.jp October 7, 2021 …","url":["https://arxiv.org/pdf/2110.02501"]}
{"year":"2021","title":"Should we find another model?: Improving Neural Machine Translation Performance with ONE-Piece Tokenization Method without Model Modification","authors":["C Park, S Eo, H Moon, HS Lim - Proceedings of the 2021 Conference of the North …, 2021"],"snippet":"… corpus. In mBART (Liu et al., 2020), the CC25 corpus was composed of a total of 25 languages extracted from CommonCrawl (CC) (Lample and Conneau, 2019; Wenzek et al., 2019) and used for unified vocabulary extraction …","url":["https://www.aclweb.org/anthology/2021.naacl-industry.13.pdf"]}
{"year":"2021","title":"Show, tell and summarise: learning to generate and summarise radiology findings from medical images","authors":["S Singh, S Karimi, K Ho-Shon, L Hamey - Neural Computing and Applications, 2021"],"snippet":"Radiology plays a vital role in health care by viewing the human body for diagnosis, monitoring, and treatment of medical problems. In radiology practice,.","url":["https://link.springer.com/article/10.1007/s00521-021-05943-6"]}
{"year":"2021","title":"SJ_AJ@ DravidianLangTech-EACL2021: Task-Adaptive Pre-Training of Multilingual BERT models for Offensive Language Identification","authors":["SM Jayanthi, A Gupta - arXiv preprint arXiv:2102.01051, 2021"],"snippet":"… mon Crawl data6. The common crawl data consists of textual data both in native scripts as well as in romanized script for some of the languages. Among the three languages in this task, only Tamil has Romanized data in the Common Crawl …","url":["https://arxiv.org/pdf/2102.01051"]}
{"year":"2021","title":"Sketch-Based Creativity Support Tools Using Deep Learning","authors":["F Huang, E Schoop, D Ha, J Nichols, J Canny - Artificial Intelligence for Human …, 2021"],"snippet":"Abstract Sketching is a natural and effective visual communication medium commonly used in creative processes. Recent developments in deep-learning models drastically improved machines’ ability in understanding and generating …","url":["https://link.springer.com/chapter/10.1007/978-3-030-82681-9_12"]}
{"year":"2021","title":"SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training","authors":["A Bapna, Y Chung, N Wu, A Gulati, Y Jia, JH Clark… - arXiv preprint arXiv …, 2021"],"snippet":"Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream …","url":["https://arxiv.org/pdf/2110.10329"]}
{"year":"2021","title":"Slav-NER: the 3rd Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic languages","authors":["J Piskorski, B Babych, Z Kancheva, O Kanishcheva… - The 8th Workshop onBalto …, 2021"],"snippet":"Page 1. Slav-NER: the 3rd Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic languages Jakub Piskorski,1 Bogdan Babych,2 Zara Kancheva,3 Olga Kanishcheva …","url":["https://tuhat.helsinki.fi/ws/files/161998023/Slav_NER_Shared_Task.pdf"]}
{"year":"2021","title":"SlovakBERT: Slovak Masked Language Model","authors":["M Pikuliak, Š Grivalský, M Konôpka, M Blšták… - arXiv preprint arXiv …, 2021"],"snippet":"… is a suite of RoBERTa-style LMs. The models support 100 languages, including Slovak. Training data are based on CommonCrawl Web-crawled corpus. Slovak part has 23.2 GB (3.5B tokens). The XLM-R models differ in their …","url":["https://arxiv.org/pdf/2109.15254"]}
{"year":"2021","title":"Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-Resource Languages","authors":["K Ogueji, Y Zhu, J Lin"],"snippet":"… We select these languages because they are the languages supported by the British Broadcasting Corporation (BBC) News, which was our main source of data.4 We also obtain additional data from the Common Crawl …","url":["https://cs.uwaterloo.ca/~jimmylin/publications/Ogueji_etal_MRL2021.pdf"]}
{"year":"2021","title":"Smart integration of sensors, computer vision and knowledge representation for intelligent monitoring and verbal human-computer interaction","authors":["T Mavropoulos, S Symeonidis, A Tsanousa… - Journal of Intelligent …, 2021"],"snippet":"The details presented in this article revolve around a sophisticated monitoring framework equipped with knowledge representation and computer vision capabi.","url":["https://link.springer.com/article/10.1007/s10844-021-00648-7"]}
{"year":"2021","title":"SOAR: A Synthesis Approach for Data Science API Refactoring","authors":["A Ni, D Ramos, A Yang, I Lynce, V Manquinho… - arXiv preprint arXiv …, 2021"],"snippet":"… is greatly reduced. However, GloVe is trained with Common Crawl [30] which contains raw webpages, which is a mismatch from our domain of textual data (ie, data science and programming). API matching. Given the representation …","url":["https://arxiv.org/pdf/2102.06726"]}
{"year":"2021","title":"Social network analysis using deep learning: applications and schemes","authors":["AM Abbas - Social Network Analysis and Mining, 2021"],"snippet":"Abstract Online social networks (OSNs) are part of daily life of human beings. Millions of users are connected through online social networks. Due to very large number of users and huge amount of data, social network analysis is a challenging …","url":["https://link.springer.com/article/10.1007/s13278-021-00799-z"]}
{"year":"2021","title":"Social Networks Analysis to Retrieve Critical Comments on Online Platforms","authors":["S Bhandari, R Raju - arXiv preprint arXiv:2102.10495, 2021"],"snippet":"… 2019 IEEE International Conference on Healthcare Informatics (ICHI), pp. 1–3, 2019. [13] S. Nagel, “Cc-news.” https://commoncrawl.org/2016/10/news-datasetavailable/, 2016. [14] M. Heidari and JH Jones, “Using bert to extract …","url":["https://arxiv.org/pdf/2102.10495"]}
{"year":"2021","title":"Social Norm Bias: Residual Harms of Fairness-Aware Algorithms","authors":["M Cheng, M De-Arteaga, L Mackey, AT Kalai - arXiv preprint arXiv:2108.11056, 2021"],"snippet":"… We use the dataset1 and task described by De-Arteaga et al. [25]. The dataset, containing 397,340 biographies spanning twenty-eight occupations, is obtained by using regular expressions to filter the Common Crawl for online biographies …","url":["https://arxiv.org/pdf/2108.11056"]}
{"year":"2021","title":"SOLID: A Large-Scale Semi-Supervised Dataset for Offensive Language Identification","authors":["S Rosenthal, P Atanasova, G Karadzhov, M Zampieri… - Proceedings of the Findings …, 2021"],"snippet":"… relations between words. First is an embedding layer initialized with a concatenation of the GloVe 300-dimensional (Pennington et al., 2014) and FastText's Common Crawl 300-dimensional embeddings (Grave et al., 2018). It is …","url":["https://aclanthology.org/2021.findings-acl.80.pdf"]}
{"year":"2021","title":"Spanish Biomedical Crawled Corpus: A Large, Diverse Dataset for Spanish Biomedical Language Models","authors":["CP Carrino, J Armengol-Estapé, OG Bonet… - arXiv preprint arXiv …, 2021"],"snippet":"… The CommonCrawl1 is very a large repository of crawled websites. However, it needs preprocessing to extract the text relevant to the user. The OSCAR corpus [10] was built applying language identification to CommonCrawl …","url":["https://arxiv.org/pdf/2109.07765"]}
{"year":"2021","title":"Sparse Distillation: Speeding Up Text Classification by Using Bigger Models","authors":["Q Ye, M Khabsa, M Lewis, S Wang, X Ren, A Jaech - arXiv preprint arXiv:2110.08536, 2021"],"snippet":"Distilling state-of-the-art transformer models into lightweight student models is an effective way to reduce computation cost at inference time. However, the improved inference speed may be still unsatisfactory for certain time-sensitive applications. In …","url":["https://arxiv.org/pdf/2110.08536"]}
{"year":"2021","title":"Spreadsheet Data Transformation for Ontology Engineering in Petrochemical Equipment Inspection Tasks","authors":["NO Dorodnykh, AY Yurin - … Conference on Intelligent Information Technologies for …, 2021"],"snippet":"… world. The exploration of the Web crawl (“Google Crawl”, “Common Crawl”, “ClueWeb”) discovered hundreds of millions of tables containing relational data [3]. Presumably, billions of valuable facts can be extracted from them …","url":["https://link.springer.com/chapter/10.1007/978-3-030-87178-9_55"]}
{"year":"2021","title":"SQYQP@ Vaxxstance: Stance Detection for the Antivaxxers Movement","authors":["J Calleja, A Méndez - 2021"],"snippet":"… embeddings, Multilingual cased BERT embeddings are employed, which are trained on cased text in the top 104 languages with the largest Wikipedias, as well as XML-RoBERTa embeddings, which are trained on 2.5 TB of newly …","url":["http://ceur-ws.org/Vol-2943/vaxx_paper2.pdf"]}
{"year":"2021","title":"Stacked Embeddings and Multiple Fine-Tuned XLM-RoBERTa Models for Enhanced Hostility Identification","authors":["XLM Fine-Tuned - Combating Online Hostile Posts in Regional …"],"snippet":"… XLM-RoBERTa. XLM-RoBERTa [10] is a large multilingual model trained on the CommonCrawl Dataset. There are two versions: base and large; both have around 250k words in the vocabulary, and the base has 250M parameters, while large has 560M …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=yD8oEAAAQBAJ&oi=fnd&pg=PA224&dq=commoncrawl&ots=ai1kFeEaRU&sig=UkWmp4nmnIiSJ004dO8RQkmOvmk"]}
{"year":"2021","title":"StarryThoughts: Facilitating Diverse Opinion Exploration on Social Issues","authors":["H Kim, H Kim, KJ Jo, J Kim - Proceedings of the ACM on Human-Computer …, 2021"],"snippet":"… After translation, we computed each opinion's embedded vector with the algorithm using pre-trained GloVe word embeddings built upon 42B tokens from Common Crawl [49]. 4.3 Implementation details The front-end of StarryThoughts is implemented with React …","url":["https://dl.acm.org/doi/abs/10.1145/3449140"]}
{"year":"2021","title":"Step-unrolled Denoising Autoencoders for Text Generation","authors":["N Savinov, J Chung, M Binkowski, E Elsen, A Oord - arXiv preprint arXiv:2112.06749, 2021"],"snippet":"… results on unconditional language modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python code from GitHub. … • We demonstrate good qualitative results for unconditional generation and inpainting on Colossal Clean …","url":["https://arxiv.org/pdf/2112.06749"]}
{"year":"2021","title":"Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?","authors":["R Choenni, E Shutova, R van Rooij - arXiv preprint arXiv:2109.10052, 2021","WWE Do, FLDT Fine-Tuning"],"snippet":"09/21/21 - In this paper, we investigate what types of stereotypical information are captured by pretrained language models. We present the f...","url":["https://arxiv.org/pdf/2109.10052","https://deepai.org/publication/stepmothers-are-mean-and-academics-are-pretentious-what-do-pretrained-language-models-learn-about-you"]}
{"year":"2021","title":"Stock Volume Prediction Based on Polarity of Tweets, News, and Historical Data Using Deep Learning","authors":["N Jawahar, J Chelladurai, I Sakthivel, B Bajracharya - 2020 2nd International …, 2020"],"snippet":"… token in order to predict the entity of that token. The CNN core model is pre-trained with GloVe vectors on Common Crawl, with 86.43% precision and 86.37% recall for NER. A python script is written that uses Psycopg to extract …","url":["https://dl.acm.org/doi/abs/10.1145/3440054.3440063"]}
{"year":"2021","title":"Storytelling Exhibitions: Identity, Truth and Wonder","authors":["P Hughes - 2021"],"snippet":""}
{"year":"2021","title":"Strategyproof Learning: Building Trustworthy User-Generated Datasets","authors":["S Farhadkhani, R Guerraoui, LN Hoang - arXiv preprint arXiv:2106.02398, 2021"],"snippet":"Page 1. arXiv:2106.02398v1 [cs.LG] 4 Jun 2021 Strategyproof Learning: Building Trustworthy User-Generated Datasets Sadegh Farhadkhani IC School, EPFL Lausanne, Switzerland sadegh.farhadkhani@epfl.ch Rachid Guerraoui …","url":["https://arxiv.org/pdf/2106.02398"]}
{"year":"2021","title":"Streaming cascade-based speech translation leveraged by a direct segmentation model","authors":["J Iranzo-Sánchez, J Jorge, P Baquero-Arnal… - Neural Networks, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0893608021002057"]}
{"year":"2021","title":"Streamlining the Identification of Emerging Tasks in the O* NET System Using Natural Language Processing (NLP): Technical Summary","authors":["JA Dahlke, DJ Putka - 2021"],"snippet":"… In our case, we used embeddings from the Global Vectors for Word Representation (GloVe; Pennington, Socher, & Manning, 2014) algorithm trained on a “common crawl” of the internet consisting of 42 billion tokens (ie, wordlike pieces of text) …","url":["https://www.onetcenter.org/dl_files/EmergingTasksNLP.pdf"]}
{"year":"2021","title":"Structured Text Generation for Spanish Freestyle Battles using Neural Networks","authors":["P Dal Bianco, I Mindlin, L Lanzarini, F Ronchetti… - 2021 XLVII Latin American …, 2021"],"snippet":"… It was trained on a version of the Common Crawl database [6], which consists of around a trillion words. GPT-3 achieves very good results for text generation, via training first on large databases such as the one mentioned and then performing fine-tuning …","url":["https://ieeexplore.ieee.org/abstract/document/9639929/"]}
{"year":"2021","title":"Study of automatic text summarization approaches in different languages","authors":["Y Kumar, K Kaur, S Kaur - Artificial Intelligence Review, 2021"],"snippet":"Nowadays we see huge amount of information is available on both, online and offline sources. For single topic we see hundreds of articles are available, co.","url":["https://link.springer.com/article/10.1007/s10462-021-09964-4"]}
{"year":"2021","title":"Studying the Relationship between Social Position and the Meaning of Work using Siamese BERT Networks and Generalized K-Means Clustering","authors":["S Stewart"],"snippet":"… 1https://www.glassdoor.com 2Specifically, I used the 300-dimensional embeddings with a 1.9 million-token vocabulary that was pretrained on Common Crawl data. 2 Page 3. Review Occupation Flexible working hours …","url":["http://cs230.stanford.edu/projects_fall_2020/reports/57221844.pdf"]}
{"year":"2021","title":"Style Change Detection on Real-World Data using LSTM-powered Attribution Algorithm","authors":["R Deibel, D Löfflad - CLEF, 2021"],"snippet":"… lab [18]. They were trained on Common Crawl and Wikipedia data, using continuous bag of words with position-weights, in dimension 300, with character n-grams of length five, a window of size five and ten negatives. Unlike …","url":["http://ceur-ws.org/Vol-2936/paper-163.pdf"]}
{"year":"2021","title":"SU-NLP at CheckThat! 2021: Check-Worthiness of Turkish Tweets","authors":["B Carik, R Yeniterzi - 2021"],"snippet":"… Loodos_ALBERT5: pre-trained on the same dataset as Loodos_BERT. • mBERT6: multilingual BERT model pre-trained on Wikipedia in 104 languages. • XLM-RoBERTa 7: this cross-lingual model was pretrained on the CommonCrawl corpus in 100 languages …","url":["http://ceur-ws.org/Vol-2936/paper-37.pdf"]}
{"year":"2021","title":"Sub-Subword N-Gram Features for Subword-Level Neural Machine Translation","authors":["A Martinez, K Sudoh, Y Matsumoto - Journal of Natural Language Processing, 2021"],"snippet":"Page 1. IA0248_04martinez (2021-03-05 13:42) General Paper Sub-Subword N-Gram Features for Subword-Level Neural Machine Translation Ander Martinez † , Katsuhito Sudoh † and Yuji Matsumoto †,†† Neural machine …","url":["https://www.jstage.jst.go.jp/article/jnlp/28/1/28_82/_pdf"]}
{"year":"2021","title":"Subword Pooling Makes a Difference","authors":["J Ács, Á Kádár, A Kornai - arXiv preprint arXiv:2102.10864, 2021"],"snippet":"… We pick these two since they are architecturally similar (both have 12 layers and the same hidden size) making our comparison easier. mBERT was trained on Wikipedia while XLM-RoBERTa was trained on CommonCrawl (Wenzek et al., 2020) …","url":["https://arxiv.org/pdf/2102.10864"]}
{"year":"2021","title":"Subwords-Only Alternatives to fastText for Morphologically Rich Languages","authors":["T Ghukasyan, Y Yeshilbashyan, K Avetisyan - Programming and Computer Software, 2021"],"snippet":"… Furthermore, as a result of embedding whole words, fastText models typically weigh a few gigabytes (Facebook's Common Crawl vectors' .bin and .vec files weigh 7.3 GB and 4.5 GB respectively [8]). This especially becomes …","url":["https://link.springer.com/article/10.1134/S0361768821010059"]}
{"year":"2021","title":"Surprise Language Challenge: Developing a Neural Machine Translation System between Pashto and English in Two Months","authors":["A Birch, B Haddow, AVM Barone, J Helcl, J Waldendorf…"],"snippet":"… Finally, we also use the Pashto-English corpus that was submitted by the Bytedance team to the WMT 2020 cleaning shared task (Koehn et al., 2020). For pretraining the German–English model we use exisiting WMT data …","url":["https://aclanthology.org/2021.mtsummit-research.8.pdf"]}
{"year":"2021","title":"Surveillance of COVID-19 Pandemic using Social Media: A Reddit Study in North Carolina","authors":["C Whitfield, Y Liu, M Anwar - arXiv preprint arXiv:2106.04515, 2021"],"snippet":"… The custom NER models trained for this study were based on spaCy's multi-task convolutional neural network (CNN) which was trained using the OntoNotes 6 corpus, and contain GloVe vectors [31] that were trained on Common Crawl …","url":["https://arxiv.org/pdf/2106.04515"]}
{"year":"2021","title":"Survey of Generative Methods for Social Media Analysis","authors":["S Matwin, A Milios, P Prałat, A Soares, F Théberge - arXiv preprint arXiv:2112.07041, 2021"],"snippet":"This survey draws a broad-stroke, panoramic picture of the State of the Art (SoTA) of the research in generative methods for the analysis of social media data. It fills a void, as the existing survey articles are either much narrower in their scope or are dated …","url":["https://arxiv.org/pdf/2112.07041"]}
{"year":"2021","title":"SwissDial: Parallel Multidialectal Corpus of Spoken Swiss German","authors":["P Dogan-Schönberger, J Mäder, T Hofmann - arXiv preprint arXiv:2103.11401, 2021"],"snippet":"… Page 2. 2. Related Work For various high-resource languages, there are parallel text corpora [2, 3, 4] and monolingual text corpora such as [5], Google News, Common Crawl that are used for the tasks of language modeling and machine translation …","url":["https://arxiv.org/pdf/2103.11401"]}
{"year":"2021","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity","authors":["W Fedus, B Zoph, N Shazeer - arXiv preprint arXiv:2101.03961, 2021"],"snippet":"Page 1. SWITCH TRANSFORMERS: SCALING TO TRILLION PARAMETER MODELS WITH SIMPLE AND EFFICIENT SPARSITY William Fedus∗ Google Brain liamfedus@google.com Barret Zoph∗ Google Brain barretzoph@google.com …","url":["https://arxiv.org/pdf/2101.03961"]}
{"year":"2021","title":"SYMBOLIC AND NEURAL APPROACHES TO NATURAL LANGUAGE INFERENCE","authors":["H Hu - 2021"],"snippet":"Page 1. SYMBOLIC AND NEURAL APPROACHES TO NATURAL LANGUAGE INFERENCE Hai Hu Submitted to the faculty of the University Graduate School in partial fulfillment of the requirements for the degree …","url":["https://scholarworks.iu.edu/dspace/bitstream/handle/2022/26642/dissertation_final_hai_hu.pdf?sequence=1"]}
{"year":"2021","title":"Symbolic Knowledge Distillation: from General Language Models to Commonsense Models","authors":["P West, C Bhagavatula, J Hessel, JD Hwang, L Jiang… - arXiv preprint arXiv …, 2021"],"snippet":"The common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-to-machine …","url":["https://arxiv.org/pdf/2110.07178"]}
{"year":"2021","title":"Syntax-Enhanced Pre-trained Model","authors":["Z Xu, D Guo, D Tang, Q Su, L Shou, M Gong, W Zhong… - arXiv preprint arXiv …, 2020"],"snippet":"… pre-training and fine-tuning stages. We construct a pre-training dataset by applying an offthe-shell dependency parser (Qi et al., 2020) to one billion sentences from common crawl news. With these data, we introduce a syntax …","url":["https://arxiv.org/pdf/2012.14116"]}
{"year":"2021","title":"Systemic racism: individuals and interactions, institutions and society","authors":["MR Banaji, ST Fiske, DS Massey - Cognitive Research: Principles and Implications, 2021","ST Fiske, DS Massey - Cognitive Research, 2021"],"snippet":"Systemic racism is a scientifically tractable phenomenon, urgent for cognitive scientists to address. This tutorial reviews the built-in systems that undermine life opportunities and outcomes by racial category, with a focus on challenges to Black …","url":["https://cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-021-00349-3","https://search.proquest.com/openview/05ee76c9a6696a3ec2aa4bcd2f966d1d/1?pq-origsite=gscholar&cbl=4402895"]}
{"year":"2021","title":"Tab2KG: Semantic Table Interpretation with Lightweight Semantic Profiles","authors":["S Gottschalk, E Demidova"],"snippet":"… Subsequent steps such as property mapping are based on the results of this lookup step. However, as shown by Ritze et al. [14], only about 3% of the tables contained in the 3.5 billion HTML pages of the Common …","url":["http://semantic-web-journal.net/system/files/swj2731.pdf"]}
{"year":"2021","title":"TABBIE: Pretrained Representations of Tabular Data","authors":["H Iida, D Thai, V Manjunatha, M Iyyer - arXiv preprint arXiv:2105.02584, 2021"],"snippet":"… TaBERT's pretraining data was not publicly released at the time of our work, but their dataset consists of 26.6M ta- bles from Wikipedia and the Common Crawl. We thus form a pretraining dataset of equivalent size by combining …","url":["https://arxiv.org/pdf/2105.02584"]}
{"year":"2021","title":"Tabular Data Concept Type Detection Using Star-Transformers","authors":["Y Zhou, S Singh, C Christodoulopoulos - 2021"],"snippet":"… We directly fine-tune TURL's concept type detection model on our dataset using its re- leased code. 4.2 Training Details We use 300 dimensional Glove [12] word embeddings pre-trained on 42 billion uncased tokens from Common …","url":["https://assets.amazon.science/80/73/db622f0344878705986e0a99dc1f/tabular-data-concept-type-detection-using-star-transformers.pdf"]}
{"year":"2021","title":"Tackling Cyber-Aggression: Identification and Fine-Grained Categorization of Aggressive Texts on Social Media using Weighted Ensemble of Transformers","authors":["O Sharif, MM Hoque - Neurocomputing, 2021"],"snippet":"… This model is trained with monolingual Bengali CommonCrawl corpus and utilises the BERT base model’s architecture. XLM-R is a cross-lingual model which outdoes m-BERT in various benchmarks. This model is built over the of 100 …","url":["https://www.sciencedirect.com/science/article/pii/S0925231221018567"]}
{"year":"2021","title":"Tackling Italian University Assessment Tests with Transformer-Based Language Models","authors":["D Puccinelli, S Demartini, PL Ferrari - 2021"],"snippet":"Cloze tests are a great tool to asses reading proficiency as well as analytical thinking, and are therefore employed in admission and assessment tests at various levels of the education system in multiple countries. In Italy, cloze tests are administered to …","url":["http://ceur-ws.org/Vol-3033/paper20.pdf"]}
{"year":"2021","title":"Targeted Feedback Generation for Constructed-Response Questions","authors":["J Zhao, K Larson, W Xu, N Gattani, C Thille - 2020"],"snippet":"… TextRank, BERT basic, and BERT fine-tuned models achieve better FPR scores. For non-transformer-based approaches, we ap- ply multitask CNN (trained on OntoNotes) and GloVe (1.2m 300d vectors trained on Common Crawl) as our embedding methods …","url":["https://assets.amazon.science/bb/f6/ea2108414befb188d20e2b75ac87/targeted-feedback-generation-for-constructed-response-questions.pdf"]}
{"year":"2021","title":"Task Aware Multi-Task Learning for Speech to Text Tasks","authors":["S Indurthi, MA Zaidi, NK Lakumarapu, B Lee, H Han… - ICASSP 2021-2021 IEEE …, 2021"],"snippet":"… We use the following datasets for ST English-German, ASR English, MT English-German tasks: MT En-De: We use the Open Subtitles [9] and WMT 19 corpora. WMT 19 consists of Common Crawl, Europarl v9, and News …","url":["https://ieeexplore.ieee.org/abstract/document/9414703/"]}
{"year":"2021","title":"Taxonomy Enrichment with Text and Graph Vector Representations","authors":["I Nikishina, M Tikhomirov, V Logacheva, Y Nazarov…"],"snippet":"Page 1. Semantic Web 0 (0) 1 1 IOS Press 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 11 11 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 29 30 30 31 31 32 32 33 …","url":["http://www.semantic-web-journal.net/system/files/swj2729.pdf"]}
{"year":"2021","title":"TBCOV: Two Billion Multilingual COVID-19 Tweets with Sentiment, Entity, Geo, and Gender Labels","authors":["M Imran, U Qazi, F Ofli - arXiv preprint arXiv:2110.03664, 2021"],"snippet":"… Although the original XLM-R model is trained on one hundred languages using more than two terabytes of filtered CommonCrawl data4, its Twitter variant XLM-T achieves better performance on a large multilingual benchmark for sentiment analysis17 …","url":["https://arxiv.org/pdf/2110.03664"]}
{"year":"2021","title":"Technology-Skill Complementarity and Labor Displacement: Evidence from Linking Two Centuries of Patents with Occupations","authors":["L Kogan, D Papanikolaou, LDW Schmidt, B Seegmiller"],"snippet":"Abstract We construct new technology indicators using textual analysis of patent documents and occupation task descriptions that span of two centuries (1850–2010). At the industry level, improvements in technology are associated with higher labor …","url":["https://www.bryanseegmiller.com/files/Draft_v2021-1017.pdf"]}
{"year":"2021","title":"Temporal Effects on Pre-trained Models for Language Processing Tasks","authors":["O Agarwal, A Nenkova - arXiv preprint arXiv:2111.12790, 2021"],"snippet":"Keeping the performance of language technologies optimal as time passes is of great practical interest. Here we survey prior work concerned with the effect of time on system performance, establishing more nuanced terminology for discussing the …","url":["https://arxiv.org/pdf/2111.12790"]}
{"year":"2021","title":"Term-community-based topic detection with variable resolution","authors":["A Hamm, S Odrowski - arXiv preprint arXiv:2103.13550, 2021"],"snippet":"… The pre-trained fastText models are shallow neural nets with output vectors of dimension 300 that were trained on huge text collections from Wikipedia and Common Crawl, using the so called CBOW task of predicting a word by its surrounding words …","url":["https://arxiv.org/pdf/2103.13550"]}
{"year":"2021","title":"Term-Community-Based Topic Detection with Variable Resolution. Information 2021, 12, 221","authors":["A Hamm, S Odrowski - 2021"],"snippet":"Page 1. information Article Term-Community-Based Topic Detection with Variable Resolution Andreas Hamm * and Simon Odrowski Citation: Hamm, A.; Odrowski, S. Term-Community-Based Topic Detection with …","url":["https://128.84.4.13/pdf/2103.13550.pdf"]}
{"year":"2021","title":"Text Analysis for Psychology: Methods, Principles, and Practices","authors":["B Kennedy, A Ashokkumar, RL Boyd, M Dehghani - 2021"],"snippet":"Page 1. Running Head: TEXT ANALYSIS METHODS FOR PSYCHOLOGY Text Analysis for Psychology: Methods, Principles, and Practices Brendan Kennedy​1​, Ashwini Ashokkumar​2​, Ryan Boyd​3–5​, Morteza …","url":["https://psyarxiv.com/h2b8t/download?format=pdf"]}
{"year":"2021","title":"Text Classification for Predicting Multi-level Product Categories","authors":["H Jahanshahi, O Ozyegen, M Cevik, B Bulut, D Yigit… - arXiv preprint arXiv …, 2021"],"snippet":"… next-sentence pretraining objectives, and training with larger batch sizes. For our experiments, we use the multi-lingual version of this architecture pretrained on 2.5TB of CommonCrawl data in 100 languages using a …","url":["https://arxiv.org/pdf/2109.01084"]}
{"year":"2021","title":"Text Normalization for Low-Resource Languages of Africa","authors":["A Zupon, E Crew, S Ritchie - arXiv preprint arXiv:2103.15845, 2021"],"snippet":"… The size of available corpora also varies greatly from 10K sentences (Swahili Newscrawl) to 1M sentences (Afrikaans Web and Mixed). The OSCAR9 corpus is a multilingual corpus of data obtained by language classification and filtering of the Common Crawl corpus …","url":["https://arxiv.org/pdf/2103.15845"]}
{"year":"2021","title":"Text Similarity Techniques for Matching Employee Objectives","authors":["A Elnaggar, M Ghanem, F Matthes, AD Mckinnon…"],"snippet":"… https://huggingface.co/albert-xxlarge-v2 11. https://huggingface.co/distilbertbase-uncased 12. https://huggingface.co/roberta-base 13. https:// commoncrawl.org/2016/10/news-dataset-available/ 14. https://github.com …","url":["https://wwwmatthes.in.tum.de/file/wpu1f6pvjvt4/Sebis-Public-Website/-/Guided-Research-Mohab-Ghanem/Text_Similarity_Techniques_For_Matching_Employee_Objective.pdf"]}
{"year":"2021","title":"Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents","authors":["U Bhattacharya, N Rewkowski, A Banerjee, P Guhan… - arXiv preprint arXiv …, 2021"],"snippet":"… W = [w1 ... ws ... wTsen ], with Tsen being the maximum sentence length, using word embeddings ws ∈ R300. We obtain the word em- beddings using the GloVe model pre-trained on the Common Crawl corpus [52]. We opt …","url":["https://arxiv.org/pdf/2101.11101"]}
{"year":"2021","title":"Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents** This work has been supported in part by ARO Grants …","authors":["U Bhattacharya, N Rewkowski, A Banerjee, P Guhan… - 2021 IEEE Virtual Reality …, 2021"],"snippet":"… W = [w1 ... ws ... wTsen], with Tsen being the maximum sentence length, using word embeddings ws ∈ R300. We obtain the word embeddings using the GloVe model pre-trained on the Common Crawl corpus [52]. We opt …","url":["https://ieeexplore.ieee.org/abstract/document/9417647/"]}
{"year":"2021","title":"TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling","authors":["P Rileya, N Constantb, M Guob, G Kumarc, D Uthusb…"],"snippet":"… For this purpose, we train a single general-purpose TextSETTR model, with the same configuration as our model from Section 3, except fine-tuned for 200k steps on English Common Crawl data (the same “C4” data that T5 pretrained on) instead of Amazon reviews …","url":["https://aclanthology.org/2021.acl-long.293.pdf"]}
{"year":"2021","title":"Textual Analysis of News for Stock Market Prediction","authors":["AV Bogdanov, M Bogan, A Stankus - … Conference on Computational Science and Its …, 2021"],"snippet":"… So, for Word2Vec there are pre-trained word-vectors on Google news with a dimension of 300. There is a dictionary for GloVe trained on various textual data, including on news with Common Crawl with a dimension of 300 …","url":["https://link.springer.com/chapter/10.1007/978-3-030-87010-2_22"]}
{"year":"2021","title":"TGIF: Tree-Graph Integrated-Format Parser for Enhanced UD with Two-Stage Generic-to Individual-Language Finetuning","authors":["T Shi, L Lee - arXiv preprint arXiv:2107.06907, 2021"],"snippet":"… Representation The first step is to extract contextual representations. For this purpose, we use the pre-trained XLM-R model (Conneau et al., 2020), which is trained on multilingual CommonCrawl data and supports all 17 languages in the shared task …","url":["https://arxiv.org/pdf/2107.06907"]}
{"year":"2021","title":"The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task Overview and Evaluation Results (WebNLG","authors":["T Ferreira, C Gardent, N Ilinykh, C van der Lee, S Mille…"],"snippet":"… 3.3 Mono-task, Bilingual Approaches cuni-ufal. The mBART model (Liu et al., 2020) is pre-trained for multilingual denoising on the large-scale multilingual CC25 corpus extracted from Common Crawl, which contains …","url":["https://pure.uvt.nl/ws/portalfiles/portal/48450135/2020.webnlg_1.7.pdf"]}
{"year":"2021","title":"The Advantages of Human Evaluation of Sociomedical Question Answering Systems","authors":["V Firsanova - International Journal of Open Information Technologies, 2021"],"snippet":"… XLM-RoBERTa is a model for one hundred languages trained on CommonCrawl data. According to the developers, this model improved the performance on low-resource languages of other crossand multilingual models developed earlier. This model is …","url":["http://injoit.ru/index.php/j1/article/download/1227/1166"]}
{"year":"2021","title":"The Croatian psycholinguistic database: Estimates for 6000 nouns, verbs, adjectives and adverbs","authors":["A Peti-Stantić, M Anđel, V Gnjidić, G Keresteš… - Behavior Research Methods, 2021"],"snippet":"Psycholinguistic databases containing ratings of concreteness, imageability, age of acquisition, and subjective frequency are used in psycholinguistic and.","url":["https://link.springer.com/article/10.3758/s13428-020-01533-x"]}
{"year":"2021","title":"The Danish Gigaword Corpus","authors":["L Derczynski, ITU Copenhagen, MR Ciosici, R Baglini…"],"snippet":"… Massive, monolithic, automatically collected datasets of web content, such as Common Crawl, support the training of large language models but suffer from quality issues (Radford et al., 2019) and bias (Ferrer et al., 2021) …","url":["http://derczynski.com/papers/dagw.pdf"]}
{"year":"2021","title":"The Data Set Knowledge Graph: Creating a Linked Open Data Source for Data Sets","authors":["M Färber, D Lamprecht - 2021"],"snippet":"ABSTRACT Several scholarly knowledge graphs have been proposed to model and analyze the academic landscape. However, although the number of data sets has increased remarkably in recent years, these knowledge graphs do not primarily …","url":["https://www.aifb.kit.edu/images/6/66/DSKG_QSS2021_v0.pdf"]}
{"year":"2021","title":"The Demand for Executive Skills","authors":["J Fuller, S Hansen, T Ramdas, R Sadun - 2021"],"snippet":"… tions. 'Off-the-shelf' models—typically estimated on large corpora that are representative of written language such as Wikipedia and Common Crawl for English—do exist, but se- mantic relatedness in generic English may …","url":["https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3886751"]}
{"year":"2021","title":"The Effect of Domain and Diacritics in Yorubá–English Neural Machine Translation","authors":["DI Adelani, D Ruiter, JO Alabi, D Adebonojo, A Ayeni… - Proceedings of Machine …, 2021"],"snippet":"… Fine-tuning mT5 We examine a transfer learning approach by fine-tuning a massively multilingual model mT5 (Xue et al., 2021). mT5 had been pre-trained on 6.3T tokens originating from Common Crawl in 101 languages (including Yor`ubá) …","url":["https://www.cs.upc.edu/~cristinae/CV/docs/2021.mtsummit.adelani.pdf"]}
{"year":"2021","title":"The Effectiveness of Intermediate-Task Training for Code-Switched Natural Language Understanding","authors":["A Prasad, MA Rehan, S Pathak, P Jyothi - arXiv preprint arXiv:2107.09931, 2021"],"snippet":"… XLM-R uses a similar training objective as mBERT but is trained on orders of magnitude more data from the CommonCrawl corpus spanning 100 languages and yields competitive results on low-resource languages(Conneau et al., 2020) …","url":["https://arxiv.org/pdf/2107.09931"]}
{"year":"2021","title":"The FinSim-2 2021 Shared Task: Learning Semantic Similarities for the Financial Domain","authors":["Y Mansar, J Kang, IE Maarouf - Companion Proceedings of the Web Conference 2021, 2021"],"snippet":"… where english words of similar meaning are grouped into sets called synsets, Wikidata is a knowledge base maintained by the Wikimedia Foundation and contains entities like \"MSCI World\" and finally WebIsALOD is a hypernymy …","url":["https://dl.acm.org/doi/abs/10.1145/3442442.3451381"]}
{"year":"2021","title":"The Impact of Main Content Extraction on Near-Duplicate Detection","authors":["M Fröbe, M Hagen, J Bevendorff, M Völske, B Stein… - arXiv preprint arXiv …, 2021"],"snippet":"… Widely available web crawls—most notably the ClueWebs† and the Common Crawl‡—contain the (near-)duplicate documents that the crawler encountered during the crawling process. While the inclusion of near-duplicates enables many …","url":["https://arxiv.org/pdf/2111.10864"]}
{"year":"2021","title":"The Impact of Positional Encodings on Multilingual Compression","authors":["V Ravishankar, A Søgaard - arXiv preprint arXiv:2109.05388, 2021"],"snippet":"… The second corpus uses the English Wikipedia as the training split, and Common Crawl as validation. We present corpus statistics in Table 3. For each corpus, we learn and apply a BPE vocabulary of size 2048. Train Validation Bible 30602 9080 Wikipedia 50000 20000 …","url":["https://arxiv.org/pdf/2109.05388"]}
{"year":"2021","title":"The information retrieval anthology 2021: inaugural status report and challenges ahead","authors":["M Potthast, B Stein, M Hagen - ACM SIGIR Forum, 2021"],"snippet":"… Originally developed as a reproducible baseline search engine for the ClueWeb09, the ClueWeb12, and the Common Crawl, ChatNoir's distributed Elasticsearch backend also enables the inclusion of other kinds of indexes …","url":["https://dl.acm.org/doi/abs/10.1145/3476415.3476417"]}
{"year":"2021","title":"The Information Retrieval Anthology","authors":["M Potthast, S Günther, J Bevendorff, JP Bittner… - 2021"],"snippet":"… To allow users to easily search the IR Anthology, we provide a dedicated search index accessible via our search engine ChatNoir [6], a proven web search engine indexing around 5 billion web pages from the ClueWeb crawls and Common Crawl versions …","url":["https://webis.de/downloads/publications/papers/potthast_2021j.pdf"]}
{"year":"2021","title":"The IWSLT 2021 BUT Speech Translation Systems","authors":["HK Vydana, M Karafi'at, L Burget - arXiv preprint arXiv:2107.06155, 2021"],"snippet":"… Corpora #Sentences Audio Source text Target Text MT -Train-set ParaCrawl v3 31M - OpenSubtitles 2018 12M - Rapid 2019 1.5M - Europarl v9 1.81M - News Commentary 365K - Common Crawl 2.4M - Wikititles …","url":["https://arxiv.org/pdf/2107.06155"]}
{"year":"2021","title":"The Language of Situational Empathy","authors":["KE ZHOU, LM AIELLO, S SCEPANOVIC, D QUERCIA… - 2021"],"snippet":"… Specifically, we averaged the 300-dimensional embedding vectors for each lexicon word using GloVe embeddings [55] trained on the Common Crawl corpus8. The above two approaches used a machine learning algorithm on top of our lexicon for the final prediction …","url":["https://social-dynamics.net/docs/empathy.pdf"]}
{"year":"2021","title":"The NiuTrans End-to-End Speech Translation System\\\\for IWSLT 2021 Offline Task","authors":["C Xu, X Liu, X Liu, L Wang, C Huang, T Xiao, J Zhu - arXiv preprint arXiv:2107.02444, 2021"],"snippet":"… Task Corpora Size Time ASR LibriSpeech 281241 960h Common Voice 1000000 1387h Total 1281241 2347h MT CommonCrawl 2014304 - Europarl 1802849 - ParaCrawl 31528317 - Wiki 5714363 - OpenSubtitles 14449099 - Total 55508932 - ST …","url":["https://arxiv.org/pdf/2107.02444"]}
{"year":"2021","title":"The NiuTrans Machine Translation Systems for WMT21","authors":["S Zhou, T Zhou, B Wei, Y Luo, Y Mu, Z Zhou, C Wang… - arXiv preprint arXiv …, 2021"],"snippet":"… For the ParaCrawl v7.1, we only selected 8.5 million data according to the score of sentences provided by the dataset. We chose all of News Crawl and News Commentary and 12 million data sampled from Common Crawl for the Japanese monolingual data …","url":["https://arxiv.org/pdf/2109.10485"]}
{"year":"2021","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling","authors":["L Gao, S Biderman, S Black, L Golding, T Hoppe… - arXiv preprint arXiv …, 2020"],"snippet":"… The growing need for data in language modeling has caused most existing large-scale language models to turn to the Common Crawl for most or all of their data (Brown et al., 2020; Raffel et al., 2019). While training on the …","url":["https://arxiv.org/pdf/2101.00027"]}
{"year":"2021","title":"The Potential of Automated Text Analytics in Social Knowledge Building","authors":["R Németh, J Koltai - Pathways Between Social Science and Computational …, 2021"],"snippet":"… Finally, big data are often samples themselves, resulting from an unknown or hard-to-formalize sampling procedure. See, eg, Common Crawl (http://commoncrawl.org), an open repository of textual web page data …","url":["https://link.springer.com/chapter/10.1007/978-3-030-54936-7_3"]}
{"year":"2021","title":"The Power of Anchor Text in the Neural Retrieval Era","authors":["M Fröbe, S Günther, M Probst, M Potthast, M Hagen"],"snippet":"… and Eiron and McCurley on MS MARCO, we extract anchor texts from Common Crawl snapshots. We randomly select one snapshot from each year from 2016 to 2021 (each containing 1.7–3.4 billion documents) and extract the anchor texts of …","url":["https://webis.de/downloads/publications/papers/froebe_2022a.pdf"]}
{"year":"2021","title":"The practical ethics of bias reduction in machine translation: why domain adaptation is better than data debiasing","authors":["M Tomalin, B Byrne, S Concannon, D Saunders… - Ethics and Information …, 2021"],"snippet":"This article probes the practical ethical implications of AI system design by reconsidering the important topic of bias in the datasets used to train auton.","url":["https://link.springer.com/article/10.1007/s10676-021-09583-1"]}
{"year":"2021","title":"The Rediscovery Hypothesis: Language Models Need to Meet Linguistics","authors":["T Maxat - 2022","V Nikoulina, M Tezekbayev, N Kozhakhmet… - arXiv preprint arXiv …, 2021"],"snippet":"… English-to-German translation. The authors used the CommonCrawl-840B GloVe model (Pennington et al., 2014) for English word vectors, which was completely fixed during pretraining, and we follow their setup. This entails that …","url":["https://arxiv.org/pdf/2103.01819","https://nur.nu.edu.kz/bitstream/handle/123456789/6141/Tezekbayev,%20Maxat.pdf?sequence=1"]}
{"year":"2021","title":"The SPECTRANS System Description for the WMT21 Terminology Task","authors":["NBDCB Faye, ZYKHM Mojca, PJBYG Wisniewski…"],"snippet":"… The latter did better for sacreBLEU and Window Overlap Accuracy (n=3) but probably having seen the terminological resources in the training data gave an edge for Exact-Match Accuracy to our model trained with Common Crawl and the terminological …","url":["http://www.statmt.org/wmt21/pdf/2021.wmt-1.80.pdf"]}
{"year":"2021","title":"The Trade-offs of Domain Adaptation for Neural Language Models","authors":["D Iter, D Grangier - arXiv preprint arXiv:2109.10274, 2021"],"snippet":"Page 1. The Trade-offs of Domain Adaptation for Neural Language Models David Grangier Google, Mountain View, CA grangier@google.com Dan Iter∗ Stanford, Palo Alto, CA daniter@stanford.edu Abstract This work connects …","url":["https://arxiv.org/pdf/2109.10274"]}
{"year":"2021","title":"The University of Edinburgh's Bengali-Hindi Submissions to the WMT21 News Translation Task","authors":["P Pal, AF Aji, P Chen, S Sen"],"snippet":"Abstract We describe the University of Edinburgh’s Bengali↔ Hindi constrained systems submitted to the WMT21 News Translation task. We submitted ensembles of Transformer models built with large-scale back-translation and finetuned on subsets …","url":["https://www.statmt.org/wmt21/pdf/2021.wmt-1.16.pdf"]}
{"year":"2021","title":"The University of Edinburgh's English-German and English-Hausa Submissions to the WMT21 News Translation Task","authors":["PCJHU Germann, LBN Bogoychev…"],"snippet":"… Page 2. • Crawled parallel: ParaCrawl, WikiMatrix, CommonCrawl, and WikiTitles … For back-translation, we use 9.5 million monolingual Hausa sentences from Common Crawl, Extended Common Crawl, and News Crawl provided by the task organizers …","url":["https://pinzhenchen.github.io/paper/2021-wmt-deen.pdf"]}
{"year":"2021","title":"The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021","authors":["D Liu, M Du, X Li, Y Hu, L Dai - arXiv preprint arXiv:2107.00279, 2021"],"snippet":"… sentences from it. Corpus Sentences EN→DE MuST-C(v2) 229.7k Europarl 1828.5k Rapid-2019 1531.3k WIT3-TED 209.5k Commoncrawl 2399.1k WikiMatrix 6227.2k Wikititles 1382.6k Paracrawl 82638.2k EN→JA WIT3-TED 225.0 …","url":["https://arxiv.org/pdf/2107.00279"]}
{"year":"2021","title":"The USYD-JD Speech Translation System for IWSLT 2021","authors":["L Ding, D Wu, D Tao - arXiv preprint arXiv:2107.11572, 2021"],"snippet":"… Corpus #Sent. commoncrawl English 4,366,344 commoncrawl Swahili 38,928 + upsampling (14×) 544,992 … Mono. Corpus for Tagged BT #Sent. Totally collected corpus commoncrawl English 30,513,498 Cleaned corpus …","url":["https://arxiv.org/pdf/2107.11572"]}
{"year":"2021","title":"The Utility and Interplay of Gazetteers and Entity Segmentation for Named Entity Recognition in English","authors":["O Agarwal, A Nenkova"],"snippet":"… We use two input word representations: the 300-d GloVe vectors trained on Common Crawl (Pennington et al., 2014), which is the dominant representation in NER, and the 1024-d contextual ELMo (Peters et al …","url":["https://aclanthology.org/2021.findings-acl.349.pdf"]}
{"year":"2021","title":"The Volctrans Neural Speech Translation System for IWSLT 2021","authors":["C Zhao, Z Liu, J Tong, T Wang, M Wang, R Ye, Q Dong… - arXiv preprint arXiv …, 2021"],"snippet":"… Data Combination and Sampling Strategy We train transformer models with different combina5http://www.statmt.org/wmt20/ translation-task.html, including Common Crawl, tions of data sets because increasing the model's diversity can benefit the model ensemble …","url":["https://arxiv.org/pdf/2105.07319"]}
{"year":"2021","title":"The Web Is Your Oyster--Knowledge-Intensive NLP against a Very Large Web Corpus","authors":["A Piktus, F Petroni, V Karpukhin, D Okhonko… - arXiv preprint arXiv …, 2021"],"snippet":"… CCNet processes Common Crawl by performing deduplication, language identification and quality filtering (articles are split into three quality tiers: head, … We pick the CCNet snapshot corresponding to the August 2019 Common Crawl …","url":["https://arxiv.org/pdf/2112.09924"]}
{"year":"2021","title":"TheEyeCorpus: Experiments in Reducing NLP Bias and Identifiability for Large LMs","authors":["J Herrera, D Bernal"],"snippet":"… 2020), a state of the art large language model, that used terabytes of the mostly unfiltered Common Crawl. Making corpuses is usually as simple as downloading and extracting the common crawl database. This isn’t the most ideal approach …","url":["https://openreview.net/pdf?id=CxMuhSuXIW"]}
{"year":"2021","title":"Thor at Touché 2021: Argument Retrieval for Comparative Questions","authors":["E Shirshakova, A Wattar - Working Notes of CLEF, 2021"],"snippet":"… doi:10.1145/2348283.2348429. [4] J. Bevendorff, B. Stein, M. Hagen, M. Potthast, Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl, in: L. Azzopardi, A. Hanbury, G. Pasi, B. Piwowarski (Eds.), Advances in Information Retrieval …","url":["http://ceur-ws.org/Vol-2936/paper-219.pdf"]}
{"year":"2021","title":"Three Essays on the Role of Unstructured Data in Marketing Research","authors":["ISK Chakraborty - 2021"],"snippet":"Page 1. Abstract Three Essays on the Role of Unstructured Data in Marketing Research Ishita Sunity Kumar Chakraborty 2021 This thesis studies the use of firm and user-generated unstructured data (eg, text and videos) for …","url":["https://search.proquest.com/openview/c265768db80d44b3e2c4fef8b9b0df95/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2021","title":"Time and Cost Prediction Models for Language Classification Over a Large Corpus on Spark","authors":["JB Rodrigues, GC Vasconcelos, PRM Maciel - 2020 IEEE Symposium Series on …, 2020"],"snippet":"… All nodes were managed by a Debian GNU/Linux 9 OS, Apache Spark 2.3, and Apache Hadoop 2.9. 2 The Common Crawl Foundation - http://commoncrawl.org/ 1) Analysis of Variance: Fig. 1 displays the results over the log-transformed data …","url":["https://ieeexplore.ieee.org/abstract/document/9308299/"]}
{"year":"2021","title":"TMEKU System for the WAT2021 Multimodal Translation Task","authors":["Y Zhao, M Komachi, T Kajiwara, C Chu - red"],"snippet":"… Subsequently, we utilize fastText (Bojanowski et al., 2017) to learn subword embeddings. We use a pre-trained model4 containing two million word vectors trained with subword information on Common Crawl (600B tokens) …","url":["https://aclanthology.org/2021.wat-1.20.pdf"]}
{"year":"2021","title":"Token-Level Multilingual Epidemic Dataset for Event Extraction","authors":["S Mutuvi, E Boros, A Doucet, G Lejeune, A Jatowt… - International Conference on …, 2021"],"snippet":"… 8. XLM-RoBERTa-base was trained on 2.5TB of CommonCrawl data in 100 languages. References. 1. Aiello, AE, Renson, A., Zivich, PN: Social media-and internet-based disease surveillance for public health. Annu. Rev. Public …","url":["https://link.springer.com/chapter/10.1007/978-3-030-86324-1_6"]}
{"year":"2021","title":"Touché Task 2: Comparative Argument Retrieval. A Document-based Search Engine for Answering Comparative Questions","authors":["D Helmrich, D Streitmatter, F Fuchs, M Heykeroth - Working Notes of CLEF, 2021"],"snippet":"… eg faster). The results are sentences from the Common Crawl2 retrieved via ElasticSearch [7] and then reduced to comparative sentences (either by certain keywords or a machine learning approach) and ranked. The system …","url":["http://ceur-ws.org/Vol-2936/paper-214.pdf"]}
{"year":"2021","title":"Toward Gender-Inclusive Coreference Resolution: An Analysis of Gender and Bias throughout the Machine Learning Lifecyle","authors":["YT Cao, H Daumé - Computational Linguistics, 2021"],"snippet":"Page 1. Toward Gender-Inclusive Coreference Resolution An Analysis of Gender and Bias throughout the Machine Learning Lifecyle∗ Yang Trista Cao∗∗ University of Maryland Hal Daumé III† Microsoft Research University of Maryland …","url":["https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli_a_00413/1928379/coli_a_00413.pdf"]}
{"year":"2021","title":"Toward the Understanding of Deep Text Matching Models for Information Retrieval","authors":["L Chen, Y Lan, L Pang, J Guo, X Cheng - arXiv preprint arXiv:2108.07081, 2021"],"snippet":"… For the input word embeddings, we initialize the embedding layer with the 300-dimensional Glove[29] word vectors pre-trained in the 840B Common Crawl corpus4. For the out-of … 3https://github.com/NTMC-Community …","url":["https://arxiv.org/pdf/2108.07081"]}
{"year":"2021","title":"Towards a Machine Learning Based Generalizable Framework for Detecting COVID-19 Misinformation on Social Media","authors":["Y Chen - 2021"],"snippet":"Page 1. University of Nebraska - Lincoln DigitalCommons@University of Nebraska - Lincoln …","url":["https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1232&context=computerscidiss"]}
{"year":"2021","title":"TOWARDS A ROMANIAN END-TO-END AUTOMATIC SPEECH RECOGNITION BASED ON DEEPSPEECH2","authors":["AM AVRAM, P Vasile, D Tufiș"],"snippet":"… [17]) layers that have 768 units. The output of the LSTM layers is 7 https://commoncrawl org/ 8 We used langdetect to detect Romanian sentences: https://github.com/Mimino666/ langdetect. Page 4. Andrei-Marius AVRAM, Vasile PĂIȘ, Dan TUFIȘ 398 …","url":["https://academiaromana.ro/sectii2002/proceedings/doc2020-4/11-Avram_Tufis.pdf"]}
{"year":"2021","title":"Towards better subtitles: A multilingual approach for punctuation restoration of speech transcripts","authors":["NM Guerreiro, R Rei, F Batista - Expert Systems with Applications, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0957417421011180"]}
{"year":"2021","title":"Towards Change Detection in Privacy Policies with Natural Language Processing","authors":["A Adhikari, R Dewri - 2021 18th International Conference on Privacy …, 2021"],"snippet":"Privacy policies notify users about the privacy practices of websites, mobile apps, and other products and services. However, users rarely read them and struggle to understand their contents. Due to the complicated nature of these documents, it gets …","url":["https://ieeexplore.ieee.org/abstract/document/9647767/"]}
{"year":"2021","title":"Towards Continual Knowledge Learning of Language Models","authors":["J Jang, S Ye, S Yang, J Shin, J Han, G Kim, SJ Choi… - arXiv preprint arXiv …, 2021"],"snippet":"… 3CC-RECENTNEWS consists of 221,779 articles (∼168M tokens), which is estimated to be about 750 times smaller than C4, a cleansed version of the April 2019 Common Crawl dataset (https://commoncrawl.org/) that was …","url":["https://arxiv.org/pdf/2110.03215"]}
{"year":"2021","title":"Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution","authors":["X Garcia, N Constant, AP Parikh, O Firat - arXiv preprint arXiv:2103.06799, 2021"],"snippet":"Page 1. Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution Xavier Garcia and Noah Constant and Ankur P. Parikh and Orhan Firat Google Research Mountain View California …","url":["https://arxiv.org/pdf/2103.06799"]}
{"year":"2021","title":"Towards Controllable Neural Generation of Arguments","authors":["X Hua - 2021"],"snippet":"Page 1. Page 2. TOWARDS CONTROLLABLE NEURAL GENERATION OF ARGUMENTS A Dissertation Presented to The Khoury College of Computer Sciences of Northeastern University in Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy …","url":["https://search.proquest.com/openview/b002097758dccea01c78e5461afe631f/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2021","title":"Towards Generalization in Dialog through Inductive Biases","authors":["S Mehri - 2021"],"snippet":"Abstract Generalization is imperative in dialog research. Data-driven models have been shown to be capable of performing specific tasks in constrained contexts, given ample data. However, the complexity of human communication necessitates …","url":["http://shikib.com/proposal.pdf"]}
{"year":"2021","title":"Towards Learning a Joint Representation from Transformer in Multimodal Emotion Recognition","authors":["JJ Deng, CHC Leung - International Conference on Brain Informatics, 2021"],"snippet":"… modality of speech audio. As for text representation, we adopt fine-tuned Text-To-Text Transfer Transformer (T5) [18] model trained by a common crawl (C4) dataset to extract text embeddings. Operations of transfer learning …","url":["https://link.springer.com/chapter/10.1007/978-3-030-86993-9_17"]}
{"year":"2021","title":"Towards Learning Language Agnostic Features for NLP in Low-resource Languages","authors":["AJ Antony - 2020"],"snippet":"Page 1. Towards Learning Language Agnostic Features for NLP in Low-resource Languages Thesis submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science and …","url":["http://web2py.iiit.ac.in/research_centres/publications/download/mastersthesis.pdf.bfe7226242875ee5.5468657369735f416c6c656e2e706466.pdf"]}
{"year":"2021","title":"Towards Learning Terminological Concept Systems from Multilingual Natural Language Text","authors":["L Wachowiak, C Lang, B Heinisch, D Gromann - 3rd Conference on Language, Data …, 2021"],"snippet":"… [6]. We further added to the data by manually annotating around 200 acronym-term pairs from ACTER with relations other than synonymy and another 271 sample sentences from the Common Crawl News Corpus5 … 5 …","url":["https://drops.dagstuhl.de/opus/volltexte/2021/14558/pdf/OASIcs-LDK-2021-22.pdf"]}
{"year":"2021","title":"Towards Making the Most of Dialogue Characteristics for Neural Chat Translation","authors":["Y Liang, C Zhou, F Meng, J Xu, Y Chen, J Su, J Zhou - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. Towards Making the Most of Dialogue Characteristics for Neural Chat Translation Yunlong Liang1∗, Chulun Zhou2∗, Fandong Meng3, Jinan Xu1†, Yufeng Chen1, Jinsong Su2 and Jie Zhou3 1Beijing Key Lab of Traffic …","url":["https://arxiv.org/pdf/2109.00668"]}
{"year":"2021","title":"Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation","authors":["G Chen, S Ma, Y Chen, D Zhang, J Pan, W Wang… - arXiv preprint arXiv …, 2021"],"snippet":"… 2018, SPM) learned on the full Common Crawl data that includes 250k subword tokens. We do not apply additional preprocessing, such as truecasing or normalizing punctuation/characters. Following XLMR, we add the [BOS] and [EOS] tokens at the …","url":["https://arxiv.org/pdf/2110.08547"]}
{"year":"2021","title":"Towards Model Understanding","authors":["D Pruthi - 2021"],"snippet":"While deep learning models have become increasingly accurate over the last decade, concerns about their (lack of) interpretability have taken a center stage. In response, a growing sub-field on interpretability and analysis of these models has …","url":["https://www.cs.cmu.edu/~ddanish/files/thesis.pdf"]}
{"year":"2021","title":"Towards More Effective and Economic Sparsely-Activated Model","authors":["H Jiang, K Zhan, J Qu, Y Wu, Z Fei, X Zhang, L Chen… - arXiv preprint arXiv …, 2021"],"snippet":"… RealNews is a large scale dataset about news articles from Common Crawl OpenWebText dataset contains a large amount of text data from social media platform Reddit. They scraped all outbound links from Reddit as data sources and …","url":["https://arxiv.org/pdf/2110.07431"]}
{"year":"2021","title":"Towards Multi-modal Entity Resolution for Product Matching","authors":["M Wilke, E Rahm"],"snippet":"… Finally a procedure to query the internet archive2 for missing images is used. Due to its slow speed this method is only applied selectively to achieve higher coverage of images for the experiments. 1http://commoncrawl.org/ 2https://archive.org/ Page 3 …","url":["https://dbs.uni-leipzig.de/file/GvDB21_multi_modal_product_matching.pdf"]}
{"year":"2021","title":"Towards Reinforcement Learning for Pivot-based Neural Machine Translation with Non-autoregressive Transformer","authors":["E Tokarchuk, J Rosendahl, W Wang, P Petrushkov… - arXiv preprint arXiv …, 2021"],"snippet":"Page 1. Towards Reinforcement Learning for Pivot-based Neural Machine Translation with Non-autoregressive Transformer Evgeniia Tokarchuk 1 2 Jan Rosendahl 2 Weiyue Wang 2 Pavel Petrushkov 1 Tomer Lancewicki 1 Shahram Khadivi 1 Hermann Ney 2 …","url":["https://arxiv.org/pdf/2109.13097"]}
{"year":"2021","title":"Towards Selecting Informative Content for Cyber Threat Intelligence","authors":["P Panagiotou, C Iliou, K Apostolou, T Tsikrika… - 2021 IEEE International …, 2021"],"snippet":"… NER model. For their implementation, we used the Delft tool3, and the Glove Common Crawl embeddings4. The Bi- directional LSTM-CNNs-CRF architecture of [29] performs the best according to all metrics considered. We …","url":["https://ieeexplore.ieee.org/abstract/document/9527909/"]}
{"year":"2021","title":"Towards Target-dependent Sentiment Classification in News Articles","authors":["F Hamborg, K Donnay, B Gipp"],"snippet":"… category-dataset Page 10. 10 F. Hamborg et al. 23. Nagel, S.: Common Crawl: News Crawl (2016), https://web. archive.org/web/20191118111519/https://commoncrawl org/2016/10/ news-dataset-available/ 24. Nakov, P., Ritter …","url":["https://www.gipp.com/wp-content/papercite-data/pdf/hamborg2021.pdf"]}
{"year":"2021","title":"Towards the smart use of embedding and instance features for property matching","authors":["D Ayala, I Hernández, D Ruiz, E Rahm - 2021 IEEE 37th International Conference on …, 2021"],"snippet":"… To compute embeddings, we use the pre-trained GloVe approach [25]1, specifically for the uncased Common Crawl corpus that includes 300-dimensional vectors for 1.9 million words. Unknown words are mapped to a vector filled with zeroes …","url":["https://ieeexplore.ieee.org/abstract/document/9458651/"]}
{"year":"2021","title":"Towards zero-shot cross-lingual named entity disambiguation","authors":["A Barrena Madinabeitia, A Soroa Echave… - 2021","A Barrena, A Soroa, E Agirre - Expert Systems with Applications, 2021"],"snippet":"[EN]In cross-Lingual Named Entity Disambiguation (XNED) the task is to link Named Entity mentions in text in some native language to English entities in a knowledge graph. XNED systems usually require training data for each native language, limiting …","url":["https://addi.ehu.es/bitstream/handle/10810/54482/1-s2.0-S0957417421009490-main.pdf?sequence=1&isAllowed=y","https://www.sciencedirect.com/science/article/pii/S0957417421009490"]}
{"year":"2021","title":"Traceability recovery between bug reports and test cases-a Mozilla Firefox case study","authors":["G Gadelha, F Ramalho, T Massoni - Automated Software Engineering, 2021"],"snippet":"Automatic recovery of traceability between software artifacts may promote early detection of issues and better calculate change impact. Information Retriev.","url":["https://link.springer.com/article/10.1007/s10515-021-00287-w"]}
{"year":"2021","title":"Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction","authors":["A Barbaresi - 2021"],"snippet":"… Data collection approaches using the CommonCrawl1 have flourished as they allow for faster download and processing by skipping (or more precisely outsourcing) the crawling phase … 1https://commoncrawl.org Page 2. 123 …","url":["https://aclanthology.org/2021.acl-demo.15.pdf"]}
{"year":"2021","title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation","authors":["O Press, NA Smith, M Lewis - arXiv preprint arXiv:2108.12409, 2021"],"snippet":"Page 1. TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION Ofir Press1,2 Noah A. Smith1,3 Mike Lewis2 1Paul G. Allen School of Computer Science & Engineering …","url":["https://arxiv.org/pdf/2108.12409"]}
{"year":"2021","title":"Training Domain Specific Multilingually Aligned Word Embeddings","authors":["X Wang, C Fan, M Frantzen - 2021"],"snippet":"… FastText The pre-trained Fasttext embeddings follow the training procedure proposed in [5]. They use a mixture of Wikipedia and CommonCrawl as training data and learn 300 dimensional embeddings with a CBOW model defined …","url":["https://xinpeng-wang.github.io/pdfs/nlp_final_report.pdf"]}
{"year":"2021","title":"Training ELECTRA Augmented with Multi-word Selection","authors":["J Shen, J Liu, T Liu, C Yu, J Han - arXiv preprint arXiv:2106.00139, 2021"],"snippet":"Page 1. Training ELECTRA Augmented with Multi-word Selection Jiaming Shen∇⋆, Jialu Liu O , Tianqi Liu O , Cong Yu O , Jiawei Han∇ ∇University of Illinois Urbana-Champaign, IL, USA, 3 Google Research, NY, USA ∇{js2 …","url":["https://arxiv.org/pdf/2106.00139"]}
{"year":"2021","title":"Transfer Learning for Automated Responses to the BDI Questionnaire","authors":["C Spartalis, G Drosatos, A Arampatzis - Working Notes of CLEF, 2021"],"snippet":"… batches, and over more data. More specifically, its pretraining corpus also includes CC-News (portion of the CommonCrawl News dataset [44]), OpenWebText [45], and Stories [46]). Furthermore, there are some modifications …","url":["http://ceur-ws.org/Vol-2936/paper-84.pdf"]}
{"year":"2021","title":"Transfer Learning for Sequence Generation: from Single-source to Multi-source","authors":["X Huang, J Xu, M Sun, Y Liu - arXiv preprint arXiv:2105.14809, 2021"],"snippet":"Page 1. Transfer Learning for Sequence Generation: from Single-source to Multi-source Xuancheng Huang1, Jingfang Xu4, Maosong Sun1,3, and Yang Liu1,2,3∗ 1Dept. of Comp. Sci. & Tech., BNRist Center, Institute for AI, Tsinghua …","url":["https://arxiv.org/pdf/2105.14809"]}
{"year":"2021","title":"Transfer Learning Model for Disrupting Misinformation During a COVID-19 Pandemic","authors":["R Raju, S Bhandari, SA Mohamud, EN Ceesay - 2021 IEEE 11th Annual Computing …, 2021"],"snippet":"… 19–27, IEEE Computer Society, 2015. [30] S. Nagel, “Cc-news.” https://commoncrawl.org/ 2016/10/news-datasetavailable/, 2016. [31] A. Gokaslan and V. Cohen, “Openwebtext corpus.” https://skylion007.github.io/OpenWebTextCorpus/, 2016 …","url":["https://ieeexplore.ieee.org/abstract/document/9376066/"]}
{"year":"2021","title":"Transfer Learning Techniques for Sequence Labeling in Network File System Specifications","authors":["A Singh - 2021"],"snippet":"… English Wikipedia and BookCorpus (Zhu et al., 2015), and RoBERTa (Liu et al., 2019) is pre-trained on Common-Crawl News (Nagel, 2016), OpenWebText (Radford et al., 2019), and Stories (Trinh and Le, 2019) …","url":["https://search.proquest.com/openview/754b6463cd54e1b58537a7d6558d4825/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2021","title":"Transformer-Based Automatic Punctuation Prediction and Word Casing Reconstruction of the ASR Output","authors":["J Švec, J Lehečka, L Šmídl, P Ircing - International Conference on Text, Speech, and …, 2021"],"snippet":"… For experiments with English data, we used the Google's T5-base English model (version from October 21th, 2019) 3 trained from Common Crawl data 4 . We replicated the same pre-processing procedure to obtain the …","url":["https://link.springer.com/chapter/10.1007/978-3-030-83527-9_7"]}
{"year":"2021","title":"Transformer-based identification of stochastic information cascades in social networks using text and image similarity","authors":["P Kasnesis, R Heartfield, X Liang, L Toumanidis… - Applied Soft Computing, 2021"],"snippet":"… An advantage of RoBERTa against BERT for text-based information cascade identification is its pretrained architecture which benefits from a more diverse range of datasets (larger corpus). For example, its training corpus includes the CommonCrawl News dataset.","url":["https://www.sciencedirect.com/science/article/pii/S1568494621003367"]}
{"year":"2021","title":"TRANSFORMERS AND THEIR APPLICATIONS IN NATURAL LANGUAGE PROCESSING","authors":["N Sarzhan"],"snippet":"Page 1. Page 2. E¨OTV¨OS LORÁND UNIVERSITY FACULTY OF SCIENCE TRANSFORMERS AND THEIR APPLICATIONS IN NATURAL LANGUAGE PROCESSING MASTER'S THESIS Nurzhan Sarzhan MSc mathematics …","url":["https://web.cs.elte.hu/blobs/diplomamunkak/msc_alkmat/2021/nurzhan_sarzhan.pdf"]}
{"year":"2021","title":"Transforming Term Extraction: Transformer-Based Approaches to Multilingual Term Extraction Across Domains","authors":["C Lang, L Wachowiak, B Heinisch, D Gromann"],"snippet":"… XLM-R (Conneau et al., 2020) is a multilingual variant of BERT, which was pretrained in 100 languages using 2.5 terabytes of Common Crawl data. Moreover, it makes use of the improved training routine introduced by RoBERTa (Liu et al., 2019) …","url":["https://aclanthology.org/2021.findings-acl.316.pdf"]}
{"year":"2021","title":"Translate and Classify: Improving Sequence Level Classification for English-Hindi Code-Mixed Data","authors":["D Gautam, K Gupta, M Shrivastava - Proceedings of the Fifth Workshop on …, 2021"],"snippet":"… auto-encoder. It has been pre-trained using the BART (Lewis et al., 2020) objective on large-scale monolingual corpora of 25 languages extracted from Common Crawl1 (Wenzek et al., 2020; Conneau et al., 2020). Both English …","url":["https://www.aclweb.org/anthology/2021.calcs-1.3.pdf"]}
{"year":"2021","title":"TrollsWithOpinion: A Dataset for Predicting Domain-specific Opinion Manipulation in Troll Memes","authors":["S Suryawanshi, BR Chakravarthi, M Arcan, S Little… - arXiv preprint arXiv …, 2021"],"snippet":"… 5.2 Deep Learning baselines Bag of words (BOW) : This unimodal baseline used text representation in the form of 300-dimensional pre-trained GloVe Pennington et al. (2014) (common crawl) word embeddings, which were fed to a classification layer …","url":["https://arxiv.org/pdf/2109.03571"]}
{"year":"2021","title":"TunBERT: Pretrained Contextualized Text Representation for Tunisian Dialect","authors":["A Messaoudi, A Cheikhrouhou, H Haddad, N Ferchichi… - arXiv preprint arXiv …, 2021"],"snippet":"… They also concluded that a model trained on a CommonCrawl-based corpus performed consistently better than the one trained on the French Wikipedia. They suggested that a 4 GB heterogeneous dataset in terms of genre and style is large enough as a …","url":["https://arxiv.org/pdf/2111.13138"]}
{"year":"2021","title":"Turkic Interlingua: A Case Study of Machine Translation in Low-resource Languages","authors":["J Mirzakhalov - 2021"],"snippet":"… production. Table 4 shows results from the CCAligned dataset, which is a parallel dataset compiled from 68 Common Crawl snapshots … shown in Table 6. Data for OSCAR is also a compilation of Common Crawl snapshots and follows a similar filtering process …","url":["https://scholarcommons.usf.edu/cgi/viewcontent.cgi?article=10026&context=etd"]}
{"year":"2021","title":"Twin Question Pair Classification","authors":["A Sharma, SS Jha, S Arora, S Garg, S Tayal - Smart and Sustainable Intelligent …, 2021"],"snippet":"… 4 Embedding Embedding maps words in a text document to number space (these number-space representations are called word vectors). We have to use Fast text embedding [6]. The Fast text model for English is pre-trained …","url":["http://books.google.de/books?hl=en&lr=lang_en&id=7fEhEAAAQBAJ&oi=fnd&pg=PA215&dq=commoncrawl&ots=oxs_QNhHWM&sig=OazT1mzKhUG_fOXx1LjGsEMtRKI"]}
{"year":"2021","title":"Twitter Dataset and Evaluation of Transformers for Turkish Sentiment Analysis","authors":["A Köksal, A Özgür - 2021 29th Signal Processing and Communications …, 2021"],"snippet":"… Kelime Büyüklü˘gü 128,000 119,547 250,002 Kullanılan Diller Türkçe Türkçe + 103 dil Türkçe + 99 dil E˘gitim Derlemi Oscar + Wikipedia + Opus Wikipedia CommonCrawl vektör uzunlu˘gu, dikkat ve gizli katmanların sayısı bu üç mo- delde de aynıdır …","url":["https://ieeexplore.ieee.org/abstract/document/9477814/"]}
{"year":"2021","title":"Two Parents, One Child: Dual Transfer for Low-Resource Neural Machine Translation","authors":["M Zhang, L Li, Q Liu"],"snippet":"Page 1. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2726–2738 August 1–6, 2021. ©2021 Association for Computational Linguistics 2726 Two Parents, One Child: Dual Transfer for Low-Resource Neural Machine Translation …","url":["https://aclanthology.org/2021.findings-acl.241.pdf"]}
{"year":"2021","title":"Two-stage Visual Cues Enhancement Network for Referring Image Segmentation","authors":["Y Jiao, Z Jie, W Luo, J Chen, YG Jiang, X Wei, L Ma - arXiv preprint arXiv:2110.04435, 2021"],"snippet":"… As for language processing, GloVe [27] word embeddings pretrained on Common Crawl 840B tokens are firstly used for constructing word embedding look-up table. Then a LSTM takes GloVe word embeddings as input for capturing context information …","url":["https://arxiv.org/pdf/2110.04435"]}
{"year":"2021","title":"Type-and Token-based Word Embeddings in the Digital Humanities","authors":["A Ehrmanntraut, T Hagen, L Konle, F Jannidis - Proceedings http://ceur-ws. org ISSN, 2021"],"snippet":"Abstract In the general perception of the NLP community, the new dynamic, context-sensitive, token-based embeddings from language models like BERT have replaced the older static, type-based embeddings like word2vec or fastText, due to their better …","url":["http://ceur-ws.org/Vol-2989/long_paper35.pdf"]}
{"year":"2021","title":"UGTO: Uncommon Words and Proper Nouns","authors":["X Zhong, E Cambria - Time Expression and Named Entity Recognition, 2021"],"snippet":"… they are (1) word2vec, which is trained on the Google News dataset [13], (2) GloVe, which is trained on the Wikipedia 2014 and Gigaword 5 corpora [20], and (3) FastText, which is trained on the Wikipedia 2017, UMBC corpus …","url":["https://link.springer.com/chapter/10.1007/978-3-030-78961-9_6"]}
{"year":"2021","title":"Uncovering the Limits of Text-based Emotion Detection","authors":["N Alvarez-Gonzalez, A Kaltenbrunner, V Gómez - arXiv preprint arXiv:2109.01900, 2021"],"snippet":"Page 1. Uncovering the Limits of Text-based Emotion Detection Nurudin Alvarez-Gonzalez ∗, Andreas Kaltenbrunner, Vicenç Gómez nuralgon@gmail.com, andreas.kaltenbrunner@ upf.edu, vicen.gomez@upf.edu Universitat Pompeu Fabra. Barcelona, Spain. Abstract …","url":["https://arxiv.org/pdf/2109.01900"]}
{"year":"2021","title":"Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model","authors":["KC Fraser, I Nejadgholi, S Kiritchenko"],"snippet":"… trained word em- beddings in the gensim library ( ˇRehurek and Sojka, 2010), with the results given in Table 2. The FastText embeddings generally outperform the other embeddings on this task, with the 2M word model trained …","url":["https://www.svkir.com/papers/Fraser-et-al-SCM-ACL2021.pdf"]}
{"year":"2021","title":"Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making","authors":["H Liu, V Lai, C Tan - arXiv preprint arXiv:2101.05303, 2021"],"snippet":"… Race. There are 7,214 defendants in this dataset. • BIOS [9]. This dataset contains hundreds of thousands of online biographies from the Common Crawl corpus. The task is to predict a person's profession given a biography …","url":["https://arxiv.org/pdf/2101.05303"]}
{"year":"2021","title":"Ungoliant: An optimized pipeline for the generation of a very large-scale multilingual web corpus","authors":["J Abadji, PJ Ortiz Suárez, L Romary, B Sagot - … of the Workshop on Challenges in the …, 2021"],"snippet":"… The most recent Common Crawl dump contains 64,000 shards. Each shard is composed of numerous records, and each record holds textual content along with metadata. While CommonCrawl shards hold document-level …","url":["https://ids-pub.bsz-bw.de/files/10468/Abadji_Suarez_Romary_Ungoliant_2021.pdf"]}
{"year":"2021","title":"Unified Open-Domain Question Answering with Structured and Unstructured Knowledge","authors":["B Oguz, X Chen, V Karpukhin, S Peshterliev… - arXiv preprint arXiv …, 2020"],"snippet":"… KBQA is ultimately limited in its coverage of facts and the types of questions it can answer. On the other hand, large collections of text such as Wikipedia or CommonCrawl promise to be a richer source of knowledge for truly open do- main question answering systems …","url":["https://arxiv.org/pdf/2012.14610"]}
{"year":"2021","title":"Universal Sentence Representation Learning with Conditional Masked Language Model","authors":["Z Yang, Y Yang, D Cer, J Law, E Darve - arXiv preprint arXiv:2012.14388, 2020"],"snippet":"… For training English sentence encoders with CMLM, we use three Common Crawl dumps … Similarly, we also train QuickThought, a competitive unsupervised sentence representations learning model, on the same …","url":["https://arxiv.org/pdf/2012.14388"]}
{"year":"2021","title":"University of Copenhagen Participation in TREC Health Misinformation Track 2020","authors":["LC Lima, DB Wright, I Augenstein, M Maistro - arXiv preprint arXiv:2103.02462, 2021"],"snippet":"… The corpus of the Total Recall and Ad Hoc Tasks consits of news articles in the CommonCrawl News crawl, sampled from January, 1st 2020 … Since there are no credibility assessments for the Common Crawl News sample …","url":["https://arxiv.org/pdf/2103.02462"]}
{"year":"2021","title":"University of Regensburg@ PAN: Profiling Hate Speech Spreaders on Twitter","authors":["KO Akomeah, U Kruschwitz, B Ludwig - CLEF, 2021"],"snippet":"… This preprocessor is a companion to the BERT models for preprocsssing plain text inputs into the input format expected by BERT. The model uses a vocabulary for multilingual models extracted from Wikipedia, CommonCrawl, and translation pairs from the Web …","url":["http://ceur-ws.org/Vol-2936/paper-184.pdf"]}
{"year":"2021","title":"Unraveling Social Perceptions & Behaviors towards Migrants on Twitter","authors":["A Khatua, W Nejdl - arXiv preprint arXiv:2112.06642, 2021"],"snippet":"We draw insights from the social psychology literature to identify two facets of Twitter deliberations about migrants, ie, perceptions about migrants and behaviors towards mi-grants. Our theoretical anchoring helped us in identifying two prevailing …","url":["https://arxiv.org/pdf/2112.06642"]}
{"year":"2021","title":"Unsupervised Clustering of Structured and Unstructured Text Collections","authors":["L Bungum - 2021"],"snippet":"Page 1. Lars B u n g u m Doctoral theses at NTNU, 202 1 :179 ISBN 978-82-326-5390-4 (printed ver.) ISBN 978-82-326-6966-0 (electronic ver.) ISSN 1503-8181 (printed ver.) ISSN 2703-8084 (electronic ver.) Do cto ral …","url":["https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/2759145/Lars%20Bungum_PhD.pdf?sequence=1"]}
{"year":"2021","title":"Unsupervised Cross-Domain and Cross-Lingual Methods for Text Classification, Slot-Filling, and Question-Answering","authors":["J Krishnan - 2021"],"snippet":"Page 1. UNSUPERVISED CROSS-DOMAIN AND CROSS-LINGUAL METHODS FOR TEXT CLASSIFICATION, SLOT-FILLING, AND QUESTION-ANSWERING by Jitin Krishnan A Dissertation Submitted to the Graduate Faculty …","url":["https://search.proquest.com/openview/5ef2690ad3bc61870ee72a9048d3a4f9/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2021","title":"Unsupervised Neural Machine Translation with Generative Language Models Only","authors":["JM Han, I Babuschkin, H Edwards, A Neelakantan… - arXiv preprint arXiv …, 2021"],"snippet":"We show how to derive state-of-the-art unsupervised neural machine translation systems from generatively pre-trained language models. Our method consists of three steps: few-shot amplification, distillation, and backtranslation. We first use the …","url":["https://arxiv.org/pdf/2110.05448"]}
{"year":"2021","title":"Uppsala NLP at SemEval-2021 Task 2: Multilingual Language Models for Fine-tuning and Feature Extraction in Word-in-Context Disambiguation","authors":["H You, X Zhu, S Stymne - arXiv preprint arXiv:2104.03767, 2021"],"snippet":"… 3 Multilingual Language Models 3.1 XLMR XLMR (XLM-RoBERTa) is a scaled cross-lingual sentence encoder (Conneau et al., 2020), which is trained on 2.5T of data obtained from Common Crawl that covers more than 100 languages …","url":["https://arxiv.org/pdf/2104.03767"]}
{"year":"2021","title":"UPV at TREC Health Misinformation Track 2021 Ranking with SBERT and Quality Estimators","authors":["IB Schlicht, AFM de Paula, P Rosso - arXiv preprint arXiv:2112.06080, 2021"],"snippet":"Health misinformation on search engines is a significant problem that could negatively affect individuals or public health. To mitigate the problem, TREC organizes a health misinformation track. This paper presents our submissions to this …","url":["https://arxiv.org/pdf/2112.06080"]}
{"year":"2021","title":"UQuAD1. 0: Development of an Urdu Question Answering Training Data for Machine Reading Comprehension","authors":["S Kazi, S Khoja - arXiv preprint arXiv:2111.01543, 2021"],"snippet":"… The Third model used is XLM-Roberta, a multi-lingual pre-trained transformer model from common Crawl trained on 100 languages, including Urdu. XLM-Roberta outperformed previous multi-lingual models such as mBERT and XLM on a variety of …","url":["https://arxiv.org/pdf/2111.01543"]}
{"year":"2021","title":"User Identification in Online Social Networks using Graph Transformer Networks","authors":["KNP Kumar, ML Gavrilova - 2021 18th International Conference on Privacy …, 2021"],"snippet":"… We used the model with 2 million word vectors trained with subword information on Common Crawl (600B tokens). (v) Skip-thought Vectors (StV) [30]: A Gated Recurrent Unit (GRU) encoder trained to predict adjacent sentences in a books …","url":["https://ieeexplore.ieee.org/abstract/document/9647749/"]}
{"year":"2021","title":"Using Embedding-Based Similarities to Improve Lexical Resources","authors":["NV Loukachevitch, MM Tikhomirov, EA Parkhomenko - Lobachevskii Journal of …, 2021"],"snippet":"In this paper we discuss the usefulness of applying semi-automatic checking procedures to existing thesauri for natural language processing—large man.","url":["https://link.springer.com/article/10.1134/S1995080221070167"]}
{"year":"2021","title":"Using mixed methods to study the historical use of web beacons in web tracking","authors":["J Nielsen - International Journal of Digital Humanities, 2021"],"snippet":"… pixel. The approach is inspired by the study of web beacons by Ruohonen and Leppänen (2018) and Schelter and Kunegis' study of third-party embeddings in the Common Crawl 2012 corpus (Schelter & Kunegis, 2016, 2018) …","url":["https://link.springer.com/article/10.1007/s42803-021-00033-4"]}
{"year":"2021","title":"USING SEMANTIC RELATIONSHIPS TO ENHANCE NEURAL WORD EMBEDDINGS","authors":["Y Yin, X Sun, H Lv, P Wang, H Ma, D Yang"],"snippet":"Neural language models have significantly improved current natural language understanding tasks. However, distributional semantics, derived from neural language models is less competitive in computing semantic relatedness or similarity …","url":["https://www.researchgate.net/profile/Dongqiangdongqiang-Yang/publication/356592344_USING_SEMANTIC_RELATIONSHIPS_TO_ENHANCE_NEURAL_WORD_EMBEDDINGS/links/61a2f3196a0f6e76c2b43389/USING-SEMANTIC-RELATIONSHIPS-TO-ENHANCE-NEURAL-WORD-EMBEDDINGS.pdf"]}
{"year":"2021","title":"Using Visual Features to Generate Language Context","authors":["I Zachmann - 2021"],"snippet":"Page 1. Using Visual Features to Generate Language Context Undergraduate Honors Research Thesis Presented in Partial Fulfillment of the Requirements for Undergraduate Honors Research Thesis in the College of Engineering at The Ohio State University By …","url":["https://kb.osu.edu/bitstream/handle/1811/92666/Thesis.pdf?sequence=1"]}
{"year":"2021","title":"Using Word Embeddings to Determine Concepts of Values In Insurance Claim Spreadsheets","authors":["V Justnes - 2021"],"snippet":"Page 1. Using Word Embeddings to Determine Concepts of Values In Insurance Claim Spreadsheets Vemund Justnes Thesis submitted for the degree of Master in Informatics: Programming and System Architecture 60 credits Institute for Informatics …","url":["https://www.duo.uio.no/bitstream/handle/10852/87784/1/using_word_embeddings_for_concept_determiniation.pdf"]}
{"year":"2021","title":"Using word embeddings to probe sentiment associations of politically loaded terms in news and opinion articles from news media outlets","authors":["D Rozado, M al-Gharbi - Journal of Computational Social Science, 2021"],"snippet":"… word embeddings also absorb many of the biases that are prevalent within a cultural system, as a result of embedding models being trained on culturally relevant language artifacts that are presumed to contain cultural biases, such …","url":["https://link.springer.com/article/10.1007/s42001-021-00130-y"]}
{"year":"2021","title":"Utility-Preserving Anonymization of Textual Documents","authors":["FAM Hassan - 2021"],"snippet":"Page 1. UTILITY-PRESERVING ANONYMIZATION OF TEXTUAL DOCUMENTS Fadi Abdulfattah Mohammed Hassan ADVERTIMENT. L'accés als continguts d'aquesta tesi doctoral i la seva utilització ha de respectar els drets de la persona autora …","url":["https://84.88.27.106/bitstream/handle/10803/672012/TESI%20Fadi%20Abdulfattah%20Mohammed%20Hassan.pdf?sequence=1&isAllowed=y"]}
{"year":"2021","title":"Utility-Preserving Privacy Protection of Textual Documents via Word Embeddings","authors":["F Hassan, D Sanchez, J Domingo-Ferrer - IEEE Transactions on Knowledge and Data …, 2021"],"snippet":"Page 1. 1041-4347 (c) 2021 IEEE. Personal use is permitted, but republication/ redistribution requires IEEE permission. See http://www.ieee.org/ publications_standards/publications/rights/index.html for more information. This …","url":["https://ieeexplore.ieee.org/abstract/document/9419784/"]}
{"year":"2021","title":"UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resource Cross-Lingual NLP","authors":["MS Bari, T Mohiuddin, S Joty"],"snippet":"… Manning, 2019). We use XLM-R masked LM (Conneau et al., 2020) as our vicinity model θmlm, which is trained on massive multilingual corpora (2.5 TB of Common-Crawl data in 100 languages). The Page 4. 1981 Algorithm 1 …","url":["https://aclanthology.org/2021.acl-long.154.pdf"]}
{"year":"2021","title":"UzBERT: pretraining a BERT model for Uzbek","authors":["B Mansurov, A Mansurov - arXiv preprint arXiv:2108.09814, 2021"],"snippet":"… Page 3. 3 Uzbek Wikipedia and the Common Crawl corpus. According … 2006], where full words are replaced by their abbreviations to save space in print. The filtered Common Crawl corpus5 used by XLM-R [Conneau et al. 2019 …","url":["https://arxiv.org/pdf/2108.09814"]}
{"year":"2021","title":"Validating Word Embedding as a Tool for the Psychological Sciences","authors":["V Swift - 2021"],"snippet":"Page 1. Validating Word Embedding as a Tool for the Psychological Sciences by Victor Swift A thesis submitted in conformity with the requirements for the degree of Doctorate of Philosophy Department of Psychology University of …","url":["https://tspace.library.utoronto.ca/bitstream/1807/104927/3/Swift_Victor_202103_PhD_thesis.pdf"]}
{"year":"2021","title":"Variational Autoencoder with Interactive Attention for Affective Text Generation","authors":["R Chen, J Wang, X Zhang - CCF International Conference on Natural Language …, 2021"],"snippet":"… The word embeddings used in this experiment was pre-trained on Common Crawl 840B 1 by GloVe [10]. For baselines, the polarity label \\(\\mathbf {c}\\) was treated as a scalar (ie, 0 or 1) and directly concatenated into the latent variables. For …","url":["https://link.springer.com/chapter/10.1007/978-3-030-88483-3_9"]}
{"year":"2021","title":"VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation","authors":["F Luo, W Wang, J Liu, Y Liu, B Bi, S Huang, F Huang…"],"snippet":"… mBART (Liu et al., 2020b) Encoder-decoder 680M 12 12 25 250k CommonCrawl VECO Flexible 662M 24* 50 250k CommonCrawl + Translation … For monolingual training datasets, we reconstruct CommonCrawl Corpus used in XLM-R (Conneau et al., 2019) …","url":["https://aclanthology.org/2021.acl-long.308.pdf"]}
{"year":"2021","title":"Vera: Prediction Techniques for Reducing Harmful Misinformation in Consumer Health Search","authors":["R Pradeep, X Ma, R Nogueira, J Lin - Proceedings of the 44th Annual International …, 2021"],"snippet":"… The document collection comprises news articles from the CommonCrawl news crawl spanning the first four months of 2020 (January 1st, 2020 to April 30th, 2020), covering the onset of the COVID-19 pandemic. This corpus …","url":["https://cs.uwaterloo.ca/~jimmylin/publications/Pradeep_etal_SIGIR2021.pdf"]}
{"year":"2021","title":"VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations","authors":["A Rathore, S Dev, JM Phillips, V Srikumar, Y Zheng… - arXiv preprint arXiv …, 2021"],"snippet":"… It also provides a larger GLoVe embedding (300-dimensional with the 100K most frequent words) from the Common Crawl corpus (https://commoncrawl.org/). 5 USE CASES We demonstrate the efficacy of VERB on several examples …","url":["https://arxiv.org/pdf/2104.02797"]}
{"year":"2021","title":"VieSum: How Robust Are Transformer-based Models on Vietnamese Summarization?","authors":["H Nguyen, L Phan, J Anibal, A Peltekian, H Tran - arXiv preprint arXiv:2110.04257, 2021"],"snippet":"… Common Crawl is a publicly web archive that provides text scraped from websites. These datasets (C4, CC25, and Common Crawl) may include Vietnamese Wikipedia languages website that help the model excel in constrained domain corpus (Wikipedia) …","url":["https://arxiv.org/pdf/2110.04257"]}
{"year":"2021","title":"Visual and Affective Multimodal Models of Word Meaning in Language and Mind","authors":["S De Deyne, DJ Navarro, G Collell, A Perfors - Cognitive Science, 2021"],"snippet":"Abstract One of the main limitations of natural language‐based approaches to meaning is that they do not incorporate multimodal representations the way humans do. In this study, we evaluate how wel...","url":["https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12922"]}
{"year":"2021","title":"Visual Grounding Strategies for Text-Only Natural Language Processing","authors":["D Sileo - arXiv preprint arXiv:2103.13942, 2021"],"snippet":"… A linear projection maps the region features to the input space of the cross-modal encoder. We also provide K rank 6We use the CommonCrawl version https:// fasttext.cc/docs/en/english-vectors.html. and discard stop-words with NLTK …","url":["https://arxiv.org/pdf/2103.13942"]}
{"year":"2021","title":"Visualizing large-scale high-dimensional data via hierarchical embedding of KNN graphs","authors":["H Zhu, M Zhu, Y Feng, D Cai, Y Hu, S Wu, X Wu… - Visual Informatics, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S2468502X21000292"]}
{"year":"2021","title":"VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics","authors":["A Narechania, A Karduni, R Wesslen, E Wall - arXiv preprint arXiv:2108.03366, 2021"],"snippet":"… Unlike many pre-trained language models that use a general corpus like Wikipedia or the Common Crawl [14, 43, 49], SPECTER was pre-trained on academic literature (sciBERT [?]) and fine-tuned with citations which …","url":["https://arxiv.org/pdf/2108.03366"]}
{"year":"2021","title":"VOH. CoLAB at TREC 2020 Health Misinformation Track⋆","authors":["SN Gonçalves, F Martins"],"snippet":"… 2.1 Collection The collection contains 65 million news articles from CommonCrawl News4 corresponding to the period from January to April of 2020 … Recently, vinegar has been promoted as a disinfectant (...) 4 …","url":["https://trec.nist.gov/pubs/trec29/papers/vohcolab.HM.pdf"]}
{"year":"2021","title":"Voted In, Standing Out: Public Response to Immigrants' Political Accession","authors":["G Grossman, S Zonszein - 2021"],"snippet":"… newspapers, covering the general elections from 2010–2019.8 This data is from Common Crawl, which is an open repository of web crawl data. We assume that an article refers to a candidate's ethnic group when three conditions are met: 1) the publication date is …","url":["https://files.osf.io/v1/resources/xd4wk/providers/osfstorage/614782978ae0920335d8c84c?action=download&direct&version=2"]}
{"year":"2021","title":"We Need to Talk About Data: The Importance of Data Readiness in Natural Language Processing","authors":["F Olsson, M Sahlgren - arXiv preprint arXiv:2110.05464, 2021"],"snippet":"In this paper, we identify the state of data as being an important reason for failure in applied Natural Language Processing (NLP) projects. We argue that there is a gap between academic research in NLP and its application to problems outside …","url":["https://arxiv.org/pdf/2110.05464"]}
{"year":"2021","title":"Web Archive Analytics","authors":["M Völske, J Bevendorff, J Kiesel, B Stein, M Fröbe… - INFORMATIK 2020, 2021"],"snippet":"… in Figure 5, beginning with the bottom-most data acquisition layerȷ Primary sources for data ingestion include web crawls and web archives, such as the aforementioned Internet Archive, the Common Crawl,13 the older … 13 …","url":["https://dl.gi.de/bitstream/handle/20.500.12116/34759/A8-1.pdf?sequence=1&isAllowed=y"]}
{"year":"2021","title":"Web Content Authentication: A Machine Learning Approach to Identify Fake and Authentic Web Pages on Internet","authors":["J Ashok, P Badoni - … Technology for Competitive Strategies (ICTCS 2020) …"],"snippet":"… SpringerLink (Springer, 21 June 2017). www. link. springer. com/chapter/10.1007/978- 3-319-69784-0_15 42. Link to GitHub where code is hosted. https://github. com/Jkrish1011/Web-Content-Authentic ator 43. https://commoncrawl. org/","url":["http://books.google.de/books?hl=en&lr=lang_en&id=Dwo3EAAAQBAJ&oi=fnd&pg=PA85&dq=commoncrawl&ots=RCTLIhR7zF&sig=_9zgxY48BSktz0Jk-c6Y7XTbVf0"]}
{"year":"2021","title":"Web Table Classification based on Visual Features","authors":["B Bühler, H Paulheim - arXiv preprint arXiv:2103.05110, 2021"],"snippet":"… Lehmberg et al. [17] conduct web table classification in order to create a large open corpus of relational HTML tables, ie, the WebDataCommons HTML Ta- bles Dataset3 based on the 2012 version of the Common Crawl web corpus …","url":["https://arxiv.org/pdf/2103.05110"]}
{"year":"2021","title":"WeChat Neural Machine Translation Systems for WMT21","authors":["X Zeng, Y Liu, E Li, Q Ran, F Meng, P Li, J Xu, J Zhou - arXiv preprint arXiv …, 2021"],"snippet":"… 4.2 Dataset For each language pair, the bilingual training data is the combination of all parallel data released by WMT21. For monolingual data, we filter out clean data from News Crawl, Common Crawl and Ex- tended Common Crawl …","url":["https://arxiv.org/pdf/2108.02401"]}
{"year":"2021","title":"What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?","authors":["K Marchisio, M Freitag, D Grangier - arXiv preprint arXiv:2106.15818, 2021"],"snippet":"… As bilingual training data, we use News Commentary v13, Europarl v7, Common Crawl and EU Press Release. We deduplicate and filter out pairs with > 250 to- kens in either language or length ratio over 1.5. This results in 5.2 million parallel sentence pairs …","url":["https://arxiv.org/pdf/2106.15818"]}
{"year":"2021","title":"What Does Your Bio Say? Inferring Twitter Users' Depression Status From Multimodal Profile Information Using Deep Learning","authors":["S Ghosh, A Ekbal, P Bhattacharyya - IEEE Transactions on Computational Social …, 2021"],"snippet":"… Word embedding has been trained on the Common Crawl corpus (840 billion tokens). Padding arrays of zeros are done to make input sequences the same length. In our training set, we create an embedding matrix (E) that contains the embedding …","url":["https://ieeexplore.ieee.org/abstract/document/9567734/"]}
{"year":"2021","title":"What is this cluster about? Explaining textual clusters by extracting relevant keywords","authors":["A Penta, A Pal - Knowledge-Based Systems, 2021"],"snippet":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article …","url":["https://www.sciencedirect.com/science/article/pii/S0950705121006043"]}
{"year":"2021","title":"What Makes Sentences Semantically Related: A Textual Relatedness Dataset and Empirical Study","authors":["M Abdalla, K Vishnubhotla, SM Mohammad - arXiv preprint arXiv:2110.04845, 2021"],"snippet":"The degree of semantic relatedness (or, closeness in meaning) of two units of language has long been considered fundamental to understanding meaning. Automatically determining relatedness has many applications such as question …","url":["https://arxiv.org/pdf/2110.04845"]}
{"year":"2021","title":"What Transformers Might Know About the Physical World: T5 and the Origins of Knowledge","authors":["H Shi, P Wolff - Proceedings of the Annual Meeting of the Cognitive …, 2021"],"snippet":"… stack of decoders. All versions of T5 were trained on a cleaned version of the common crawl called the Colossal Cleaned Common Crawl (C4). The training corpus is over two times larger than Wikipedia. The largest version …","url":["https://escholarship.org/content/qt0kr3t179/qt0kr3t179.pdf"]}
{"year":"2021","title":"What Vision-Language ModelsSee'when they See Scenes","authors":["M Cafagna, K van Deemter, A Gatt - arXiv preprint arXiv:2109.07301, 2021"],"snippet":"Page 1. What Vision-Language Models 'See' when they See Scenes Michele Cafagna1 Kees van Deemter2 Albert Gatt1,2 1University of Malta, Institute of Linguistics and Language Technology 2Universiteit Utrecht, Information …","url":["https://arxiv.org/pdf/2109.07301"]}
{"year":"2021","title":"What's in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus","authors":["A Sasha Luccioni, JD Viviano - arXiv e-prints, 2021","AS Luccioni, JD Viviano - arXiv preprint arXiv:2105.02732, 2021"],"snippet":"Whereas much of the success of the current generation of neural language models has been driven by increasingly large training corpora, relatively little research has been dedicated to analyzing these massive sources of textual data. In this …","url":["https://arxiv.org/pdf/2105.02732","https://ui.adsabs.harvard.edu/abs/2021arXiv210502732S/abstract"]}
{"year":"2021","title":"When FastText Pays Attention","authors":["V Novotný, M Štefánik, EF Ayetiran, P Sojka - 2021"],"snippet":"… (Hyper)-Parameter Optimization 3. Experiments & 4. Results I 3.3. (Hyper-)Parameter Optimization Dataset Number of tokens 2017 English Wikipedia 2,423,655,228 English Common Crawl 823,575,128,431 Table: Our datasets and their sizes in tokens …","url":["https://nlp.fi.muni.cz/trac/research/chrome/site/seminar2020/VNovotny-FastText_Attention_slides.pdf"]}
{"year":"2021","title":"When FastText Pays Attention: Efficient Estimation of Word Representations using Constrained Positional Weighting","authors":["V Novotný, M Štefánik, EF Ayetiran, P Sojka - arXiv preprint arXiv:2104.09691, 2021"],"snippet":"… For qualitative evaluation and performance estimation, we use the deduplicated English Common Crawl4 as the training corpus. See the statistics in Table 1. Dataset Number of tokens 2017 English Wikipedia 2,423,655,228 English Common Crawl 823,575,128,431 …","url":["https://arxiv.org/pdf/2104.09691"]}
{"year":"2021","title":"When few-shot learning meets large-scale knowledge-enhanced pre-training: Alibaba at FewCLUE","authors":["Z Xu, C Wang, P Li, Y Li, M Wang, B Hou, M Qiu… - … International Conference on …, 2021"],"snippet":"… In the pre-training steps, we use CLUECorpus2020 [28], which contains 100 GB Chinese raw texts retrieved from Common Crawl. We also employ 100 million high-quality Chinese knowledge SPOs from our in-house KG as the structured knowledge source …","url":["https://link.springer.com/chapter/10.1007/978-3-030-88483-3_34"]}
{"year":"2021","title":"When in Doubt, Summon the Titans: Efficient Inference with Large Models","authors":["AS Rawat, M Zaheer, AK Menon, A Ahmed, S Kumar - arXiv preprint arXiv …, 2021"],"snippet":"Scaling neural networks to \"large\" sizes, with billions of parameters, has been shown to yield impressive results on many challenging problems. However, the inference cost incurred by such large models often prevents their application in most …","url":["https://arxiv.org/pdf/2110.10305"]}
{"year":"2021","title":"When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer","authors":["A Deshpande, P Talukdar, K Narasimhan - arXiv preprint arXiv:2110.14782, 2021"],"snippet":"… (same) which uses different splits of Wikipedia data (hence, same domain), and (2) non-parallel (diff) which uses Wikipedia data for the original and common crawl data (web text) for the derived language (hence, different domain). We use the …","url":["https://arxiv.org/pdf/2110.14782"]}
{"year":"2021","title":"Which Apple Keeps Which Doctor Away Colorful Word Representations with Visual Oracles","authors":["Z Zhang, H Yu, H Zhao, M Utiyama - IEEE/ACM Transactions on Audio, Speech, and …, 2021"],"snippet":"Recent pre-trained language models (PrLMs) offer a new performant method of contextualized word representations by leveraging the sequence-level context for modeling. Although the PrLMs generally provide more effective contextualized word …","url":["https://ieeexplore.ieee.org/abstract/document/9627795/"]}
{"year":"2021","title":"Which is Better for Deep Learning: Python or MATLAB? Answering Comparative Questions in Natural Language","authors":["V Chekalina, A Bondarenko, C Biemann, M Beloucif…"],"snippet":"… with respect to an aspect specified by the user. First, using the Elasticsearch BM25, CAM retrieves sentences containing the two compared objects and the comparison aspect from the Common Crawl-based corpus featuring …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/publications/2021-chekalinaetal-eacldemo-coqas.pdf"]}
{"year":"2021","title":"Which model is helpful in solving privacy, memorization, and bias problems?","authors":["S Yoon, H Lee"],"snippet":"Page 1. Which model is helpful in solving privacy, memorization, and bias problems? Soyoung Yoon * 1 Hyunji Lee * 1 Abstract Recently, language models of huge size are achieving stunning performance on natural language understanding tasks …","url":["https://soyoung97.github.io/profile/assets/papers/CS774.pdf"]}
{"year":"2021","title":"Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text","authors":["K Park, Z Pan, J Joo - arXiv preprint arXiv:2106.01033, 2021"],"snippet":"… For each of the target media outlets, we collected news articles shared throughout 2020 until September from the Common Crawl corpus, which has been collecting web data since 20088. The total number of retrieved …","url":["https://arxiv.org/pdf/2106.01033"]}
{"year":"2021","title":"Who is Accountable for Data Bias?","authors":["C Parkey - CHANCE, 2021"],"snippet":"… The OpenAI researchers use the data from Common Crawl, which provides a corpus for collaborative research, analysis, and education. It is an open data set containing petabytes of data collected since 2008 from raw web …","url":["https://www.tandfonline.com/doi/abs/10.1080/09332480.2021.1915032"]}
{"year":"2021","title":"Why Do Masked Neural Language Models Still Need Semantic Knowledge in Question Answering?","authors":["C Kang - 2021"],"snippet":"… Lastly, RoBERTa is pre-trained with much larger data. Especially, the data is about 160GB, which is composed of CommonCrawl News dataset [40], Open WebText corpus [41], and STORIES corpus [42] in addition to the pre-training data of BERT …","url":["http://sailab.kaist.ac.kr/wp-content/uploads/2021/03/MS_Thesis_cheongwoong.pdf"]}
{"year":"2021","title":"Wide and Deep Learning in Crowdfunding Success Prediction","authors":["D George"],"snippet":"The thesis studies to which extent deep learning models can predict whether crowdfunding campaigns will be successfully funded by drawing upon a dataset of 246,891 projects and 40 structured and text features from the crowdfunding platform …","url":["https://research.cbs.dk/files/68329943/1186110_Master_Thesis_133104_Wide_and_Deep_Learning_in_Crowdfunding_Success_Prediction.pdf"]}
{"year":"2021","title":"with Graph Neural Networks","authors":["M House, M Rules"],"snippet":"… example “pen” and “pencil”, or “salt” and “pepper”. We load word embeddings from a FastText 155 model [18], pre-trained on the Common Crawl dataset of over 600 billion tokens. Names of objects 156 are converted to word …","url":["https://openreview.net/pdf?id=Ei3MOY2rDHB"]}
{"year":"2021","title":"WITS: Wikipedia for Italian Text Summarization","authors":["S Casola, A Lavelli - 2021"],"snippet":"Abstractive text summarization has recently improved its performance due to the use of sequence to sequence models. However, while these models are extremely data-hungry, datasets in languages other than English are few. In this work, we introduce WITS (Wikipedia …","url":["http://ceur-ws.org/Vol-3033/paper65.pdf"]}
{"year":"2021","title":"Word embeddings for the classification of main conditions with samples of patient text describing symptoms","authors":["A Lamproudis - 2020"],"snippet":"Page 1. IN DEGREE PROJECT COMPUTER SCIENCE AND ENGINEERING, SECOND CYCLE, 30 CREDITS , STOCKHOLM SWEDEN 2020 Word embeddings for the classification of main conditions with samples of patient text describing symptoms …","url":["https://www.diva-portal.org/smash/get/diva2:1523304/FULLTEXT01.pdf"]}
{"year":"2021","title":"Word embeddings reveal social group attitudes and stereotypes in large language corpora","authors":["TES Charlesworth, MR Banaji"],"snippet":"… Surprisingly, less than half of the 45 languages studied by DeFranza and colleagues revealed a male-good/female-bad2 association across data from Wikipedia and the Common Crawl. This … For example, in the Common Crawl, 67% of languages with …","url":["http://www.people.fas.harvard.edu/~banaji/research/publications/articles/2021_Charlesworth_TAL.pdf"]}
{"year":"2021","title":"Word Vector Representations using Shallow Neural Networks","authors":["O Adewumi - 2021"],"snippet":"… We obtain better performance in both languages on the downstream task with far smaller training data, compared to recently released, common crawl versions; and character n-grams appear useful for Swedish, a morphologically rich language. Keywords …","url":["https://www.diva-portal.org/smash/record.jsf?pid=diva2:1543295"]}
{"year":"2021","title":"Word-level Korean-English Quality Estimation","authors":["S Eo, C Park, J Seo, H Moon, H Lim - Annual Conference on Human and Language …, 2021"],"snippet":"Abstract 기계번역 품질 예측 (Quality Estimation, QE) 은 정답 문장에 대한 참조없이 소스 문장과 기계번역 결과를 통해 기계번역 결과에 대한 품질을 수준별 주석으로 나타내주는 태스크이며, 다양한 활용도가 있다는 점에서 꾸준히 연구가 수행되고 있다 …","url":["https://www.koreascience.or.kr/article/CFKO202130060780846.pdf"]}
{"year":"2021","title":"Words as a window: Using word embeddings to explore the learned representations of Convolutional Neural Networks","authors":["D Dharmaretnam, C Foster, A Fyshe - Neural Networks, 2021"],"snippet":"… here. Elmo is trained on a random sample of Wikipedia and the common crawl, and we use an average of the three 512-dimensional output layers … 2016). The Lexvec model has 300 dimensions and is trained on the common crawl …","url":["https://www.sciencedirect.com/science/article/pii/S0893608020304263"]}
{"year":"2021","title":"WTR: A Test Collection for Web Table Retrieval","authors":["Z Chen, S Zhang, BD Davison - arXiv e-prints, 2021"],"snippet":"… PA, USA ABSTRACT We describe the development, characteristics and availability of a test collection for the task of Web table retrieval, which uses a largescale Web Table Corpora extracted from the Common Crawl. Since …","url":["https://arxiv.org/pdf/2105.02354"]}
{"year":"2021","title":"XLM-E: Cross-lingual Language Model Pre-training via ELECTRA","authors":["Z Chi, S Huang, L Dong, S Ma, S Singhal, P Bajaj… - arXiv preprint arXiv …, 2021"],"snippet":"… different languages. 4 Experiments 4.1 Setup Data We use the CC-100 (Conneau et al., 2020) dataset for the replaced token detection task. CC- 100 contains texts in 100 languages collected from the CommonCrawl dump. We use …","url":["https://arxiv.org/pdf/2106.16138"]}
{"year":"2021","title":"XLM-K: Improving Cross-Lingual Language Model Pre-Training with Multilingual Knowledge","authors":["X Jiang, Y Liang, W Chen, N Duan - arXiv preprint arXiv:2109.12573, 2021"],"snippet":"… 4.1 Implementation Details Data and Model Structure For the multilingual masked language modeling task, we use Common Crawl dataset (Wenzek et al. 2020) … 2020) was trained on Common Crawl dataset (Wenzek et al. 2020), which covers our training data Wikipedia …","url":["https://arxiv.org/pdf/2109.12573"]}
{"year":"2021","title":"XLM-T: A Multilingual Language Model Toolkit for Twitter","authors":["F Barbieri, LE Anke, J Camacho-Collados - arXiv preprint arXiv:2104.12250, 2021"],"snippet":"… 2.1). For the monolingual experiments we also include a FastText (FT) baseline (Joulin et al., 2017), which relies on monolingual FT embeddings trained on Common Crawl and Wikipedia (Grave et al., 2018) as initialization for each language lookup table …","url":["https://arxiv.org/pdf/2104.12250"]}
{"year":"2021","title":"XLM-T: Scaling up Multilingual Machine Translation with Pretrained Cross-lingual Transformer Encoders","authors":["S Ma, J Yang, H Huang, Z Chi, L Dong, D Zhang… - arXiv preprint arXiv …, 2020"],"snippet":"… cross-lingual representations. In this work, we adopt XLM-R BASE (Conneau et al., 2020) as the pretrained encoder. It was trained in 100 languages, using more than two terabytes of filtered CommonCrawl data. XLM-R is …","url":["https://arxiv.org/pdf/2012.15547"]}
{"year":"2021","title":"XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale","authors":["A Babu, C Wang, A Tjandra, K Lakhotia, Q Xu, N Goyal… - arXiv preprint arXiv …, 2021"],"snippet":"This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 …","url":["https://arxiv.org/pdf/2111.09296"]}
{"year":"2021","title":"XRR: Explainable Risk Ranking for Financial Reports","authors":["TW Lin, RY Sun, HL Chang, CJ Wang, MF Tsai"],"snippet":"… permutation test with p < 0.05. Table 1: Performance comparison 3. GloVe [20] representations are 300-dimensional word vectors6 trained on 840 billion tokens of Common Crawl data. In the following experiments, we denote …","url":["https://2021.ecmlpkdd.org/wp-content/uploads/2021/07/sub_506.pdf"]}
{"year":"2021","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation","authors":["S Ruder, N Constant, J Botha, A Siddhant, O Firat, J Fu… - arXiv preprint arXiv …, 2021"],"snippet":"… Rebalanced mBERT (RemBERT; Chung et al., 2021) is a more efficient, scaled-up reparameterization of mBERT. These models have been pre-trained on unlabeled data in around 100 languages from Wikipedia …","url":["https://arxiv.org/pdf/2104.07412"]}
{"year":"2021","title":"XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation","authors":["S Mukherjee, AH Awadallah, J Gao - arXiv preprint arXiv:2106.04563, 2021"],"snippet":"… To address these issues, we ex- plore techniques to automatically generate largescale task-specific transfer sets leveraging a very large bank of web sentences from Common Crawl in Section 4.1. 3 Distillation Framework Overview …","url":["https://arxiv.org/pdf/2106.04563"]}
{"year":"2021","title":"You too Brutus! Trapping Hateful Users in Social Media: Challenges, Solutions & Insights","authors":["M Das, P Saha, R Dutt, P Goyal, A Mukherjee… - arXiv preprint arXiv …, 2021"],"snippet":"… We experiment with the following text-classifiers. fastText: We leverage the pre-trained fastText embedding by Grave et al. [14] which has been trained on Common Crawl and Wikipedia. Each user's document has been represented as a 100 dimensional vector …","url":["https://arxiv.org/pdf/2108.00524"]}
{"year":"2021","title":"Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning","authors":["S Wu, X Zhao, T Yu, R Zhang, C Shen, H Liu, F Li… - arXiv preprint arXiv …, 2021"],"snippet":"… c) After this step, the data size of Common Crawl is decreased from 866,304GB to 12,200GB. • Traditional to Simplified Chinese: Converts traditional Chinese characters to simplified Chinese characters. • Sensitive Words Filtering: Removes …","url":["https://arxiv.org/pdf/2110.04725"]}
{"year":"2021","title":"Zero-Resource Multi-Dialectal Arabic Natural Language Understanding","authors":["M Khalifa, H Hassan, A Fahmy"],"snippet":"… XLM-R is a cross-lingual model, and we choose it since it is reported to perform better than mBERT, the multilingual model from Google [5]. XLM-R also uses Common Crawl for training, which is more likely to have …","url":["https://www.researchgate.net/profile/Muhammad-Khalifa/publication/350530331_Zero-Resource_Multi-Dialectal_Arabic_Natural_Language_Understanding/links/6064d0ba458515614d272082/Zero-Resource-Multi-Dialectal-Arabic-Natural-Language-Understanding.pdf"]}
{"year":"2021","title":"Zeroshot Crosslingual Transfer of a Gloss Language Model for Semantic Change Detection","authors":["M Rachinskiy, N Arefyev"],"snippet":"… finetuned. data with MLM (Masked Language Modeling) objective. But in contrast to BERT and RoBERTa, it is pretrained not on monolingual data but on 2.5 TB of texts from CommonCrawl in 100 languages. 3 System overview …","url":["http://www.dialog-21.ru/media/5278/rachinskiymplusarefyevn130.pdf"]}
{"year":"2021","title":"ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation","authors":["KK Maurya, MS Desarkar, Y Kano, K Deepshikha - arXiv preprint arXiv:2106.01597, 2021"],"snippet":"… languages. It is a transformer-based sequence- to-sequence pre-trained model. The model is trained on monolingual data in many languages from Wikipedia Common Crawl corpus with BART language model objective. Particularly …","url":["https://arxiv.org/pdf/2106.01597"]}
{"year":"2022","title":"4 Touché Task 1: Conversational Argument Retrieval","authors":["A Bondarenko, M Fröbe, M Beloucif, L Gienapp…"],"snippet":"… Other argument retrieval systems, such as ArgumenText [40] and TARGER [9], use the larger Common Crawl, requiring additionally also argument … scenarios, based on billions of sentences from the Common Crawl, however, it still lacks a …","url":["https://9dok.net/document/y4w4p04v-touch%C3%A9-task-conversational-argument-retrieval.html"]}
{"year":"2022","title":"\" Way back then\": A Data-driven View of 25+ years of Web Evolution","authors":["V Agarwal, N Sastry - arXiv preprint arXiv:2202.08239, 2022"],"snippet":"… For example, the use of historical crawls of the web such as the Common Crawl5. [30] is a brief history of web crawlers. [9] considers … Similarly, the common crawl12 represents a huge dataset of Web URLs and their content, that can provide …","url":["https://arxiv.org/pdf/2202.08239"]}
{"year":"2022","title":"A Comparative Study of Pre-trained Encoders for Low-Resource Named Entity Recognition","authors":["Y Chen, J Mikkelsen, A Binder, C Alt, L Hennig - arXiv preprint arXiv:2204.04980, 2022"],"snippet":"Pre-trained language models (PLM) are effective components of few-shot named entity recognition (NER) approaches when augmented with continued pre-training on task-specific out-of-domain data or fine-tuning on in-domain data. However, their …","url":["https://arxiv.org/pdf/2204.04980"]}
{"year":"2022","title":"A comparative study on vectorization methods for non-functional requirements classification","authors":["P Leelaprute, S Amasaki - Information and Software Technology, 2022"],"snippet":"… The pre-trained model with 840 billion tokens of Common Crawl was used. This study used a vectorization method [30] based on the … For fastText, the pre-trained model with Common Crawl and Wikipedia was used. This study also used a …","url":["https://www.sciencedirect.com/science/article/pii/S0950584922001239"]}
{"year":"2022","title":"A Comparative Text Classification Study with Deep Learning-Based Algorithms","authors":["Ö Köksal, Ö Akgül - 2022 9th International Conference on Electrical and …, 2022"],"snippet":"… The vectors are trained from Wikipedia and Common Crawl, and each has a length of 300. Furthermore, the vectors are generated for n-grams, n length sequences of characters, instead of words. This property allows FastText to …","url":["https://ieeexplore.ieee.org/abstract/document/9772587/"]}
{"year":"2022","title":"A Comparison of Approaches for Imbalanced Classification Problems in the Context of Retrieving Relevant Documents for an Analysis","authors":["S Wankmüller - arXiv preprint arXiv:2205.01600, 2022"],"snippet":"One of the first steps in many text-based social science studies is to retrieve documents that are relevant for the analysis from large corpora of otherwise irrelevant documents. The conventional approach in social science to address this …","url":["https://arxiv.org/pdf/2205.01600"]}
{"year":"2022","title":"A Corpus for Suggestion Mining of German Peer Feedback","authors":["R Rietsche, E Ritz, J Janda, D Pfütze"],"snippet":"Peer feedback in online education becomes increasingly important to meet the demand for feedback in large scale classes, such as eg Massive Open Online Courses (MOOCs). However, students are often not experts in how to write helpful …","url":["https://www.researchgate.net/profile/Roman-Rietsche/publication/362013990_A_Corpus_for_Suggestion_Mining_of_German_Peer_Feedback/links/62d10a069b8b7d1f6f712393/A-Corpus-for-Suggestion-Mining-of-German-Peer-Feedback.pdf"]}
{"year":"2022","title":"A Critique Empirical Evaluation of Relevance Computation for Focused Web Crawlers","authors":["JDPNR Mary, S Balasubramanian, RSP Raj - Brazilian Archives of Biology and …, 2022"],"snippet":"HIGHLIGHTS This paper presents a survey on focused web crawlers. This paper presents the challenges in focused crawling research. This paper presents the highlights and hindrances of existing focused web crawlers. This paper also …","url":["https://www.scielo.br/j/babt/a/yG9Bw9htLXd554BfFcwwLCy/abstract/?lang=en"]}
{"year":"2022","title":"A Deep Learning approach to real-world Entity Linking: extracting and matching organisation mentions from unstructured text","authors":["T Bonomo - 2022"],"snippet":"… Specifically, the authors built a clean CommonCrawl Corpus composed of text scraped from the web in 100 different languages, amounting to around 2.5 TB of text data. The amount of data obviously varies from language to language, but Conneau …","url":["https://aaltodoc.aalto.fi/bitstream/handle/123456789/112889/master_Bonomo_Tommaso_2022.pdf?sequence=1"]}
{"year":"2022","title":"A Dual Attention-Based Representation for the Detection of Abusive Language in Texts and Memes. Technical Report No. CCC-22-005","authors":["HJJ Vásquez - 2022"],"snippet":"… For word representation we used pretrained fastText embeddings [83], trained with subword information on Common Crawl, which have been recognized as useful for this task according to the study presented in [8]. All the non-BERT based DNNs …","url":["https://ccc.inaoep.mx/archivos/2021/CCC-22-005.pdf"]}
{"year":"2022","title":"A Format-Aware Reducer for Scriptable Rewriting of PDF Files","authors":["P Anantharaman, S Cheung, N Boorman, ME Locasto"],"snippet":"Sanitizing untrusted input is a significant unsolved problem in defensive cybersecurity and input handling. Even if we assume that a safe, provably correct parser exists to validate the input syntax, processing logic may still require the …","url":["https://prashant.at/files/pdffixer.pdf"]}
{"year":"2022","title":"A Framework for Deprecating Datasets: Standardizing Documentation, Identification, and Communication","authors":["AS Luccioni, F Corry, H Sridharan, M Ananny, J Schultz… - 2022"],"snippet":"… , given their sheer size (C4 represents 2.3 TB of data, whereas the Common Crawl has 139TB), filtering them is complex and time-consuming, … In practice, documenting and deprecating these datasets is akin to a game of whack-a-mole …","url":["https://facctconference.org/static/pdfs_2022/facct22-17.pdf"]}
{"year":"2022","title":"A generating model for Finnish nominal inflection using distributional semantics","authors":["A Nikolaev, YY Chuang, RH Baayen - 2022"],"snippet":"Finnish nouns are characterized by rich inflectional variation, with obligatory marking of case and number, with optional possessive suffixes and with the possibility of further cliticization. We present a model for the conceptualization of …","url":["https://psyarxiv.com/ndtv2/download"]}
{"year":"2022","title":"A Holistic Assessment of the Carbon Footprint of Noor, a Very Large Arabic Language Model","authors":["I Lakim, E Almazrouei, IA Alhaol, M Debbah, J Launay - Challenges {\\&, 2022"],"snippet":"… We use Common Crawl (CC) for acquiring large amounts of web data. Each CC dump is on average around 10TB, and we discard it immediately after processing it. On average, it takes 24 hours to fully process a dump: we used 21 dumps from CC …","url":["https://openreview.net/pdf?id=B-lS3zH8Zq"]}
{"year":"2022","title":"A hybrid quantum approach to leveraging data from HTML tables","authors":["P Jiménez Aguirre, JC Roldán Salvador… - Knowledge and Information …, 2022","P Jiménez, JC Roldán, R Corchuelo - Knowledge and Information Systems, 2022"],"snippet":"… Unfortunately, a recent analysis of the 32.04 million domains in the November 2019 Common Crawl has revealed that only 11.92 million domains provide such tags [5], which means that there are roughly 20.12 million domains that do not …","url":["https://idus.us.es/bitstream/handle/11441/131991/Jim%C3%A9nez2022_Article_AHybridQuantumApproachToLevera.pdf?sequence=1&isAllowed=y","https://link.springer.com/article/10.1007/s10115-021-01636-7"]}
{"year":"2022","title":"A Knowledge-Enhanced Adversarial Model for Cross-lingual Structured Sentiment Analysis","authors":["Q Zhang, J Zhou, Q Chen, Q Bai, J Xiao, L He - arXiv preprint arXiv:2205.15514, 2022"],"snippet":"… XLM-RoBERTa model is pre-trained on the 100 languages with 2.5TB of filtered CommonCrawl data. Moreover, [35] finetuned XLM-RoBERTa models on named-entity-recognition (NER) task with multilingual datasets. However, most existing works use one of …","url":["https://arxiv.org/pdf/2205.15514"]}
{"year":"2022","title":"A Large and Diverse Arabic Corpus for Language Modeling","authors":["AR Ali, W Antoun"],"snippet":"… The Open Super-large Crawled ALMAnaCH coRpus (OSCAR) is a multi-lingual corpus, extracted from Common Crawl (CC) by performing … N-gram counts and language models from the common crawl. In Proceedings of the Language Resources …","url":["http://percipience.io/papers/14-A%20Large%20and%20Diverse%20Arabic%20Corpus%20for%20Language%20Modeling.pdf"]}
{"year":"2022","title":"A Lite Romanian BERT: ALR-BERT","authors":["DC Nicolae, RK Yadav, D Tufiş - Computers, 2022"],"snippet":"… OSCAR—OSCAR, or Open Super-large Crawled ALMAnaCH corpora, is a massive multilingual corpus derived from the Common Crawl corpus using language categorization and filtering [16]. There are approximately 11 GB of text in …","url":["https://www.mdpi.com/2073-431X/11/4/57/htm"]}
{"year":"2022","title":"A Literature Survey of Recent Advances in Chatbots. Information 2022, 13, 41","authors":["G Caldarini, S Jaf, K McGarry - 2022"],"snippet":"… This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from transformers) [39] and GPT (Generative Pre-trained Transformer), which were trained with huge language datasets, such as Wikipedia …","url":["https://www.researchgate.net/profile/Sardar-Jaf-2/publication/357839467_A_Literature_Survey_of_Recent_Advances_in_Chatbots/links/61e2ca739a753545e2d1d107/A-Literature-Survey-of-Recent-Advances-in-Chatbots.pdf"]}
{"year":"2022","title":"A mechanism for personalized Automatic Speech Recognition for less frequently spoken languages: the Greek case","authors":["P Antoniadis, E Tsardoulias, A Symeonidis"],"snippet":"Automatic Speech Recognition (ASR) has become increasingly popular since it significantly simplifies human-computer interaction, providing a more intuitive way of communication. Building an accurate, general-purpose ASR system is a challenging …","url":["https://panosantoniadis.github.io/files/personalized_asr.pdf"]}
{"year":"2022","title":"A Multi-task Multi-modal Framework for Sentiment and Emotion aided Cyberbully Detection","authors":["K Maity, A Kumar, S Saha - IEEE Internet Computing, 2022"],"snippet":"… The model uses a BERT based architecture pretrained by the Wikipedia and Common Crawl, along with PMINDIA and Dakshina corpora … So to overcome this issue, we have chosen MuRIL BERT, which has been pre-trained by Wikipedia and …","url":["https://ieeexplore.ieee.org/abstract/document/9733228/"]}
{"year":"2022","title":"A Multilingual Approach to Scene Text Visual Question Answering","authors":["D Karatzas - Document Analysis Systems: 15th IAPR International …","J Brugués i Pujolràs, L Gómez i Bigordà, D Karatzas - International Workshop on …, 2022"],"snippet":"… The provided FastText embeddings are either trained on Wikipedia (Wiki) or Common Crawl (CC) corpora and are available in 157 languages Footnote 3 . We have used the monolingual embeddings trained in both Wiki and CC, and we have …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=81xwEAAAQBAJ&oi=fnd&pg=PA65&dq=commoncrawl&ots=vQu3SxHxPs&sig=P3my4WCb63x3Ok5GPvjtzpy_FcI","https://link.springer.com/chapter/10.1007/978-3-031-06555-2_5"]}
{"year":"2022","title":"A Multilingual Simplified Language News Corpus","authors":["R Hauser, J Vamvas, S Ebling, M Volk - 2nd Workshop on Tools and Resources for …, 2022"],"snippet":"Simplified language news articles are being offered by specialized web portals in several countries. The thousands of articles that have been published over the years are a valuable resource for natural language processing, especially for efforts …","url":["https://cental.uclouvain.be/readi2022/readi_2022_proceedings.pdf#page=35"]}
{"year":"2022","title":"A multistage retrieval system for health-related misinformation detection","authors":["M Fernández-Pichel, DE Losada, JC Pichel - Engineering Applications of Artificial …, 2022"],"snippet":"Web search is widely used to find online medical advice. As such, health-related information access requires retrieval algorithms capable of promoting reliable documents and filtering out unreliable ones. To this end, different types of …","url":["https://www.sciencedirect.com/science/article/pii/S0952197622002950"]}
{"year":"2022","title":"A Novel Approach to Train Diverse Types of Language Models for Health Mention Classification of Tweets","authors":["PI Khan, I Razzak, A Dengel, S Ahmed - arXiv preprint arXiv:2204.06337, 2022"],"snippet":"Health mention classification deals with the disease detection in a given text containing disease words. However, non-health and figurative use of disease words adds challenges to the task. Recently, adversarial training acting as a means of …","url":["https://arxiv.org/pdf/2204.06337"]}
{"year":"2022","title":"A Part-of-Speech Tagger for Yiddish: First Steps in Tagging the Yiddish Book Center Corpus","authors":["S Kulick, N Ryant, B Santorini, J Wallenberg - arXiv preprint arXiv:2204.01175, 2022"],"snippet":"We describe the construction and evaluation of a part-of-speech tagger for Yiddish (the first one, to the best of our knowledge). This is the first step in a larger project of automatically assigning part-of-speech tags and syntactic structure to Yiddish text for …","url":["https://arxiv.org/pdf/2204.01175"]}
{"year":"2022","title":"A Projection-Based Asymmetric Similarity Measure for Distributional Semantic Models","authors":["R Das"],"snippet":"… We apply our model to GloVe vector representations (trained on the Common Crawl 42B corpus) of pairs of words taken from two databases on free association norms, and compute the correlation between the human responses and our model’s …","url":["https://riadas.github.io/files/6_804_Final_Project.pdf"]}
{"year":"2022","title":"A Review on Phishing Websites Revealing through Machine Learning","authors":["AS Sengar, A Bhola, RK Shukla, A Gupta - … on System Modeling & Advancement in …, 2021"],"snippet":"Phishing is a frequent assault in which unsuspecting people’s unique, private, and sensitive information is stolen through fake websites. The primary objective of phishing websites’consistent resource allocators isto steal unique, private, and …","url":["https://ieeexplore.ieee.org/abstract/document/9676288/"]}
{"year":"2022","title":"A Review on Text-Based Emotion Detection--Techniques, Applications, Datasets, and Future Directions","authors":["S Kusal, S Patil, J Choudrie, K Kotecha, D Vora… - arXiv preprint arXiv …, 2022"],"snippet":"Artificial Intelligence (AI) has been used for processing data to make decisions, interact with humans, and understand their feelings and emotions. With the advent of the internet, people share and express their thoughts on day-to-day activities and …","url":["https://arxiv.org/pdf/2205.03235"]}
{"year":"2022","title":"A Roadmap for Big Model","authors":["S Yuan, H Zhao, S Zhao, J Leng, Y Liang, X Wang… - arXiv preprint arXiv …, 2022"],"snippet":"… C4 is a colossal, cleaned version of Common Crawl’s web crawl corpus. It is based on Common Crawl dataset and was used to train the … CLUECorpus2020 is a high-quality Chinese pre-training corpus obtained by cleaning the Chinese part of …","url":["https://arxiv.org/pdf/2203.14101"]}
{"year":"2022","title":"A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation","authors":["Y Chen, F Wei, X Sun, Z Wu, S Lin - arXiv preprint arXiv:2203.04287, 2022"],"snippet":"This paper proposes a simple transfer learning baseline for sign language translation. Existing sign language datasets (eg PHOENIX-2014T, CSL-Daily) contain only about 10K-20K pairs of sign videos, gloss annotations and texts, which …","url":["https://arxiv.org/pdf/2203.04287"]}
{"year":"2022","title":"A Study of Commonsense Reasoning with Language Models","authors":["RMR Branco - 2021"],"snippet":"Artificial Intelligence (AI) has gone through an increasing growth in the past decades, which in the present day translates to its usage in almost every sector of society. From its inception, AI pursues the reproduction of human intelligence. Currently, AI¬equipped …","url":["https://repositorio.ul.pt/bitstream/10451/51357/1/TM_Ruben_Branco.pdf"]}
{"year":"2022","title":"A Study of Various Word Embeddings in Deep Learning","authors":["P Shah, S Shah, S Joshi - 2022 3rd International Conference for Emerging …, 2022"],"snippet":"With the number of users and reviews rising on the internet, and the liberty was given to post anything on it, it is still a major issue to identify the sentiment of the reviews. The traditional machine learning models are taking a lot of computational …","url":["https://ieeexplore.ieee.org/abstract/document/9824963/"]}
{"year":"2022","title":"A Study on Chinese-English Machine Translation Based on Transfer Learning and Neural Networks","authors":["C Li - Wireless Communications and Mobile Computing, 2022"],"snippet":"The existing Chinese-English machine translation has problems such as inaccurate word translation and difficult translation of long sentences. To this end, this paper proposes a new machine translation model based on bidirectional Chinese-English …","url":["https://www.hindawi.com/journals/wcmc/2022/8282164/"]}
{"year":"2022","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective","authors":["E Al-Hossami, S Shaikh - arXiv preprint arXiv:2202.04847, 2022"],"snippet":"In this survey paper, we overview major deep learning methods used in Natural Language Processing (NLP) and source code over the last 35 years. Next, we present a survey of the applications of Artificial Intelligence (AI) for source code, also …","url":["https://arxiv.org/pdf/2202.04847"]}
{"year":"2022","title":"A Survey on Gender Bias in Natural Language Processing","authors":["K Stanczak, I Augenstein - arXiv preprint arXiv:2112.14168, 2021"],"snippet":"Language can be used as a means of reproducing and enforcing harmful stereotypes and biases and has been analysed as such in numerous research. In this paper, we present a survey of 304 papers on gender bias in natural language …","url":["https://arxiv.org/pdf/2112.14168"]}
{"year":"2022","title":"A Survey on NLP resources, tools and techniques for Marathi Language Processing","authors":["P Lahoti, N Mittal, G Singh - Transactions on Asian and Low-Resource Language …, 2022"],"snippet":"… using goclassy architecture through language classiication and iltering the Common Crawl2 corpus. The current version of OSCAR corpus … CC-Net corpus applied an automatic pipeline to extract massive high-quality monolingual text from …","url":["https://dl.acm.org/doi/abs/10.1145/3548457"]}
{"year":"2022","title":"A Survey on Semantics in Automated Data Science","authors":["U Khurana, K Srinivas, H Samulowitz - arXiv preprint arXiv:2205.08018, 2022"],"snippet":"Data Scientists leverage common sense reasoning and domain knowledge to understand and enrich data for building predictive models. In recent years, we have witnessed a surge in tools and techniques for {\\em automated machine learning} …","url":["https://arxiv.org/pdf/2205.08018"]}
{"year":"2022","title":"A survey on text classification: Practical perspectives on the Italian language","authors":["A Gasparetto, A Zangari, M Marcuzzo, A Albarelli - PloS one, 2022"],"snippet":"Text Classification methods have been improving at an unparalleled speed in the last decade thanks to the success brought about by deep learning. Historically, state-of-the-art approaches have been developed for and benchmarked against English datasets …","url":["https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0270904"]}
{"year":"2022","title":"A Symmetric Fusion Learning Model for Detecting Visual Relations and Scene Parsing","authors":["X Liu, X Jing, Z Zheng, W Du, X Ding, Q Zhu - Scientific Programming, 2022"],"snippet":"… We employ a pretrained word vector fastText trained on Common Crawl [38] to implement our purpose. Unlike the word vector models that ignore the morphological features inside words, the fastText model utilizes a bag of n-grams to …","url":["https://www.hindawi.com/journals/sp/2022/5985392/"]}
{"year":"2022","title":"A Systematic Review on Machine Learning and Deep Learning Models for Electronic Information Security in Mobile Networks","authors":["C Gupta, I Johri, K Srinivasan, YC Hu, SM Qaisar… - Sensors, 2022"],"snippet":"Today’s advancements in wireless communication technologies have resulted in a tremendous volume of data being generated. Most of our information is part of a widespread network that connects various devices across the globe. The …","url":["https://www.mdpi.com/1424-8220/22/5/2017/pdf"]}
{"year":"2022","title":"A thesis that writes itself: On the threat of AI-generated essays within academia","authors":["A Olsson, O Engelbrektsson - 2022"],"snippet":"… 3 The CommonCrawl dataset is a large dataset of web crawl data that is freely available. The dataset is made up of data from various web crawls and contains a wealth of information on the web. The dataset used by GPT-3 is 570GB. …","url":["https://www.diva-portal.org/smash/get/diva2:1669744/FULLTEXT02"]}
{"year":"2022","title":"A Tool for Study on Impact of Big Data Technologies on firm performance","authors":["C Lotfi, S Srinivasan, M Ertz, I Latrous"],"snippet":"Organizations can use big data analytics to evaluate large data volumes and collect new information. It aids in answering basic inquiries concerning business operations and performance. It also aids in the discovery of unknown patterns in massive …","url":["https://www.researchgate.net/profile/Myriam-Ertz/publication/358504492_A_Tool_for_Study_on_Impact_of_Big_Data_Technologies_on_Firm_Performance/links/62053348634ff774f4c0b07c/A-Tool-for-Study-on-Impact-of-Big-Data-Technologies-on-Firm-Performance.pdf"]}
{"year":"2022","title":"A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model","authors":["X Sun, T Ge, S Ma, J Li, F Wei, H Wang - arXiv preprint arXiv:2201.10707, 2022"],"snippet":"Synthetic data construction of Grammatical Error Correction (GEC) for non-English languages relies heavily on human-designed and language-specific rules, which produce limited error-corrected patterns. In this paper, we propose a generic and …","url":["https://arxiv.org/pdf/2201.10707"]}
{"year":"2022","title":"A User Study on Clarifying Comparative Questions","authors":["A Bondarenko, E Shirshakova, M Hagen - Proceedings of CHIIR, 2022"],"snippet":"… From the obtained entities, we selected the pairs with the highest sentence-wise co-occurrence frequencies in the Common Crawl snapshot 2014154 (eg, ‘drummer’ and ‘guitarist’ for occupation or ‘amoxicillin’ and ‘ciprofloxacin’ for antibiotics). As for …","url":["https://webis.de/downloads/publications/papers/bondarenko_2022b.pdf"]}
{"year":"2022","title":"A Warm Start and a Clean Crawled Corpus--A Recipe for Good Language Models","authors":["V Snæbjarnarson, HB Símonarson, PO Ragnarsson… - arXiv preprint arXiv …, 2022"],"snippet":"… We also demonstrate how to directly extract Icelandic text from the Common Crawl corpus in … that in mC4, which is also sourced from Common Crawl, there are 107 labelled languages, with … We believe that by using text extracted from the …","url":["https://arxiv.org/pdf/2201.05601"]}
{"year":"2022","title":"A Web-Scale Analysis of the Community Origins of Image Memes","authors":["D Morina, MS Bernstein - Proceedings of the ACM on Human-Computer …, 2022"],"snippet":"… So, we adopted a common revision called harmonic centrality [43], endorsed and used by the Common Crawl foundation in their web-wide web crawl snapshot [13]. We obtained harmonic centrality scores for each community from a full web crawl by …","url":["https://dl.acm.org/doi/abs/10.1145/3512921"]}
{"year":"2022","title":"Accurate Dependency Parsing and Tagging of Latin","authors":["S Nehrdich, O Hellwig"],"snippet":"Having access to high-quality grammatical annotations is important for downstream tasks in NLP as well as for corpus-based research. In this paper, we describe experiments with the Latin BERT word embeddings that were recently be made …","url":["http://www.lrec-conf.org/proceedings/lrec2022/workshops/LT4HALA/pdf/2022.lt4hala2022-1.3.pdf"]}
{"year":"2022","title":"Ad creative generation using reinforced generative adversarial network","authors":["S Terzioğlu, KN Çoğalmış, A Bulut - Electronic Commerce Research, 2022"],"snippet":"… T5 and PEGASUS models were trained on 750 GB of English-language text from the public Common Crawl web scrape. BART model was trained on the CNN/Daily Mail dataset, which contains over 300k unique news articles from CNN and the Daily …","url":["https://link.springer.com/article/10.1007/s10660-022-09564-6"]}
{"year":"2022","title":"Adam mickiewicz university's english-hausa submissions to the wmt 2021 news translation task","authors":["A Nowakowski, T Dwojak - Proceedings of the Sixth Conference on Machine …, 2021"],"snippet":"This paper presents the Adam Mickiewicz University’s (AMU) submissions to the WMT 2021 News Translation Task. The submissions focus on the English↔ Hausa translation directions, which is a low-resource translation scenario between distant …","url":["https://aclanthology.org/2021.wmt-1.14.pdf"]}
{"year":"2022","title":"Adapting Automatic Speech Recognition to the Radiology Domain for a Less-Resourced Language: The Case of Latvian","authors":["N Gruzitis, R Dargis, VJ Lasmanis, G Garkaje, D Gosko - Intelligent Sustainable …, 2022"],"snippet":"Automatic speech recognition (ASR) is becoming available to more and more languages. Training a quality ASR system typically requires significant amount of language resources—representative speech and text corpora. Recent advances in …","url":["https://link.springer.com/chapter/10.1007/978-981-16-6309-3_27"]}
{"year":"2022","title":"Adversarial Cross-domain Community Question Retrieval","authors":["A Guo, X Li, N Pang, X Zhao - Transactions on Asian and Low-Resource Language …, 2022"],"snippet":"… In practice, we choose the embedding set (Common Crawl, glove.840B.300d), which contains 840B tokens and 2.2M vocabulary. The word embeddings are of 300 dimensions. The word embedding is updated during the training process. …","url":["https://dl.acm.org/doi/abs/10.1145/3487291"]}
{"year":"2022","title":"AI Personification: Estimating the Personality of Language Models","authors":["SR Karra, S Nguyen, T Tulabandhula - arXiv preprint arXiv:2204.12000, 2022"],"snippet":"… GPT-3 was pretrained on an open-source dataset called Common Crawl, and other text corpora from sources such as Wikipedia (a popular online encyclopedia). TransformerXL: TransformerXL is a transformer-based language model capable of …","url":["https://arxiv.org/pdf/2204.12000"]}
{"year":"2022","title":"AI WITH ALIEN CONTENT AND ALIEN METASEMANTICS","authors":["H Cappelen, J Dever"],"snippet":"… by the Common Crawl archiving service. Give the role that the Common Crawl database plays in the training of GPT-3, maybe the Common Crawl … in some way we can’t fully comprehend) by the way in which Common Crawl extracts and stores …","url":["https://hermancappelen.net/docs/AIAlienMetasemantics.pdf"]}
{"year":"2022","title":"AI-based Structured Web Data Extraction","authors":["J Joneš - 2022"],"snippet":"In this thesis, we explore current approaches for automatic web data extraction, define their limitations, and aim to overcome them. We propose a deep learning model to extract structured data from graph and visual representations of web pages …","url":["https://dspace.cuni.cz/bitstream/handle/20.500.11956/174143/120418601.pdf?sequence=1"]}
{"year":"2022","title":"AI-supported Natural Language Processing in project management–capabilities and research agenda","authors":["H NUHN, A OSWALD, A FLORE, R LANG"],"snippet":"AI-based natural language processing (NLP) models show remarkable performance in tasks like question answering or text generation in general. We argue that recent NLP-AI models will play a major role in the transformation of our societies, an …","url":["https://www.researchgate.net/profile/Helge-Nuhn/publication/361439333_AI-supported_Natural_Language_Processing_in_project_management_-capabilities_and_research_agenda/links/62b1979189e4f1160c8fde4d/AI-supported-Natural-Language-Processing-in-project-management-capabilities-and-research-agenda.pdf"]}
{"year":"2022","title":"An Approach to Generate Realistic HTTP Parameters for Application Layer Deception","authors":["M Sahin, C Hébert, R Cabrera Lozoya - International Conference on Applied …, 2022"],"snippet":"… While the names of the honey HTML elements are ideally chosen by the user, authors also implement a suggestion tool based on a Markov model of URLs gathered from the Common Crawl [2] dataset. However, the paper does not provide …","url":["https://link.springer.com/chapter/10.1007/978-3-031-09234-3_17"]}
{"year":"2022","title":"An Empirical Study on the Fairness of Pre-trained Word Embeddings","authors":["E Sesari, M Hort, F Sarro - 2022"],"snippet":"… from 42 billion and 840 billion tokens of Common Crawl corpus. Pre-trained embeddings trained on … + UMBCWeb Base + statmt.org News and 600 billion tokens from Common Crawl. … Also according to these data, we can infer that …","url":["https://discovery.ucl.ac.uk/id/eprint/10149529/1/ACL___Word_Embedding_Bias.pdf"]}
{"year":"2022","title":"An Ensemble Approach to Acronym Extraction using Transformers","authors":["P Sharma, H Saadany, L Zilio, D Kanojia, C Orasan - 2022"],"snippet":"… 2019) is a multilingual contextualised Language Model (LM) pre-trained on filtered CommonCrawl data from 100+ languages. Each … feed-forward hidden states, 8 heads; and is pre-trained on CommonCrawl data in over 100 languages …","url":["https://dipteshkanojia.github.io/files/sdu-ST-aaai-2022-acronym.pdf"]}
{"year":"2022","title":"An ensemble multilingual model for toxic comment classification","authors":["G Xie - … Conference on Algorithms, Microchips and Network …, 2022"],"snippet":"… Unlabeled text in 100 languages is extracted from CommonCrawl datasets, totaling 2.5TB of text. It is trained in a RoBERTa fashion, that is, only using the MLM objective. XLM-R is a transformer-based multilingual masked language model pre-trained …","url":["https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12176/121761P/An-ensemble-multilingual-model-for-toxic-comment-classification/10.1117/12.2636419.full"]}
{"year":"2022","title":"An integrated data framework for policy guidance during the coronavirus pandemic: Towards real-time decision support for economic policymakers","authors":["JO Dörr, J Kinne, D Lenz, G Licht, P Winker - PloS one, 2022"],"snippet":"… Specifically, XLM-RoBERTa has been trained on more than two terabyte of filtered common-crawl data [39]. It has acquired its basic language understanding using the masked language model approach [40], ie given a sequence of text—eg a …","url":["https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0263898"]}
{"year":"2022","title":"An Interactive Visual Demo of Bias Mitigation Techniques for Word Representations From a Geometric Perspective","authors":["A Rathore, S Dev, V Srikumar, JM Phillips, Y Zheng… - NeurIPS 2021 Competitions …, 2022"],"snippet":"Abstract Language representations are known to encode and propagate biases, ie, stereotypical associations between words or groups of words that may cause representational harm. In this demo, we utilize interactive visualization to increase …","url":["https://proceedings.mlr.press/v176/rathore22a/rathore22a.pdf"]}
{"year":"2022","title":"An Interpretable Word Sense Classifier for Human Explainable Chatbot","authors":["RK Yadav, L Jiao, OC Granmo, M Goodwin - International Conference on Agents and …, 2022"],"snippet":"… The three models employ static embedding obtained from two sources, CommonCrawl and Base, and are trained using 1 layer neural network … This is because of the enriched information in CommonCrawl in comparison to Base word …","url":["https://link.springer.com/chapter/10.1007/978-3-031-10161-8_13"]}
{"year":"2022","title":"Analysis and Application of Language Models to Human-Generated Textual Content","authors":["M Di Giovanni - 2022"],"snippet":"… The authors trained the model on corpora of different sizes; the larger includes 42 billion tokens of web data from Common Crawl, tokenized and lowercased. A clear correlation between corpus size and performance is observed for syntactic tasks but …","url":["http://amsdottorato.unibo.it/10057/1/Tesi%20con%20Frontespizio.pdf"]}
{"year":"2022","title":"Analysis of Scientific Literature of LDOW Workshops: A Scientometric and NLP approach","authors":["S Shekarpour"],"snippet":"This paper contributes to compiling and publishing a structured dataset from the scientific literature of the Linked Data on the Web (LDOW) workshop series. This workshop was the primary venue for publishing the frontier topics related to …","url":["https://www.researchgate.net/profile/Enayat-Rajabi/publication/358973156_Analysis_of_Scientific_Literature_of_LDOW_Workshops_A_Scientometric_and_NLP_approach/links/62201e29c4c4fa27cd236da6/Analysis-of-Scientific-Literature-of-LDOW-Workshops-A-Scientometric-and-NLP-approach.pdf"]}
{"year":"2022","title":"Analysis of the Full-Size Russian Corpus of Internet Drug Reviews with Complex NER Labeling Using Deep Learning Neural Networks and Language Models","authors":["A Sboev, S Sboeva, I Moloshnikov, A Gryaznov… - Applied Sciences, 2022"],"snippet":"The paper presents the full-size Russian corpus of Internet users’ reviews on medicines with complex named entity recognition (NER) labeling of pharmaceutically relevant entities. We evaluate the accuracy levels reached on this …","url":["https://www.mdpi.com/2076-3417/12/1/491/pdf"]}
{"year":"2022","title":"Analysis of Word-level Embeddings for Indic Languages on AI4Bharat-IndicNLP Corpora","authors":["D Goswami, S Malviya, R Mishra, US Tiwary - 2021 IEEE 8th Uttar Pradesh Section …, 2021"],"snippet":"This paper presents the analysis of non-contextual word embeddings trained on AI4Bharat-IndicNLP corpus containing 2.7 billion words covering 10 Indian languages. We share the pre-trained embeddings for research and development in …","url":["https://ieeexplore.ieee.org/abstract/document/9667615/"]}
{"year":"2022","title":"Analyzing Gender Representation in Multilingual Models","authors":["H Gonen, S Ravfogel, Y Goldberg - arXiv preprint arXiv:2204.09168, 2022"],"snippet":"Multilingual language models were shown to allow for nontrivial transfer across scripts and languages. In this work, we study the structure of the internal representations that enable this transfer. We focus on the representation of gender …","url":["https://arxiv.org/pdf/2204.09168"]}
{"year":"2022","title":"Analyzing the Mono-and Cross-Lingual Pretraining Dynamics of Multilingual Language Models","authors":["T Blevins, H Gonen, L Zettlemoyer - arXiv preprint arXiv:2205.11758, 2022"],"snippet":"… This dataset consists of filtered Common Crawl data for 100 languages, with a wide range of data quantities ranging from 0.1 GiB for languages like Xhosa and Scottish Gaelic to over 300 Gib for English. As with XLM-R, we train on CC100 for 1.5M …","url":["https://arxiv.org/pdf/2205.11758"]}
{"year":"2022","title":"ANNA: Enhanced Language Representation for Question Answering","authors":["C Jun, H Jang, M Sim, H Kim, J Choi, K Min, K Bae - arXiv preprint arXiv:2203.14507, 2022"],"snippet":"… of 127,490 wordpieces that are extracted from the English Common Crawl corpus (Raffel et al.… corpora such as a ColossalCleaned version of Common Crawl (C4) corpus (Raffel et al.… for the deletion of documents written in non-English words in …","url":["https://arxiv.org/pdf/2203.14507"]}
{"year":"2022","title":"Annotation Projection-based Dependency Parser Development for Nepali","authors":["P Rai, S Chatterji - Transactions on Asian and Low-Resource Language …, 2022"],"snippet":"Building computational resources and tools for the under-resourced languages is strenuous for any Natural Language Processing (NLP) task. This paper presents the first dependency parser for an under-resourced Indian language, Nepali. A …","url":["https://dl.acm.org/doi/pdf/10.1145/3542696"]}
{"year":"2022","title":"APIRO: A Framework for Automated Security Tools API Recommendation","authors":["ZT Sworna, C Islam, MA Babar - arXiv preprint arXiv:2201.07959, 2022"],"snippet":"… Moreover, for augmenting data with the same Aug and same action, we propose to use a wide variety of data sources, ie, thesaurus and embedding (eg, PPDB, GloVe Common Crawl). Hence, a set of 𝛼 number of Data Augmentation …","url":["https://arxiv.org/pdf/2201.07959"]}
{"year":"2022","title":"Application of neural network language models based on distributive semantics for ontological modeling of the domain","authors":["MG Shishaev, VV Dikovitsky, VK Pimeshkov - Journal of Physics: Conference Series, 2022"],"snippet":"… Common Crawl'. The network architecture included 5 layers with a 631-500-350-250-1503 funnel, the ReLU activation function was used, and training was carried out in 15 epochs. The results of testing the resulting model on verification data are shown in …","url":["https://iopscience.iop.org/article/10.1088/1742-6596/2182/1/012033/pdf"]}
{"year":"2022","title":"Approaching what and how people with mental disorders communicate in social media–Introducing a multi-channel representation","authors":["ME Aragón, AP López-Monroy, LC González… - Neural Computing and …, 2022"],"snippet":"… This model was pre-trained with high-dimensional corpora from Twitter, Common Crawl, and Wikipedia. GloVe presents a lower dimensionality in their word vectors, ranging between 50 and 300 dimensions. GloVe pre-trained with information from …","url":["https://link.springer.com/article/10.1007/s00521-022-07569-8"]}
{"year":"2022","title":"Arabic Fake News Detection: A Fact Checking Based Deep Learning Approach","authors":["F Harrag, MK Djahli - Transactions on Asian and Low-Resource Language …, 2022"],"snippet":"Fake news stories can polarize society, particularly during political events. They undermine confidence in the media in general. Current NLP systems are still lacking the ability to properly interpret and classify Arabic fake news. Given the high stakes …","url":["https://dl.acm.org/doi/abs/10.1145/3501401"]}
{"year":"2022","title":"AraNPCC: The Arabic Newspaper COVID-19 Corpus","authors":["A Al-Thubaity, S Alkhereyf, A Bahanshal - Politics"],"snippet":"This paper introduces a corpus for Arabic newspapers during COVID-19: AraNPCC. The AraNPCC corpus covers 2019 until 2021 via automatically-collected data from 12 Arab countries. It comprises more than 2 billion words and 7.2 million texts …","url":["http://www.lrec-conf.org/proceedings/lrec2022/workshops/OSACT/pdf/2022.osact-1.4.pdf"]}
{"year":"2022","title":"ArSphere: Arabic word vectors embedded in a polar sphere","authors":["S Rizkallah, AF Atiya, S Shaheen, HED Mahgoub - International Journal of Speech …, 2022"],"snippet":"Word embeddings mean the mapping of words into vectors in an N-dimensional space. ArSphere: is an approach that designs word embeddings for the Arabic language. This approach overcomes one of the shortcomings of word embeddings (for …","url":["https://link.springer.com/article/10.1007/s10772-022-09966-9"]}
{"year":"2022","title":"Artificial intelligence and the affective labour of understanding: The intimate moderation of a language model","authors":["C Perrotta, N Selwyn, C Ewin - New Media & Society, 2022"],"snippet":"Interest in artificial intelligence (AI) language models has grown considerably following the release of ‘generative pre-trained transformer’ (GPT). Framing AI as an extractive technology, this article details how GPT harnesses human labour and …","url":["https://journals.sagepub.com/doi/abs/10.1177/14614448221075296"]}
{"year":"2022","title":"Artificial Intelligence, Deepfakes, and Disinformation","authors":["TC HELMUS - 2022"],"snippet":"CAI Content Authenticity Initiative GAN generative adversarial network GPT-3 Generative Pre-Trained Transformer 3 OSINT open-source intelligence technique in American society: increasing disagreement in evaluations of facts and analytical …","url":["https://www.rand.org/content/dam/rand/pubs/perspectives/PEA1000/PEA1043-1/RAND_PEA1043-1.pdf"]}
{"year":"2022","title":"Artificial Intelligence: A Medium that Hides Its Nature","authors":["A Huxor - Artificial Intelligence and Its Discontents, 2022"],"snippet":"… This technology builds a vast language model based on a corpus of many written texts, drawn from the Internet which themselves previously created by humans, including texts garnered by crawling the web and made available at Common Crawl …","url":["https://link.springer.com/chapter/10.1007/978-3-030-88615-8_6"]}
{"year":"2022","title":"Assessment of Massively Multilingual Sentiment Classifiers","authors":["K Rajda, Ł Augustyniak, P Gramacki, M Gruza… - arXiv preprint arXiv …, 2022"],"snippet":"… We also included models trained on multilingual corpus like Wikipedia (Wiki) or Common Crawl (CC) as well as models trained with the use of parallel datasets. Selected models differ in size - from LASER with 52M parameters to LaBSE with …","url":["https://arxiv.org/pdf/2204.04937"]}
{"year":"2022","title":"Assisting Children with Special Needs in Their Daily Interaction with Other People","authors":["M Allouche - 2022"],"snippet":"Children and adults with special needs may find it difficult to recognize danger and threats as well as socially complex situations. They are thus at risk of becoming victims of exploitation and violence. In addition, they may find themselves …","url":["http://azariaa.com/Content/Theses/PhD_MeravA.pdf"]}
{"year":"2022","title":"Attack or Block? Repertoires of Digital Censorship in Autocracies","authors":["L Kawerau, NB Weidmann, A Dainotti - Journal of Information Technology & Politics, 2022"],"snippet":"… When a given host is visited by CommonCrawl multiple times and has the same IP address in two adjacent observations, we assume that the host had the same address on every day between those two observations. We call this period a “stable …","url":["https://www.tandfonline.com/doi/abs/10.1080/19331681.2022.2037118"]}
{"year":"2022","title":"Attention Fusion: a light yet efficient late fusion mechanism for task adaptation in NLU","authors":["J Cao, CS Prakash, W Hamza, AI Amazon"],"snippet":"Fine-tuning a pre-trained language model using annotated data has become the de-facto standard for adapting general-purpose pretrained models like BERT to downstream tasks. However, given the trend of larger pretrained models, fine-tuning these …","url":["https://assets.amazon.science/99/58/7342d03044d7a4f83324191c4bd3/attention-fusion-a-light-yet-efficient-late-fusion-mechanism-for-task-adaptation-in-nlu.pdf"]}
{"year":"2022","title":"Attention-Based Bi-LSTM Network for Abusive Language Detection","authors":["KB Nelatoori, HB Kommanti - IETE Journal of Research, 2022"],"snippet":"… These GloVe embeddings are trained on Common Crawl which contains 42 billion tokens and a Twitter corpus with 2 billion tweets. GloVe trained on Twitter contains 25, 50, 100 and 200-dimensional word embeddings with 1.2 million …","url":["https://www.tandfonline.com/doi/abs/10.1080/03772063.2022.2034534"]}
{"year":"2022","title":"Automated Fake News Detection using cross-checking with reliable sources","authors":["Z Ghadiri, M Ranjbar, F Ghanbarnejad, S Raeisi - arXiv preprint arXiv:2201.00083, 2022"],"snippet":"… We trained this word embedding on Common Crawl and Wikipedia [9]. For two semantically close tweets, their embedding vector should be relatively close. To find the distance between two tweets, we use the cosine between their corresponding vectors. …","url":["https://arxiv.org/pdf/2201.00083"]}
{"year":"2022","title":"Automated Identification of Toxic Code Reviews: How Far Can We Go?","authors":["J Sarker, AK Turzo, M Dong, A Bosu - arXiv preprint arXiv:2202.13056, 2022"],"snippet":"Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express …","url":["https://arxiv.org/pdf/2202.13056"]}
{"year":"2022","title":"Automated text-level semantic markers of Alzheimer's disease","authors":["C Sanz, F Carrillo, A Slachevsky, G Forno… - Alzheimer's & Dementia …, 2022"],"snippet":"Methods Combining statistical and machine learning analyses of natural speech data, we aimed to discriminate ADD patients from healthy controls (HCs) based on automated measures of domains typically affected in ADD: semantic granularity (coarseness …","url":["https://alz-journals.onlinelibrary.wiley.com/doi/pdf/10.1002/dad2.12276"]}
{"year":"2022","title":"Automatic and Manual Detection of Generated News: Case Study, Limitations and Challenges","authors":["J Bogaert, A Descampe, MC de Marneffe, FX Standaert - 2022"],"snippet":"In this paper, we study the exploitation of language generation models for disinformation purposes from two viewpoints. Quantitatively, we argue that language models hardly deal with domain adaptation (ie, the ability to generate text on topics …","url":["https://perso.uclouvain.be/fstandae/PUBLIS/277.pdf"]}
{"year":"2022","title":"Automatic Generation and Detection of Motivational-Interviewing-Style Reflections for Smoking Cessation Therapeutic Conversations Using Transformer-based …","authors":["I Ahmed - 2022"],"snippet":"… The GPT-3 model [9] has 175 billion parameters and is trained on 499GB of text data from multiple datasets: Common Crawl from 2016-2019 [48], WebText, Books1 and Books2 (collection of books and movie script text data) [49], and English-language …","url":["https://www.eecg.utoronto.ca/~jayar/download/ahmed_imtihan_202201_masc_thesis.pdf"]}
{"year":"2022","title":"Automatic Pull Request Title Generation","authors":["T Zhang, IC Irsan, F Thung, DG Han, D Lo, L Jiang - arXiv preprint arXiv:2206.10430, 2022"],"snippet":"… Unlike BART, T5 was pre-trained with the Colossal Clean Crawled Corpus (C4) dataset, which consists of 750GB of English text from the public Common Crawl web scrape. T5 was reported to achieve state-of-the-art results on many benchmarks …","url":["https://arxiv.org/pdf/2206.10430"]}
{"year":"2022","title":"Automatic question answering for multiple stakeholders, the epidemic question answering dataset","authors":["TR Goodwin, D Demner-Fushman, K Lo, LL Wang… - Scientific Data, 2022"],"snippet":"… In the end-to-end collection, we also included 265 Reddit threads as well as a subset of the CommonCrawl News Crawl (CCNC) from … e2e/ccns_trec/ contains 114,645 JSON files, each corresponding to an HTML-parsed website included in the …","url":["https://www.nature.com/articles/s41597-022-01533-w"]}
{"year":"2022","title":"Automatic Text Summarization for Moroccan Arabic Dialect Using an Artificial Intelligence Approach","authors":["I Benelallam - … Intelligence: 7th International Conference, CBI 2022 …"],"snippet":"A major advantage of artificial intelligence is its ability to automatically perform tasks at a human-like level quickly; this is needed in many fields, and more particularly in Automatic Text Summarization (ATS). Several advances related to this technique …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=mZxvEAAAQBAJ&oi=fnd&pg=PA158&dq=commoncrawl&ots=R3bP_X-osy&sig=StHLey4LgAHWwHnofHZYygV5YzA"]}
{"year":"2022","title":"Automatic text summarization of konkani texts using pre-trained word embeddings and deep learning.","authors":["J D'Silva, U Sharma - International Journal of Electrical & Computer …, 2022"],"snippet":"… The models are trained on documents from Common Crawl and Wikipedia to obtain embeddings for words in the languages offered [12], [13]. With Konkani, the number of Konkani language articles is significantly low on Wikipedia compared to …","url":["http://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=20888708&AN=154204023&h=YgeWf2sTl1O%2FMKtu7OeHz%2FHcc2De9w9RwaYA14TSEkzxauPv0vrRqD%2Bdn4LVWiwHi7XNZke8S5Wuv%2BL1wGg8hQ%3D%3D&crl=c"]}
{"year":"2022","title":"Automating Idea Unit Segmentation and Alignment for Assessing Reading Comprehension via Summary Protocol Analysis","authors":["M Gecchele, H Yamada, T Tokunaga, Y Sawaki…"],"snippet":"In this paper, we approach summary evaluation from an applied linguistics (AL) point of view. We provide computational tools to AL researchers to simplify the process of Idea Unit (IU) segmentation. The IU is a segmentation unit that can …","url":["http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.498.pdf"]}
{"year":"2022","title":"Balancing between holistic and cumulative sentiment classification","authors":["P Agathangelou, I Katakis - Online Social Networks and Media, 2022"],"snippet":"Sentiment analysis is a fast-accelerating discipline that develops algorithms for knowledge discovery from opinionated content. The challenges however, when it comes to analyzing user reviews are plenty. Bad-quality, informal use of language …","url":["https://www.sciencedirect.com/science/article/pii/S2468696422000039"]}
{"year":"2022","title":"Balancing novelty and appropriateness leads to creative associations in children","authors":["C Rastelli, A Greco, N De Pisapia, C Finocchiaro - 2022"],"snippet":"Creative cognition is conceived as the process whereby something novel and appropriate is generated. However, the contribution of novelty and appropriateness to creativity is far from being understood, especially during developmental age. Here …","url":["https://psyarxiv.com/k5pz3/download?format=pdf"]}
{"year":"2022","title":"BanglaNLG: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in Bangla","authors":["A Bhattacharjee, T Hasan, WU Ahmad, R Shahriyar - arXiv preprint arXiv:2205.11081, 2022"],"snippet":"This work presents BanglaNLG, a comprehensive benchmark for evaluating natural language generation (NLG) models in Bangla, a widely spoken yet low-resource language in the web domain. We aggregate three challenging conditional text …","url":["https://arxiv.org/pdf/2205.11081"]}
{"year":"2022","title":"Based on billions of words on the internet,< scp> people</scp>=< scp> men</scp","authors":["AH Bailey, A Williams, A Cimpian"],"snippet":"Recent advances have made it possible to precisely measure the extent to which any two words are used in similar contexts. In turn, this measure of similarity in linguistic context also captures the extent to which the concepts being denoted are …","url":["https://ouci.dntb.gov.ua/en/works/733X5j07/"]}
{"year":"2022","title":"Being Polite: Modeling Politeness Variation in a Personalized Dialog Agent","authors":["M Firdaus, A Shandilya, A Ekbal, P Bhattacharyya - IEEE Transactions on …, 2022"],"snippet":"… The word embedding dimension is set as 300 for which we use FastText Embeddings [52] pretrained on the common crawl. We apply 0.5 dropout rate [53] to the layer before softmax. All the models are trained on a single NVIDIA 1080Ti GPU. …","url":["https://ieeexplore.ieee.org/abstract/document/9801557/"]}
{"year":"2022","title":"Benchmarking Library Recognition in Tweets","authors":["T Zhang, DP Chandrasekaran, F Thung, D Lo - 2022"],"snippet":"… News dataset); OpenWebText [2] (an open-source recreation of the WebText corpus), and Stories [40] (a subset of CommonCrawl data that … Compared to BERT, XLNet also involved more pre-training data, such as Common Crawl [1]. XLNet …","url":["https://happygirlzt.com/files/icpc22.pdf"]}
{"year":"2022","title":"Benchmarking Transformers-based models on French Spoken Language Understanding tasks","authors":["O Cattan, S Ghannay, C Servan, S Rosset - INTERSPEECH 2022, 2022"],"snippet":"… Most models are pre-trained on the content of either Wikipedia, the web pages gathered by Common Crawl (CC) or a combination of diverse corpora. This represents a few gigabytes to several hundred or even several terabytes of text. The …","url":["https://hal.archives-ouvertes.fr/hal-03715340/document"]}
{"year":"2022","title":"Benefits from Variational Regularization in Language Models","authors":["C Ferner, S Wegenkittl - Machine Learning and Knowledge Extraction, 2022"],"snippet":"… WMT19 contains data from news commentaries, Wiki titles, Europarl, ParaCrawl, and Common Crawl corpora. The data are tokenized using … Both BERT and MiniBERT were trained on a much larger collection of datasets (Wikipedia …","url":["https://www.mdpi.com/2504-4990/4/2/25/pdf?version=1654773213"]}
{"year":"2022","title":"Bengali POS Tagging Using Bi-LSTM with Word Embedding and Character-Level Embedding","authors":["K Bose, K Sarkar - COMSYS 2021"],"snippet":"Part-of-speech tagging (POS) is an important and very fundamental process in natural language processing (NLP). POS tagging is required as a preprocessing task in many types of linguistic research such as named entity recognition (NER) …","url":["https://link.springer.com/content/pdf/10.1007/978-981-19-0105-8.pdf#page=563"]}
{"year":"2022","title":"BERT based Multiple Parallel Co-attention Model for Visual Question Answering","authors":["M Dias, H Aloj, N Ninan, D Koshti - 2022 6th International Conference on Intelligent …, 2022"],"snippet":"Humans can easily interpret visual and textual content whereas for a machine this is a challenging task. Visual question answering is a well-known problem in the field of computer vision and NLP where an image and a question related to the image are …","url":["https://ieeexplore.ieee.org/abstract/document/9788253/"]}
{"year":"2022","title":"BERTuit: Understanding Spanish language in Twitter through a native transformer","authors":["J Huertas-Tato, A Martin, D Camacho - arXiv preprint arXiv:2204.03465, 2022"],"snippet":"The appearance of complex attention-based language models such as BERT, Roberta or GPT-3 has allowed to address highly complex tasks in a plethora of scenarios. However, when applied to specific domains, these models encounter …","url":["https://arxiv.org/pdf/2204.03465"]}
{"year":"2022","title":"Bilingual attention based neural machine translation","authors":["L Kang, S He, M Wang, F Long, J Su - Applied Intelligence, 2022"],"snippet":"… Our contrast models mainly include (1) the winning system in WMT2014 [3], a phrase-based system of which language models were trained on the Common Crawl corpus; (2) GNMT [16] that is the most competitive RNN-based NMT model to …","url":["https://link.springer.com/article/10.1007/s10489-022-03563-8"]}
{"year":"2022","title":"Bilingual Tabular Inference: A Case Study on Indic Languages","authors":["C Agarwal, V Gupta, A Kunchukuttan, M Shrivastava"],"snippet":"Existing research on Tabular Natural Language Inference (TNLI) exclusively examines the task in a monolingual setting where the tabular premise and hypothesis are in the same language. However, due to the uneven distribution of …","url":["https://vgupta123.github.io/docs/NAACL2022.pdf"]}
{"year":"2022","title":"Biomedical Semantic Textual Similarity: Evaluation of Sentence Representations Enhanced with Principal Component Reduction and Word Frequency Weighting","authors":["K Kantor, M Morzy - International Conference on Artificial Intelligence in …, 2022"],"snippet":"… We examine 50-dimension and 300-dimension GloVe models built on Wikipedia corpus and 300-dimension models trained on the Common Crawl. We choose the BERT model [7] trained on BookCorpus and English Wikipedia for neural-based …","url":["https://link.springer.com/chapter/10.1007/978-3-031-09342-5_39"]}
{"year":"2022","title":"Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages","authors":["K Heffernan, O Çelebi, H Schwenk - arXiv preprint arXiv:2205.12654, 2022"],"snippet":"… Our monolingual data comes mostly from Common Crawl and other public sources like ParaCrawl5, and some additional targeted … Our main source of monolingual data was Common Crawl, complemented and targeted crawling (see …","url":["https://arxiv.org/pdf/2205.12654"]}
{"year":"2022","title":"Boosting named entity recognition in domain-specific and low-resource settings","authors":["S Najem-Meyer - 2022"],"snippet":"Recent researches in natural language processing have leveraged attention-based models to produce state-of-the-art results in a wide variety of tasks. Using transfer learning, generic models like BERT can be fine-tuned for domain-specific tasks …","url":["https://infoscience.epfl.ch/record/291236/files/2021SemProj___Boosting_NER%20%281%29.pdf"]}
{"year":"2022","title":"Bridging Cross-Lingual Gaps During Leveraging the Multilingual Sequence-to-Sequence Pretraining for Text Generation","authors":["C Zan, L Ding, L Shen, Y Cao, W Liu, D Tao - arXiv preprint arXiv:2204.07834, 2022"],"snippet":"For multilingual sequence-to-sequence pretrained language models (multilingual Seq2Seq PLMs), eg mBART, the self-supervised pretraining task is trained on a wide range of monolingual languages, eg 25 languages from commoncrawl, while …","url":["https://arxiv.org/pdf/2204.07834"]}
{"year":"2022","title":"Building a comprehensive NER model for Satellite Domain","authors":["P Maurya, O Jafari, B Thatte, C Ingram, P Nagarkar - SN Computer Science, 2022"],"snippet":"… A model trained on the OntoNotes 5 and GloVe Common Crawl datasets. All of these models can recognize types of entities such as PERSON, NORP, FAC, ORG, GPE, LOC, PRODUCT, EVENT, WORK_OF_ART, LAW, LANGUAGE, DATE, TIME …","url":["https://link.springer.com/article/10.1007/s42979-022-01085-1"]}
{"year":"2022","title":"Building a Connected Web Observatory","authors":["T Tiropanis, X Wang, R Tinati, W Hall"],"snippet":"… For the SUWO, we are harvesting a variety of Web feeds, which include real-time streams from social networking and user-generated content sites (Twitter, Facebook, YouTube), Web-crawled datasets using existing resources such as the ’CommonCrawl’1 …","url":["https://www.academia.edu/download/78199864/web-obs-arch-bwow14.pdf"]}
{"year":"2022","title":"Building a Multilingual Taxonomy of Olfactory Terms with Timestamps","authors":["S Menini, T Paccosi, SS Tekiroglu, S Tonelli - To"],"snippet":"Olfactory references play a crucial role in our memory and, more generally, in our experiences, since researchers have shown that smell is the sense that is most directly connected with emotions. Nevertheless, only few works in NLP have tried to …","url":["http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.429.pdf"]}
{"year":"2022","title":"Building AI Enabled Note-taking Interfaces for Patient Encounters","authors":["J Wang - 2022"],"snippet":"Current clinical note-taking approaches cannot capture the entirety of information available from patient encounters and detract from patient-clinician interactions. To address those issues we investigate the feasibility and suitability of using artificial …","url":["https://tspace.library.utoronto.ca/bitstream/1807/123369/1/Wang_Jixuan__202206_PhD_thesis.pdf"]}
{"year":"2022","title":"Building better machine learning models for rhetorical analyses: The use of rhetorical feature sets for training artificial neural network models","authors":["ZP Majdik, J Wynn - Technical Communication Quarterly, 2022"],"snippet":"In this paper, we investigate two approaches to building artificial neural network models to compare their effectiveness for accurately classifying rhetorical structures across multiple (non-binary) classes in small textual datasets. We find that the most …","url":["https://www.tandfonline.com/doi/abs/10.1080/10572252.2022.2077452"]}
{"year":"2022","title":"Building Sentiment Lexicons for Mainland Scandinavian Languages Using Machine Translation and Sentence Embeddings","authors":["P Liu, C Marco, JA Gulla"],"snippet":"This paper presents a simple but effective method to build sentiment lexicons for the three Mainland Scandinavian languages: Danish, Norwegian and Swedish. This method benefits from the English Sentiwordnet and a thesaurus in one of the target …","url":["http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.301.pdf"]}
{"year":"2022","title":"Business Document Information Extraction: Towards Practical Benchmarks","authors":["M Skalický, Š Šimsa, M Uřičář, M Šulc - arXiv preprint arXiv:2206.11229, 2022"],"snippet":"… [93] was not published, while pre-training on it was crucial for the proposed method, and the C4 dataset [64] is shared in the form of code that extracts it directly from Common Crawl. Publicly available datasets for KI(L)E from business …","url":["https://arxiv.org/pdf/2206.11229"]}
{"year":"2022","title":"Can Foundation Models Help Us Achieve Perfect Secrecy?","authors":["S Arora, C Ré - arXiv preprint arXiv:2205.13722, 2022"],"snippet":"… 4A particular challenge was obtaining an FM for Reddit, since most FMs are trained on generic Common Crawl — we thus use Grover FMs, which are pretrained only on news articles and find the performance of Grover models directly match or …","url":["https://arxiv.org/pdf/2205.13722"]}
{"year":"2022","title":"Can linguistic features extracted from geo-referenced tweets help building function classification in remote sensing?","authors":["M Häberle, EJ Hoffmann, XX Zhu - ISPRS Journal of Photogrammetry and Remote …, 2022"],"snippet":"… To add more heterogeneous language, they additionally utilized CommonCrawl 13 to train the word embedding using fastText, and words were left in upper and lower case to add some semantics and context to words (cf. Section 3.4). For our …","url":["https://www.sciencedirect.com/science/article/pii/S0924271622001058"]}
{"year":"2022","title":"Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content?","authors":["P Schramowski, C Tauchmann, K Kersting - arXiv preprint arXiv:2202.06675, 2022"],"snippet":"… The most recent and successful models, such as GPT-3 [5], CLIP [36], DALL-E [38] and other similar models, are trained on data scraped from the web, eg using CommonCrawl. The information they acquire from this data is largely uncontrolled …","url":["https://arxiv.org/pdf/2202.06675"]}
{"year":"2022","title":"Capturing Failures of Large Language Models via Human Cognitive Biases","authors":["E Jones, J Steinhardt - arXiv preprint arXiv:2202.12299, 2022"],"snippet":"Large language models generate complex, open-ended outputs: instead of outputting a single class, they can write summaries, generate dialogue, and produce working code. In order to study the reliability of these open-ended systems, we must …","url":["https://arxiv.org/pdf/2202.12299"]}
{"year":"2022","title":"Catching a Phish: Frontiers of Deep Learning-Based Anticipating Detection Engines","authors":["H Salah, H Zuhair - International Conference of Reliable Information and …, 2022"],"snippet":"In recent years, cyber-security gains high attention in the light of ethical hacking and social engineering attacks like phishing that riskily overshadow the development of social networking, e commerce, and information technology. Thus, mitigation of such …","url":["https://link.springer.com/chapter/10.1007/978-3-030-98741-1_40"]}
{"year":"2022","title":"Cedille: A large autoregressive French language model","authors":["M Müller, F Laurent - arXiv preprint arXiv:2202.03371, 2022"],"snippet":"… Experiments conducted on GPT-3 (which has been trained on 570GB of text data from Common Crawl) show that the model may generate toxic sentences even when prompted with non-toxic text [11]. Although applying filtering of training data using …","url":["https://arxiv.org/pdf/2202.03371"]}
{"year":"2022","title":"Challenges and limits of an open source approach to Artificial Intelligence","authors":["A THEBEN, L GUNDERSON, L LÓPEZ-FORÉS…"],"snippet":"… It takes the GPT model to a whole new level as it is trained on a massive number of parameters (ie 175 billions, which is over 10x the size of its predecessor) on an open source dataset called 'Common Crawl,' and other texts from OpenAI such as …","url":["https://www.staten-generaal.nl/eu/documenteu/pe_662908_challenges_and_limits_of/f=/vlkvn20v47an.pdf"]}
{"year":"2022","title":"Challenges in Measuring Bias via Open-Ended Language Generation","authors":["AF Akyürek, MY Kocyigit, S Paik, D Wijaya - arXiv preprint arXiv:2205.11601, 2022"],"snippet":"Researchers have devised numerous ways to quantify social biases vested in pretrained language models. As some language models are capable of generating coherent completions given a set of textual prompts, several prompting datasets …","url":["https://arxiv.org/pdf/2205.11601"]}
{"year":"2022","title":"Changing the Narrative Perspective: A New Language Processing Task and Machine Learning Approaches","authors":["M Chen - 2022"],"snippet":"In this dissertation I introduce the novel text processing task of changing the narrative perspective, where characters in a story are assigned a point of view that is di erent from the one originally used by the writer. The resulting shift in the narrative …","url":["https://etd.ohiolink.edu/apexprod/rws_etd/send_file/send?accession=ohiou1647655979201374&disposition=inline"]}
{"year":"2022","title":"ChapterBreak: A Challenge Dataset for Long-Range Language Models","authors":["S Sun, K Thai, M Iyyer - arXiv preprint arXiv:2204.10878, 2022"],"snippet":"While numerous architectures for long-range language models (LRLMs) have recently been proposed, a meaningful evaluation of their discourse-level language understanding capabilities has not yet followed. To this end, we introduce …","url":["https://arxiv.org/pdf/2204.10878"]}
{"year":"2022","title":"Characterization and Prediction of Mobile Tasks","authors":["Y Tian, K Zhou, D Pelleg - ACM Transactions on Information Systems (TOIS), 2022"],"snippet":"Mobile devices have become an increasingly ubiquitous part of our everyday life. We use mobile services to perform a broad range of tasks (eg, booking travel or conducting remote office work), leading to often lengthy interactions with several …","url":["https://dl.acm.org/doi/abs/10.1145/3522711"]}
{"year":"2022","title":"Characterizing accident narratives with word embeddings: Improving accuracy, richness, and generalizability","authors":["DM Goldberg - Journal of Safety Research, 2021"],"snippet":"Introduction: Ensuring occupational health and safety is an enormous concern for organizations, as accidents not only harm workers but also result in financial losses. Analysis of accident data has the potential to reveal insights that may improve …","url":["https://www.sciencedirect.com/science/article/pii/S0022437521001754"]}
{"year":"2022","title":"Characterizing and Resolving Degeneracies in Neural Autoregressive Text Generation","authors":["I Kulikov - 2022"],"snippet":"… We use the subset of WMT’19 training set consisting of news commentary v12 and common crawl resulting in slightly more than 1M and 2M training sentence pairs for Ru→En and De↔En pairs, respectively. We ne-tuned single model checkpoints …","url":["https://search.proquest.com/openview/b0268b79afd971636daec6ad90c0ec2c/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2022","title":"Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering","authors":["M Luo, K Hashimoto, S Yavuz, Z Liu, C Baral, Y Zhou - arXiv preprint arXiv …, 2022"],"snippet":"While both extractive and generative readers have been successfully applied to the Question Answering (QA) task, little attention has been paid toward the systematic comparison of them. Characterizing the strengths and weaknesses of the two …","url":["https://arxiv.org/pdf/2203.07522"]}
{"year":"2022","title":"City of Disguise: A Query Obfuscation Game on the ClueWeb","authors":["M Fröbe, NL Libera, M Hagen"],"snippet":"We present City of Disguise, a retrieval game that tests how well searchers are able to reformulate some sensitive query in a ‘Taboo’-style setup but still retrieve good results. Given one of 200 sensitive information needs and a relevant example …","url":["https://webis.de/downloads/publications/papers/froebe_2022b.pdf"]}
{"year":"2022","title":"Classification of Construction Accident Court Cases Via Natural Language Processing in Hong Kong","authors":["RYM Li, HCY Li, B Tang, WC Au - Current State of Art in Artificial Intelligence and …, 2022"],"snippet":"… The fastText system has a trained English language vector model derived from Wikipedia, Common Crawl and other sources. Every word is stored along with several different tokens, and the morphology and word representations are replaced …","url":["https://link.springer.com/chapter/10.1007/978-981-19-0737-1_5"]}
{"year":"2022","title":"Classification of German Jungian Extraversion and Introversion Texts with Assessment of Changes during the COVID-19 Pandemic","authors":["D Johannßen, C Biemann, D Scheffer"],"snippet":"The corona pandemic and countermeasures such as social distancing and lockdowns have confronted individuals with new challenges for their mental health and well-being. It can be assumed that the Jungian psychology types of extraverts …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/publications/2022-johannssen-biemann-scheffer-rapid4.pdf"]}
{"year":"2022","title":"Classification of Indian Media Titles using Deep Learning Techniques","authors":["S Kumar, DD Rajesh, S Pranesh, VNH Kollipara… - International Journal of …, 2022"],"snippet":"Automatic speech recognition is being used everywhere these days. An essential part of this is language identification. Our goal here is to identify the language of the media title, such as song names and movie titles, to help in speech recognition. The …","url":["https://www.sciencedirect.com/science/article/pii/S2666307422000109"]}
{"year":"2022","title":"Classifying Documents by Viewpoint Using Word2Vec and Support Vector Machines","authors":["J Harwell, Y Li - International Conference on Applications of Natural …, 2022"],"snippet":"Ensuring viewpoint diversity in mass media is a historical challenge and recent political events, and the ever-increased use of the Internet, have made it an increasingly critical and contentious issue. This research explores the relationship …","url":["https://link.springer.com/chapter/10.1007/978-3-031-08473-7_13"]}
{"year":"2022","title":"Classifying multi-level product categories using dynamic masking and transformer models","authors":["O Ozyegen, H Jahanshahi, M Cevik, B Bulut, D Yigit… - Journal of Data, Information …, 2022"],"snippet":"In an online shopping platform, a detailed categorization of the products greatly enhances user navigation. Online retailers also benefit from well-defined product categories as various sales and marketing operations such as special discounts and …","url":["https://link.springer.com/article/10.1007/s42488-022-00066-6"]}
{"year":"2022","title":"ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization","authors":["J Wang, F Meng, Z Lu, D Zheng, Z Li, J Qu, J Zhou - arXiv preprint arXiv:2202.05599, 2022"],"snippet":"We present ClidSum, a benchmark dataset for building cross-lingual summarization systems on dialogue documents. It consists of 67k+ dialogue documents from two subsets (ie, SAMSum and MediaSum) and 112k+ annotated summaries in different …","url":["https://arxiv.org/pdf/2202.05599"]}
{"year":"2022","title":"ClueWeb22: 10 Billion Web Documents with Rich Information","authors":["A Overwijk, C Xiong, J Callan - Proceedings of the 45th International ACM SIGIR …, 2022"],"snippet":"… One approach is to sift CommonCrawl data, eg, the C4 dataset used to pretrain T5 [10], which provides sufficient quantity, but the quality quickly becomes a concern. For example, the cleaned CommonCrawl reflects a quite weird distribution of the …","url":["https://dl.acm.org/doi/abs/10.1145/3477495.3536321"]}
{"year":"2022","title":"CM3: A Causal Masked Multimodal Model of the Internet","authors":["A Aghajanyan, B Huang, C Ross, V Karpukhin, H Xu… - arXiv preprint arXiv …, 2022"],"snippet":"… on close to a Terabyte of multi-modal simplified HTML data from the common crawl. • We present the causally masked objective, a hybrid of … all of Common Crawl and instead opt into using a subset of the Common Crawl News (CC-NEWS) …","url":["https://arxiv.org/pdf/2201.07520"]}
{"year":"2022","title":"Cognitive linguistics meets computational linguistics: Construction grammar, dialectology, and linguistic diversity","authors":["J Dunn - Data Analytics in Cognitive Linguistics: Methods and …, 2022"],"snippet":"Computational linguistics and cognitive linguistics come together when we use data-driven methods to conduct linguistic experiments on corpora. This chapter uses usage-based construction grammar to model geographic variation in language. The basic …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=mpZuEAAAQBAJ&oi=fnd&pg=PA273&dq=commoncrawl&ots=QDhNj-UIiF&sig=Gkw4yCUdjqDM0EjTNJ7mpG44xyA"]}
{"year":"2022","title":"Combining shallow and deep learning approaches against data scarcity in legal domains","authors":["F Sovrano, M Palmirani, F Vitali - Government Information Quarterly, 2022"],"snippet":"We are recently witnessing a radical shift towards digitisation in many aspects of our daily life, including law, public administration and governance. This has sometimes been done with the aim of reducing costs and human errors by improving data …","url":["https://www.sciencedirect.com/science/article/pii/S0740624X2200048X"]}
{"year":"2022","title":"Commonsense Validation for Arabic Sentences using Deep Learning","authors":["E Al-Bashabsheh, H Al-Khazaleh, O Elayan, R Duwairi - 2021 22nd International Arab …, 2021"],"snippet":"… -based masked language model trained on a massive 2.5 TB of clean CommonCrawl data. The data includes 100 languages and obtained a decent performance as it … The XLM-Roberta was trained on 2.5 TB of recently produced …","url":["https://ieeexplore.ieee.org/abstract/document/9677156/"]}
{"year":"2022","title":"Compact N-gram Language Models for Armenian","authors":["DS Karamyan, TS Karamyan - Mathematical Problems of Computer Science, 2022"],"snippet":"Applications such as speech recognition and machine translation use language models to select the most likely translation among many hypotheses. For on-device applications, inference time and model size are just as important as performance. In …","url":["https://www.mpcs.sci.am/index.php/mpcs/article/download/732/549"]}
{"year":"2022","title":"Comparative Analysis of Transformer Models on WikiHow Dataset","authors":["D Jadeja, A Khetri, A Mittal, DK Vishwakarma - 2022 International Conference on …, 2022"],"snippet":"Because of the ever-increasing amount of information around us, it has become highly essential to consume it in the most efficient way as possible. Text Summarization is a way of generating a summary for a long piece of text while …","url":["https://ieeexplore.ieee.org/abstract/document/9761043/"]}
{"year":"2022","title":"Comparative Analysis of Transformers to Support Fine-Grained Emotion Detection in Short-Text Data","authors":["RH Frye, DC Wilson - The International FLAIRS Conference Proceedings, 2022"],"snippet":"… A clean derivation of the CommonCrawl Corpus was used in pretraining XLM-R, with one English version and twelve versions inclusive of other languages. Conneau et al. reported 95.0% accuracy on the SST binary sentiment classification task. Clark …","url":["https://journals.flvc.org/FLAIRS/article/download/130612/133913"]}
{"year":"2022","title":"Comparative Study of Marathi Text Classification Using Monolingual and Multilingual Embeddings","authors":["F Eranpurwala - … Network Technologies and Intelligent Computing: First …, 2022"],"snippet":"… It was trained on Common Crawl and Wikipedia dataset using the CBOW model with position-weighted vocabulary of size 99882. IndicBERT was trained on ALBERT [18] using standard masked language model with vocabulary 200K and …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=8v9fEAAAQBAJ&oi=fnd&pg=PA441&dq=commoncrawl&ots=fyNHOEuz9Q&sig=fjU_MoFQFWMcxfuV5Mhyv1Tl15M"]}
{"year":"2022","title":"Comparative study of text representation and learning for Persian named entity recognition","authors":["MM Abdollah Pour, S Momtazi - ETRI Journal, 2022"],"snippet":"… XLM-R [6] is a multilingual language model trained using a large corpus of CommonCrawl data from 100 languages. XLM-R has been shown to significantly outperform previous multilingual models, for example, the mBERT and XLM models. In addition …","url":["https://onlinelibrary.wiley.com/doi/abs/10.4218/etrij.2021-0269"]}
{"year":"2022","title":"Comparison of Text Mining Models for Food and Dietary Constituent Named-Entity Recognition","authors":["N Perera, TTL Nguyen, M Dehmer, F Emmert-Streib - Machine Learning and …, 2022"],"snippet":"Biomedical Named-Entity Recognition (BioNER) has become an essential part of text mining due to the continuously increasing digital archives of biological and medical articles. While there are many well-performing BioNER tools for entities …","url":["https://www.mdpi.com/2504-4990/4/1/12/pdf"]}
{"year":"2022","title":"Comparison of text preprocessing methods","authors":["CP Chai - Natural Language Engineering, 2022"],"snippet":"Text preprocessing is not only an essential step to prepare the corpus for modeling but also a key area that directly affects the natural language processing (NLP) application results. For instance, precise tokenization increases the accuracy of part-of-speech …","url":["https://www.cambridge.org/core/journals/natural-language-engineering/article/comparison-of-text-preprocessing-methods/43A20821D65F1C0C4366B126FC794AE3"]}
{"year":"2022","title":"Compilation and evaluation of the Spanish SatiCorpus 2021 for satire identification using linguistic features and transformers","authors":["JA García-Díaz, R Valencia-García - Complex & Intelligent Systems, 2022"],"snippet":"Satirical content on social media is hard to distinguish from real news, misinformation, hoaxes or propaganda when there are no clues as to which medium these news were originally written in. It is important, therefore, to provide Information Retrieval …","url":["https://link.springer.com/article/10.1007/s40747-021-00625-1"]}
{"year":"2022","title":"Complex Word Identification for Language Learners","authors":["A Srivastava, C Biemann, Ö Alaçam, X Wang - 2022"],"snippet":"… In my experiment, I used the pre-trained English word vectors from FastText, which contains 2 million word vectors with subword information on Common Crawl (600B tokens). Table 6.3 displays the results achieved by training the sequential model …","url":["https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2022-ma-srivastava.pdf"]}
{"year":"2022","title":"COMPUTATIONAL INFERENCE OF TRUSTWORTHINESS IN SOCIAL FIGURES THROUGH ANALYSIS OF SPEECH ACOUSTIC, TEXTUAL, AND VISUAL SIGNALS","authors":["SMS Shah - 2022"],"snippet":"This chapter provides a general introduction to the field of Social Signal Processing (SSP) and its application areas. It commences by providing a brief description of motivation for this research and use of SSP for behavioural analysis, presenting the research …","url":["https://researchrepository.rmit.edu.au/view/pdfCoverPage?instCode=61RMIT_INST&filePid=13293466670001341&download=true"]}
{"year":"2022","title":"Computer Vision based Automated Spare Part Finder App","authors":["G Jugroop, P Seegolum, A Chiniah, BB Humaïra - 2022 International Conference on …, 2022"],"snippet":"The “Automated car spare part finder app” aims at building a spare part detection mobile application using image recognition technique that will assist users in finding desired spare parts and provide a list of websites where those spare parts can be …","url":["https://ieeexplore.ieee.org/abstract/document/9792778/"]}
{"year":"2022","title":"Conditional Generation with a Question-Answering Blueprint","authors":["S Narayan, J Maynez, RK Amplayo, K Ganchev… - arXiv preprint arXiv …, 2022"],"snippet":"The ability to convey relevant and faithful information is critical for many tasks in conditional generation and yet remains elusive for neural seq-to-seq models whose outputs often reveal hallucinations and fail to correctly cover important details. In this …","url":["https://arxiv.org/pdf/2207.00397"]}
{"year":"2022","title":"Conditional Supervised Contrastive Learning for Fair Text Classification","authors":["J Chi, W Shand, Y Yu, KW Chang, H Zhao, Y Tian - arXiv preprint arXiv:2205.11485, 2022"],"snippet":"… The data consist of nearly 400,000 online biographies collected from the Common Crawl corpus. These biographies are annotated with one of the 28 professions to which their subject belongs. The data are mapped to a binary gender …","url":["https://arxiv.org/pdf/2205.11485"]}
{"year":"2022","title":"Considerations for Multilingual Wikipedia Research","authors":["I Johnson, E Lescak - arXiv preprint arXiv:2204.02483, 2022"],"snippet":"English Wikipedia has long been an important data source for much research and natural language machine learning modeling. The growth of non-English language editions of Wikipedia, greater computational resources, and calls for equity in the …","url":["https://arxiv.org/pdf/2204.02483"]}
{"year":"2022","title":"Considerations for the social impact of natural language processing","authors":["AGJ Paullada - 2021"],"snippet":"Natural language processing (NLP) technologies have transformed how people access information and communicate with one another. It has thus become critical to take stock of the social impact of natural language processing technologies. In this …","url":["https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/48281/Paullada_washington_0250E_23796.pdf?sequence=1&isAllowed=y"]}
{"year":"2022","title":"Consolidation of Crowd-Sourced Geo-Tagged Data for Parameterized Travel Recommendations","authors":["A Luberg"],"snippet":"The general subject area of this dissertation is automated creation of recommendations for tourism, with the focus on sights and visit-worthy locations. This stands in contrast to the recommendations provided by mainstream travel …","url":["https://scholar.archive.org/work/2pvxz3c6nvdelbkan4y5l5ykdy/access/wayback/https://digikogu.taltech.ee/et/Download/e6d95cdb-97f5-405e-af8a-dd617fa4c922/ConsolidationofCrowdSourcedGeoTaggedDatafor.pdf"]}
{"year":"2022","title":"Constructing novel datasets for intent detection and ner in a korean healthcare advice system: guidelines and empirical results","authors":["YM Kim, TH Lee, SO Na - Applied Intelligence, 2022"],"snippet":"The demand for intelligent dialogue systems has increased rapidly in recent years. However, building such systems involves numerous complicated processes, including training data construction for language understanding. Although training …","url":["https://link.springer.com/article/10.1007/s10489-022-03400-y"]}
{"year":"2022","title":"Construction and Evaluation of Sentiment Datasets for Low-Resource Languages: The Case of Uzbek","authors":["C Gómez-Rodrıguez - … Technology. Challenges for Computer Science and …"],"snippet":"To our knowledge, the majority of human language processing technologies for low-resource languages don’t have well-established linguistic resources for the development of sentiment analysis applications. Therefore, it is in dire need of such tools and …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=sF1zEAAAQBAJ&oi=fnd&pg=PA232&dq=commoncrawl&ots=DEPUp0f29K&sig=ynTMQqraKZa4eht38ObNrhsNhPw"]}
{"year":"2022","title":"Contextual Answer Validation","authors":["A Baksi - 2022"],"snippet":"… Among these is the Common Crawl dataset[2]. It is acquired via scraping web pages, skipping over any HTML markup. Each month, it generates about 20TB of data via scraping. However, a significant quantity of Common Crawl contains …","url":["http://library.isical.ac.in:8080/xmlui/bitstream/handle/10263/7340/Arkadeep_Thesis-dissertation-18-7-22.pdf?sequence=1&isAllowed=y"]}
{"year":"2022","title":"Contextualization and Generalization in Entity and Relation Extraction","authors":["B Taillé - arXiv preprint arXiv:2206.07558, 2022"],"snippet":"During the past decade, neural networks have become prominent in Natural Language Processing (NLP), notably for their capacity to learn relevant word representations from large unlabeled corpora. These word embeddings can then be …","url":["https://arxiv.org/pdf/2206.07558"]}
{"year":"2022","title":"Controlling Formality in Low-Resource NMT with Domain Adaptation and Re-Ranking: SLT-CDT-UoS at IWSLT2022","authors":["ST Vincent, L Barrault, C Scarton - arXiv preprint arXiv:2205.05990, 2022"],"snippet":"This paper describes the SLT-CDT-UoS group's submission to the first Special Task on Formality Control for Spoken Language Translation, part of the IWSLT 2022 Evaluation Campaign. Our efforts were split between two fronts: data engineering …","url":["https://arxiv.org/pdf/2205.05990"]}
{"year":"2022","title":"Conversational Humor Analysis: Developing Data, Annotation Schema and Models","authors":["V Pamulapati - 2021"],"snippet":"Conversational humor (CH) is a sub-domain of humor where the participants (speakers or listeners) engage in different types of humor such as retorts or teasing for various purposes. There are shared complexities between this phenomenon and the larger …","url":["https://web2py.iiit.ac.in/research_centres/publications/download/mastersthesis.pdf.8971852bf2270413.76616973686e6176695f70616d756c61706174695f66696e616c5f7468657369732e706466.pdf"]}
{"year":"2022","title":"Core words in semantic representation","authors":["A Wang, S De Deyne, M McKague, A Perfors - Proceedings of the Annual Meeting of …, 2022"],"snippet":"A central question in cognitive science is how semantic information is mentally represented. Two dominant theories of semantic representation are language-based distributional semantic models (which suggest that word meaning is based on which …","url":["https://escholarship.org/content/qt8t34p5t8/qt8t34p5t8.pdf"]}
{"year":"2022","title":"Correcting diacritics and typos with ByT5 transformer model","authors":["L Stankevičius, M Lukoševičius, J Kapočiūtė-Dzikienė… - arXiv preprint arXiv …, 2022"],"snippet":"… Good examples of the first side are the Common Crawl dataset of more than 20TB of data and its version OSCAR [107] filtered by language. Such huge datasets are now one of the main building blocks of popular transformer model pre-training but …","url":["https://arxiv.org/pdf/2201.13242"]}
{"year":"2022","title":"CoSearcher: studying the effectiveness of conversational search refinement and clarification through user simulation","authors":["A Salle, S Malmasi, O Rokhlenko, E Agichtein - Information Retrieval Journal, 2022"],"snippet":"A key application of conversational search is refining a user’s search intent by asking a series of clarification questions, aiming to improve the relevance of search results. Training and evaluating such conversational systems currently requires …","url":["https://link.springer.com/article/10.1007/s10791-022-09404-z"]}
{"year":"2022","title":"Cost-Effective Updating of Distributed Reordered Indexes","authors":["J Mackenzie, A Moffat - 2021"],"snippet":"Index reordering techniques allow document collections to be renumbered, with the goal of developing a permutation of the initial document ordinal identifiers that places documents that are (somehow) like each other into positions near each other …","url":["https://jmmackenzie.io/pdf/mm21-adcs.pdf"]}
{"year":"2022","title":"COVID-19 and cyberbullying: deep ensemble model to identify cyberbullying from code-switched languages during the pandemic","authors":["S Paul, S Saha, JP Singh - Multimedia Tools and Applications, 2022"],"snippet":"… We have experimented with two different language specific fasttext Footnote 6 word embeddings, ie, English and Hindi as our code-switched corpus contains aforementioned two languages which are trained on Common Crawl and Wikipedia …","url":["https://link.springer.com/article/10.1007/s11042-021-11601-9"]}
{"year":"2022","title":"COVID-19 Tweets Classification Based on a Hybrid Word Embedding Method. Big Data Cogn. Comput. 2022, 6, 58","authors":["Y Didi, A Walha, A Wali - 2022"],"snippet":"… pre-trained Word2Vec, FastText, and GloVe embedding trained on Common Crawl and Wikipedia with 300-D vectors, for word embedding. … vectors freely available on corpora, which is the combination of Gigaword5 and Wikipedia2014 …","url":["https://www.researchgate.net/profile/Yosra-Didi/publication/360686590_COVID-19_Tweets_Classification_Based_on_a_Hybrid_Word_Embedding_Method/links/62877bdb8ecbaa07fcc54606/COVID-19-Tweets-Classification-Based-on-a-Hybrid-Word-Embedding-Method.pdf"]}
{"year":"2022","title":"Crawling Under-Resourced Languages–A Portal for Community-Contributed Corpus Collection","authors":["E Körner, F Helfer, C Schröder, T Eckart, D Goldhahn"],"snippet":"… The full texts might also be contained and accessible in larger web crawls and collections such as archive.org or commoncrawl.org. To alleviate the likelihood of copyright and license infringement, the largest related text segments published are …","url":["http://www.lrec-conf.org/proceedings/lrec2022/workshops/DCLRL/pdf/2022.dclrl-1.5.pdf"]}
{"year":"2022","title":"CRIM's Speech Recognition system description for OpenASR21 evaluation","authors":["V Gupta, G Boulianne"],"snippet":"CRIM participated in all the 15 low resource languages and the three languages with case sensitive scoring in the OpenASR21 Challenge for the constrained condition. For acoustic modeling, we developed both hybrid DNN-HMM systems and …","url":["https://www.nist.gov/document/crim-openasr21-system-description"]}
{"year":"2022","title":"Cross-Language Learning for Product Matching","authors":["R Peeters, C Bizer - 2022"],"snippet":"… As multilingual models, we use multi-lingual BERT (’bert-base-multilingual-uncased’), which is trained on the top 100 largest Wikipedias, as well as XLMRoBERTa (’xlm-roberta-base’), which is trained on a CommonCrawl corpus consisting of 100 different languages …","url":["https://www2022.thewebconf.org/PaperFiles/48.pdf"]}
{"year":"2022","title":"Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure","authors":["Y Chai, Y Liang, N Duan - arXiv preprint arXiv:2203.08430, 2022"],"snippet":"Multilingual pre-trained language models, such as mBERT and XLM-R, have shown impressive cross-lingual ability. Surprisingly, both of them use multilingual masked language model (MLM) without any cross-lingual supervision or aligned data …","url":["https://arxiv.org/pdf/2203.08430"]}
{"year":"2022","title":"Cross-Lingual Approaches to Identifying Argument Components and Relations in Norwegian Reviews","authors":["Y Khutarniuk - 2022"],"snippet":"Argument mining is the process of automatic extraction of certain argumentation structures from data. Argument mining consists of several stages such as argument component detection, argument component classification, and argumentative …","url":["https://www.duo.uio.no/bitstream/handle/10852/94450/yauhenk_master_thesis.pdf?sequence=5"]}
{"year":"2022","title":"Crosslingual Sharing for Low-Resource Natural Language Processing","authors":["P Mulcaire - 2022"],"snippet":"… Common Crawl, a repository of content scraped from the web which is frequently used as a source of text data, includes text from hundreds of languages—but more than 45% of the documents crawled are in English, just ten languages together …","url":["https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/48884/Mulcaire_washington_0250E_24163.pdf?sequence=1"]}
{"year":"2022","title":"Customer Support Chat Intent Classification using Weak Supervision and Data Augmentation","authors":["S Prabhu, AK Brahma, H Misra - 5th Joint International Conference on Data Science …, 2022"],"snippet":"Understanding the actual intent of customers is an essential step in automating the conversational experience on a chat platform. Typically, chatbots are powered by machine learning algorithms that rely on the acquisition of a large amount of high …","url":["https://dl.acm.org/doi/abs/10.1145/3493700.3493729"]}
{"year":"2022","title":"Cyber-Phishing Website Detection Using Fuzzy Rule Interpolation","authors":["M Almseidin, M Alkasassbeh, M Alzubi, J Al-Sawwa - Cryptography, 2022"],"snippet":"This paper introduces a novel detection method for phishing website attacks while avoiding the issues associated with the deficiencies of the knowledge-based representation and the binary decision. The suggested detection method was …","url":["https://www.mdpi.com/2410-387X/6/2/24/pdf?version=1651912543"]}
{"year":"2022","title":"Data Analytics and Modeling in IoT-Fog Environment for Resourceconstrained IoT-Applications: A Review","authors":["O Farooq, P Singh - Recent Advances in Computer Science and …, 2022"],"snippet":"Objective: The emergence of the concepts like Big Data, Data Science, Machine Learning (ML), and the Internet of Things (IoT) in recent years has added the potential of research in today's world. The continuous use of IoT devices, sensors, etc …","url":["https://www.ingentaconnect.com/content/ben/rascs/2022/00000015/00000007/art00008"]}
{"year":"2022","title":"Data Augmentation to Address Out-of-Vocabulary Problem in Low-Resource Sinhala-English Neural Machine Translation","authors":["A Fernando, S Ranathunga - arXiv preprint arXiv:2205.08722, 2022"],"snippet":"Out-of-Vocabulary (OOV) is a problem for Neural Machine Translation (NMT). OOV refers to words with a low occurrence in the training data, or to those that are absent from the training data. To alleviate this, word or phrase-based Data Augmentation (DA) …","url":["https://arxiv.org/pdf/2205.08722"]}
{"year":"2022","title":"Data Bootstrapping Approaches to Improve Low Resource Abusive Language Detection for Indic Languages","authors":["M Das, S Banerjee, A Mukherjee - arXiv preprint arXiv:2204.12543, 2022"],"snippet":"Abusive language is a growing concern in many social media platforms. Repeated exposure to abusive speech has created physiological effects on the target users. Thus, the problem of abusive language should be addressed in all forms for online …","url":["https://arxiv.org/pdf/2204.12543"]}
{"year":"2022","title":"Data Governance in the Age of Large-Scale Data-Driven Language Technology","authors":["Y JERNITE, HUU NGUYEN, S BIDERMAN, A ROGERS… - 2022"],"snippet":"New families of algorithms relying on deep learning have made it possible to extract ever more complex language statistics from growing numbers of text and speech records to drastically improve the performance and applicability of data-driven …","url":["https://yjernite.github.io/content/LangDataGov.pdf"]}
{"year":"2022","title":"Data integration and ethical quality: fundamental steps of the data analysis pipeline","authors":["F Azzalini - 2022"],"snippet":"… and the corpus on which it was trained is Common Crawl2. Regarding GloVe we use the model glove.840B.300d [106] and again each word is represented as a 300-dimensional vector and the training corpus was still Common Crawl. Once the single words are …","url":["https://www.politesi.polimi.it/bitstream/10589/187689/1/Azzalini%20Fabio%20PhD%20Thesis%20-%20POLITESI.pdf"]}
{"year":"2022","title":"Data Selection Curriculum for Neural Machine Translation","authors":["T Mohiuddin, P Koehn, V Chaudhary, J Cross… - arXiv preprint arXiv …, 2022"],"snippet":"… In our experiments, we use the newscrawl data as in-domain and commoncrawl data combined with newscrawl as general-domain for training the LMs. After scoring each parallel sentence pair (xi,yi) ∈ Dd by any of the above methods, we rank Dd …","url":["https://arxiv.org/pdf/2203.13867"]}
{"year":"2022","title":"Database Systems for Advanced Applications: 27th International Conference, DASFAA 2022, Virtual Event, April 11–14, 2022, Proceedings, Part III","authors":["A Bhattacharya, JLM Li, D Agrawal, PK Reddy… - 2022"],"snippet":"The three-volume set LNCS 13245, 13246 and 13247 constitutes the proceedings of the 26th International Conference on Database Systems for Advanced Applications, DASFAA 2022, held online, in April 2021. The total of 72 full papers, along with 76 …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=4XtsEAAAQBAJ&oi=fnd&pg=PR5&dq=commoncrawl&ots=GfE_2IqJct&sig=IIumPcUnOioFQO9gemsHu9RotXc"]}
{"year":"2022","title":"Datasheet for the Pile","authors":["S Biderman, K Bicheno, L Gao - arXiv preprint arXiv:2201.07311, 2022"],"snippet":"This datasheet describes the Pile, a 825 GiB dataset of human-authored text compiled by EleutherAI for use in large-scale language modeling. The Pile is comprised of 22 different text sources, ranging from original scrapes done for this …","url":["https://arxiv.org/pdf/2201.07311"]}
{"year":"2022","title":"Deep cascaded multitask framework for detection of temporal orientation, sentiment and emotion from suicide notes","authors":["S Ghosh, A Ekbal, P Bhattacharyya - Scientific Reports, 2022"],"snippet":"… To capture the syntactic and semantic information of the words in a phrase, we use the efficacy of a 300-dimensional pre-trained GloVe word embedding 51 that is trained on the Common Crawl (42 billion tokens) corpus. …","url":["https://www.nature.com/articles/s41598-022-08438-z"]}
{"year":"2022","title":"Deep Learning Approach for Aspect-Based Sentiment Analysis of Restaurants Reviews in Spanish","authors":["BC Martínez-Seis, O Pichardo-Lagunas, S Miranda… - Computación y Sistemas, 2022"],"snippet":"Online reviews of products and services have become important for customers and enterprises. Recent research focuses on analyzing and managing those kinds of reviews using natural language processing. This paper focuses on aspect-based …","url":["https://www.cys.cic.ipn.mx/ojs/index.php/CyS/article/download/4258/3387"]}
{"year":"2022","title":"Deep learning based semantic textual similarity for applications in translation technology","authors":["T Ranasinghe - 2021"],"snippet":"Semantic Textual Similarity (STS) measures the equivalence of meanings between two textual segments. It is a fundamental task for many natural language processing applications. In this study, we focus on employing STS in the context of translation …","url":["https://wlv.openrepository.com/bitstream/handle/2436/624530/Ranasinghe_PhD_Thesis.pdf?sequence=1&isAllowed=y"]}
{"year":"2022","title":"Deep Learning in Sentiment Analysis: A Survey of Recent Architectures","authors":["T Abdullah, A Ahmet - ACM Computing Surveys (CSUR), 2022"],"snippet":"Humans are increasingly integrated with devices that enable the collection of vast unstructured opinionated data. Accurately analysing subjective information from this data is the task of sentiment analysis (an actively researched area in NLP). Deep …","url":["https://dl.acm.org/doi/abs/10.1145/3548772"]}
{"year":"2022","title":"Deep Lexical Hypothesis: Identifying personality structure in natural language","authors":["A Cutler, DM Condon - 2022"],"snippet":"Recent advances in natural language processing (NLP) have produced general models that can perform complex tasks such as summarizing long passages and translating across languages. Here, we introduce a method to extract adjective …","url":["https://psyarxiv.com/gdm5v/download?format=pdf"]}
{"year":"2022","title":"Demonstrator of Automatic Minuting","authors":["M Singh, R Kumar, T Ghosal, O Bojar, CC PV, AS PV… - 2022"],"snippet":"… framework to suit a variety of challenges in the domain of natural language understanding T5 also explained the advantage of scaling up the model size (to 11B parameters) and pre-training corpus, by introducing C4 (a massive text corpus which …","url":["https://elitr.eu/wp-content/uploads/2022/03/D65.FINAL_.pdf"]}
{"year":"2022","title":"Dense-to-Sparse Gate for Mixture-of-Experts","authors":["X Nie, S Cao, X Miao, L Ma, J Xue, Y Miao, Z Yang… - arXiv preprint arXiv …, 2021"],"snippet":"… Wikipedia is a single domain of text, which contains about 2,500 words, and WebText is a large and diverse dataset in various domains, which is collected using 45 million links from Common Crawl. We apply GPT2-BPE (vocabulary size of 50257) …","url":["https://arxiv.org/pdf/2112.14397"]}
{"year":"2022","title":"DESIGN AND IMPLEMENTATION OF A REALWORLD SEARCH ENGINE BASED ON OKAPI BM25 AND SENTENCEBERT","authors":["L Bonetti, P Torroni"],"snippet":"The work conducted in this thesis aims to present an hybrid model for a real world application search engine. The project presented was part of an intern ship work carried out in a startup which deals with Knowledge Management and Artificial …","url":["https://amslaurea.unibo.it/24774/1/Thesis_Bonetti.pdf"]}
{"year":"2022","title":"Design of an GIS-based Investment Heatmap System using Topic Classification and NER","authors":["TT Van, KV Sy, TT Anh, TL Quang, PH Xuan, HL Viet… - 2021 13th International …, 2021"],"snippet":"… A multilingual fastText model [7], pre-trained on Common Crawl and Wikipedia corpus, is used to train on our collected corpus using transfer learning approach. The model was trained using CBOW with position-weights, in dimension 300, with …","url":["https://ieeexplore.ieee.org/abstract/document/9648730/"]}
{"year":"2022","title":"Design of Oral English Intelligent Evaluation System Based on DTW Algorithm","authors":["Y Fang - Mobile Networks and Applications, 2022"],"snippet":"The accuracy of the existing spoken English intelligent evaluation system is not high, and the spoken English evaluation effect is poor. To improve the accuracy and speed of spoken English evaluation, this paper puts forward the design and …","url":["https://link.springer.com/article/10.1007/s11036-022-01925-7"]}
{"year":"2022","title":"Designing Machine Learning Systems","authors":["C Huyen - 2022"],"snippet":"Machine learning systems are both complex and unique. Complex because they consist of many different components and involve many different stakeholders. Unique because they're data dependent, with data varying wildly from one use case …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=EThwEAAAQBAJ&oi=fnd&pg=PT51&dq=commoncrawl&ots=puuVz2C8kK&sig=TP64dzZehXbV5rEytHJY3Yhl3ow"]}
{"year":"2022","title":"Detecting ambiguity in conversational systems","authors":["A Banerjee - 2021"],"snippet":"The question of detection of user search queries has been explored by many authors. With the advent of speech based search interfaces, narrowing down the scope of search based on user intent becomes even more important. A prominent part of …","url":["https://elib.uni-stuttgart.de/bitstream/11682/11953/1/Banerjee_Master%20Thesis.pdf"]}
{"year":"2022","title":"Detecting autism from picture book narratives using deep neural utterance embeddings","authors":["A Wawer, I Chojnicka - International Journal of Language & Communication …, 2022"],"snippet":"… Pre-training of ELMoForManyLangs was performed on a 20 million-word data set sampled from Wikipedia and Common Crawl. For each input utterance, the representation obtained from ELMo was a vector of 1024 elements. The vectors …","url":["https://onlinelibrary.wiley.com/doi/pdf/10.1111/1460-6984.12731"]}
{"year":"2022","title":"Detecting explicit lyrics: a case study in Italian music","authors":["M Rospocher - Language Resources and Evaluation, 2022"],"snippet":"Preventing the reproduction of songs whose textual content is offensive or inappropriate for kids is an important issue in the music industry. In this paper, we investigate the problem of assessing whether music lyrics contain content unsuitable …","url":["https://link.springer.com/article/10.1007/s10579-022-09595-3"]}
{"year":"2022","title":"Detection and Performance Evaluation of Online-Fraud Using Deep Learning Algorithms","authors":["A Khan, M Patil - Intelligent Computing and Networking, 2022"],"snippet":"Online News Portals are currently one of the primary sources used by people, though its credibility is under serious question. Because the problem associated with this is Click-bait. Click-baiting being the growing phenomenon on internet has the …","url":["https://link.springer.com/chapter/10.1007/978-981-16-4863-2_16"]}
{"year":"2022","title":"Detection and Prevention of Sophisticated Cyberattacks","authors":["P Zhang - 2022"],"snippet":"Despite extensive research by the security community, cyberattacks such as phishing and Internet of Things (IoT) attacks remain profitable to criminals and continue to cause substantial damage not only to the victim users that they target …","url":["https://search.proquest.com/openview/3fdbd0bb47f35012a7243652fa10f045/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2022","title":"Developing Technology Tools to Combat Fake Science","authors":["C Impey, A Danehy - Future of Information and Communication Conference, 2022"],"snippet":"… In the next phase of the project, this technology will be scaled to large samples of content drawn from CommonCrawl, and it will be applied across more domains of science. Then it will be deployed as a web browser extension that presents the …","url":["https://link.springer.com/chapter/10.1007/978-3-030-98012-2_25"]}
{"year":"2022","title":"Development of lumbar spine MRI referrals vetting models using machine learning and deep learning algorithms: Comparison models vs healthcare professionals","authors":["AH Alanazi, A Cradock, L Rainford - Radiography, 2022"],"snippet":"Introduction Referrals vetting is a necessary daily task to ensure the appropriateness of radiology referrals. Vetting requires extensive clinical knowledge and may challenge those responsible. This study aims to develop AI models to automate the …","url":["https://www.sciencedirect.com/science/article/pii/S1078817422000682"]}
{"year":"2022","title":"Dialog Inpainting: Turning Documents into Dialogs","authors":["Z Dai, AT Chaganty, V Zhao, A Amini, QM Rashid… - arXiv preprint arXiv …, 2022"],"snippet":"Many important questions (eg \"How to eat healthier?\") require conversation to establish context and explore in depth. However, conversational question answering (ConvQA) systems have long been stymied by scarce training data that is expensive …","url":["https://arxiv.org/pdf/2205.09073"]}
{"year":"2022","title":"DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings","authors":["YS Chuang, R Dangovski, H Luo, Y Zhang, S Chang… - arXiv preprint arXiv …, 2022"],"snippet":"… However, they use 1TB of the training data from Common Crawl dumps while our model only use 115MB of the Wikipedia data for pretraining. We put their scores in Table 2 for reference. In SimCSE, the authors propose to use MLM as an auxiliary …","url":["https://arxiv.org/pdf/2204.10298"]}
{"year":"2022","title":"Differentially Private Decoding in Large Language Models","authors":["J Majmudar, C Dupuy, C Peris, S Smaili, R Gupta… - arXiv preprint arXiv …, 2022"],"snippet":"Recent large-scale natural language processing (NLP) systems use a pre-trained Large Language Model (LLM) on massive and diverse corpora as a headstart. In practice, the pre-trained model is adapted to a wide array of tasks via fine-tuning on …","url":["https://arxiv.org/pdf/2205.13621"]}
{"year":"2022","title":"Dilated Convolutional Neural Networks for Lightweight Diacritics Restoration","authors":["B Csanády, A Lukács - arXiv preprint arXiv:2201.06757, 2022"],"snippet":"… In the case of HunWeb2, we used the \"2017-2018\" part of the Common Crawl subcorpus. We decided not to split up the documents to multiple chunks or sentences, as we have found that there can be a lot of repeated sentences within one document …","url":["https://arxiv.org/pdf/2201.06757"]}
{"year":"2022","title":"Distillation or loss of information?: The effects of distillation on model redundancy","authors":["EE Sventickaite - 2022"],"snippet":"… The XLM-Roberta could be comparable to mBERT, as the model can process 100 languages, it was trained on 2.1 terabytes of data extracted from CommonCrawl. It is important to note, that although lacking the next sentence prediction functionality …","url":["https://www.diva-portal.org/smash/get/diva2:1681230/FULLTEXT01.pdf"]}
{"year":"2022","title":"Do We Need a Specific Corpus and Multiple High-Performance GPUs for Training the BERT Model? An Experiment on COVID-19 Dataset","authors":["N Nuntachit, P Sugunnasil - Machine Learning and Knowledge Extraction, 2022"],"snippet":"The COVID-19 pandemic has impacted daily lives around the globe. Since 2019, the amount of literature focusing on COVID-19 has risen exponentially. However, it is almost impossible for humans to read all of the studies and classify them. This article …","url":["https://www.mdpi.com/2504-4990/4/3/30/pdf?version=1656917033"]}
{"year":"2022","title":"Documenting Geographically and Contextually Diverse Data Sources: The BigScience Catalogue of Language Data and Resources","authors":["A McMillan-Major, Z Alyafeai, S Biderman, K Chen… - arXiv preprint arXiv …, 2022"],"snippet":"… Typically, this data is collected from online sources, ranging from highly edited and structured text such as Wikipedia to the myriad text and audiovisual components of web pages collected by Common Crawl1. Several issues concerning their …","url":["https://arxiv.org/pdf/2201.10066"]}
{"year":"2022","title":"Does Corpus Quality Really Matter for Low-Resource Languages?","authors":["M Artetxe, I Aldabe, R Agerri, O Perez-de-Viñaspre… - arXiv preprint arXiv …, 2022"],"snippet":"… 2020), respectively, and were built by filtering CommonCrawl. Wikipedia has been a … Instead of filtering CommonCrawl, we do tailored crawling on 33 websites with high-quality content in … First of all, we find that, even if CommonCrawl derived …","url":["https://arxiv.org/pdf/2203.08111"]}
{"year":"2022","title":"Domain Adaptation for Sparse-Data Settings: What Do We Gain by Not Using Bert?","authors":["M Sedinkina, M Schmitt, H Schütze - arXiv preprint arXiv:2203.16926, 2022"],"snippet":"The practical success of much of NLP depends on the availability of training data. However, in real-world scenarios, training data is often scarce, not least because many application domains are restricted and specific. In this work, we compare …","url":["https://arxiv.org/pdf/2203.16926"]}
{"year":"2022","title":"Domain Parking: Largely Present, Rarely Considered!","authors":["J Zirngibl, S Deusch, P Sattler, J Aulbach, G Carle…"],"snippet":"… Common Crawl While visual identification allowed us to validate the inferences to a reasonable extent, we wanted to upscale validation. Therefore, we consider Common Crawl (CC) data [21] and calculate the similarity of pages. Common Crawl …","url":["https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/zirngibl2022prevalenceofparking.pdf"]}
{"year":"2022","title":"Domain-aware Multi-modality Fusion Network for Generalized Zero-shot Learning","authors":["J Wang, X Wang, H Zhang - Neurocomputing, 2022"],"snippet":"… Specifically, we investigate two word embedding methods including Word2vec[30] and Glove [36], which are trained on 1.4M Google News 2 and 2.2M Common Crawl 3 . We also consider hierarchical embeddings where similarity between objects can …","url":["https://www.sciencedirect.com/science/article/pii/S0925231222002168"]}
{"year":"2022","title":"Domain-specific language models for multi-label classification of medical text","authors":["V Yogarajan - 2022"],"snippet":"Recent advancements in machine learning-based medical text multi-label classifications can be used to enhance the understanding of the human body and aid the need for patient care. This research considers predicting medical codes from …","url":["https://researchcommons.waikato.ac.nz/bitstream/handle/10289/14757/thesis.pdf?sequence=4&isAllowed=y"]}
{"year":"2022","title":"Domain-Specific Machine Learning-A Human Learning Perspective","authors":["C Shan - 2022"],"snippet":"This dissertation focuses on Domain-Specific Machine Learning (DSML) with applications in the semiconductor industry. We first illustrate characteristics of DSML in view of semiconductor design and test processes, where a task is usually carried …","url":["https://escholarship.org/content/qt3m62p0hk/qt3m62p0hk.pdf"]}
{"year":"2022","title":"Double Retrieval and Ranking for Accurate Question Answering","authors":["Z Zhang, T Vu, A Moschitti - arXiv preprint arXiv:2201.05981, 2022"],"snippet":"… Then, we used it to build a large index having up to 130MM passages extracted from 54MM documents of CommonCrawl2. We selected English Web documents of 5,000 most popular domains, including Wikipedia, from the recent releases of …","url":["https://arxiv.org/pdf/2201.05981"]}
{"year":"2022","title":"DS4DH at TREC Health Misinformation 2021: Multi-Dimensional Ranking Models with Transfer Learning and Rank Fusion","authors":["B Zhang, N Naderi, F Jaume-Santero, D Teodoro - arXiv preprint arXiv:2202.06771, 2022"],"snippet":"This paper describes the work of the Data Science for Digital Health (DS4DH) group at the TREC Health Misinformation Track 2021. The TREC Health Misinformation track focused on the development of retrieval methods that provide relevant, correct …","url":["https://arxiv.org/pdf/2202.06771"]}
{"year":"2022","title":"Dutch SQuAD and Ensemble Learning for Question Answering from Labour Agreements","authors":["NJ Rouws, S Vakulenko, S Katrenko - Benelux Conference on Artificial Intelligence, 2021"],"snippet":"The Dutch Ministry of Social Affairs and Employment has to regularly explore the content of labour agreements. Studies on topics such as diversity and work flexibility are conducted on the regular basis by means of specialised questionnaires. We …","url":["https://link.springer.com/chapter/10.1007/978-3-030-93842-0_9"]}
{"year":"2022","title":"Early depression detection in social media based on deep learning and underlying emotions","authors":["JSL Figuerêdo, ALLM Maia, RT Calumby - Online Social Networks and Media, 2022"],"snippet":"Depression is a challenge to public health, frequently related to disability and one of the reasons that lead to suicide. Many of the ones who suffer depression use social media to obtain information or even to talk about their problem. Some studies have …","url":["https://www.sciencedirect.com/science/article/pii/S2468696422000283"]}
{"year":"2022","title":"EDIN: An End-to-end Benchmark and Pipeline for Unknown Entity Discovery and Indexing","authors":["N Kassner, F Petroni, M Plekhanov, S Riedel… - arXiv preprint arXiv …, 2022"],"snippet":"Existing work on Entity Linking mostly assumes that the reference knowledge base is complete, and therefore all mentions can be linked. In practice this is hardly ever the case, as knowledge bases are incomplete and because novel concepts arise …","url":["https://arxiv.org/pdf/2205.12570"]}
{"year":"2022","title":"Effective Approaches to Neural Query Language Identification","authors":["X Ren, B Yang, D Liu, H Zhang, X Lv, L Yao, J Xie - Computational Linguistics, 2022"],"snippet":"… Commoncow: Massively huge web corpora from commoncrawl data and a method to distribute them freely under restrictive EU copyright laws. In Proceedings of the Tenth International Conference on Language Resources and Evaluation …","url":["https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli_a_00451/2035281/coli_a_00451.pdf"]}
{"year":"2022","title":"Effectiveness of French Language Models on Abstractive Dialogue Summarization Task","authors":["Y Zhou, F Portet, F Ringeval - arXiv preprint arXiv:2207.08305, 2022"],"snippet":"Pre-trained language models have established the state-of-the-art on various natural language processing tasks, including dialogue summarization, which allows the reader to quickly access key information from long conversations in meetings …","url":["https://arxiv.org/pdf/2207.08305"]}
{"year":"2022","title":"Effects of Physiological Signals in Different Types of Multimodal Sentiment Estimation","authors":["S Katada, S Okada, K Komatani - IEEE Transactions on Affective Computing, 2022"],"snippet":"… FastText word vectors FastText is a recently developed language model for training word vectors with subword information on Wikipedia and the Common Crawl corpus [41]. This language model is not restricted to English and is available …","url":["https://ieeexplore.ieee.org/abstract/document/9726810/"]}
{"year":"2022","title":"Effects of Similarity Score Functions in Attention Mechanisms on the Performance of Neural Question Answering Systems","authors":["Y Shen, EMK Lai, M Mohaghegh - Neural Processing Letters, 2022"],"snippet":"Attention mechanisms have been incorporated into many neural network-based natural language processing (NLP) models. They enhance the ability of these models to learn and reason with long input texts. A critical part of such mechanisms …","url":["https://link.springer.com/article/10.1007/s11063-021-10730-4"]}
{"year":"2022","title":"Efficient Language Modeling with Sparse all-MLP","authors":["P Yu, M Artetxe, M Ott, S Shleifer, H Gong, V Stoyanov… - arXiv preprint arXiv …, 2022"],"snippet":"All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks. In this work, we …","url":["https://arxiv.org/pdf/2203.06850"]}
{"year":"2022","title":"ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models","authors":["J Li, T Tang, Z Gong, L Yang, Z Yu, Z Chen, J Wang… - arXiv preprint arXiv …, 2022"],"snippet":"Nowadays, pretrained language models (PLMs) have dominated the majority of NLP tasks. While, little research has been conducted on systematically evaluating the language abilities of PLMs. In this paper, we present a large-scale empirical study …","url":["https://arxiv.org/pdf/2205.01523"]}
{"year":"2022","title":"Emerging trends: General fine-tuning (gft)","authors":["KW Church, X Cai, Y Ying, Z Chen, G Xun, Y Bian - Natural Language Engineering, 2022"],"snippet":"This paper describes gft (general fine-tuning), a little language for deep nets, introduced at an ACL-2022 tutorial. gft makes deep nets accessible to a broad audience including non-programmers. It is standard practice in many fields to use …","url":["https://www.cambridge.org/core/services/aop-cambridge-core/content/view/909A753B5EC40310A8DC2A3093CAE9F6/S1351324922000237a.pdf/emerging-trends-general-fine-tuning-gft.pdf"]}
{"year":"2022","title":"Emerging Trends: SOTA-Chasing","authors":["KW Church, V Kordoni - Natural Language Engineering, 2022"],"snippet":"Many papers are chasing state-of-the-art (SOTA) numbers, and more will do so in the future. SOTA-chasing comes with many costs. SOTA-chasing squeezes out more promising opportunities such as coopetition and interdisciplinary collaboration. In …","url":["https://www.cambridge.org/core/services/aop-cambridge-core/content/view/5E9F9F796159040973053C52C443C1D6/S1351324922000043a.pdf/emerging_trends_sotachasing.pdf"]}
{"year":"2022","title":"Emotion Detection using Deep Learning","authors":["Y Shaaban, H Korashy, W Medhat - 2021 16th International Conference on Computer …, 2021"],"snippet":"Emotion detection is one of the challenging tasks in Natural Language Processing. The applications of emotion detection can be utilized in different fields like data mining, psychology, and human-computer interaction. The paper aims to train …","url":["https://ieeexplore.ieee.org/abstract/document/9686108/"]}
{"year":"2022","title":"Emotion-Aware Multimodal Pre-training for Image-Grounded Emotional Response Generation","authors":["Z Tian, Z Wen, Z Wu, Y Song, J Tang, D Li, NL Zhang - International Conference on …, 2022"],"snippet":"Face-to-face communication leads to better interactions between speakers than text-to-text conversations since the speakers can capture both textual and visual signals. Image-grounded emotional response generation (IgERG) tasks requires chatbots to generate a …","url":["https://link.springer.com/chapter/10.1007/978-3-031-00129-1_1"]}
{"year":"2022","title":"Empirical Evaluation and Theoretical Analysis for Representation Learning: A Survey","authors":["K Nozawa, I Sato - arXiv preprint arXiv:2204.08226, 2022"],"snippet":"Representation learning enables us to automatically extract generic feature representations from a dataset to solve another machine learning task. Recently, extracted feature representations by a representation learning algorithm and a …","url":["https://arxiv.org/pdf/2204.08226"]}
{"year":"2022","title":"Energy-Efficient Data Transfer Optimization via Decision-Tree Based Uncertainty Reduction","authors":["H Jamil, L Rodolph, J Goldverg, T Kosar - arXiv preprint arXiv:2204.07601, 2022"],"snippet":"… These include: (1) the small file size dataset consisting of 20,000 HTML files derived from the common crawl project [24]; (2) the medium file size dataset consisting of 5,000 image files derived from Flickr [25]; (3) the large file size dataset …","url":["https://arxiv.org/pdf/2204.07601"]}
{"year":"2022","title":"Enhanced Distance-aware Self-attention and Multi-level Match for Sentence Semantic Matching","authors":["Y Deng, X Li, M Zhang, X Lu, X Sun - Neurocomputing, 2022"],"snippet":"Sentence semantic matching is a core research area in natural language processing, which is widely used in various natural language tasks. In recent years, attention mechanism has shown good performance in deep neural networks for sentence …","url":["https://www.sciencedirect.com/science/article/pii/S0925231222006877"]}
{"year":"2022","title":"Enhancing Cross-lingual Prompting with Mask Token Augmentation","authors":["M Zhou, X Li, Y Jiang, L Bing - arXiv preprint arXiv:2202.07255, 2022"],"snippet":"… Model Details XLM-R base model, containing 270M parameters, is pretrained on 2.5TB of filtered CommonCrawl on 100 languages. It contains 12 Transformer layers with hidden space dimensions of 768 and 12 attention heads in each layer. …","url":["https://arxiv.org/pdf/2202.07255"]}
{"year":"2022","title":"Enhancing Mobile App Bug Reporting via Real-time Understanding of Reproduction Steps","authors":["M Fazzini, K Moran, CB Cardenas, T Wendland, A Orso… - arXiv preprint arXiv …, 2022"],"snippet":"… The module uses the FastText model trained on the Common Crawl dataset [39] to identify the semantic similarity between S2R descriptions and GUI element properties. The web application also provides access to bug reports saved in a …","url":["https://arxiv.org/pdf/2203.12093"]}
{"year":"2022","title":"Enhancing the Performance of Text Mining","authors":["FM Al Shanik - 2021"],"snippet":"The amount of text data produced in science, finance, social media, and medicine is growing at an unprecedented pace. The raw text data typically introduces major computational and analytical obstacles (eg, extremely high dimensionality) to data …","url":["https://tigerprints.clemson.edu/cgi/viewcontent.cgi?article=3929&context=all_dissertations"]}
{"year":"2022","title":"Ensembling of Various Transformer Based Models for the Fake News Detection Task in the Urdu Language","authors":["S Kalraa, P Vermaa, Y Sharmaa, GS Chauhanb"],"snippet":"The spread of misinformation has become a severe issue affecting society. Inaccurate information has enormous potential to cause real-world impacts. Developing algorithms to detect fake news automatically will be very useful in …","url":["http://ceur-ws.org/Vol-3159/T7-10.pdf"]}
{"year":"2022","title":"esCorpius: A Massive Spanish Crawling Corpus","authors":["A Gutiérrez-Fandiño, D Pérez-Fernández… - arXiv e-prints, 2022"],"snippet":"… In this paper, we introduce esCorpius, a Spanish crawling corpus obtained from near 1 Pb of Common Crawl data. It is the most extensive corpus in Spanish with this level of quality in the extraction, purification and deduplication of web textual content …","url":["https://ui.adsabs.harvard.edu/abs/2022arXiv220615147G/abstract"]}
{"year":"2022","title":"Estimating Confidence of Predictions of Individual Classifiers and Their Ensembles for the Genre Classification Task","authors":["M Lepekhin, S Sharoff"],"snippet":"… It has a similar architecture but uses a bigger and more genre-diverse corpus based on Common Crawl (instead of Wikipedia for the multilingual BERT). Therefore, we choose XLM-RoBERTa as the classifier for the experiments in our research. One …","url":["http://corpus.leeds.ac.uk/serge/publications/2022-LREC-genre.pdf"]}
{"year":"2022","title":"Estimating Intelligence Quotient Using Stylometry and Machine Learning Techniques: A Review","authors":["GO Adebayo, RV Yampolskiy - Big Data Mining and Analytics, 2022"],"snippet":"… the curve of the collegiate word ratio (CWR) — the ratio of the total count of collegiate words (words SAT consider a part of strong vocabulary usage) used in a written text to the total count of words in the text, of sample texts with more than 100 …","url":["https://ieeexplore.ieee.org/iel7/8254253/9793354/09793359.pdf"]}
{"year":"2022","title":"Estimation on the Importance of Semantic Web Integration for Art and Culture Related Online Media Outlets","authors":["A Giannakoulopoulos, M Pergantis, N Konstantinou… - Future Internet, 2022"],"snippet":"… Such a list was acquired through Common Crawl a “non-profit organization dedicated to … This process, which was developed in PHP, used Common Crawl’s API to receive a list of domains … The language of a website was identified based …","url":["https://www.mdpi.com/1999-5903/14/2/36/pdf"]}
{"year":"2022","title":"Ethnic Bias in Judicial Decision-making: Evidence from Criminal Appeals in Kenya","authors":["DD Choi, JA Harris, F Shen-Bayh"],"snippet":"… ), we used the Common Crawl GLoVe model that was trained using a . million word vocabulary. Rodriguez and Spirling (Nd) further nd that pretrained word vectors perform well against both locally trained vectors and human coders. …","url":["https://www.dhdannychoi.com/files/CHS_EthnicBias.pdf"]}
{"year":"2022","title":"European Language Equality-Report on the French Language","authors":["G Adda, A Braffort, I Vasilescu, F Yvon - 2022"],"snippet":"This report presents a survey of the current state of technologies for the automatic processing of the French language as well as French Sign Language (FSL). Similar reports have been prepared independently for all languages of the European Union …","url":["https://hal.archives-ouvertes.fr/hal-03637776/document"]}
{"year":"2022","title":"Evaluate and Visualize Legal Embeddings for Explanation Purpose","authors":["NH Thanh, DT Binh, BM Quan, N Le Minh - 2021 13th International Conference on …, 2021"],"snippet":"Deep learning models have recently demonstrated promising results on legal-related tasks. However, the performance of the models is not enough for us to have a clear understanding of their behavior and the reason for such performance. This paper is …","url":["https://ieeexplore.ieee.org/abstract/document/9648655/"]}
{"year":"2022","title":"Evaluate Similarity of Requirements with Multilingual Natural Language Processing","authors":["U Bisang, J Brünnhäußer, P Lünnemann, L Kirsch… - Proceedings of the Design …, 2022"],"snippet":"Finding redundant requirements or semantically similar ones in previous projects is a very time-consuming task in engineering design, especially with multilingual data. Due to modern NLP it is possible to automate such tasks. In this paper we compared …","url":["https://www.cambridge.org/core/services/aop-cambridge-core/content/view/2733FA14767D66DEE559E39594732BDD/S2732527X22001535a.pdf/div-class-title-evaluate-similarity-of-requirements-with-multilingual-natural-language-processing-div.pdf"]}
{"year":"2022","title":"Evaluating Pre-training Objectives for Low-Resource Translation into Morphologically Rich Languages","authors":["P Dhar, A Bisazza, G van Noord"],"snippet":"The scarcity of parallel data is a major limitation for Neural Machine Translation (NMT) systems, in particular for translation into morphologically rich languages (MRLs). An important way to overcome the lack of parallel data is to leverage target monolingual …","url":["http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.527.pdf"]}
{"year":"2022","title":"Evaluating the Construct Validity of Text Embeddings with Application to Survey Questions","authors":["Q Fang, D Nguyen, DL Oberski - arXiv preprint arXiv:2202.09166, 2022"],"snippet":"Text embedding models from Natural Language Processing can map text data (eg words, sentences, documents) to supposedly meaningful numerical representations (aka text embeddings). While such models are increasingly applied in social science …","url":["https://arxiv.org/pdf/2202.09166"]}
{"year":"2022","title":"Evaluating transfer learning approach for detecting Arabic anti-refugee/migrant speech on social media","authors":["D Mohdeb, M Laifa, F Zerargui, O Benzaoui - Aslib Journal of Information …, 2022"],"snippet":"Purpose The present study was designed to investigate eight research questions that are related to the analysis and the detection of dialectal Arabic hate speech that targeted African refugees and illegal migrants on the YouTube Algerian space …","url":["https://www.emerald.com/insight/content/doi/10.1108/AJIM-10-2021-0293/full/html"]}
{"year":"2022","title":"Evaluation of Transfer Learning for Polish with a Text-to-Text Model","authors":["A Chrabrowa, Ł Dragan, K Grzegorczyk, D Kajtoch… - arXiv preprint arXiv …, 2022"],"snippet":"We introduce a new benchmark for assessing the quality of text-to-text models for Polish. The benchmark consists of diverse tasks and datasets: KLEJ benchmark adapted for text-to-text, en-pl translation, summarization, and question answering. In …","url":["https://arxiv.org/pdf/2205.08808"]}
{"year":"2022","title":"Evaluation of Word Embedding Models in Latvian NLP Tasks Based on Publicly Available Corpora","authors":["R Laucis, G Jēkabsons - Applied Computer Systems, 2021"],"snippet":"Nowadays, natural language processing (NLP) is increasingly relaying on pre-trained word embeddings for use in various tasks. However, there is little research devoted to Latvian–a language that is much more morphologically complex than English. In …","url":["https://sciendo.com/pdf/10.2478/acss-2021-0016"]}
{"year":"2022","title":"Event prediction from news text using subgraph embedding and graph sequence mining","authors":["RF Cekinel, P Karagoz - World Wide Web, 2022"],"snippet":"Event detection from textual content by using text mining concepts is a well-researched field in the literature. On the other hand, graph modeling and graph embedding techniques in recent years provide an opportunity to represent textual contents as …","url":["https://link.springer.com/article/10.1007/s11280-021-01002-1"]}
{"year":"2022","title":"Explainable detection of adverse drug reaction with imbalanced data distribution","authors":["J Wang, LC Yu, X Zhang - PLOS Computational Biology, 2022"],"snippet":"… For BiLSTM, the word vectors were pre-trained using GloVe on the 840B Common Crawl corpus [28]. The dimensionality of the word vectors is 300. Words that don’t appear in GloVe were initialized with a uniform distribution U(−0.25, 0.25). …","url":["https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010144"]}
{"year":"2022","title":"Exploiting Document-based Features for Clarification in Conversational Search","authors":["I Sekulić, M Aliannejadi, F Crestani - European Conference on Information Retrieval, 2022"],"snippet":"… For that purpose, we utilise Chatnoir [7] – a freely accessible Elasticsearch-based search engine with indexes of ClueWeb and CommonCrawl … Elastic ChatNoir: search engine for the ClueWeb and the common crawl. In: Pasi, G., Piwowarski, B …","url":["https://link.springer.com/chapter/10.1007/978-3-030-99736-6_28"]}
{"year":"2022","title":"Exploiting Ensemble of Transformer Models for Detecting Informative Tweets","authors":["AN Chowdhury, S Guha, N Amin, SI Khan - 2021 IEEE 18th India Council …, 2021"],"snippet":"Microblogging platforms especially Twitter is considered as one of the prominent medium of getting user-generated information. Millions of tweets were posted daily during COVID-19 pandemic days and the rate increases gradually. Tweets include a …","url":["https://ieeexplore.ieee.org/abstract/document/9691734/"]}
{"year":"2022","title":"Exploration of text features representation on applications to the insurance domain","authors":["M Garcia Ribes - 2022"],"snippet":"The recent popularization of a more data-centric paradigm in machine learning has turned the study of the quality of feature representation into a very relevant topic. In this document an exploration of different text features representation applied to a …","url":["https://upcommons.upc.edu/bitstream/handle/2117/363257/165184.pdf?sequence=1"]}
{"year":"2022","title":"Exploring Capabilities of Monolingual Audio Transformers using Large Datasets in Automatic Speech Recognition of Czech","authors":["J Lehečka, J Švec, A Pražák, JV Psutka - arXiv preprint arXiv:2206.07627, 2022"],"snippet":"… is a huge collection of cleaned and deduplicated web pages from Common Crawl project4. Since this text corpus contains almost 13 billion … • adding LM into the beam search CTC decoder; we found the Common Crawl project to be a useful …","url":["https://arxiv.org/pdf/2206.07627"]}
{"year":"2022","title":"Exploring Dimensionality Reduction Techniques in Multilingual Transformers","authors":["Á Huertas-García, A Martín, J Huertas-Tato… - arXiv preprint arXiv …, 2022"],"snippet":"Both in scientific literature and in industry,, Semantic and context-aware Natural Language Processing-based solutions have been gaining importance in recent years. The possibilities and performance shown by these models when dealing with …","url":["https://arxiv.org/pdf/2204.08415"]}
{"year":"2022","title":"Exploring Swedish & English fastText Embeddings","authors":["T Adewumi, F Liwicki, M Liwicki"],"snippet":"In this paper, we show that embeddings from relatively smaller corpora sometimes outperform those from larger corpora and we introduce a new Swedish analogy test set and make it publicly available. To achieve good performance in Natural …","url":["https://aic20.aass.oru.se/aic-pdfs/AIC-2022-camera-ready-short-paper-07.pdf"]}
{"year":"2022","title":"Exposing Cross-Lingual Lexical Knowledge from Multilingual Sentence Encoders","authors":["I Vulić, G Glavaš, F Liu, N Collier, EM Ponti… - arXiv preprint arXiv …, 2022"],"snippet":"Pretrained multilingual language models (LMs) can be successfully transformed into multilingual sentence encoders (SEs; eg, LaBSE, xMPNET) via additional fine-tuning or model distillation on parallel data. However, it remains uncertain how to best …","url":["https://arxiv.org/pdf/2205.00267"]}
{"year":"2022","title":"Extracting Impact Model Narratives from Social Services' Text","authors":["B Gajderowicz, D Rosu, MS Fox - 2022"],"snippet":"Named entity recognition (NER) is an important task in narration extraction. Narration, as a system of stories, provides insights into how events and characters in the stories develop over time. This paper proposes an architecture for NER on a corpus …","url":["http://ceur-ws.org/Vol-3117/paper6.pdf"]}
{"year":"2022","title":"Extreme Multilabel Text Classification on Indonesian Tax Court Ruling using Single Channel CNN and IndoBERT Embedding","authors":["IN Khasanah, AA Krisnadhi - 2021 6th International Workshop on Big Data and …, 2021"],"snippet":"Manual searching for legal basis such as paragraphs, articles, and laws when preparing for a tax court hearing is time-consuming. In this paper, we use extreme multilabel text classification approach to predict paragraphs, articles, and laws …","url":["https://ieeexplore.ieee.org/abstract/document/9631855/"]}
{"year":"2022","title":"Face-Dubbing++: Lip-Synchronous, Voice Preserving Translation of Videos","authors":["A Waibel, M Behr, FI Eyiokur, D Yaman, TN Nguyen… - arXiv preprint arXiv …, 2022"],"snippet":"In this paper, we propose a neural end-to-end system for voice preserving, lip-synchronous translation of videos. The system is designed to combine multiple component models and produces a video of the original speaker speaking in the target …","url":["https://arxiv.org/pdf/2206.04523"]}
{"year":"2022","title":"FactPEGASUS: Factuality-Aware Pre-training and Fine-tuning for Abstractive Summarization","authors":["D Wan, M Bansal - arXiv preprint arXiv:2205.07830, 2022"],"snippet":"We present FactPEGASUS, an abstractive summarization model that addresses the problem of factuality during pre-training and fine-tuning: (1) We augment the sentence selection strategy of PEGASUS's (Zhang et al., 2020) pre-training objective …","url":["https://arxiv.org/pdf/2205.07830"]}
{"year":"2022","title":"Factual Consistency of Multilingual Pretrained Language Models","authors":["C Fierro, A Søgaard - arXiv preprint arXiv:2203.11552, 2022"],"snippet":"Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work …","url":["https://arxiv.org/pdf/2203.11552"]}
{"year":"2022","title":"Fake news detection: When complex problems demand complex solutions","authors":["C Oliva, IP Marín, LF Lago-Fernández… - 2021"],"snippet":"Fake news detection is one of the most challenging problems in today's information and communication systems. In this article we address the challenge of detecting the generation and spreading of misleading information in the specific scenario of …","url":["https://digital.csic.es/bitstream/10261/257188/1/main_applied_intelligence.pdf"]}
{"year":"2022","title":"Fake or Genuine? Contextualised Text Representation for Fake Review Detection","authors":["R Mohawesh, S Xu, M Springer, M Al-Hawawreh… - arXiv preprint arXiv …, 2021"],"snippet":"… Along with the English Wikipedia and book corpora, RoBERTa is pre-trained on the Common Crawl News dataset, comprising 63 million English-language news stories. RoBERTa base architecture consists of 768 hidden layers, 12 layers, 125 …","url":["https://arxiv.org/pdf/2112.14343"]}
{"year":"2022","title":"FD-GATDR: A Federated-Decentralized-Learning Graph Attention Network for Doctor Recommendation Using EHR","authors":["L Bi, Y Wang, F Zhang, Z Liu, Y Cai, E Zhao - arXiv preprint arXiv:2207.05750, 2022"],"snippet":"… The GloVe model is loaded with the pretrained parameters on Common Crawl which has 840B tokens and 2.2M vocab [11]. Each word is converted to a 300-dimension vector. Then the model is fine-tuned with LSTM for the classification. …","url":["https://arxiv.org/pdf/2207.05750"]}
{"year":"2022","title":"Feasibility study of long-form question answering","authors":["S Antikainen - 2022"],"snippet":"Long-form question answering (LFQA) is a challenging natural language processing task as it asks to produce paragraph-length answers to questions. Meaningful progress in LFQA would significantly speed up how fast people can seek information …","url":["https://aaltodoc.aalto.fi/bitstream/handle/123456789/112850/master_Antikainen_Simo_2022.pdf?sequence=1"]}
{"year":"2022","title":"Feature-based Question Routing in Community Question Answering Platforms","authors":["S Sorkhani, R Etemadi, A Bigdeli, M Zihayat, E Bagheri - Information Sciences, 2022"],"snippet":"… To do so, we use the Fasttext model with the pre-trained embeddings on the Common Crawl dataset 4 to obtain embedding vectors for question’s tags and tags of the questions answered by the user. Subsequently, we compute this feature for …","url":["https://www.sciencedirect.com/science/article/pii/S0020025522006661"]}
{"year":"2022","title":"Few-shot learning with language models: Learning from instructions and contexts","authors":["T Schick - 2022"],"snippet":"Pretraining deep neural networks to perform language modeling–that is, to reconstruct missing words from incomplete pieces of text–has brought large improvements throughout natural language processing (NLP). However, even …","url":["https://edoc.ub.uni-muenchen.de/29867/1/Schick_Timo.pdf"]}
{"year":"2022","title":"Few-Shot Natural Language Processing by Meta-Learning Without Labeled Data","authors":["T Bansal - 2022"],"snippet":"Humans show a remarkable capability to accurately solve a wide range of problems efficiently--utilizing a limited amount of computation and experience. Deep learning models, by stark contrast, can be trained to be highly accurate on a narrow task …","url":["https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=3507&context=dissertations_2"]}
{"year":"2022","title":"Fight Against COVID-19 Misinformation via Clustering-Based Subset Selection Fusion Methods","authors":["Y Huang, Q Xu, S Wu, C Nugent, A Moore - 2022"],"snippet":"The worldwide COVID-19 pandemic has brought about a lot of changes in people’s life. It also emerges as a new challenge to information search services. This is because up to now our understanding about the virus is still limited, and there is a lot …","url":["http://ceur-ws.org/Vol-3138/paper2_jot.pdf"]}
{"year":"2022","title":"Finding and Fixing Undesirable Behaviors in Pretrained Language Models","authors":["E Perez - 2022"],"snippet":"In machine learning, we often do not know how to specify the desired objective for our learning algorithms. For example, we typically train models to answer questions correctly by using a proxy objective: training models to answer questions as people …","url":["https://ethanperez.net/thesis.pdf"]}
{"year":"2022","title":"Fine-grained semantic type discovery for heterogeneous sources using clustering","authors":["F Piai, P Atzeni, P Merialdo, D Srivastava - The VLDB Journal, 2022"],"snippet":"We focus on the key task of semantic type discovery over a set of heterogeneous sources, an important data preparation task. We consider the challenging setting of multiple Web data sources in a vertical domain, which present sparsity of data and a …","url":["https://link.springer.com/article/10.1007/s00778-022-00743-3"]}
{"year":"2022","title":"Fine-tuning Pre-trained Language Models with Noise Stability Regularization","authors":["H Hua, X Li, D Dou, CZ Xu, J Luo - arXiv preprint arXiv:2206.05658, 2022"],"snippet":"The advent of large-scale pre-trained language models has contributed greatly to the recent progress in natural language processing. Many state-of-the-art language models are first trained on a large text corpus and then fine-tuned on downstream …","url":["https://arxiv.org/pdf/2206.05658"]}
{"year":"2022","title":"FINE-TUNING TRANSFORMERS: ASSESSMENTS OF","authors":["Z Sun - 2022"],"snippet":"… in the development of pre-trained systems such as BERT [4](Bidirectional Encoder Representations from Transformers) and GPT [23](Generative Pre-trained Transformer), which was trained on large language datasets such as the Wikipedia …","url":["https://scholarworks.csun.edu/bitstream/handle/10211.3/223189/Sun-Zhen-thesis-2022.pdf?sequence=1"]}
{"year":"2022","title":"Forecasting Future World Events with Neural Networks","authors":["A Zou, T Xiao, R Jia, J Kwon, M Mazeika, R Li, D Song… - arXiv preprint arXiv …, 2022"],"snippet":"… To this end, we provide a corpus of news articles scraped from CommonCrawl news (Nagel, 2016; Hamborg et al., 2017) that is organized by publish date. The articles were generated between 2016 to mid-2022 and total more than 200GB of …","url":["https://arxiv.org/pdf/2206.15474"]}
{"year":"2022","title":"Formal Mathematics Statement Curriculum Learning","authors":["S Polu, JM Han, K Zheng, M Baksys, I Babuschkin…"],"snippet":"We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms …","url":["https://cdn.openai.com/papers/Formal_Mathematics_Statement_Curriculum_Learning__ICML_2022.pdf"]}
{"year":"2022","title":"Formulating Automated Responses to Cognitive Distortions for CBT Interactions","authors":["I de Toledo Rodriguez, G Salton, R Ross - … of The Fourth International Conference on …, 2021"],"snippet":"… 2020), trained on a cleaned version of the Common Crawl corpus (C4). The published model checkpoints have been pre-trained in a diverse variety of unsupervised and supervised tasks (language modelling, word embedding, machine translation …","url":["https://aclanthology.org/2021.icnlsp-1.13.pdf"]}
{"year":"2022","title":"FPSRS: A Fusion Approach for Paper Submission Recommendation System","authors":["ST Huynh, N Dang, DH Nguyen, PT Huynh, BT Nguyen - arXiv preprint arXiv …, 2022"],"snippet":"… We remove single letters that likely do not have pretrained weights in wellknown pretrained word vectors like FastText Common Crawl4. 4. We remove words within stopwords downloaded from the Natural Language Toolkit (NLTK5) and additional …","url":["https://arxiv.org/pdf/2205.05965"]}
{"year":"2022","title":"French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English","authors":["A Névéol, Y Dupont, J Bezançon, K Fort - ACL 2022-60th Annual Meeting of the …, 2022"],"snippet":"Warning: This paper contains explicit statements of offensive stereotypes which may be upsetting. Much work on biases in natural language processing has addressed biases linked to the social and cultural experience of English speaking individuals in …","url":["https://hal.inria.fr/hal-03629677/document"]}
{"year":"2022","title":"Fully Automated Detection of Formal Thought Disorder with Time-series Augmented Representations for Detection of Incoherent Speech (TARDIS)","authors":["W Xu, W Wang, J Portanova, A Chander, A Campbell… - Journal of Biomedical …, 2022"],"snippet":"… For the current research, we used publicly available pre-trained vectors derived using the FastText [39] package 8 consisting of 2-million-word vectors trained on a corpus derived from Common Crawl 9 . Individual words are represented by their …","url":["https://www.sciencedirect.com/science/article/pii/S1532046422000144"]}
{"year":"2022","title":"FuzzyDedup: Secure Fuzzy Deduplication for Cloud Storage","authors":["T Jiang, X Yuan, Y Chen, K Cheng, L Wang, X Chen… - IEEE Transactions on …, 2022"],"snippet":"Data deduplication is of critical importance to reduce the storage cost for clients and to relieve the unnecessary storage pressure for cloud servers. While various techniques have been proposed for secure deduplication of identical files/blocks …","url":["https://ieeexplore.ieee.org/abstract/document/9804218/"]}
{"year":"2022","title":"Gauging Engagement: Measuring Student Response to a Large-Scale College Advising Field Experiment","authors":["BH Kim, K Meyer, A Choe"],"snippet":"Interactive, text message-based advising programs have become an increasingly common strategy to support college access and success for underrepresented student populations. Despite the proliferation of these programs, we know relatively …","url":["https://edworkingpapers.org/sites/default/files/ai22-567.pdf"]}
{"year":"2022","title":"Gender and Racial Stereotype Detection in Legal Opinion Word Embeddings","authors":["S Matthews, J Hudzina, D Sepehr - 2022"],"snippet":"… In addition, the instrument/weapons control display equivalent effect sizes between Caliskan’s Common Crawl corpus experiments and the legal … Gender Effects: While the effect sizes were comparable between the Common Crawl corpus …","url":["https://www.aaai.org/AAAI22Papers/AISI-10870.MatthewsS.pdf"]}
{"year":"2022","title":"Gender Bias in Machine Translation Systems","authors":["S Ullmann - Artificial Intelligence and Its Discontents, 2022"],"snippet":"… The data was originally obtained from Common Crawl, a free online repository consisting of large amounts of internet data. Consequently, the data represents a variety of registers from formal to informal, slang and even archaic vocabulary due to …","url":["https://link.springer.com/chapter/10.1007/978-3-030-88615-8_7"]}
{"year":"2022","title":"Gender Bias in Meta-Embeddings","authors":["M Kaneko, D Bollegala, N Okazaki - arXiv preprint arXiv:2205.09867, 2022"],"snippet":"Combining multiple source embeddings to create meta-embeddings is considered effective to obtain more accurate embeddings. Different methods have been proposed to develop meta-embeddings from a given set of source embeddings …","url":["https://arxiv.org/pdf/2205.09867"]}
{"year":"2022","title":"Generating Full Length Wikipedia Biographies: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies","authors":["A Fan, C Gardent - arXiv preprint arXiv:2204.05879, 2022"],"snippet":"… Given a person’s name, one or more occupation(s), and CommonCrawl as a source of … name, one or more occupations, a section header, and CommonCrawl as a retrieval corpus. … We reject all CommonCrawl links from Wikipedia, to prevent …","url":["https://arxiv.org/pdf/2204.05879"]}
{"year":"2022","title":"Generating Role-Playing Game Quest Descriptions With the GPT-2 Language Model","authors":["S Värtinen - 2022"],"snippet":"Recent advances in artificial intelligence research have brought forth text-generating language models with promising computational storytelling capabilities. This thesis leveraged one of the most successful Transformer models, GPT-2, to automatically …","url":["https://aaltodoc.aalto.fi/bitstream/handle/123456789/112852/master_V%C3%A4rtinen_Susanna_2022.pdf?sequence=1"]}
{"year":"2022","title":"Generating Scientific Definitions with Controllable Complexity","authors":["T August, K Reinecke, NA Smith"],"snippet":"… We use scientific abstracts, rather than general audience text like Wikipedia or the Common Crawl, for two reasons. First, scientific terms are originally introduced and most commonly used in research papers, making them the most reliable source for …","url":["https://homes.cs.washington.edu/~reinecke/Publications_files/August_ACL2022.pdf"]}
{"year":"2022","title":"Generating Word Embeddings for the Azerbaijani Language and Their Empirical Evaluation on Intrinsic and Extrinsic Evaluation Tasks","authors":["U Suleymanov - 2021"],"snippet":"… al. has also partitioned their data corpora into smaller datasets, such as Common Crawl, Wikipedia, WebText and etc. The machine learning architecture they have built, is one of the largest language models ever built with 175 billion parameters …","url":["http://dspace.khazar.org/bitstream/20.500.12323/5448/1/Generating%20Word%20Embeddings%20for%20the%20Azerbaijani%20Language%20and%20Their%20Empirical%20Evaluation%20on%20Intrinsic%20and%20Extrinsic%20Evaluation%20Tasks.pdf"]}
{"year":"2022","title":"Getting along to get ahead: Predictors of success in an online competitive social environment","authors":["SR Curtis - 2022"],"snippet":"The University of Nevada, Reno Getting along to get ahead: Predictors of success in an online competitive social environment A d Page 1 The University of Nevada, Reno Getting along to get ahead: Predictors of success in an online competitive …","url":["https://scholarworks.unr.edu/bitstream/handle/11714/8172/Curtis_unr_0139D_13732.pdf?sequence=1&isAllowed=y"]}
{"year":"2022","title":"Getting to the root of linguistic alignment: Testing the predictions of Interactive Alignment across developmental and biological variation in language skill","authors":["R Foushee, D Byrne, M Casillas, S Goldin-Meadow - 2022"],"snippet":"Linguistic alignment—the contingent reuse of our interlocutors' language at all levels of linguistic structure—pervades human dialogue. Here, we design unique measures to capture the degree of linguistic alignment between interlocutors' …","url":["https://osf.io/tqrew/download"]}
{"year":"2022","title":"Global Readiness of Language Technology for Healthcare: What would it Take to Combat the Next Pandemic?","authors":["I Mondal, K Ahuja, M Jain, JO Neil, K Bali… - arXiv preprint arXiv …, 2022"],"snippet":"The COVID-19 pandemic has brought out both the best and worst of language technology (LT). On one hand, conversational agents for information dissemination and basic diagnosis have seen widespread use, and arguably, had an important …","url":["https://arxiv.org/pdf/2204.02790"]}
{"year":"2022","title":"Global-locality preserving projection for word embedding","authors":["B Wang, Y Sun, Y Chu, Z Yang, H Lin - … Journal of Machine Learning and Cybernetics, 2022"],"snippet":"… -the Common Crawl corpus consisting of 840b tokens and a vocabulary containing 2.2 M words with 300-dimensional, the Common Crawl (… Here we use the pre-trained word embedding of the Common Crawl corpus (840b tokens, 2.2 M …","url":["https://link.springer.com/article/10.1007/s13042-022-01574-y"]}
{"year":"2022","title":"GPT-3 and InstructGPT: technological dystopianism, utopianism, and “Contextual” perspectives in AI ethics and industry","authors":["A Chan - AI and Ethics, 2022"],"snippet":"This paper examines the ethical solutions raised in response to OpenAI’s language model Generative Pre-trained Transformer-3 (GPT-3) a year and a half from its release. I argue that hype and fear about GPT-3, even within the Natural Language …","url":["https://link.springer.com/article/10.1007/s43681-022-00148-6"]}
{"year":"2022","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","authors":["S Black, S Biderman, E Hallahan, Q Anthony, L Gao…"],"snippet":"GPT-NeoX-20B is a 20 billion parameter autoregressive language model whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model …","url":["http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf"]}
{"year":"2022","title":"GPTs at Factify 2022: Prompt Aided Fact-Verification","authors":["PK Sahu, S Aggarwal, T Gupta, G Das - arXiv preprint arXiv:2206.14913, 2022"],"snippet":"One of the most pressing societal issues is the fight against false news. The false claims, as difficult as they are to expose, create a lot of damage. To tackle the problem, fact verification becomes crucial and thus has been a topic of interest …","url":["https://arxiv.org/pdf/2206.14913"]}
{"year":"2022","title":"GRILLBot: A flexible conversational agent for solving complex real-world tasks","authors":["C Gemmell, S Fischer, I Mackie, P Owoicho, F Rossetto…"],"snippet":"… We use Common Crawl to download the raw HTML for target domains and develop website-specific wrappers to extract semi-structured information about each task, ie title, author, description, ingredients, images, steps, ratings, videos, etc. We …","url":["https://assets.amazon.science/0c/2c/f214256a43bba8d97ade42c56be0/grillbot-a-flexible-conversational-agent-for-solving-complex-real-world-tasks.pdf"]}
{"year":"2022","title":"Grounding Language by Seeing, Hearing, and Interacting","authors":["R Zellers - 2022"],"snippet":"As humans, our understanding of language is grounded in a rich mental model about \"how the world works.\" As children, we learn this mental model gradually. We take in raw perceptual input about the world through all of our senses, and learn to …","url":["https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/48898/Zellers_washington_0250E_24408.pdf?sequence=1&isAllowed=y"]}
{"year":"2022","title":"Grounding Language Models on External Sources: A Comparison of Knowledge Graphs and Unstructured Text","authors":["T Kaminski"],"snippet":"… To get an idea of the size, WebText, for example, consists of 40GB of text from 8 million documents scraped from the internet, but an even larger example is the corpus used to train T5 [81], named C4, which consists of 750GB of text derived from …","url":["https://isl.anthropomatik.kit.edu/downloads/BA-Tom-Kaminski-final.pdf"]}
{"year":"2022","title":"Handling and Presenting Harmful Text","authors":["L Derczynski, HR Kirk, A Birhane, B Vidgen - arXiv preprint arXiv:2204.14256, 2022"],"snippet":"… enormous internet-scraped datasets such as Common Crawl Corpus or WebText. As these unstructured corpora become larger, the risk of them containing harmful content increases, and the larger the dataset, the more difficult it is for humans to …","url":["https://arxiv.org/pdf/2204.14256"]}
{"year":"2022","title":"Hate speech and offensive language detection in Dravidian languages using deep ensemble framework","authors":["PK Roy, S Bhawal, CN Subalalitha - Computer Speech & Language, 2022"],"snippet":"… It is pretrained with 2.5 TB of CommonCrawl data in 100 languages. Due to the absence of language embedding it can result in much better performances in the case of code-switching and are much better compared to multilingual BERT …","url":["https://www.sciencedirect.com/science/article/pii/S0885230822000250"]}
{"year":"2022","title":"HC4: A New Suite of Test Collections for Ad Hoc CLIR","authors":["D Lawrie, J Mayfield, D Oard, E Yang - arXiv preprint arXiv:2201.09992, 2022","DW Oard, E Yang"],"snippet":"… First, to develop a multilingual document collection that was easy to distribute, we chose the Common Crawl News Collection as the basis for the suite of collections. We applied automatic language identification to determine the language of each …","url":["https://arxiv.org/pdf/2201.09992","https://terpconnect.umd.edu/~oard/pdf/ecir22lawrie.pdf"]}
{"year":"2022","title":"HeLI-OTS, Off-the-shelf Language Identifier for Text","authors":["T Jauhiainen, H Jauhiainen, K Lindén"],"snippet":"This paper introduces HeLI-OTS, an off-the-shelf text language identification tool using the HeLI language identification method. The HeLI-OTS language identifier is equipped with language models for 200 languages and licensed for academic as …","url":["http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.416.pdf"]}
{"year":"2022","title":"HICEM: A High-Coverage Emotion Model for Artificial Emotional Intelligence","authors":["B Wortman, JZ Wang - arXiv preprint arXiv:2206.07593, 2022"],"snippet":"… Trained on Common Crawl and Wikipedia, this model provides a 300-dimensional embedding for each word. FastText was chosen over … We then once again proceeded with Facebook’s FastText models which have been trained on Common …","url":["https://arxiv.org/pdf/2206.07593"]}
{"year":"2022","title":"Hierarchical Agglomerative Graph Clustering in Poly-Logarithmic Depth","authors":["L Dhulipala, D Eisenstat, J Łącki, V Mirronki, J Shi - arXiv preprint arXiv:2206.11654, 2022"],"snippet":"Obtaining scalable algorithms for hierarchical agglomerative clustering (HAC) is of significant interest due to the massive size of real-world datasets. At the same time, efficiently parallelizing HAC is difficult due to the seemingly sequential nature of the …","url":["https://arxiv.org/pdf/2206.11654"]}
{"year":"2022","title":"Hierarchical Graph Decomposition for Natural Language Generation","authors":["L Jin - 2022"],"snippet":"Central to natural language understanding and generation is the ability to accurately model the semantics of text. We can frame understanding as parsing a semantic form from a given sentence that represents all sentences of equivalent meaning …","url":["https://search.proquest.com/openview/8bba4ec7c1c89c32f0af8202a5fd0c9b/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2022","title":"Hierarchical Label-wise Attention Transformer Model for Explainable ICD Coding","authors":["L Liu, O Perez-Concha, A Nguyen, V Bennett, L Jorm - arXiv preprint arXiv …, 2022"],"snippet":"International Classification of Diseases (ICD) coding plays an important role in systematically classifying morbidity and mortality data. In this study, we propose a hierarchical label-wise attention Transformer model (HiLAT) for the explainable …","url":["https://arxiv.org/pdf/2204.10716"]}
{"year":"2022","title":"High-frequency words have higher frequencies in Turkish social sciences article","authors":["N Gürsakal, S Çelik, S Özdemir - Quality & Quantity, 2022"],"snippet":"Abstract Words, sentences, and paragraphs are the basis of texts. When we consider texts as data and want to establish a relationship between qualitative and quantitative perspectives, we can do this with the word frequencies in a text. We aim …","url":["https://link.springer.com/article/10.1007/s11135-022-01444-3"]}
{"year":"2022","title":"Hints of Independence in a Pre-scripted World: On Controlled Usage of Open-domain Language Models for Chatbots in Highly Sensitive Domains","authors":["E Basar, I Hendrickx, E Krahmer, GJ de Bruijn, T Bosse - Proceedings of the 14th …, 2022"],"snippet":"… Because the training approach is unsupervised, it is possible to include a massive amount of data from various online sources, such as Wikipedia and CommonCrawl, with minimal human labour required. Hence, these neural language …","url":["https://www.scitepress.org/Papers/2022/109143/109143.pdf"]}
{"year":"2022","title":"Hints of Independence Independence Independence in a Pre-scripted e-scripted e-scripted World: On Controlled Usage of Open-domain Open-domain Open-domain …","authors":["EBE Basar, I Hendrickx, E Krahmer, GJ de Bruijn… - 2022"],"snippet":"… Because the Because the Because the training approach is unsupervised, unsupervised, unsupervised, it is possible to include a massive amount of data from various online sources, such as Wikipedia and CommonCrawl, CommonCrawl …","url":["https://repository.uantwerpen.be/docstore/d:irua:11354"]}
{"year":"2022","title":"Hipsters and the cool: A game theoretic analysis of identity expression, trends, and fads.","authors":["R Golman, EH Bugbee, A Jain, S Saraf - Psychological Review, 2021"],"snippet":"Cultural trends and popularity cycles can be observed all around us, yet our theories of social influence and identity expression do not explain what perpetuates these complex, often unpredictable social dynamics. We propose a theory of social identity …","url":["https://psycnet.apa.org/record/2022-16685-001"]}
{"year":"2022","title":"Homepage2Vec: Language-Agnostic Website Embedding and Classification","authors":["S Lugeon, T Piccardi, R West - Proceedings of the International AAAI Conference on …, 2022"],"snippet":"… Following this intuition, we collected from Common Crawl,5 a large-scale sample of the Web, the 19 most frequent TLDs: .com, .org, .net, .info, … We consider the 30 most frequent metatags from a sample of the Common Crawl and represent their …","url":["https://ojs.aaai.org/index.php/ICWSM/article/download/19380/19152"]}
{"year":"2022","title":"Homophobic and Hate Speech Detection Using Multilingual-BERT Model on Turkish Social Media","authors":["H Karayiğit, A Akdagli, Çİ ACI - Information Technology and Control, 2022"],"snippet":"… The GloVe algorithm used in this study is trained on Common Crawl [24]. There are 253K words in the vocabulary and the dimension size is 300. Training data is web-crawled multilingual text with 2,736B … Available: https:// commoncrawl.org/2021 …","url":["http://apbs.mersin.edu.tr/files/caci/Publications_003.pdf"]}
{"year":"2022","title":"How Using a Foreign Language Influences Perceived Fairness","authors":["G Yu-Buck, A Mishra, H Mishra - ACR North American Advances, 2021"],"snippet":"… We used pre-trained word embedding vectors that were trained with the fastText algorithm and the Common Crawl corpus. The association between the fairness construct and each review is the dependent variable. We followed the procedure of …","url":["https://www.acrwebsite.org/volumes/v49/acr_vol49_3001028.pdf"]}
{"year":"2022","title":"Human Behavior in Domestic Environments: Prediction and Applications","authors":["S Zehtabian - 2021"],"snippet":"… However, many deep learning algorithms work best under big data regimes, where the number of data samples is counted from the tens of thousands (eg the MNIST dataset) to 500 billion (the Common Crawl dataset used to train the GPT-3 …","url":["https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1944&context=etd2020"]}
{"year":"2022","title":"HUMAN-INSPIRED COMPUTER VISION & REASONING","authors":["HJ Yi, J Tang, LJ Yu, WM Xuan"],"snippet":"… We used 300-dimensional GloVe vectors for the word embedding layer, which are pre-trained on the Common Crawl corpus consisting of 840B tokens and a 2.2M words vocabulary list [6]. GloVe vectors act as a non-trainable base to learn …","url":["https://www.dsta.gov.sg/ydsp/projects/pdf/research/reports/human-inspired-computer-vision-reasoning.pdf"]}
{"year":"2022","title":"Hypernymy Detection for Low-resource Languages: A Study for Hindi, Bengali, and Amharic","authors":["A Jana, G Venkatesh, SM Yimam, C Biemann - Transactions on Asian and Low …, 2022"],"snippet":"… Even though the focus of our study is not claiming superiority of the distributional semantic models over fastText, given that DT embeddings are prepared from a smaller corpus as compared to the corpus (Wikipedia, Common Crawl) on which …","url":["https://dl.acm.org/doi/abs/10.1145/3490389"]}
{"year":"2022","title":"HyperTree Proof Search for Neural Theorem Proving","authors":["G Lample, MA Lachaux, T Lavril, X Martinet, A Hayat… - arXiv preprint arXiv …, 2022"],"snippet":"We propose an online training procedure for a transformer-based automated theorem prover. Our approach leverages a new search algorithm, HyperTree Proof Search (HTPS), inspired by the recent success of AlphaZero. Our model learns from …","url":["https://arxiv.org/pdf/2205.11491"]}
{"year":"2022","title":"iCAR: Bridging Image Classification and Image-text Alignment for Visual Recognition","authors":["Y Wei, Y Cao, Z Zhang, Z Yao, Z Xie, H Hu, B Guo - arXiv preprint arXiv:2204.10760, 2022"],"snippet":"… It extracts image-caption pairs in random web pages crawled between 2014 and 2021 from the Common Crawl web data, and de-noises the dataset by scoring and filtering numerous raw pairs with a threshold using a CLIP ViT-B/32 [40] model …","url":["https://arxiv.org/pdf/2204.10760"]}
{"year":"2022","title":"Identifying Implicitly Abusive Remarks about Identity Groups using a Linguistically Informed Approach","authors":["M Wiegand, E Eder, J Ruppenhofer - Proceedings of the 2022 Conference of the …, 2022"],"snippet":"… regression classifier where each verb was represented by its (publicly available) word embedding induced on Common Crawl (Mikolov et al.… As features, we represented each verb by its word embedding from Common Crawl. (Using such a …","url":["https://aclanthology.org/2022.naacl-main.410.pdf"]}
{"year":"2022","title":"IDEOLOGY DETECTION USING TRANSFORMER-BASED MACHINE LEARNING MODELS","authors":["O ÖZTÜRK, A ÖZCAN"],"snippet":"Ideology detection has been a challenging but essential problem that has been studied for a long time. Certain groups and organizations, such as politicians, rely on people’s political views to make wise or forward-looking decisions. In the previous …","url":["https://www.researchgate.net/profile/Oktay-Ozturk-3/publication/357660800_Ideology_Detection_Using_Transformer-Based_Machine_Learning_Models/links/61d87712e669ee0f5c8f1b97/Ideology-Detection-Using-Transformer-Based-Machine-Learning-Models.pdf"]}
{"year":"2022","title":"IgboBERT Models: Building and Training Transformer Models for the Igbo Language","authors":["C Chukwuneke, I Ezeani, P Rayson, M El-Haj"],"snippet":"This work presents a standard Igbo named entity recognition (IgboNER) dataset as well as the results from training and fine-tuning state-of-the-art transformer IgboNER models. We discuss the process of our dataset creation-data collection and …","url":["http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.547.pdf"]}
{"year":"2022","title":"Imbalanced data as risk factor of discriminating automated decisions","authors":["A Vetrò - 2021"],"snippet":"… experiment on the search engine Common Crawl [17]. The authors compared three techniques of machine learning for occupational classification with almost 400.000 collected biographies. In all cases, even without explicitly using gender …","url":["https://www.jipitec.eu/issues/jipitec-12-4-2021/5452/vetro_pdf.pdf"]}
{"year":"2022","title":"Impact of preprocessing and word embedding on extreme multi-label patent classification tasks","authors":["G Jung, J Shin, S Lee - Applied Intelligence, 2022"],"snippet":"… The Word2Vec pre-trained model learned approximately 3 million words using GoogleNews as a dataset, whereas the GloVe pre-trained model learned approximately 2.2 million words using the Common Crawl dataset [30, 31]. Because …","url":["https://link.springer.com/article/10.1007/s10489-022-03655-5"]}
{"year":"2022","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning","authors":["Y Razeghi, RL Logan IV, M Gardner, S Singh - arXiv preprint arXiv:2202.07206, 2022"],"snippet":"Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper …","url":["https://arxiv.org/pdf/2202.07206"]}
{"year":"2022","title":"Impact of Sentence Representation Matching in Neural Machine Translation","authors":["H Jung, K Kim, JH Shin, SH Na, S Jung, S Woo - Applied Sciences, 2022"],"snippet":"… To build a training set, we merged the Europarl parallel corpus and the Common-crawl corpus released from WMT-14 (http://www.statmt.org/wmt14/, accessed on 18 January 2022). We applied tokenizing and lowercasing and limited the length of …","url":["https://www.mdpi.com/2076-3417/12/3/1313/pdf"]}
{"year":"2022","title":"Impact of Tokenization on Language Models: An Analysis for Turkish","authors":["C Toraman, EH Yilmaz, F Şahinuç, O Ozcelik - arXiv preprint arXiv:2204.08832, 2022"],"snippet":"… OSCAR is a multilingual corpus that is obtained by filtering of the Common Crawl corpus that maintain an open repository of publicly available web pages. We use the split of this corpus prepared for Turkish. However, we observe that this split includes …","url":["https://arxiv.org/pdf/2204.08832"]}
{"year":"2022","title":"Implicit Social Cognition","authors":["B Kurdi, MR Banaji"],"snippet":"Among the central features of the mind that drive social behavior are the attitudes and beliefs that humans bring to their engagement with other humans. Accordingly, ever since the inception of the field, research in social psychology has been guided …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=wD90EAAAQBAJ&oi=fnd&pg=PA323&dq=commoncrawl&ots=cFuiqkafcD&sig=PPJezjT4cihMsBdiO_1wITIQ09w"]}
{"year":"2022","title":"Improving Address Sequence Tagger Using Geographical Context","authors":["L Eriksson, M Olsson"],"snippet":"For humans, parsing an address into its components such as street and city is easy but time-consuming. Computers can have a harder time with such tasks. There are existing tools that use machine learning which has proven to be effective but still …","url":["https://lup.lub.lu.se/student-papers/record/9085667/file/9085668.pdf"]}
{"year":"2022","title":"Improving BERTScore for Machine Translation Evaluation Through Contrastive Learning","authors":["O Yousuf - 2022"],"snippet":"… Scaled to one hundred languages, XLM-RoBERTa was pre-trained on 2.5TB of filtered CommonCrawl data (CC-100). However, it is … Furthermore, quality of data also poses challenges, as CommonCrawl or parallel localization data from projects …","url":["https://www.diva-portal.org/smash/get/diva2:1671848/FULLTEXT01.pdf"]}
{"year":"2022","title":"Improving searchability of datasets","authors":["E Kacprzak - 2022"],"snippet":"Data is one of the most important digital assets in the world thanks to its business and social value. As is becoming increasingly available online, in order to use it effectively, we need tools that allow us to retrieve the most relevant datasets that …","url":["https://eprints.soton.ac.uk/457260/1/Emilia_Kacprzak_PhD_WAIS_27_March.pdf"]}
{"year":"2022","title":"Improving Short Text Classification With Augmented Data Using GPT-3","authors":["S Balkus, D Yan - arXiv preprint arXiv:2205.10981, 2022"],"snippet":"GPT-3 is a large-scale natural language model developed by OpenAI that can perform many different tasks, including topic classification. Although researchers claim that it requires only a small number of in-context examples to learn a task, in …","url":["https://arxiv.org/pdf/2205.10981"]}
{"year":"2022","title":"Improving Taxonomy-based Categorization with Categorical Graph Neural Networks","authors":["T Du, K Chang, P Liu, R Zhang - 2021 IEEE International Conference on Big Data …, 2021"],"snippet":"… Then each query was converted to a 300-dimensional embedding by averaging each word’s FastText embedding [23] (pretrained on the common-crawl dataset). Each FastText word embedding was L2 normalized before use. The query …","url":["https://ieeexplore.ieee.org/abstract/document/9671372/"]}
{"year":"2022","title":"Improving the Accessibility of Arabic Electronic Theses and Dissertations (ETDs) with Metadata and Classification","authors":["E Abdelrahman - 2021"],"snippet":"Much research work has been done to extract data from scientific papers, journals, and articles. However, Electronic Theses and Dissertations (ETDs) remain an unexplored genre of data in the research fields of natural language processing and …","url":["https://vtechworks.lib.vt.edu/bitstream/handle/10919/107790/Abdelrahman_E_T_2021.pdf?sequence=1&isAllowed=y"]}
{"year":"2022","title":"Improving Word Translation via Two-Stage Contrastive Learning","authors":["Y Li, F Liu, N Collier, A Korhonen, I Vulić - arXiv preprint arXiv:2203.08307, 2022"],"snippet":"Word translation or bilingual lexicon induction (BLI) is a key cross-lingual task, aiming to bridge the lexical gap between different languages. In this work, we propose a robust and effective two-stage contrastive learning framework for the BLI …","url":["https://arxiv.org/pdf/2203.08307"]}
{"year":"2022","title":"In Situ Answer Sentence Selection at Web-scale","authors":["Z Zhang, T Vu, A Moschitti - arXiv preprint arXiv:2201.05984, 2022"],"snippet":"… Question Sampling We sample the questions from a large collection of information inquiries collected from Web data, eg, CommonCrawl. … The candidates are extracted from 30MM passages, retrieved from a large index having …","url":["https://arxiv.org/pdf/2201.05984"]}
{"year":"2022","title":"INAUGURAL ARTICLE by a Recently Elected Academy Member: When danger strikes: A linguistic tool for tracking America's collective response to threats","authors":["VK Choi, S Shrestha, X Pan, MJ Gelfand - Proceedings of the National Academy of …, 2022"],"snippet":"In today’s vast digital landscape, people are constantly exposed to threatening language, which attracts attention and activates the human brain’s fear circuitry. However, to date, we have lacked the tools needed to identify threatening language …","url":["https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8795557/"]}
{"year":"2022","title":"IndicXNLI: Evaluating Multilingual Inference for Indian Languages","authors":["D Aggarwal, V Gupta, A Kunchukuttan - arXiv preprint arXiv:2204.08776, 2022"],"snippet":"… XLM-RoBERTa is pre-trained using the common crawl monolingual data. mBERT (cased/uncased) includes pre-training on nine of eleven Indic lan… These model use the Indic enrich monolingual corpora: Common Crawl, Oscar and IndicCorp and …","url":["https://arxiv.org/pdf/2204.08776"]}
{"year":"2022","title":"InfoDCL: A Distantly Supervised Contrastive Learning Framework for Social Meaning","authors":["C Zhang, M Abdul-Mageed, G Jawahar - arXiv preprint arXiv:2203.07648, 2022"],"snippet":"Existing supervised contrastive learning frameworks suffer from two major drawbacks: (i) they depend on labeled data, which is limited for the majority of tasks in real-world, and (ii) they incorporate inter-class relationships based on instance-level information …","url":["https://arxiv.org/pdf/2203.07648"]}
{"year":"2022","title":"Instance-Specific Feature Propagation for Referring Segmentation","authors":["C Liu, X Jiang, H Ding - IEEE Transactions on Multimedia, 2022"],"snippet":"… We follow prior work [12], [36] and adopt the GloVe word embeddings [32] on Common Crawl 840B tokens. We use Adam with the base learning rate of 0.001 for optimization. Images are resized to 416 × 416 before sending to the network, and …","url":["https://ieeexplore.ieee.org/abstract/document/9745353/"]}
{"year":"2022","title":"Integrating character-level and word-level representation for affect in Arabic tweets","authors":["AI Alharbi, P Smith, M Lee - Data & Knowledge Engineering, 2022"],"snippet":"Affect tasks, which range from sentiment polarity classification to finer grained sentiment strength and emotional intensity detection, have become of increasing interest due to the vast amount of user-generated content and advanced learning …","url":["https://www.sciencedirect.com/science/article/pii/S0169023X21000938"]}
{"year":"2022","title":"Intelligent Techniques to Accelerate Everyday Text Communication","authors":["JK Adhikary - 2022"],"snippet":"People with some form of speech-or motor-impairments usually use a high-tech augmentative and alternative communication (AAC) device to communicate with other people in writing or in face-to-face conversations. Their text entry rate on these …","url":["https://digitalcommons.mtu.edu/cgi/viewcontent.cgi?article=2550&context=etdr"]}
{"year":"2022","title":"Interactive capsule network for implicit sentiment analysis","authors":["Y Qian, J Wang, D Li, X Zhang - Applied Intelligence, 2022"],"snippet":"Existing sentiment analysis models mainly rely on evident emotive words within phrases. When the apparent emotional words within phrases are eliminated, the performance of these models will inevitably decrease. The implicit communication of …","url":["https://link.springer.com/article/10.1007/s10489-022-03584-3"]}
{"year":"2022","title":"Interactive Question Answering","authors":["E Toledo"],"snippet":"Artificial intelligence has long aspired to create systems that can perform real-world activities and converse with humans using natural language. In the pursuit of this goal, Interactive Question Answering was proposed. Specifically for language …","url":["https://showcase.sit.uct.ac.za/static/media/122/9ed8c65d-3e35-43d9-86ab-acd83c3abb7c.pdf"]}
{"year":"2022","title":"International Journal of Knowledge Processing Studies","authors":["SS Moosavi, S Ciruskabiri, A Varnaseri"],"snippet":"The speed and ease of production and loading of content on the web, the exponential increase in the volume and short life cycle of data in this space, have made the processes of collection, refinement, organization, storage, management …","url":["http://kps.artahub.ir/article_152778_d3fab5eab308c5bb383c1a874e0354d4.pdf"]}
{"year":"2022","title":"Internet-augmented language models through few-shot prompting for open-domain question answering","authors":["A Lazaridou, E Gribovskaya, W Stokowiec, N Grigorev - arXiv preprint arXiv …, 2022"],"snippet":"In this work, we aim to capitalize on the unique few-shot capabilities offered by large-scale language models to overcome some of their challenges with respect to grounding to factual and up-to-date information. Motivated by semi-parametric language models …","url":["https://arxiv.org/pdf/2203.05115"]}
{"year":"2022","title":"Interpreting Arabic Transformer Models","authors":["A Abdelali, N Durrani, F Dalvi, H Sajjad - arXiv preprint arXiv:2201.07434, 2022"],"snippet":"Arabic is a Semitic language which is widely spoken with many dialects. Given the success of pre-trained language models, many transformer models trained on Arabic and its dialects have surfaced. While these models have been compared with …","url":["https://arxiv.org/pdf/2201.07434"]}
{"year":"2022","title":"Interpreting Logical Metonymy through Dense Paraphrasing","authors":["B Ye, J Tu, E Jezek, J Pustejovsky - Proceedings of the Annual Meeting of the …, 2022"],"snippet":"… The extensive usage of C4 and the nature of it being sourced from the public Common Crawl web scrape ensure its huge volume and various language styles compared to other public datasets, which provides a “real world” text distribution for our task. …","url":["https://escholarship.org/content/qt19k4w0c1/qt19k4w0c1.pdf"]}
{"year":"2022","title":"Investigating language relationships in multilingual sentence encoders through the lens of linguistic typology","authors":["R Choenni, E Shutova - Computational Linguistics, 2022"],"snippet":"Multilingual sentence encoders have seen much success in cross-lingual model transfer for downstream NLP tasks. The success of this transfer is, however, dependent on the model’s ability to encode the patterns of cross-lingual similarity …","url":["https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli_a_00444/2008428/coli_a_00444.pdf"]}
{"year":"2022","title":"Investigating the Country of Origin and the Role of the. eu TLD in External Trade of European Union Member States","authors":["A Giannakoulopoulos, M Pergantis, L Limniati… - Future Internet, 2022"],"snippet":"… that queried the Common Crawl Index servers for Web pages and collected all the unique domains found. Common Crawl is a non-profit foundation providing access to web information through an open database of web crawl data [28]. The …","url":["https://www.mdpi.com/1999-5903/14/6/174/pdf?version=1654339602"]}
{"year":"2022","title":"Investigating the Efficient Use of Word Embedding with Neural-Topic Models for Interpretable Topics from Short Texts","authors":["R Murakami, B Chakraborty - Sensors, 2022"],"snippet":"With the rapid proliferation of social networking sites (SNS), automatic topic extraction from various text messages posted on SNS are becoming an important source of information for understanding current social trends or needs. Latent …","url":["https://www.mdpi.com/1424-8220/22/3/852/pdf"]}
{"year":"2022","title":"Investigating the Quality of Static Anchor Embeddings from Transformers for Under-Resourced Languages","authors":["P Singh, O De Clercq, E Lefever - The 1st Annual Meeting of the ELRA/ISCA Special …, 2022"],"snippet":"… This problem could be solved by adding more monolingual data (from Common Crawl, for example) for the anchor extraction step. We also … Since most methodologies use the Wikipedia and/or the Common Crawl corpus as initial pre-training …","url":["https://biblio.ugent.be/publication/8759657/file/8759658.pdf"]}
{"year":"2022","title":"Investigations of Performance and Bias in Human-AI Teamwork in Hiring","authors":["A Peng, B Nushi, E Kiciman, K Inkpen, E Kamar - arXiv preprint arXiv:2202.11812, 2022"],"snippet":"In AI-assisted decision-making, effective hybrid (human-AI) teamwork is not solely dependent on AI performance alone, but also on its impact on human decision-making. While prior work studies the effects of model accuracy on humans, we endeavour …","url":["https://arxiv.org/pdf/2202.11812"]}
{"year":"2022","title":"Is this Question Real? Dataset Collection on Perceived Intentions and Implicit Attack Detection","authors":["MS Mirzaei, K Meshgi, S Sekine - Proceedings of the ACM Web Conference 2022, 2022"],"snippet":"… We used GloVe [38] word embedding (300d version on 840B Common Crawl data) for obtaining vector representations for words and Xavier initialization for the parameters. The batch size is set to 32, and the loss is calculated using binary cross-entropy …","url":["https://dl.acm.org/doi/abs/10.1145/3485447.3512005"]}
{"year":"2022","title":"Isomorphic Cross-lingual Embeddings for Low-Resource Languages","authors":["S Sannigrahi, J Read - arXiv preprint arXiv:2203.14632, 2022"],"snippet":"… 2018) which uses Wikipedia dumps and Common Crawl for all languages. In addition to this, we use available parallel data between the following related language pairs respectively: English-Hindi (hi) for Nepali, English-Estonian (et) for …","url":["https://arxiv.org/pdf/2203.14632"]}
{"year":"2022","title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation","authors":["G Sarti, M Nissim - arXiv preprint arXiv:2203.03759, 2022"],"snippet":"… 2020), a large collection including roughly 750GB of webscraped English texts sourced from the CommonCrawl. This was subsequently cleaned employing heuristics aimed at removing templated fillers, text deduplication, Javascript code …","url":["https://arxiv.org/pdf/2203.03759"]}
{"year":"2022","title":"Jointly Multi-Similarity Loss for Deep Metric Learning","authors":["L Zhang, S Shen, L Li, H Wang, X Li, J Lang - 2021 IEEE International Conference on …, 2021"],"snippet":"… In particular, all of the input sentences are split and initialized in the word representation layer with the 300-dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus [24], and the embeddings of the out-of-vocabulary …","url":["https://ieeexplore.ieee.org/abstract/document/9679139/"]}
{"year":"2022","title":"JParaCrawl v3. 0: A Large-scale English-Japanese Parallel Corpus","authors":["M Morishita, K Chousa, J Suzuki, M Nagata - arXiv preprint arXiv:2202.12607, 2022"],"snippet":"… To list such parallel websites, we analyzed all the Common Crawl text archive data released from March 2019 to August 20213. We identified the language in the archive by CLD24 and listed 100,000 large websites that roughly have the same …","url":["https://arxiv.org/pdf/2202.12607"]}
{"year":"2022","title":"Just Rank: Rethinking Evaluation with Word and Sentence Similarities","authors":["B Wang, CCJ Kuo, H Li - arXiv preprint arXiv:2203.02679, 2022"],"snippet":"Word and sentence embeddings are useful feature representations in natural language processing. However, intrinsic evaluation for embeddings lags far behind, and there has been no significant update since the past decade. Word and sentence …","url":["https://arxiv.org/pdf/2203.02679"]}
{"year":"2022","title":"Just-In-Time Obsolete Comment Detection and Update","authors":["Z Liu, X Xia, D Lo, M Yan, S Li - IEEE Transactions on Software Engineering, 2021"],"snippet":"Comments are valuable resources for the development, comprehension and maintenance of software. However, while changing code, developers sometimes neglect the evolution of the corresponding comments, resulting in obsolete …","url":["https://ieeexplore.ieee.org/abstract/document/9664004/"]}
{"year":"2022","title":"k 𝑘 k italic_k-Nearest Neighbor Augmented Neural Networks for Text Classification","authors":["Z Wang, W Hamza, L Song"],"snippet":"In recent years, many deep-learning based models are proposed for text classification. This kind of models well fits the training set from the statistical point of view. However, it lacks the capacity of utilizing instance-level information from …","url":["https://ar5iv.labs.arxiv.org/html/1708.07863"]}
{"year":"2022","title":"Keyword Extraction in Scientific Documents","authors":["SX Rao, P Piriyatamwong, P Ghoshal, S Nasirian… - arXiv preprint arXiv …, 2022"],"snippet":"… As an additional hint to participants, document embeddings for each item in the train and test sets, as well as word embeddings for the entire corpus, were generated from a fastText model2 trained on the English Common Crawl dataset (cc.en.300.bin)3. …","url":["https://arxiv.org/pdf/2207.01888"]}
{"year":"2022","title":"Knowledge Complementarity between Seed Investors and Startup Founders: Lessons from Accelerators","authors":["S Santamaria, S Breschi"],"snippet":"Which entrepreneurs benefit most from accelerator programs? In this paper, we argue that the value-added by startup accelerators depends critically on whether the knowledge these supporting institutions provide is complementary to or a substitute …","url":["https://kid2021.sciencesconf.org/data/program/kid2021_breschi.pdf"]}
{"year":"2022","title":"Knowledge extraction from fictional texts","authors":["CX Chu - 2022"],"snippet":"Knowledge extraction from text is a key task in natural language processing, which involves many sub-tasks, such as taxonomy induction, named entity recognition and typing, relation extraction, knowledge canonicalization and so on. By constructing …","url":["https://publikationen.sulb.uni-saarland.de/bitstream/20.500.11880/32914/1/thesis-twoside-toprint.pdf"]}
{"year":"2022","title":"KNOWLEDGE REPRESENTATIONS+ SCHEMATA","authors":["LJ Martin"],"snippet":"… Data Source: Common Crawl …","url":["https://interactive-fiction-class.org/slides/CIS700-KnowledgeBases.pdf"]}
{"year":"2022","title":"Korean and Multilingual Language Models Study for Cross-Lingual Post-Training (XPT)","authors":["S Son, C Park, J Lee, M Shim, C Lee, K Park, H Lim - Journal of the Korea …, 2022"],"snippet":"It has been proven through many previous researches that the pretrained language model with a large corpus helps improve performance in various natural language processing tasks. However, there is a limit to building a large-capacity corpus for …","url":["https://www.koreascience.or.kr/article/JAKO202210459401088.pdf"]}
{"year":"2022","title":"L3Cube-MahaCorpus and MahaBERT: Marathi Monolingual Corpus, Marathi BERT Language Models, and Resources","authors":["R Joshi - arXiv preprint arXiv:2202.01159, 2022"],"snippet":"We present L3Cube-MahaCorpus a Marathi monolingual data set scraped from different internet sources. We expand the existing Marathi monolingual corpus with 24.8M sentences and 289M tokens. We further present, MahaBERT, MahaAlBERT …","url":["https://arxiv.org/pdf/2202.01159"]}
{"year":"2022","title":"L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and BERT models","authors":["A Velankar, H Patil, A Gore, S Salunke, R Joshi - arXiv preprint arXiv:2203.13778, 2022"],"snippet":"Social media platforms are used by a large number of people prominently to express their thoughts and opinions. However, these platforms have contributed to a substantial amount of hateful and abusive content as well. Therefore, it is important …","url":["https://arxiv.org/pdf/2203.13778"]}
{"year":"2022","title":"Label Semantic Aware Pre-training for Few-shot Text Classification","authors":["A Mueller, J Krone, S Romeo, S Mansour, E Mansimov… - arXiv preprint arXiv …, 2022"],"snippet":"In text classification tasks, useful information is encoded in the label names. Label semantic aware systems have leveraged this information for improved text classification performance during fine-tuning and prediction. However, use of label-semantics …","url":["https://arxiv.org/pdf/2204.07128"]}
{"year":"2022","title":"Landing AI on Networks: An equipment vendor viewpoint on Autonomous Driving Networks","authors":["D Rossi, L Zhang"],"snippet":"… Similarly, recently hyped advances [21] in NLP relied on hundred billions of text tokens corpus such as Common Crawl [47]. Even in lesser massmediatized fields such as computational biology, it is fairly well recognized that astonishing advances [48] …","url":["https://nonsns.github.io/paper/rossi22ai4net_vision.pdf"]}
{"year":"2022","title":"Language and Gender","authors":["J Berger, R Boghrati, S Rathee, R Pogacar, A Mecit - ACR North American Advances, 2021"],"snippet":"… We use Common Crawl and 11 million product reviews from Amazon and Yelp, to ask whether algorithms learn to associate women with … Common Crawl is a snapshot of the contents of the World Wide Web, a text repository of 2.45 billion web …","url":["https://www.acrwebsite.org/volumes/v49/acr_vol49_3000129.pdf"]}
{"year":"2022","title":"Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion","authors":["K Shuster, M Komeili, L Adolphs, S Roller, A Szlam… - arXiv preprint arXiv …, 2022"],"snippet":"… Knowledge Module Task To construct our knowledge task, we also start with Common Crawl, splitting it into sentences. We construct a Lucene5 search over Common Crawl, and then, for a given target sentence of a document, we find the …","url":["https://arxiv.org/pdf/2203.13224"]}
{"year":"2022","title":"Language Pre-Training and Auxiliary Tasks for Vision and Language Navigation","authors":["S Bhatt - 2022"],"snippet":"The Vision and Language Navigation task came to life from the idea that we can build a robot or an autonomous system that can be instructed in human language and that will navigate using the instructions given. For example, we tell the agent to “Go …","url":["https://rc.library.uta.edu/uta-ir/bitstream/handle/10106/30243/BHATT-THESIS-2021.pdf?sequence=1&isAllowed=y"]}
{"year":"2022","title":"Language-Agnostic Website Embedding and Classification","authors":["S Lugeon, T Piccardi, R West - arXiv preprint arXiv:2201.03677, 2022"],"snippet":"… Following this intuition, we collected from Common Crawl4, a large-scale sample of the Web, the 19 most frequent TLDs: .com, .org, .net, .info, … We consider the 30 most frequent metatags from a sample of the Common Crawl and represent their …","url":["https://arxiv.org/pdf/2201.03677"]}
{"year":"2022","title":"Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation","authors":["Z Ding, T Hui, J Huang, X Wei, J Han, S Liu - … of the IEEE/CVF Conference on …, 2022"],"snippet":"… tic inputs, we adopt LSTM [9] to extract language features from GloVe word embeddings [32] pretrained on Common Crawl with 840B tokens. The maximum length of the input sentence is 25. We set the frame interval δ = 6 for calculating the …","url":["https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Language-Bridged_Spatial-Temporal_Interaction_for_Referring_Video_Object_Segmentation_CVPR_2022_paper.pdf"]}
{"year":"2022","title":"Large pre-trained language models contain human-like biases of what is right and wrong to do","authors":["P Schramowski, C Turan, N Andersen, CA Rothkopf… - Nature Machine Intelligence, 2022"],"snippet":"Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, GPT-2 and GPT-3. Using them as pre-trained models and fine-tuning them for specific tasks, researchers have …","url":["https://www.nature.com/articles/s42256-022-00458-8"]}
{"year":"2022","title":"Large-scale Bilingual Language-Image Contrastive Learning","authors":["B Ko, G Gu - arXiv preprint arXiv:2203.14463, 2022"],"snippet":"This paper is a technical report to share our experience and findings building a Korean and English bilingual multimodal model. While many of the multimodal datasets focus on English and multilingual multimodal research uses machine-translated …","url":["https://arxiv.org/pdf/2203.14463"]}
{"year":"2022","title":"Large-scale product classification for efficient matching in procurement systems","authors":["I Hrysha - 2022"],"snippet":"We consider the problem of recommending relevant suppliers given detailed request context in a procurement setting. The fundamental recommendation in procurement systems is that a single query has potentially hundreds of relevant …","url":["http://www.er.ucu.edu.ua:8080/bitstream/handle/1/3154/Hrysha%2C%20Ihor%20-%20Large-scale%20product%20classification%20for%20efficient%20matching%20in%20procurement%20systems.pdf?sequence=1&isAllowed=y"]}
{"year":"2022","title":"Large-scale Semi-supervised Learning of Neural NLP Models","authors":["S Kiyono - 2022"],"snippet":"Deep neural network (DNN) based approaches have achieved remarkable performance in multiple research fields, such as natural language processing, computer vision, and speech processing. However, it is widely acknowledged that …","url":["https://www.cl.ecei.tohoku.ac.jp/publications/2022/kiyono-dthesis.pdf"]}
{"year":"2022","title":"Laying the foundations for benchmarking open data automatically","authors":["AS Correa, FSC Da Silva"],"snippet":"… Common Crawl And Unlocking Web Archives For Research. Retrieved 2019-01-13 from https://www.forbes.com/sites/kalevleetaru/2017/09… Common Crawl And Unlocking Web Archives For Research. Retrieved 2019-01-13 from https://www.forbes.com/sites/kalevleetaru/2017/09…","url":["https://ouci.dntb.gov.ua/en/works/9j0eJBj4/"]}
{"year":"2022","title":"Learner Profiling: Demographics Identification Based on NLP, Machine Learning, and MOOCs Metadata","authors":["T ALJOHANI, M MUSLIH - 2022"],"snippet":"Massive Open Online Courses (MOOCs) have become universal learning resources, and the COVID- 19 pandemic is rendering these platforms even more necessary. Many types of research are ongoing to improve the learning resources provided to …","url":["http://etheses.dur.ac.uk/14486/1/Thesis__Final_Version.pdf"]}
{"year":"2022","title":"Learning GUI Completions with User-defined Constraints","authors":["L Brückner, LA Leiva, A Oulasvirta - ACM Transactions on Interactive Intelligent …, 2022","LA LEIVA - 2022"],"snippet":"A key objective in the design of graphical user interfaces (GUIs) is to ensure consistency across screens of the same product. However, designing a compliant layout is time-consuming and can distract designers from creative thinking. This …","url":["https://dl.acm.org/doi/abs/10.1145/3490034","https://research.aalto.fi/files/82497867/learning_gui_completions.pdf"]}
{"year":"2022","title":"Learning interpretable word embeddings via bidirectional alignment of dimensions with semantic concepts","authors":["LK Şenel, F Şahinuç, V Yücesoy, H Schütze, T Çukur… - Information Processing & …, 2022"],"snippet":"We propose bidirectional imparting or BiImp, a generalized method for aligning embedding dimensions with concepts during the embedding learning phase. While preserving the semantic structure of the embedding space, BiImp makes dimensions …","url":["https://www.sciencedirect.com/science/article/pii/S0306457322000498"]}
{"year":"2022","title":"Learning Meta Word Embeddings by Unsupervised Weighted Concatenation of Source Embeddings","authors":["D Bollegala - arXiv preprint arXiv:2204.12386, 2022"],"snippet":"… Note that this is significantly smaller compared to vocabulary sizes and training corpora used in publicly available word embeddings (eg GloVe embeddings are trained on 42B tokens from Common Crawl corpus covering …","url":["https://arxiv.org/pdf/2204.12386"]}
{"year":"2022","title":"Learning on Streaming Graphs with Experience Replay","authors":["M Perini, G Ramponi, P Carbone, V Kalavri - 2022"],"snippet":"… Text features have been generated using a 300-dimensional GloVe CommonCrawl word vector. Posts are linked with an edge if they have been commented by the same user and each edge bears the timestamp of the first comment. The …","url":["https://sites.bu.edu/casp/files/2021/12/online-gnn-SAC-preprint.pdf"]}
{"year":"2022","title":"Learning Sample Importance for Cross-Scenario Video Temporal Grounding","authors":["P Bao, Y Mu - arXiv preprint arXiv:2201.02848, 2022"],"snippet":"The task of temporal grounding aims to locate video moment in an untrimmed video, with a given sentence query. This paper for the first time investigates some superficial biases that are specific to the temporal grounding task, and proposes a …","url":["https://arxiv.org/pdf/2201.02848"]}
{"year":"2022","title":"Learning the Morphological and Syntactic Grammars for Named Entity Recognition","authors":["M Sun, Q Yang, H Wang, M Pasquine, IA Hameed - Information, 2022"],"snippet":"In some languages, Named Entity Recognition (NER) is severely hindered by complex linguistic structures, such as inflection, that will confuse the data-driven models when perceiving the word’s actual meaning. This work tries to alleviate …","url":["https://www.mdpi.com/2078-2489/13/2/49/pdf"]}
{"year":"2022","title":"Learning the Morphological and Syntactic Grammars for Named Entity Recognition. Information 2022, 13, 49","authors":["M Sun, Q Yang, H Wang, M Pasquine, IA Hameed - 2022"],"snippet":"In some languages, Named Entity Recognition (NER) is severely hindered by complex linguistic structures, such as inflection, that will confuse the data-driven models when perceiving the word’s actual meaning. This work tries to alleviate …","url":["https://www.researchgate.net/profile/Ibrahim-Hameed/publication/357970212_Learning_the_Morphological_and_Syntactic_Grammars_for_Named_Entity_Recognition/links/61e97f849a753545e2e50b09/Learning-the-Morphological-and-Syntactic-Grammars-for-Named-Entity-Recognition.pdf"]}
{"year":"2022","title":"Learning to Enrich Query Representation with Pseudo-Relevance Feedback for Cross-lingual Retrieval","authors":["R Chandradevan, E Yang, M Yarmohammadi… - Proceedings of the 45th …, 2022"],"snippet":"… To help picking the best performing model across the training steps and tune the free parameters, we used CLIR Common Crawl Collection (HC4) [13] as the development dataset. In this dataset, the exact same English topics are queried over …","url":["https://dl.acm.org/doi/abs/10.1145/3477495.3532013"]}
{"year":"2022","title":"Learning to translate by learning to communicate","authors":["CM Downey, LZ Liu, X Zhou, S Steinert-Threlkeld - arXiv preprint arXiv:2207.07025, 2022"],"snippet":"We formulate and test a technique to use Emergent Communication (EC) with a pretrained multilingual model to improve on modern Unsupervised NMT systems, especially for low-resource languages. It has been argued that the currently …","url":["https://arxiv.org/pdf/2207.07025"]}
{"year":"2022","title":"Learning Trustworthy Web Sources to Derive Correct Answers and Reduce Health Misinformation in Search","authors":["D Zhang, A Vakili Tahami, M Abualsaud, M Smucker - 2022"],"snippet":"When searching the web for answers to health questions, people can make incorrect decisions that have a negative effect on their lives if the search results contain misinformation. To reduce health misinformation in search results, we need to be …","url":["https://uwspace.uwaterloo.ca/bitstream/handle/10012/18257/SIGIR%202022%20Misinformation.pdf?sequence=1"]}
{"year":"2022","title":"Learning Two-Step Hybrid Policy for Graph-Based Interpretable Reinforcement Learning","authors":["T Mu, K Lin, F Niu, G Thattai - arXiv preprint arXiv:2201.08520, 2022"],"snippet":"We present a two-step hybrid reinforcement learning (RL) policy that is designed to generate interpretable and robust hierarchical policies on the RL problem with graph-based input. Unlike prior deep reinforcement learning policies parameterized by an end-to-end …","url":["https://arxiv.org/pdf/2201.08520"]}
{"year":"2022","title":"Learnings from Federated Learning in the Real world","authors":["C Dupuy, TG Roosta, L Long, C Chung, R Gupta… - arXiv preprint arXiv …, 2022"],"snippet":"Federated Learning (FL) applied to real world data may suffer from several idiosyncrasies. One such idiosyncrasy is the data distribution across devices. Data across devices could be distributed such that there are some \"heavy devices\" with …","url":["https://arxiv.org/pdf/2202.03925"]}
{"year":"2022","title":"Leveraging Natural Supervision for Language Representation Learning and Generation","authors":["M Chen - arXiv preprint arXiv:2207.10617, 2022"],"snippet":"Recent breakthroughs in Natural Language Processing (NLP) have been driven by language models trained on a massive amount of plain text. While powerful, deriving supervision from textual resources is still an open question. For example, language …","url":["https://arxiv.org/pdf/2207.10617"]}
{"year":"2022","title":"Linguistic and Emotion-based Identification of Tweets with Fake News: A Case Study","authors":["VS Bernardes - 2021"],"snippet":"Since the popularization of the term in 2016, we have observed a proliferation of so called “fake news” content, assisted by the widespread use of social media platforms. The dissemination of fake content brings with it serious political, economical, and …","url":["https://repositorio-aberto.up.pt/bitstream/10216/139434/2/528149.pdf"]}
{"year":"2022","title":"Linguistic Approaches to the Analysis of Online Terrorist Threats","authors":["J Longhi - Language as Evidence, 2022"],"snippet":"This chapter focuses on online terrorist threats. For many linguists, the best training for a forensic linguist is a course in descriptive and applied linguistics because each case will normally require a different selection of tools from the linguistic toolbox …","url":["https://link.springer.com/chapter/10.1007/978-3-030-84330-4_13"]}
{"year":"2022","title":"Linguistic resources for Romanian. Quality assessed machine-translated and post-edited Romanian corpus and opportunities for interdisciplinary research","authors":["G BULGARU - REVUE INTERNATIONALE D'ÉTUDES EN LANGUES …"],"snippet":"Despite the great progress over the last years, Romanian is still a low-resourced language. In this paper, we set to make a short overview of the language technology field and the linguistic resources available for Romanian. We also sketch out a short …","url":["https://www.academia.edu/download/77452310/RIELMA_no14_2021_Supplement.pdf#page=78"]}
{"year":"2022","title":"LinkingPark: An automatic semantic table interpretation system","authors":["S Chen, A Karaoglu, C Negreanu, T Ma, JG Yao… - Journal of Web Semantics, 2022"],"snippet":"In this paper, we present LinkingPark, an automatic semantic annotation system for tabular data to knowledge graph matching. LinkingPark is designed as a modular framework which can handle Cell-Entity Annotation (CEA), Column-Type Annotation …","url":["https://www.sciencedirect.com/science/article/pii/S1570826822000233"]}
{"year":"2022","title":"liveopp/keras-quora-question-pairs","authors":["GPK Tool, JFMAM Jun, JASON Dec, JSMTW Thu, F Sat"],"snippet":"… Max bag-of-embeddings (this work) GloVe Common Crawl (840B tokens, 300D) 0.83 \"Multi-Perspective-LSTM\" [5] GloVe Common Crawl (840B tokens, 300D) 0.83 … \"Max-out Window Encoding\" with depth 2 [7] GloVe Common Crawl pruned to 1M …","url":["https://giters.com/liveopp/keras-quora-question-pairs"]}
{"year":"2022","title":"Location-based Twitter Filtering for the Creation of Low-Resource Language Datasets in Indonesian Local Languages","authors":["M Amien, C Feng, H Huang - arXiv preprint arXiv:2206.07238, 2022"],"snippet":"Twitter contains an abundance of linguistic data from the real world. We examine Twitter for user-generated content in low-resource languages such as local Indonesian. For NLP to work in Indonesian, it must consider local dialects …","url":["https://arxiv.org/pdf/2206.07238"]}
{"year":"2022","title":"LPT: Language-agnostic Prompt Tuning","authors":["Y Ji, Y Wang, Z Tang - Sci China Inf Sci"],"snippet":"… The pre-trained XLM-RoBERTa-base model contains 270M parameters, which is trained on 2.5 TB CommonCrawl data in 100 languages. Its vocabulary contains 250k tokens. The architecture of XLMRoBERTa-base is the same as RoBERTa-base. …","url":["https://lijuntaopku.github.io/SCIS_22_LPT.pdf"]}
{"year":"2022","title":"Machine Learning and Deep Learning Based Phishing Websites Detection: The Current Gaps and Next Directions","authors":["K Adane, B Beyene - Review of Computer Engineering Research, 2022"],"snippet":"There are many phishing websites detection techniques in literature, namely white-listing, black-listing, visual-similarity, heuristic-based, and others. However, detecting zero-hour or newly designed phishing website attacks is an inherent property of machine …","url":["https://archive.conscientiabeam.com/index.php/76/article/download/2983/6434"]}
{"year":"2022","title":"Machine learning and deep learning-based Natural Language Processing for auto-vetting the appropriateness of Lumbar Spine Magnetic Resonance Imaging …","authors":["A Alanazi, A Cradock, J Ryan, L Rainford - Informatics in Medicine Unlocked, 2022"],"snippet":"Manual vetting of radiology referrals is an essential daily task to ensure the appropriateness of the received referrals. Such tasks require sufficient clinical experience and may challenge the radiology staff. With the emerging of artificial …","url":["https://www.sciencedirect.com/science/article/pii/S2352914822001071"]}
{"year":"2022","title":"Machine Learning approaches for Topic and Sentiment Analysis in multilingual opinions and low-resource languages: From English to Guarani","authors":["MM Agüero Torales - 2022"],"snippet":"This dissertation has focused on the study of machine learning techniques for sentiment analysis and topic modeling in texts from social media. It puts a special emphasis on approaches and methods for handling low-resource languages, ie …","url":["https://digibug.ugr.es/bitstream/handle/10481/72863/80733.pdf?sequence=4&isAllowed=y"]}
{"year":"2022","title":"Machine Reading at Scale: A Search Engine for Scientific and Academic Research","authors":["N Sousa, N Oliveira, I Praça - Systems, 2022"],"snippet":"The Internet, much like our universe, is ever-expanding. Information, in the most varied formats, is continuously added to the point of information overload. Consequently, the ability to navigate this ocean of data is crucial in our day-to-day …","url":["https://www.mdpi.com/2079-8954/10/2/43/pdf"]}
{"year":"2022","title":"Machine Reading at Scale: A Search Engine for Scientific and Academic Research. Systems 2022, 10, 43","authors":["N Sousa, N Oliveira, I Praça - 2022"],"snippet":"The Internet, much like our universe, is ever-expanding. Information, in the most varied formats, is continuously added to the point of information overload. Consequently, the ability to navigate this ocean of data is crucial in our day-to-day …","url":["https://www.mdpi.com/2079-8954/10/2/43/pdf?version=1649908028"]}
{"year":"2022","title":"Machine Translation Performance Prediction System: Optimal Prediction for Optimal Translation","authors":["E Biçici - SN Computer Science, 2022"],"snippet":"Abstract Machine translation performance prediction (MTPP) system (MTPPS) is an automatic, accurate, language and natural language processing (NLP) output independent prediction model. MTPPS is optimal by the capability to predict …","url":["https://link.springer.com/article/10.1007/s42979-022-01183-0"]}
{"year":"2022","title":"Machine's Conceptual Development Using FNet","authors":["D Jayanna - 2022"],"snippet":"Masked language modeling (MLM) is a well-known technique in Natural languageprocessing (NLP) to train a model on randomly masked tokens and use the trainedmodel to predict the masked words. FNet is a recently developed Fourier …","url":["https://search.proquest.com/openview/8e85ce3a1dea6ec809b32142fbb3db6c/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2022","title":"Making AI work for skills-based training: A case study","authors":["R Robson, E Kelsey, A Goel, L Egerton, S Nasir…"],"snippet":"Motivated by the need to upskill workers rapidly and equitably in response to changes in work and the workplace, a team funded by the US National Science Foundation Convergence Accelerator program has developed and piloted an …","url":["https://www.researchgate.net/profile/Robby-Robson/publication/361742207_Extended_Abstract_Making_AI_work_for_skills-based_training_A_case_study/links/62c2fbd0c692f45113050e62/Extended-Abstract-Making-AI-work-for-skills-based-training-A-case-study.pdf"]}
{"year":"2022","title":"Marathi Social Media Opinion Mining using XLM-R","authors":["N Rathod, N Mistry, D Talati, M Parikh, A Kore… - … International Conference on …, 2022"],"snippet":"… XLM-R is a transformer-based multilingual masked model pre-trained on CommonCrawl data in 100 languages with base having 250M parameters, which obtains state-of-the-art results in the tasks of question answering, sequence labeling …","url":["https://ieeexplore.ieee.org/abstract/document/9793308/"]}
{"year":"2022","title":"Match the Script, Adapt if Multilingual: Analyzing the Effect of Multilingual Pretraining on Cross-lingual Transferability","authors":["Y Fujinuma, J Boyd-Graber, K Kann - arXiv preprint arXiv:2203.10753, 2022"],"snippet":"Pretrained multilingual models enable zero-shot learning even for unseen languages, and that performance can be further improved via adaptation prior to finetuning. However, it is unclear how the number of pretraining languages …","url":["https://arxiv.org/pdf/2203.10753"]}
{"year":"2022","title":"Matching Tweets With Applicable Fact-Checks Across Languages","authors":["A Kazemi, Z Li, V Pérez-Rosas, SA Hale, R Mihalcea - arXiv preprint arXiv …, 2022"],"snippet":"… The model is trained on more than 2TBs CommonCrawl data and supports one hundred languages. We also rely on recent language … language modeling trained on 17 billion monolingual sentences from CommonCrawl and 6 billion translated …","url":["https://arxiv.org/pdf/2202.07094"]}
{"year":"2022","title":"Measuring Gender Bias in Word Embeddings of Gendered Languages Requires Disentangling Grammatical Gender Signals","authors":["SO Sabbaghi, A Caliskan - arXiv preprint arXiv:2206.01691, 2022"],"snippet":"Does the grammatical gender of a language interfere when measuring the semantic gender information captured by its word embeddings? A number of anomalous gender bias measurements in the embeddings of gendered languages suggest this …","url":["https://arxiv.org/pdf/2206.01691"]}
{"year":"2022","title":"MEASURING NETWORK INTERFERENCE AND MITIGATING IT WITH DNS ENCRYPTION","authors":["SA Akhavan Niaki - 2022"],"snippet":"The Internet has emerged as one of the most important tools of communication. With around 4.5 billion active users as of July 2020, it provides people the opportunity to access a vast treasure trove of information and express their opinions online. How-ever …","url":["https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=3583&context=dissertations_2"]}
{"year":"2022","title":"Measuring Similarity by Linguistic Features rather than Frequency","authors":["R Delmonte, N Busetto"],"snippet":"… As said above, we used BERT – with the Italian model taken from UWAC corpus, Umbertocommoncrawl - and examined the output of the first or projection layer5. In this way we intended to check the predicting ability of BERT on the masked word, by …","url":["https://sigsem.uvt.nl/isa18/ISA-18_11_Paper.pdf"]}
{"year":"2022","title":"Measuring the Impact of (Psycho-) Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns","authors":["D Wiechmann, Y Qiao, E Kerz, J Mattern - arXiv preprint arXiv:2203.08085, 2022"],"snippet":"… BERT is trained on the BooksCorpus (800M words) and Wikipedia (2,500M words), whereas GPT-2 is trained on WebText, an 8-million documents subset of CommonCrawl amounting to 40 GB of text. We chose the BERT base model (cased) …","url":["https://arxiv.org/pdf/2203.08085"]}
{"year":"2022","title":"Mental Health Disorder Identification From Motivational Conversations","authors":["T Saha, SM Reddy, S Saha, P Bhattacharyya - IEEE Transactions on Computational …, 2022"],"snippet":"… 1) Embedding Features: To extract textual features of an utterance U having nu number of words, the representation of each of the words, w1,...,wu, where wi ∈ Rdu , wi s are obtained from the GloVe embeddings trained on the Common Crawl …","url":["https://ieeexplore.ieee.org/abstract/document/9729467/"]}
{"year":"2022","title":"Meta-Learning for Offensive Language Detection in Code-Mixed Texts","authors":["G Vadakkekara Suresh, BR Chakravarthi, JP McCrae - Forum for Information …, 2021"],"snippet":"This research investigates the application of Model-Agnostic Meta-Learning (MAML) and ProtoMAML to identify offensive code-mixed text content on social media in Tamil-English and Malayalam-English code-mixed texts. We follow a two-step …","url":["https://dl.acm.org/doi/pdf/10.1145/3503162.3503167"]}
{"year":"2022","title":"METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals","authors":["P Bajaj, C Xiong, G Ke, X Liu, D He, S Tiwary, TY Liu… - arXiv preprint arXiv …, 2022"],"snippet":"We present an efficient method of pretraining large-scale autoencoding language models using training signals generated by an auxiliary model. Originated in ELECTRA, this training strategy has demonstrated sample-efficiency to pretrain …","url":["https://arxiv.org/pdf/2204.06644"]}
{"year":"2022","title":"mGPT: Few-Shot Learners Go Multilingual","authors":["O Shliazhko, A Fenogenova, M Tikhonova, V Mikhailov… - arXiv preprint arXiv …, 2022"],"snippet":"Recent studies report that autoregressive language models can successfully solve many NLP tasks via zeroand few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two …","url":["https://arxiv.org/pdf/2204.07580"]}
{"year":"2022","title":"MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge","authors":["L Fan, G Wang, Y Jiang, A Mandlekar, Y Yang, H Zhu… - arXiv preprint arXiv …, 2022"],"snippet":"Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide …","url":["https://arxiv.org/pdf/2206.08853"]}
{"year":"2022","title":"minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models","authors":["K Misra - arXiv preprint arXiv:2203.13112, 2022"],"snippet":"We present minicons, an open source library that provides a standard API for researchers interested in conducting behavioral and representational analyses of transformer-based language models (LMs). Specifically, minicons enables …","url":["https://arxiv.org/pdf/2203.13112"]}
{"year":"2022","title":"Misspelling Semantics in Thai","authors":["P Nakwijit, M Purver"],"snippet":"User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained …","url":["http://www.eecs.qmul.ac.uk/~mpurver/papers/nakwijit-purver22lrec.pdf"]}
{"year":"2022","title":"MLGL-MP: a Multi-Label Graph Learning framework enhanced by pathway interdependence for Metabolic Pathway prediction","authors":["BX Du, PC Zhao, B Zhu, SM Yiu, AK Nyamabo, H Yu… - Bioinformatics, 2022"],"snippet":"Motivation During lead compound optimization, it is crucial to identify pathways where a drug-like compound is metabolized. Recently, machine learning-based methods have achieved inspiring progress to predict potential metabolic pathways …","url":["https://academic.oup.com/bioinformatics/article/38/Supplement_1/i325/6617514"]}
{"year":"2022","title":"Modeling Associative Reasoning Processes","authors":["U Furbach, C Schon, M Ragni - arXiv preprint arXiv:2201.00716, 2022"],"snippet":"The human capability to reason about one domain by using knowledge of other domains has been researched for more than 50 years, but models that are formally sound and predict cognitive process are sparse. We propose a formally sound …","url":["https://arxiv.org/pdf/2201.00716"]}
{"year":"2022","title":"Modeling Intensification for Sign Language Generation: A Computational Approach","authors":["M İnan, Y Zhong, S Hassan, L Quandt, M Alikhani - arXiv preprint arXiv:2203.09679, 2022"],"snippet":"End-to-end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor-quality generated presentations that confuse human interpreters. In this paper, we aim to …","url":["https://arxiv.org/pdf/2203.09679"]}
{"year":"2022","title":"Modeling Target-Side Morphology in Neural Machine Translation: A Comparison of Strategies","authors":["MWD Marco, M Huck, A Fraser - arXiv preprint arXiv:2203.13550, 2022"],"snippet":"Morphologically rich languages pose difficulties to machine translation. Machine translation engines that rely on statistical learning from parallel training data, such as state-of-the-art neural systems, face challenges especially with rich morphology …","url":["https://arxiv.org/pdf/2203.13550"]}
{"year":"2022","title":"MOE/RF: A Novel Phishing Detection Model based on Revised Multi-Objective Evolution Optimization Algorithm and Random Forest","authors":["E Zhu, Z Chen, J Cui, H Zhong - IEEE Transactions on Network and Service …, 2022"],"snippet":"To effectively boost computer usage, machine learning models are used in several phishing detection systems to classify enormous phishing datasets. Based on phishing patterns, researchers prefer to extract a considerable number of features to …","url":["https://ieeexplore.ieee.org/abstract/document/9744099/"]}
{"year":"2022","title":"Mokey: Enabling Narrow Fixed-Point Inference for Out-of-the-Box Floating-Point Transformer Models","authors":["AH Zadeh, M Mahmoud, A Abdelhadi, A Moshovos - arXiv preprint arXiv:2203.12758, 2022"],"snippet":"Increasingly larger and better Transformer models keep advancing state-of-the-art accuracy and capability for Natural Language Processing applications. These models demand more computational power, storage, and energy. Mokey reduces …","url":["https://arxiv.org/pdf/2203.12758"]}
{"year":"2022","title":"Mono vs Multilingual BERT for Hate Speech Detection and Text Classification: A Case Study in Marathi","authors":["A Velankar, H Patil, R Joshi - arXiv preprint arXiv:2204.08669, 2022"],"snippet":"Transformers are the most eminent architectures used for a vast range of Natural Language Processing tasks. These models are pre-trained over a large text corpus and are meant to serve state-of-the-art results over tasks like text classification. In …","url":["https://arxiv.org/pdf/2204.08669"]}
{"year":"2022","title":"MSCTD: A Multimodal Sentiment Chat Translation Dataset","authors":["Y Liang, F Meng, J Xu, Y Chen, J Zhou - arXiv preprint arXiv:2202.13645, 2022"],"snippet":"Multimodal machine translation and textual chat translation have received considerable attention in recent years. Although the conversation in its natural form is usually multimodal, there still lacks work on multimodal machine translation in …","url":["https://arxiv.org/pdf/2202.13645"]}
{"year":"2022","title":"mSLAM: Massively multilingual joint pre-training for speech and text","authors":["A Bapna, C Cherry, Y Zhang, Y Jia, M Johnson… - arXiv preprint arXiv …, 2022"],"snippet":"We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT …","url":["https://arxiv.org/pdf/2202.01374"]}
{"year":"2022","title":"MuCoT: Multilingual Contrastive Training for Question-Answering in Low-resource Languages","authors":["GK Kumar, AS Gehlot, SS Mullappilly, K Nandakumar - arXiv preprint arXiv …, 2022","GKKAS Gehlot, SSMK Nandakumar - DravidianLangTech 2022, 2022"],"snippet":"Accuracy of English-language Question Answering (QA) systems has improved significantly in recent years with the advent of Transformer-based models (eg, BERT). These models are pre-trained in a self-supervised fashion with a large English text …","url":["https://aclanthology.org/2022.dravidianlangtech-1.pdf#page=33","https://arxiv.org/pdf/2204.05814"]}
{"year":"2022","title":"Multi language Email Classification Using Transfer learning","authors":["MJC Sousa - 2022"],"snippet":"In recent years Artificial Intelligence has become a core part of many businesses, from manufacturers to service providers, AI can be found helping to improve business processes as well as providing customized experiences and support for …","url":["https://run.unl.pt/bitstream/10362/134446/1/TGI0525.pdf"]}
{"year":"2022","title":"Multi-class sentiment analysis of urdu text using multilingual BERT","authors":["L Khan, A Amjad, N Ashraf, HT Chang - Scientific Reports, 2022"],"snippet":"Sentiment analysis (SA) is an important task because of its vital role in analyzing people’s opinions. However, existing research is solely based on the English language with limited work on low-resource languages. This study introduced a new …","url":["https://www.nature.com/articles/s41598-022-09381-9"]}
{"year":"2022","title":"Multi-domain Neural Machine Translation","authors":["MQ Pham - 2021"],"snippet":"Today, neural machine translation (NMT) systems constitute state-of-the-art systems in machine translation. However, such translation models require relatively large train data and struggle to handle a specific domain text. A domain may consist of …","url":["https://tel.archives-ouvertes.fr/tel-03546910/document"]}
{"year":"2022","title":"Multi-label Movie Genre Classification Using Multiple Modalities","authors":["DA LeBaron"],"snippet":"This project explores an ensemble approach to the task of multi-label movie genre classification. The proposed model combines data from movie posters, video trailer previews, text plot summaries, and metadata using separate deep networks. Other …","url":["http://cs230.stanford.edu/projects_fall_2021/reports/102983714.pdf"]}
{"year":"2022","title":"Multilingual fine-tuning for Grammatical Error Correction","authors":["K Pajak, D Pajak - Expert Systems with Applications, 2022"],"snippet":"… mT5 was pre-trained on a large mC4 dataset covering 101 languages and containing 26 TB of the Common Crawl web scrape. The model was pre-trained on the span corruption task, where consecutive spans of input tokens were replaced …","url":["https://www.sciencedirect.com/science/article/pii/S0957417422003773"]}
{"year":"2022","title":"Multilingual Hate Speech Detection","authors":["V Lavrentiadou - 2022"],"snippet":"… It was trained on 100 languages, of a CommonCrawl Corpus, and it outperforms multilingual BERT on crosslingual classification. XLM-… The XLM-RoBERTa model, on the other hand, was trained on 2.5TB of filtered CommonCrawl data across 100 …","url":["https://ikee.lib.auth.gr/record/338409/files/GRI-2022-34410.pdf"]}
{"year":"2022","title":"Multilingual hope speech detection in English and Dravidian languages","authors":["BR Chakravarthi - International Journal of Data Science and Analytics, 2022"],"snippet":"Recent work on language technology has aimed to identify negative language such as hate speech and cyberbullying as well as improve offensive language detection to mediate social media platforms. Most of these systems rely on using machine …","url":["https://link.springer.com/article/10.1007/s41060-022-00341-0"]}
{"year":"2022","title":"Multilingual Language Model Adaptive Fine-Tuning: A Study on African Languages","authors":["JO Alabi, DI Adelani, M Mosbach, D Klakow - arXiv preprint arXiv:2204.06487, 2022"],"snippet":"Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks on both high resourced and low-resourced languages. However, there is still a large performance drop for languages unseen …","url":["https://arxiv.org/pdf/2204.06487"]}
{"year":"2022","title":"Multilingual Name Entity Recognition and Intent Classification employing Deep Learning architectures","authors":["S Rizou, A Paflioti, A Theofilatos, A Vakali… - … Modelling Practice and …, 2022"],"snippet":"… For the fastText initialization we use a model which is pre-trained in Common Crawl and Wikipedia. … For the fastText initialization we used the model in Common Crawl and Wikipedia. For the GloVe initialization we used the 300 dimensional …","url":["https://www.sciencedirect.com/science/article/pii/S1569190X22000995"]}
{"year":"2022","title":"Multilingual Transformer Encoders: a Word-Level Task-Agnostic Evaluation","authors":["F Gaschi, F Plesse, P Rastin, Y Toussaint - arXiv preprint arXiv:2207.09076, 2022"],"snippet":"… However, it could also be explained by the fact that mBERT is trained solely on Wikipedia whereas models like XLM-R are trained on the CommonCrawl corpus which might contain texts that are less comparable across languages. It is also to be …","url":["https://arxiv.org/pdf/2207.09076"]}
{"year":"2022","title":"Multilingual Transformers for Product Matching--Experiments and a New Benchmark in Polish","authors":["A Wróblewska, S Tkachuk, S Łukasik - arXiv preprint arXiv:2205.15712, 2022"],"snippet":"… The authors of these studies extracted offers from websites collected under the CommonCrawl project.The selection of offers was based on the schema.org8 tags, which Internet sellers had started using in the last five … 7commoncrawl.org 8schema.org …","url":["https://arxiv.org/pdf/2205.15712"]}
{"year":"2022","title":"Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically Ambiguous Settings for Low Resource Languages","authors":["A Pandey, S Daw, N Unnam, V Pudi - Proceedings of the 16th International Workshop …, 2022"],"snippet":"We leverage pre-trained language models to solve the task of complex NER for two low-resource languages: Chinese and Spanish. We use the technique of Whole Word Masking (WWM) to boost the performance of masked language modeling …","url":["https://aclanthology.org/2022.semeval-1.201.pdf"]}
{"year":"2022","title":"Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling","authors":["S Mai, S Xing, J He, Y Zeng, H Hu - ACM Transactions on Multimedia Computing …, 2022"],"snippet":"Multimodal sequence analysis aims to draw inferences from visual, language and acoustic sequences. A majority of existing works focus on the aligned fusion of three modalities to explore inter-modal interactions, which is impractical in real-world …","url":["https://dl.acm.org/doi/pdf/10.1145/3542927"]}
{"year":"2022","title":"Multimodality for NLP-Centered Applications: Resources, Advances and Frontiers","authors":["M Garg, S Wazarkar, M Singh, O Bojar"],"snippet":"With advancements in the methods of Natural Language Processing (NLP) by explicitly considering other modalities than just text, the resurgence of multimodal datasets has attracted significant attention. However, there remains lack of a …","url":["http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.738.pdf"]}
{"year":"2022","title":"NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis","authors":["SH Muhammad, DI Adelani, IS Ahmad, I Abdulmumin… - arXiv preprint arXiv …, 2022"],"snippet":"Sentiment analysis is one of the most widely studied applications in NLP, but most work focuses on languages with large amounts of data. We introduce the first large-scale human-annotated Twitter sentiment dataset for the four most widely spoken …","url":["https://arxiv.org/pdf/2201.08277"]}
{"year":"2022","title":"NamedEntityRangers at SemEval-2022 Task 11: Transformer-based Approaches for Multilingual Complex Named Entity Recognition","authors":["A Miftahova, A Pugachev, A Skiba, K Artemova… - Proceedings of the 16th …, 2022"],"snippet":"This paper presents the two submissions of NamedEntityRangers Team to the MultiCoNER Shared Task, hosted at SemEval-2022. We evaluate two state-of-the-art approaches, of which both utilize pre-trained multi-lingual language models …","url":["https://aclanthology.org/2022.semeval-1.216.pdf"]}
{"year":"2022","title":"NapierOne: A modern mixed file data set alternative to Govdocs1","authors":["SR Davies, R Macfarlane, WJ Buchanan - Forensic Science International: Digital …, 2022"],"snippet":"It was found when reviewing the ransomware detection research literature that almost no proposal provided enough detail on how the test data set was created, or sufficient description of its actual content, to allow it to be recreated by other …","url":["https://arxiv.org/pdf/2201.08154"]}
{"year":"2022","title":"Natural Language Correction With Focus on Czech","authors":["J Náplava - 2022"],"snippet":"Natural language correction, a subfield of natural language processing (NLP), is the task of automatically correcting user errors in written texts. It includes, but is not limited to, grammatical error correction, spelling error correction and diacritics …","url":["https://dspace.cuni.cz/bitstream/handle/20.500.11956/174730/140102103.pdf?sequence=1"]}
{"year":"2022","title":"Natural Language Inference with Self-Attention for Veracity Assessment of Pandemic Claims","authors":["M Arana-Catania, E Kochkina, A Zubiaga, M Liakata… - arXiv preprint arXiv …, 2022"],"snippet":"… on the health domain focused on information retrieval from general websites through the Common Crawl corpus (commoncrawl.org). … Research challenge using claims on the health domain focused on information retrieval from general …","url":["https://arxiv.org/pdf/2205.02596"]}
{"year":"2022","title":"Natural Questions in Icelandic","authors":["V Snæbjarnarson, H Einarsson"],"snippet":"We present the first extractive question answering (QA) dataset for Icelandic, Natural Questions in Icelandic (NQiI). Developing such datasets is important for the development and evaluation of Icelandic QA systems. It also aids in the …","url":["https://vesteinn.is/uploads/snaebjarnarson2022natural.pdf"]}
{"year":"2022","title":"Navigable Proximity Graph-Driven Native Hybrid Queries with Structured and Unstructured Constraints","authors":["M Wang, L Lv, X Xu, Y Wang, Q Yue, J Ni - arXiv preprint arXiv:2203.13601, 2022"],"snippet":"As research interest surges, vector similarity search is applied in multiple fields, including data mining, computer vision, and information retrieval. {Given a set of objects (eg, a set of images) and a query object, we can easily transform each object …","url":["https://arxiv.org/pdf/2203.13601"]}
{"year":"2022","title":"Net impact of large language models trained on code","authors":["P Gherciu - … tehnico-ştiinţifică a studenţilor, masteranzilor şi …, 2022"],"snippet":"Natural language processing has seen many improvements in recent years, particularly driven by machine learning models such as OpenAI’s GPT-3. This paper aims to present the various language models, as well as OpenAI Codex, which is …","url":["https://ibn.idsi.md/sites/default/files/imag_file/p-189-192.pdf"]}
{"year":"2022","title":"Neural Approaches to Conversational Information Retrieval","authors":["J Gao, C Xiong, P Bennett, N Craswell - arXiv preprint arXiv:2201.05176, 2022"],"snippet":"A conversational information retrieval (CIR) system is an information retrieval (IR) system with a conversational interface which allows users to interact with the system to seek information via multi-turn conversations of natural language, in spoken or …","url":["https://arxiv.org/pdf/2201.05176"]}
{"year":"2022","title":"Neural Language Models as What If?-Engines for HCI Research","authors":["P Hämäläinen, M Tavast, A Kunnari - 27th International Conference on Intelligent …, 2022"],"snippet":"Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) and user experience (UX) research. In this poster paper, we explore and critically evaluate the potential of large-scale neural language models like GPT-3 in …","url":["https://dl.acm.org/doi/abs/10.1145/3490100.3516458"]}
{"year":"2022","title":"Neural Natural Language Generation: A Survey on Multilinguality, Multimodality, Controllability and Learning","authors":["E Erdem, M Kuyu, S Yagcioglu, A Frank…"],"snippet":"Developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. Recent decades have witnessed an impressive progress on both of these problems, giving …","url":["https://web.cs.hacettepe.edu.tr/~erkut/publications/nnlg-survey.pdf"]}
{"year":"2022","title":"Neural Transfer Learning For Vietnamese Sentiment Analysis Using Pre-trained Contextual Language Models","authors":["AP Le, TV Pham, TV Le, DV Huynh - 2021 IEEE International Conference on Machine …, 2021"],"snippet":"We propose a fine-tuning methodology and a comprehensive comparison between state-of-the-art pre-trained language models (PLM) when applying to Vietnamese Sentiment Analysis. The fine-tuning architecture includes three main components: (1) …","url":["https://ieeexplore.ieee.org/abstract/document/9690554/"]}
{"year":"2022","title":"New meaning for NLP: the trials and tribulations of natural language processing with GPT-3 in ophthalmology","authors":["S Nath, A Marie, S Ellershaw, E Korot, PA Keane - British Journal of Ophthalmology, 2022"],"snippet":"Natural language processing (NLP) is a subfield of machine intelligence focused on the interaction of human language with computer systems. NLP has recently been discussed in the mainstream media and the literature with the advent of Generative …","url":["https://bjo.bmj.com/content/early/2022/05/06/bjophthalmol-2022-321141.abstract"]}
{"year":"2022","title":"New Methods and the Study of Vulnerable Groups: Using Machine Learning to Identify Immigrant-Oriented Nonprofit Organizations","authors":["C Ren, I Bloemraad - Socius, 2022"],"snippet":"Many migrants are vulnerable due to noncitizenship, linguistic or cultural barriers, and inadequate safety-net infrastructures. Immigrant-oriented nonprofits can play an important role in improving immigrant well-being. However, progress on …","url":["https://journals.sagepub.com/doi/pdf/10.1177/23780231221076992"]}
{"year":"2022","title":"NMTScore: A Multilingual Analysis of Translation-based Text Similarity Measures","authors":["J Vamvas, R Sennrich - arXiv preprint arXiv:2204.13692, 2022"],"snippet":"Being able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which …","url":["https://arxiv.org/pdf/2204.13692"]}
{"year":"2022","title":"No Language Left Behind: Scaling Human-Centered Machine Translation","authors":["N Team, MR Costa-jussà, J Cross, O Çelebi…"],"snippet":"Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving …","url":["https://arxiv.org/pdf/2207.04672"]}
{"year":"2022","title":"Noise Reduction Learning based on XLNet-CRF for Biomedical Named Entity Recognition","authors":["Z Chai, H Jin, S Shi, S Zhan, L Zhuo, Y Yang, Q Lian - IEEE/ACM Transactions on …, 2022"],"snippet":"In recent years, Biomedical Named Entity Recognition (BioNER) systems have mainly been based on deep neural networks, which are used to extract information from the rapidly expanding biomedical literature. Long-distance context …","url":["https://ieeexplore.ieee.org/abstract/document/9730067/"]}
{"year":"2022","title":"Noise-Reduction for Automatically Transferred Relevance Judgments","authors":["M Fröbe, C Akiki, M Potthast, M Hagen"],"snippet":"The TREC Deep Learning tracks used MS MARCO Version 1 as their official training data until 2020 and switched to Version 2 in 2021. For Version 2, all previously judged documents were re-crawled. Interestingly, in the track’s 2021 edition, models …","url":["https://webis.de/downloads/publications/papers/froebe_2022d.pdf"]}
{"year":"2022","title":"Non-Autoregressive Machine Translation: It's Not as Fast as it Seems","authors":["J Helcl, B Haddow, A Birch - arXiv preprint arXiv:2205.01966, 2022"],"snippet":"Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions. Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster …","url":["https://arxiv.org/pdf/2205.01966"]}
{"year":"2022","title":"Normalization and Attention","authors":["JC Ye - Geometry of Deep Learning, 2022"],"snippet":"… Sixty percent of the training data set comes from a filtered version of Common Crawl consisting of 410 billion tokens. Other sources are 19 billion tokens from WebText2, 12 billion tokens from Books1, 55 billion tokens from Books2, and 3 …","url":["https://link.springer.com/chapter/10.1007/978-981-16-6046-7_9"]}
{"year":"2022","title":"NULL at SemEval-2022 Task 6: Intended Sarcasm Detection Using Stylistically Fused Contextualized Representation and Deep Learning","authors":["M Rahgouy, HB Giglou, T Rahgooy, C Seals - … of the 16th International Workshop on …, 2022"],"snippet":"… ), which is pre-trained on 2.5TB of CommonCrawl data containing 100 languages. The model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks. The function CE ∈ R768 is a base …","url":["https://aclanthology.org/2022.semeval-1.120.pdf"]}
{"year":"2022","title":"OCHADAI at SemEval-2022 Task 2: Adversarial Training for Multilingual Idiomaticity Detection","authors":["LK Pereira, I Kobayashi - arXiv preprint arXiv:2206.03025, 2022"],"snippet":"We propose a multilingual adversarial training model for determining whether a sentence contains an idiomatic expression. Given that a key challenge with this task is the limited size of annotated data, our model relies on pre-trained contextual …","url":["https://arxiv.org/pdf/2206.03025"]}
{"year":"2022","title":"Odor Descriptor Understanding through Prompting","authors":["L Sisson - arXiv preprint arXiv:2205.03719, 2022"],"snippet":"Embeddings from contemporary natural language processing (NLP) models are commonly used as numerical representations for words or sentences. However, odor descriptor words, like \"leather\" or \"fruity\", vary significantly between their …","url":["https://arxiv.org/pdf/2205.03719"]}
{"year":"2022","title":"Offensive language detection in Tamil YouTube comments by adapters and cross-domain knowledge transfer","authors":["M Subramanian, R Ponnusamy, S Benhur… - Computer Speech & …, 2022"],"snippet":"… XLM-RoBERTa is a self-supervised transformer model that has been pre-trained on 2.5TB of filtered CommonCrawl data from 100 languages. This model does not require lang tensors to understand which language is being used, and it should be …","url":["https://www.sciencedirect.com/science/article/pii/S0885230822000407"]}
{"year":"2022","title":"Offensive Language Detection in Turkish Twitter Data with BERT Models","authors":["A Özberk - 2022"],"snippet":"As insulting statements become more frequent on online platforms, these negative statements create a reaction and disturb the peace of society. Identifying these expressions as early as possible is important to protect the victims. Offensive …","url":["http://www.openaccess.hacettepe.edu.tr:8080/xmlui/bitstream/handle/11655/26131/OFFENSIVE%20LANGUAGE%20DETECTION%20IN%20TURKISH%20TWITTER%20DATA%20WITH%20BERT%20MODELS.pdf?sequence=1"]}
{"year":"2022","title":"On Systematic Style Differences between Unsupervised and Supervised MT and an Application for High-Resource Machine Translation","authors":["K Marchisio, M Freitag, D Grangier"],"snippet":"Modern unsupervised machine translation (MT) systems reach reasonable translation quality under clean and controlled data conditions. As the performance gap between supervised and unsupervised MT narrows, it is interesting to ask …","url":["https://openreview.net/pdf?id=rqlQknbB-5"]}
{"year":"2022","title":"On the Applicability of Zero-Shot Cross-Lingual Transfer Learning for Sentiment Classification in Distant Language Pairs","authors":["A Rusli, M Shishido"],"snippet":"This research explores the applicability of cross-lingual transfer learning from English to Japanese and Indonesian using the XLM-R pre-trained model. The results are compared with several previous works, either by models using a similar …","url":["https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/A6-1.pdf"]}
{"year":"2022","title":"On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model","authors":["S Shin, SW Lee, H Ahn, S Kim, HS Kim, B Kim, K Cho… - arXiv preprint arXiv …, 2022"],"snippet":"Many recent studies on large-scale language models have reported successful in-context zeroand few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning …","url":["https://arxiv.org/pdf/2204.13509"]}
{"year":"2022","title":"On the Geometry of Concreteness","authors":["C Wartena - Proceedings of the 7th Workshop on Representation …, 2022"],"snippet":"… For English we use the 300 dimensional fastText embeddings without subword information trained on the Common Crawl with 600 billion tokens. For German we also use 300 dimensional fastText embeddings trained on the Common Crawl and …","url":["https://aclanthology.org/2022.repl4nlp-1.21.pdf"]}
{"year":"2022","title":"On the Role of Bidirectionality in Language Model Pre-Training","authors":["M Artetxe, J Du, N Goyal, L Zettlemoyer, V Stoyanov - arXiv preprint arXiv:2205.11726, 2022"],"snippet":"Prior work on language model pre-training has explored different architectures and learning objectives, but differences in data, hyperparameters and evaluation make a principled comparison difficult. In this work, we focus on bidirectionality as a key …","url":["https://arxiv.org/pdf/2205.11726"]}
{"year":"2022","title":"On the Surrogate Gap between Contrastive and Supervised Losses","authors":["H Bao, Y Nagano, K Nozawa - International Conference on Machine Learning, 2022"],"snippet":"Contrastive representation learning encourages data representation to make semantically similar pairs closer than randomly drawn negative samples, which has been successful in various domains such as vision, language, and graphs. Recent …","url":["https://proceedings.mlr.press/v162/bao22e/bao22e.pdf"]}
{"year":"2022","title":"Open-domain conversational search assistants: the Transformer is all you need","authors":["R Ferreira, M Leite, D Semedo, J Magalhaes - Information Retrieval Journal, 2022"],"snippet":"On the quest of providing a more natural interaction between users and search systems, open-domain conversational search assistants have emerged, by assisting users in answering questions about open topics in a conversational manner. In this …","url":["https://link.springer.com/article/10.1007/s10791-022-09403-0"]}
{"year":"2022","title":"OPT: Open Pre-trained Transformer Language Models","authors":["S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen… - arXiv preprint arXiv …, 2022"],"snippet":"… All corpora were previously collected or filtered to contain predominantly English text, but a small amount of non-English data is still present within the corpus via CommonCrawl. We removed duplicated documents across all datasets by filtering …","url":["https://arxiv.org/pdf/2205.01068"]}
{"year":"2022","title":"Optimize_Prime@ DravidianLangTech-ACL2022: Abusive Comment Detection in Tamil","authors":["S Patankar, O Gokhale, O Litake, A Mandke, D Kadam - arXiv preprint arXiv …, 2022"],"snippet":"This paper tries to address the problem of abusive comment detection in low-resource indic languages. Abusive comments are statements that are offensive to a person or a group of people. These comments are targeted toward individuals belonging to …","url":["https://arxiv.org/pdf/2204.09675"]}
{"year":"2022","title":"Optimize_Prime@ DravidianLangTech-ACL2022: Emotion Analysis in Tamil","authors":["O Gokhale, S Patankar, O Litake, A Mandke, D Kadam - arXiv preprint arXiv …, 2022"],"snippet":"This paper aims to perform an emotion analysis of social media comments in Tamil. Emotion analysis is the process of identifying the emotional context of the text. In this paper, we present the findings obtained by Team Optimize_Prime in the ACL 2022 …","url":["https://arxiv.org/pdf/2204.09087"]}
{"year":"2022","title":"Overview of NTCIR-16","authors":["T Yamamoto, Z Dou","Z Dou, T Yamamoto"],"snippet":"… Chuweb21 is a subset of the Common Crawl dataset and it contains 3,402,457 domains and 858,616,203 web pages. Secondly, two versions of relevance assessment are introduced: the Gold version given by the topic creators, and the …","url":["https://research.nii.ac.jp/ntcir/workshop/OnlineProceedings16/pdf/ntcir/01-NTCIR16-OV-YamamotoT-slides.pdf","https://research.nii.ac.jp/ntcir/workshop/OnlineProceedings16/pdf/ntcir/01-NTCIR16-OV-YamamotoT.pdf"]}
{"year":"2022","title":"Overview of the 2022 BUCC Shared Task: Bilingual Term Alignment in Comparable Specialized Corpora","authors":["O Adjali, E Morin, S Sharoff, R Rapp, P Zweigenbaum - LREC 2022 Workshop …, 2022"],"snippet":"The BUCC 2022 shared task addressed bilingual terminology alignment in comparable corpora. Many research groups are working on this problem using a wide variety of approaches. However, as there is no standard way to measure the …","url":["https://comparable.limsi.fr/bucc2022/BUCC2022-proceedings-20220617.pdf#page=77"]}
{"year":"2022","title":"PANGUBOT: Efficient Generative Dialogue Pre-training from Pre-trained Language Model","authors":["F Mi, Y Li, Y Zeng, J Zhou, Y Wang, C Xu, L Shang… - arXiv preprint arXiv …, 2022"],"snippet":"In this paper, we introduce PANGUBOT, a Chinese pre-trained open-domain dialogue generation model based on a large pre-trained language model (PLM) PANGU-alpha (Zeng et al.,2021). Different from other pre-trained dialogue models …","url":["https://arxiv.org/pdf/2203.17090"]}
{"year":"2022","title":"Papago's Submission for the WMT21 Quality Estimation Shared Task","authors":["S Lim, H Kim, H Kim - Proceedings of the Sixth Conference on Machine …, 2021"],"snippet":"This paper describes Papago submission to the WMT 2021 Quality Estimation Task 1: Sentence-level Direct Assessment. Our multilingual Quality Estimation system explores the combination of Pretrained Language Models and Multi-task Learning …","url":["https://aclanthology.org/2021.wmt-1.98.pdf"]}
{"year":"2022","title":"Paragraph-based Transformer Pre-training for Multi-Sentence Inference","authors":["L Di Liello, S Garg, L Soldaini, A Moschitti - arXiv preprint arXiv:2205.01228, 2022"],"snippet":"Inference tasks such as answer sentence selection (AS2) or fact verification are typically solved by fine-tuning transformer-based models as individual sentence-pair classifiers. Recent studies show that these tasks benefit from modeling …","url":["https://arxiv.org/pdf/2205.01228"]}
{"year":"2022","title":"Parquet Containers","authors":["M Coscia, A Weber - 2022"],"snippet":"Archives preserve content and support various types of analysis, study, and use. Archiving the Web and social media (eg, Twitter) content involves use of representations like Web ARChive (WARC) and JavaScript Object Notation (JSON) …","url":["https://vtechworks.lib.vt.edu/bitstream/handle/10919/109995/ParquetContainersReport.pdf?sequence=2&isAllowed=y"]}
{"year":"2022","title":"Partitioning Trillion-Edge Graphs","authors":["GM Slota, K Devine, K Madduri, S Rajamanickam - Massive Graph Analytics"],"snippet":"Graph partitioning is a key preprocessing step for distributed-memory graph analytics and scientific computations on meshes. This chapter introduces XtraPuLP, a new distributed-memory graph partitioner designed to process trillion-edge graphs …","url":["https://www.taylorfrancis.com/chapters/edit/10.1201/9781003033707-12/partitioning-trillion-edge-graphs-george-slota-karen-devine-kamesh-madduri-sivasankaran-rajamanickam"]}
{"year":"2022","title":"Pełen tekst","authors":["T Harting, S Mesbah, C Lofi"],"snippet":"… For the language-individual model, we use FastText word em-beddings [11] which are trained on Common Crawl and Wikipedia dataset. For the language-consistent model, we use pre-trained mul-tilingual embeddings which are released by FastText …","url":["https://9lib.org/document/wyejx8rz-lorem.html"]}
{"year":"2022","title":"Personalized filled-pause generation with group-wise prediction models","authors":["Y Matsunaga, T Saeki, S Takamichi, H Saruwatari - arXiv preprint arXiv:2203.09961, 2022"],"snippet":"In this paper, we propose a method to generate personalized filled pauses (FPs) with group-wise prediction models. Compared with fluent text generation, disfluent text generation has not been widely explored. To generate more human-like texts …","url":["https://arxiv.org/pdf/2203.09961"]}
{"year":"2022","title":"Perspectives on the Cognitive Unconscious","authors":["AS Reber, R Allen"],"snippet":"My interest in nonconscious processes goes back to my youth when I imagined that Sigmund Freud had discovered all that there was to be known about the human mind. But interest in the nonconscious and the role it plays in mind and behavior turned …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=wD90EAAAQBAJ&oi=fnd&pg=PA366&dq=commoncrawl&ots=cFuiqkafcD&sig=CqQx5Bv7dxtxoYrlkVD_Qp8nYdc"]}
{"year":"2022","title":"PETCI: A Parallel English Translation Dataset of Chinese Idioms","authors":["K Tang - arXiv preprint arXiv:2202.09509, 2022"],"snippet":"… RoBERTa, for example, has part of its pre-training data from Reddit and CommonCrawl (Liu … Similarly, we expect the Wikipedia dataset to have a lower PoI than CommonCrawl. The … use the cased 300-dimensional GloVe vectors pre-trained on …","url":["https://arxiv.org/pdf/2202.09509"]}
{"year":"2022","title":"Phishing Detection Using URL-based XAI Techniques","authors":["PRG Hernandes, CP Floret, KFC De Almeida… - 2021 IEEE Symposium …, 2021"],"snippet":"… As a result, 5,000 phishing web pages were selected based on URLs from PhishTank 1, OpenPhish 2, and another 5,000 legitimate pages based on URLs of Alexa 3 and the archive of CommonCrawl 4. Subsequently, this dataset was …","url":["https://ieeexplore.ieee.org/abstract/document/9659981/"]}
{"year":"2022","title":"Phishing URL Detection: A novel hybrid Approach using Long Short-Term Memory and Gated Recurrent Units","authors":["BAS Dilhara - International Journal of Computer Applications"],"snippet":"Phishing is one of the oldest types of cyber-attack, which mostly comes in the form of camouflaged URLs to delude the users to disclose their personal information for malevolent purposes of the attacker. It is one of the easiest ways of inducing people …","url":["https://www.researchgate.net/profile/Shashie-Dilhara/publication/357269439_Phishing_URL_Detection_A_novel_hybrid_Approach_using_Long_Short-Term_Memory_and_Gated_Recurrent_Units/links/61c41df0abcb1b520ada4c9c/Phishing-URL-Detection-A-novel-hybrid-Approach-using-Long-Short-Term-Memory-and-Gated-Recurrent-Units.pdf"]}
{"year":"2022","title":"Phishing URL Identification Using Machine Learning, Ensemble Learning and Deep Learning Techniques","authors":["K Laxmi Prasanna, KV Pradeepthi, A Saxena - Smart Intelligent Computing and …, 2022"],"snippet":"Demand for online applications and services is increasing day by day, whereas the security lapses in them is also becoming prominent. One of the major online threats, phishing, is consistently used to dupe users for retrieving sensitive information like …","url":["https://link.springer.com/chapter/10.1007/978-981-16-9705-0_56"]}
{"year":"2022","title":"Phrase-Based Affordance Detection via Cyclic Bilateral Interaction","authors":["L Lu, W Zhai, H Luo, Y Kang, Y Cao - arXiv preprint arXiv:2202.12076, 2022"],"snippet":"… Meanwhile, for linguistic feature extraction, we first adopt the GloVe word embeddings [42] pre-trained on Common Crawl (840B tokens) to initialize the parameters of embedding layers then a LSTM is employed as the language feature …","url":["https://arxiv.org/pdf/2202.12076"]}
{"year":"2022","title":"PLAtE: A Large-scale Dataset for List Page Web Extraction","authors":["A San, J Bakus, C Lockard, D Ciemiewicz, Y Ji, S Atluri… - arXiv preprint arXiv …, 2022"],"snippet":"… We construct PLAtE by collecting list pages from Common Crawl, then annotating them on Mechanical Turk. Quantitative and qualitative analyses are performed to demonstrate PLAtE has high-quality annotations. We establish strong baseline …","url":["https://arxiv.org/pdf/2205.12386"]}
{"year":"2022","title":"Politics and Virality in the Time of Twitter: A Large-Scale Cross-Party Sentiment Analysis in Greece, Spain and United Kingdom","authors":["D Antypas, A Preece, JC Collados - arXiv preprint arXiv:2202.00396, 2022"],"snippet":"Social media has become extremely influential when it comes to policy making in modern societies especially in the western world (eg, 48% of Europeans use social media every day or almost every day). Platforms such as Twitter allow users to follow …","url":["https://arxiv.org/pdf/2202.00396"]}
{"year":"2022","title":"Politics as Usual? Antecedents of Radical-Right Frames in US Electoral Discourse","authors":["B Bonikowski, Y Luo, O Stuhler - 2021"],"snippet":"… This extension of the original BERT model relies on the same general architecture but expands the training data to 63 million English news articles from CommonCrawl, 38 gigabytes of textual data from the OpenWebText corpus, and a 31-gigabyte …","url":["https://osf.io/preprints/socarxiv/uhvbp/download"]}
{"year":"2022","title":"POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection","authors":["Y Liu, XF Zhang, D Wegsman, N Beauchamp, L Wang - arXiv preprint arXiv …, 2022"],"snippet":"Ideology is at the core of political science research. Yet, there still does not exist general-purpose tools to characterize and predict ideology across different genres of text. To this end, we study Pretrained Language Models using novel ideology-driven …","url":["https://arxiv.org/pdf/2205.00619"]}
{"year":"2022","title":"Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?","authors":["ESA Lee, S Thillainathan, S Nayak, S Ranathunga… - arXiv preprint arXiv …, 2022"],"snippet":"… mined one like from Common Crawl? Comparing mBART trained on Common Crawl versus domain-specific data, we see that for several … both in and not in mBART, 10k high-quality in-domain sentences leads to better performance than …","url":["https://arxiv.org/pdf/2203.08850"]}
{"year":"2022","title":"Pre-trained transformers: an empirical comparison","authors":["S Casola, I Lauriola, A Lavelli - Machine Learning with Applications, 2022"],"snippet":"Pre-trained transformers have rapidly become very popular in the Natural Language Processing (NLP) community, surpassing the previous state of the art in a wide variety of tasks. While their effectiveness is indisputable, these methods are …","url":["https://www.sciencedirect.com/science/article/pii/S2666827022000445"]}
{"year":"2022","title":"Pre-training large NLP-models by utilizing low-resource NLP-pipelines","authors":["Z van Cauter"],"snippet":"The dominating approach to Natural Language Processing problems is by pre-training models on large amounts of unsupervised text and fine-tuning them for use on downstream tasks. Over the years, this amount of unsupervised text has grown …","url":["https://pure.tue.nl/ws/portalfiles/portal/199766232/Cauter_Z.pdf"]}
{"year":"2022","title":"Pre-training Transformer Models with Sentence-Level Objectives for Answer Sentence Selection","authors":["L Di Liello, S Garg, L Soldaini, A Moschitti - arXiv preprint arXiv:2205.10455, 2022"],"snippet":"An important task for designing QA systems is answer sentence selection (AS2): selecting the sentence containing (or constituting) the answer to a question from a set of retrieved relevant documents. In this paper, we propose three novel sentence-level …","url":["https://arxiv.org/pdf/2205.10455"]}
{"year":"2022","title":"Predicting Embedding Reliability in Low-Resource Settings Using Corpus Similarity Measures","authors":["J Dunn, H Li, D Sastre - arXiv preprint arXiv:2206.04330, 2022"],"snippet":"This paper simulates a low-resource setting across 17 languages in order to evaluate embedding similarity, stability, and reliability under different conditions. The goal is to use corpus similarity measures before training to predict properties of …","url":["https://arxiv.org/pdf/2206.04330"]}
{"year":"2022","title":"Predicting Fine-Tuning Performance with Probing","authors":["Z Zhu, S Shahtalebi, F Rudzicz - Challenges {\\&, 2022"],"snippet":"… For example, after pre-training a deep neural model for 1000 steps on Wikipedia, is it more beneficial to continue training on Wikipedia or proceed to Common Crawl? The probing results could help answer questions of this kind, which we leave to …","url":["https://openreview.net/pdf?id=BLXlV3MBLZ5"]}
{"year":"2022","title":"Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words","authors":["Z Feng, D Tang, C Zhou, J Liao, S Wu, X Feng, B Qin… - arXiv preprint arXiv …, 2022"],"snippet":"… Glove embedding is pretrained on Common Crawl dataset with 42B tokens 2 with 300 dimension. Given a sequence of words, we first get the corresponding 300-dimensional representation for each word through Glove Embedding. Then, we add a linear layer …","url":["https://arxiv.org/pdf/2202.12142"]}
{"year":"2022","title":"Prior knowledge guided weakly supervised object detection and semantic segmentation","authors":["F Baltacı - 2022"],"snippet":"State-of-the-art recognition models in computer vision are trained using annotated training data. Collecting manual annotation for images is a time-consuming and tedious task. Annotation time and difficulty also change across computer vision tasks …","url":["https://open.metu.edu.tr/bitstream/handle/11511/96300/METU_Thesis_fatihbaltaci.pdf"]}
{"year":"2022","title":"Privacy conflict analysis in web interaction models","authors":["P Inglis - 2022"],"snippet":"User privacy has become an important topic with strong implications for the manner by which software systems are designed and used. However, it is not a straightforward consideration on how the instrumentation of data processing …","url":["https://theses.gla.ac.uk/82875/1/2022inglisphd.pdf"]}
{"year":"2022","title":"Privacy Policies of IoT Devices: Collection and Analysis","authors":["M Kuznetsov, E Novikova, I Kotenko, E Doynikova - Sensors, 2022"],"snippet":"Currently, personal data collection and processing are widely used while providing digital services within mobile sensing networks for their operation, personalization, and improvement. Personal data are any data that identifiably describe a person …","url":["https://www.mdpi.com/1424-8220/22/5/1838/pdf"]}
{"year":"2022","title":"Proactive Interference-aware Resource Management in Deep Learning Training Cluster","authors":["GF Yeung - 2022"],"snippet":"Deep Learning (DL) applications are growing at an unprecedented rate across many domains, ranging from weather prediction, map navigation to medical imaging. However, training these deep learning models in large-scale compute clusters face …","url":["https://eprints.lancs.ac.uk/id/eprint/172142/1/2022yeungphd.pdf"]}
{"year":"2022","title":"Probing for Dutch Relative Pronoun Choice","authors":["G Bouma - Computational Linguistics in the Netherlands Journal, 2021"],"snippet":"We propose a linguistically motivated version of the relative pronoun probing task for Dutch (where a model has to predict whether a masked token is either die or dat), collect realistic data for it using a parsed corpus, and probe the performance of four …","url":["https://www.clinjournal.org/index.php/clinj/article/download/121/105"]}
{"year":"2022","title":"Probing language identity encoded in pre-trained multilingual models: a typological view","authors":["J Zheng, Y Liu - PeerJ Computer Science, 2022"],"snippet":"… It is trained on CommonCrawl corpora with 12 layers and 768 hidden states. The vocabulary size is 250k, The tool for tokenization is free, namely, the Sentence Piece (Kudo & Richardson, 2018). The only task is the dynamic mask language model. …","url":["https://peerj.com/articles/cs-899/"]}
{"year":"2022","title":"Probing Pre-Trained Language Models for Cross-Cultural Differences in Values","authors":["A Arora, LA Kaffee, I Augenstein - arXiv preprint arXiv:2203.13722, 2022"],"snippet":"… 2019) and is trained with an MLM objective on 2.5 TB of filtered CommonCrawl corpus data in 100 languages. It shows strong multilingual performance across a range of benchmarks and is commonly used for extracting multilingual sentence encodings. …","url":["https://arxiv.org/pdf/2203.13722"]}
{"year":"2022","title":"Project European Language Equality (ELE) Grant agreement no. LC-01641480–101018166 ELE Coordinator Prof. Dr. Andy Way (DCU) Co-coordinator Prof. Dr …","authors":["G Backfried, M Skowron, E Navas, A Bērziņš… - 2022"],"snippet":"D2. 14 provides an overview and describes the state of the art and developments within the field of Speech Technologies (ST). This field is interpreted to comprise technologies aimed at the processing and production of the human voice, both, from …","url":["https://european-language-equality.eu/wp-content/uploads/2022/03/ELE___Deliverable_D2_14__Speech__Technologies.pdf"]}
{"year":"2022","title":"Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models","authors":["M Xia, M Artetxe, J Du, D Chen, V Stoyanov - arXiv preprint arXiv:2205.15223, 2022"],"snippet":"Pre-trained masked language models successfully perform few-shot learning by formulating downstream tasks as text infilling. However, as a strong alternative in full-shot settings, discriminative pre-trained models like ELECTRA do not fit into the paradigm …","url":["https://arxiv.org/pdf/2205.15223"]}
{"year":"2022","title":"Promt systems for wmt21 terminology translation task","authors":["A Molchanov, V Kovalenko, F Bykov - Proceedings of the Sixth Conference on …, 2021"],"snippet":"… We first build an English-Russian system using all parallel data (except commoncrawl which we believe to be of bad quality; basic filtering is applied) including the Edinborough corpus of Russian news translated into English and …","url":["https://aclanthology.org/2021.wmt-1.83.pdf"]}
{"year":"2022","title":"Prospects for Dutch Emotion Detection: Insights from the new EmotioNL Dataset","authors":["L De Bruyne, O De Clercq, V Hoste - Computational Linguistics in the Netherlands …, 2021"],"snippet":"Although emotion detection has become a crucial research direction in NLP, the main focus is on English resources and data. The main obstacles for more specialized emotion detection are the lack of annotated data in smaller languages …","url":["https://clinjournal.org/index.php/clinj/article/download/138/144"]}
{"year":"2022","title":"Protecting Systems From Exploits Using Language-Theoretic Security","authors":["P Anantharaman - 2022"],"snippet":"Any computer program processing input from the user or network must validate the input. Input-handling vulnerabilities occur in programs when the software component responsible for filtering malicious input---the parser---does not perform …","url":["https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi?article=1081&context=dissertations"]}
{"year":"2022","title":"PsychBERT: A Mental Health Language Model for Social Media Mental Health Behavioral Analysis","authors":["V Vajre, M Naylor, U Kamath, A Shehu - 2021 IEEE International Conference on …, 2021"],"snippet":"Mental health behaviors are now recognized as primary factors contributing to suicide. This paper puts forth a novel mental health language model to address mental health and makes several contributions. First, it proposes a taxonomy and …","url":["https://ieeexplore.ieee.org/abstract/document/9669469/"]}
{"year":"2022","title":"Punctuation Restoration in Spanish Customer Support Transcripts using Transfer Learning","authors":["X Zhu, S Gardiner, D Rossouw, T Roldán… - arXiv preprint arXiv …, 2022"],"snippet":"Automatic Speech Recognition (ASR) systems typically produce unpunctuated transcripts that have poor readability. In addition, building a punctuation restoration system is challenging for low-resource languages, especially for domain-specific …","url":["https://arxiv.org/pdf/2205.13961"]}
{"year":"2022","title":"Qasar: Self-Supervised Learning Framework for Extractive Question Answering","authors":["H Assem, R Sarkar, S Dutta - 2021 IEEE International Conference on Big Data (Big …, 2021"],"snippet":"… Pretrained transformer based language models like BERT [8] are trained initially on large unlabelled data from common crawl corpus and Wikipedia dumps. With the creation of large labelled open-domain QA datasets like WikiQA [9], SQuAD [10] …","url":["https://ieeexplore.ieee.org/abstract/document/9671570/"]}
{"year":"2022","title":"QianXun: A Novel Enterprise File Search Framework","authors":["C Si, P Qi, H Xue, Y Sun - 2022 IEEE 25th International Conference on Computer …, 2022"],"snippet":"… Subsequently, Google proposed mT5 [7], a multilingual variant of T5 that is pre-trained on a new Common Crawl-based dataset covering 101 languages, which offers the possibility of processing Chinese texts. Then, based on the mT5, Su Jianlin trained …","url":["https://ieeexplore.ieee.org/abstract/document/9776101/"]}
{"year":"2022","title":"QMLEx: Data Driven Digital Transformation in Marketing Analytics","authors":["A Geronazzo, M Ziegler - 2021 IEEE International Conference on Big Data (Big …, 2021"],"snippet":"This paper presents a data driven approach to replace expert driven business processes. The QMLEx methodology combines NLP algorithms to improve data quality, ML techniques to perform the task and exploits external data sources to …","url":["https://ieeexplore.ieee.org/abstract/document/9671890/"]}
{"year":"2022","title":"Qualitative Analysis of Text Summarization Techniques and Its Applications in Health Domain","authors":["D Yadav, N Lalit, R Kaushik, Y Singh, AK Yadav… - Computational Intelligence …, 2022"],"snippet":"For the better utilization of the enormous amount of data available to us on the Internet and in different archives, summarization is a valuable method. Manual summarization by experts is an almost impossible and time-consuming activity …","url":["https://www.hindawi.com/journals/cin/2022/3411881/"]}
{"year":"2022","title":"Qualitative Measures for Ad hoc Table Retrieval","authors":["M Khodabakhsh, E Bagheri - Information Sciences, 2022"],"snippet":"The focus of our work is the ad hoc table retrieval task, which aims to rank a list of structured tabular objects in response to a user query. Given the importance of this task, various methods have already been proposed in the literature that focus on …","url":["https://www.sciencedirect.com/science/article/pii/S0020025522005126"]}
{"year":"2022","title":"Quantifying Memorization Across Neural Language Models","authors":["N Carlini, D Ippolito, M Jagielski, K Lee, F Tramer… - arXiv preprint arXiv …, 2022"],"snippet":"… These models were trained on C4, a cleaned and filtered version of the English web pages from the Common Crawl, which totals 806 GB in size. At 11 billion parameters, the largest T5 model is the largest publicly available masked language …","url":["https://arxiv.org/pdf/2202.07646"]}
{"year":"2022","title":"Query-Driven Graph-Based User Recommender System","authors":["Y Li - 2022"],"snippet":"Current Social Networking Systems (SNS) such as YouTube are creator-driven systems in which creators create content and users search among available content to find what they want. However, queries from users can be time-sensitive, such as …","url":["https://ruor.uottawa.ca/bitstream/10393/43742/1/Li_Yansong_2022_thesis.pdf"]}
{"year":"2022","title":"RAPIDS cuGraph","authors":["A Fender, B Rees, J Eaton - Massive Graph Analytics"],"snippet":"… -scale PageRank computations using the Common Crawl dataset [5].The Common Crawl dataset captures link relationships between web … Our system delivered much faster performance for the Common Crawl dataset, but the largest …","url":["https://www.taylorfrancis.com/chapters/edit/10.1201/9781003033707-22/rapids-cugraph-alex-fender-brad-rees-joe-eaton"]}
{"year":"2022","title":"Readability of English, German, and Russian Disease-Related Wikipedia Pages: Automated Computational Analysis","authors":["J Gordejeva, R Zowalla, M Pobiruchin, M Wiesner - Journal of Medical Internet …, 2022"],"snippet":"Background Wikipedia is a popular encyclopedia for healthand disease-related information in which patients seek advice and guidance on the web. Yet, Wikipedia articles can be unsuitable as patient education materials, as investigated in previous …","url":["https://www.jmir.org/2022/5/e36835/"]}
{"year":"2022","title":"Reading Comprehension","authors":["RS Roy, A Anand - Question Answering for the Curated Web, 2022"],"snippet":"… Corpora of curated text (like Wikipedia), large volumes of uncurated social-media posts, and massive Web document collection projects such as Common Crawl,1 are used to train these models. The key idea is that rather than going through the trouble …","url":["https://link.springer.com/content/pdf/10.1007/978-3-031-79512-1_9.pdf"]}
{"year":"2022","title":"RECOGNITION THROUGH SPONTANEOUS SPEECH","authors":["S Luz, F Haider, D Fromm, B MacWhinney"],"snippet":"Alzheimer’s disease (AD) is a chronic neurodegenerative disease, without any known treatment. This disease progressively destroys brain structures, such as the hippocampus and entorhinal cortex, due to the accumulation of pathological forms of …","url":["https://www.research.ed.ac.uk/files/260484259/9782889718542_3.PDF"]}
{"year":"2022","title":"Reducing Disambiguation Biases in NMT by Leveraging Explicit Word Sense Information","authors":["N Campolungo, T Pasini, D Emelin, R Navigli - … of the 2022 Conference of the North …, 2022"],"snippet":"Recent studies have shed some light on a common pitfall of Neural Machine Translation (NMT) models, stemming from their struggle to disambiguate polysemous words without lapsing into their most frequently occurring senses in the …","url":["https://aclanthology.org/2022.naacl-main.355.pdf"]}
{"year":"2022","title":"Refining Low-Resource Unsupervised Translation by Language Disentanglement of Multilingual Model","authors":["XP Nguyen, S Joty, W Kui, AT Aw - arXiv preprint arXiv:2205.15544, 2022"],"snippet":"… For each group, we use monolingual corpora from the relevant languages of the Common Crawl dataset, which contains data from totally 25 languages [21]. We limit the amount of monolingual data per language to up to 100M sentences. Similarly to …","url":["https://arxiv.org/pdf/2205.15544"]}
{"year":"2022","title":"Regressing Word and Sentence Embeddings for Low-Resource Neural Machine Translation","authors":["IJ Unanue, EZ Borzeshi, M Piccardi - IEEE Transactions on Artificial Intelligence, 2022"],"snippet":"… The training set contains over 5M sentence pairs collected from the Europarl, CommonCrawl and Newscommentary parallel corpora. As validation and test sets, we have used the newstest2017 and the newstest2018 datasets, respectively. This …","url":["https://ieeexplore.ieee.org/abstract/document/9813389/"]}
{"year":"2022","title":"Regularization-based Pruning of Irrelevant Weights in Deep Neural Architectures","authors":["G Bonetta, M Ribero, R Cancelliere - arXiv preprint arXiv:2204.04977, 2022"],"snippet":"Deep neural networks exploiting millions of parameters are nowadays the norm in deep learning applications. This is a potential issue because of the great amount of computational resources needed for training, and of the possible loss of …","url":["https://arxiv.org/pdf/2204.04977"]}
{"year":"2022","title":"Report 2 on ASR Systems","authors":["TNN KIT, CH KIT - 2022"],"snippet":"1 Executive Summary In this deliverable we describe the final automatic speech recognition (ASR) systems that we created for the ELITR speech translation system. In Section 2 we describe our work. In the subsections 2.1 and 2.2 we address tasks …","url":["https://elitr.eu/wp-content/uploads/2022/06/D2.2.pdf"]}
{"year":"2022","title":"Representations of emotion concepts: Comparison across pairwise, appraisal feature-based, and word embedding-based similarity spaces","authors":["M Kwon, T Wager, J Phillips - Proceedings of the Annual Meeting of the Cognitive …, 2022"],"snippet":"A question that has long interested cognitive scientists is how to best represent the different emotions we experience and attribute to others. For example, constructionist and appraisal theories propose that differences between emotions …","url":["https://escholarship.org/content/qt8vj3d366/qt8vj3d366.pdf"]}
{"year":"2022","title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?","authors":["S Min, X Lyu, A Holtzman, M Artetxe, M Lewis… - arXiv preprint arXiv …, 2022","WMICL Work"],"snippet":"Large language models (LMs) are able to in-context learn--perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of …","url":["https://arxiv.org/pdf/2202.12837","https://openreview.net/pdf?id=cnRGMv-Ak7u"]}
{"year":"2022","title":"Revisiting CCNet for Quality Measurements in Galician","authors":["JE Ortega, I de-Dios-Flores, JR Pichel, P Gamallo - International Conference on …, 2022","JR Pichel, P Gamallo","P Gamallo - … Processing of the Portuguese Language: 15th …"],"snippet":"… In this article, we present our findings on reproducing the introduction of the common crawl corpus by Facebook, known as the CCNet … as the Common Crawl 2 corpus based on a “snapshot” of the web at a given time – the work on CCNet is …","url":["https://books.google.de/books?hl=en&lr=lang_en&id=Df9kEAAAQBAJ&oi=fnd&pg=PA407&dq=commoncrawl&ots=UGnguc7I3u&sig=8yrKGw8bPU5TtHaLaA-1I63OUas","https://gramatica.usc.es/~gamallo/artigos-web/PROPOR2022.pdf","https://link.springer.com/chapter/10.1007/978-3-030-98305-5_38"]}
{"year":"2022","title":"Revisiting DocRED--Addressing the Overlooked False Negative Problem in Relation Extraction","authors":["Q Tan, L Xu, L Bing, HT Ng - arXiv preprint arXiv:2205.12696, 2022"],"snippet":"The DocRED dataset is one of the most popular and widely used benchmarks for document-level relation extraction (RE). It adopts a recommend-revise annotation scheme so as to have a large-scale annotated dataset. However, we find that the …","url":["https://arxiv.org/pdf/2205.12696"]}
{"year":"2022","title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Understanding","authors":["A Ghaddar, Y Wu, S Bagga, A Rashid, K Bibi… - arXiv preprint arXiv …, 2022"],"snippet":"… We collect pre-training data from the following sources: common crawl (CC), news (NEWS, ELKHAIR), and Wikipedia (WIKI).Recent … Common Crawl (CC): We used 10 shards of Common Crawl8 data from March to December 2020. After …","url":["https://arxiv.org/pdf/2205.10687"]}
{"year":"2022","title":"RigoBERTa: A State-of-the-Art Language Model For Spanish","authors":["AV Serrano, GG Subies, HM Zamorano, NA Garcia… - arXiv preprint arXiv …, 2022"],"snippet":"… OSCAR [28] [29] is a very large multilingual corpus, obtained by language classification and filtering of the CommonCrawl7. It has a portion of … For example, javascript code, automatic writings, poor automatic translations or malformed …","url":["https://arxiv.org/pdf/2205.10233"]}
{"year":"2022","title":"Robust Phishing Detection Against Adversaries","authors":["S Al-Ahmadi - Journal: WSEAS TRANSACTIONS ON COMPUTER …, 2022"],"snippet":"Phishing websites have grown more recently than ever, and they become more intelligent, even against well-designed phishing detection techniques. Formerly, we have proposed in the literature a state-of-the-art URL-exclusive phishing detection …","url":["https://ouci.dntb.gov.ua/en/works/lRjbXjjl/"]}
{"year":"2022","title":"Robustness Analysis of Grover for Machine-Generated News Detection","authors":["R Gagiano, MMH Kim, X Zhang, J Biggs"],"snippet":"Advancements in Natural Language Generation have raised concerns on its potential misuse for deep fake news. Grover is a model for both generation and detection of neural fake news. While its performance on automatically discriminating …","url":["https://alta2021.alta.asn.au/files/ALTW_2021_paper_7.pdf"]}
{"year":"2022","title":"Romantic-Computing","authors":["E Horishny - arXiv preprint arXiv:2206.11864, 2022"],"snippet":"In this paper we compare various text generation models' ability to write poetry in the style of early English Romanticism. These models include: Character-Level Recurrent Neural Networks with Long Short-Term Memory, Hugging Face's GPT-2 …","url":["https://arxiv.org/pdf/2206.11864"]}
{"year":"2022","title":"SALTED: A Framework for SAlient Long-Tail Translation Error Detection","authors":["V Raunak, M Post, A Menezes - arXiv preprint arXiv:2205.09988, 2022"],"snippet":"Traditional machine translation (MT) metrics provide an average measure of translation quality that is insensitive to the long tail of behavioral problems in MT. Examples include translation of numbers, physical units, dropped content and …","url":["https://arxiv.org/pdf/2205.09988"]}
{"year":"2022","title":"Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation","authors":["TK Lam, S Schamoni, S Riezler - arXiv preprint arXiv:2203.08757, 2022"],"snippet":"End-to-end speech translation relies on data that pair source-language speech inputs with corresponding translations into a target language. Such data are notoriously scarce, making synthetic data augmentation by back-translation or …","url":["https://arxiv.org/pdf/2203.08757"]}
{"year":"2022","title":"Sarcasm detection using deep learning and ensemble learning","authors":["P Goel, R Jain, A Nayyar, S Singhal, M Srivastava - Multimedia Tools and …, 2022"],"snippet":"Across the globe, there is a noticeable upward trend of incorporating sarcasm in everyday life. This trend can be easily attributed to the frequent use of sarcasm in everyday life, but more specifically to social media and the Internet. This study aims …","url":["https://link.springer.com/article/10.1007/s11042-022-12930-z"]}
{"year":"2022","title":"Scaling Laws and Interpretability of Learning from Repeated Data","authors":["D Hernandez, T Brown, T Conerly, N DasSarma… - arXiv preprint arXiv …, 2022"],"snippet":"Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is …","url":["https://arxiv.org/pdf/2205.10487"]}
{"year":"2022","title":"Security and Privacy Challenges for Intelligent Internet of Things Devices 2022 TADW: Traceable and Antidetection Dynamic Watermarking of Deep Neural Networks","authors":["J Dong, H Wang, Z He, J Niu, X Zhu, G Wu - Security and Communication Networks, 2022"],"snippet":"Deep neural networks (DNN) with incomparably advanced performance have been extensively applied in diverse fields (eg, image recognition, natural language processing, and speech recognition). Training a high-performance DNN model …","url":["https://www.hindawi.com/journals/scn/2022/9505808/"]}
{"year":"2022","title":"Seeing the advantage: visually grounding word embeddings to better capture human semantic knowledge","authors":["D Merkx, SL Frank, M Ernestus - arXiv preprint arXiv:2202.10292, 2022"],"snippet":"Distributional semantic models capture word-level meaning that is useful in many natural language processing tasks and have even been shown to capture cognitive aspects of word meaning. The majority of these models are purely text based, even …","url":["https://arxiv.org/pdf/2202.10292"]}
{"year":"2022","title":"SEHC: A Benchmark Setup to Identify Online Hate Speech in English","authors":["S Ghosh, A Ekbal, P Bhattacharyya, T Saha, A Kumar… - IEEE Transactions on …, 2022"],"snippet":"… The embeddings are fetched from GloVe (300-dimension) pre-trained word embedding, which was trained on the Common Crawl corpus (840 billion tokens). We pad input arrays with zeros to make input sequences the same length. On our …","url":["https://ieeexplore.ieee.org/abstract/document/9738838/"]}
{"year":"2022","title":"Semantic Annotation of Numerical Data in Web Tables","authors":["Y Su - 2021"],"snippet":"A large portion of quantitative information about entities mentioned in Web pages is expressed as Web tables, and these tables often lack proper schema and annotation, which introduces challenges for the purpose of querying and further analysis. In this …","url":["https://scholar.archive.org/work/bb4fkx7uzzbi7pnelydtz4nnsy/access/wayback/https://era.library.ualberta.ca/items/b25fe525-b33c-4954-a5d4-3066f797cd34/view/a1788204-c1a8-4a67-be55-57005a93cabc/Su_Yuchen_202109_MSc.pdf"]}
{"year":"2022","title":"Semantic Comparisons for Natural Language Processing Applications","authors":["L Lin - 2021"],"snippet":"For social scientists and other data practitioners, the abundance of available digital text data is a rich potential source for understanding social phenomena. As a result, practitioners have increasingly used text analysis methods on relevant corpora to …","url":["https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/48232/Lin_washington_0250E_23655.pdf?sequence=1&isAllowed=y"]}
{"year":"2022","title":"Semantic projection recovers rich human knowledge of multiple object features from word embeddings","authors":["G Grand, IA Blank, F Pereira, E Fedorenko - Nature Human Behaviour, 2022"],"snippet":"How is knowledge about word meaning represented in the mental lexicon? Current computational models infer word meanings from lexical co-occurrence patterns. They learn to represent words as vectors in a multidimensional space, wherein …","url":["https://www.nature.com/articles/s41562-022-01316-8"]}
{"year":"2022","title":"Semantics-driven Attentive Few-shot Learning over Clean and Noisy Samples","authors":["OB Baran, RG Cinbiş - arXiv preprint arXiv:2201.03043, 2022"],"snippet":"Over the last couple of years few-shot learning (FSL) has attracted great attention towards minimizing the dependency on labeled training examples. An inherent difficulty in FSL is the handling of ambiguities resulting from having too few training …","url":["https://arxiv.org/pdf/2201.03043"]}
{"year":"2022","title":"SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding","authors":["HT Madabushi, E Gow-Smith, M Garcia, C Scarton… - arXiv preprint arXiv …, 2022"],"snippet":"This paper presents the shared task on Multilingual Idiomaticity Detection and Sentence Embedding, which consists of two subtasks: (a) a binary classification one aimed at identifying whether a sentence contains an idiomatic expression, and (b) a …","url":["https://arxiv.org/pdf/2204.10050"]}
{"year":"2022","title":"Sentence Simplification Capabilities of Transfer-Based Models","authors":["S Štajner, KC Sheang, H Saggion - 2022"],"snippet":"… T5 is trained on Colossal Clean Crawled Corpus (C4), a dataset created by applying a set of filters to English-language text sourced from the public Common Crawl web scrape. • mT5 (Xue et al. 2021): a multilingual model based on T5 (Raffel …","url":["https://www.aaai.org/AAAI22Papers/AISI-7740.StajnerS.pdf"]}
{"year":"2022","title":"SEntFiN 1.0: Entity‐aware sentiment analysis for financial news","authors":["A Sinha, S Kedas, R Kumar, P Malo - Journal of the Association for Information Science and …"],"snippet":"Fine‐grained financial sentiment analysis on news headlines is a challenging task requiring human‐annotated datasets to achieve high performance. Limited studies have tried to address the sentiment extraction task in a setting where multiple …","url":["https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.24634"]}
{"year":"2022","title":"Sentiment Analysis of COVID-19 Vaccination Responses From Twitter Using Ensemble Learning","authors":["Q Ismail, R Obeidat, K Alissa, E Al-Sobh - 2022 13th International Conference on …, 2022"],"snippet":"… As an alternative to BERT, we also use RoBERTa, a robust pre-trained language model for English, learned from 160GB of texts covering books, Wikipedia, CommonCrawl news, CommonCrawl stories, and web text contents [11]. Our SA …","url":["https://ieeexplore.ieee.org/abstract/document/9811132/"]}
{"year":"2022","title":"Sentiment Analysis of Russian Reviews to Estimate the Usefulness of Drugs Using the Domain-Specific XLM-RoBERTa Model","authors":["A Sboev, A Naumov, I Moloshnikov, R Rybka - Biologically Inspired Cognitive …, 2022"],"snippet":"This paper considers the problem of classifying Russian-language drug reviews into five classes (corresponding to the authors’ rating scores) and into two classes (the drug is helpful or useless) in terms of the sentiment analysis task. The dataset of …","url":["https://link.springer.com/chapter/10.1007/978-3-030-96993-6_49"]}
{"year":"2022","title":"Sentiment Analysis with Linguistic Knowledge","authors":["P KLIMEŠOVÁ"],"snippet":"… This thesis employs FastText CBOW word and text vectors with dimensionality 300 pre-trained on Wikipedia and the Common Crawl1 web corpus with negative sampling and n-gram length 52. The SVM, Logistic Regression, and Naïve Bayes …","url":["https://is.muni.cz/th/n0lnb/Sentiment_Analysis_cz.pdf"]}
{"year":"2022","title":"SentSpace: Large-Scale Benchmarking and Evaluation of Text using Cognitively Motivated Lexical, Syntactic, and Semantic Features","authors":["G Tuckute, A Sathe, M Wang, H Yoder, C Shain… - Proceedings of the 2022 …, 2022"],"snippet":"SentSpace is a modular framework for streamlined evaluation of text. SentSpacecharacterizes textual input using diverse lexical, syntactic, and semantic features derivedfrom corpora and psycholinguistic experiments. Core sentence …","url":["https://aclanthology.org/2022.naacl-demo.11.pdf"]}
{"year":"2022","title":"Sequence Labeling Architectures in Diglossia","authors":["M Weidlich"],"snippet":"… FastText is a lightweight library pretrained on the Common Crawl dataset18 and Wikipedia in 157 languages including Arabic19 “using [Continuous Bag of Words] with position-weights, in 300 dimensions, with character n-grams of length 5, a …","url":["https://www.researchgate.net/profile/Mohamed-Megahed-2/publication/358956953_Sequence_Labeling_Architectures_in_Diglossia_-_a_case_study_of_Arabic_and_its_dialects/links/621f2684ef04e66eb74d6e08/Sequence-Labeling-Architectures-in-Diglossia-a-case-study-of-Arabic-and-its-dialects.pdf"]}
{"year":"2022","title":"Simple measures of bridging lexical divergence help unsupervised neural machine translation for low-resource languages","authors":["J Khatri, R Murthy, T Banerjee, P Bhattacharyya - Machine Translation, 2022"],"snippet":"… We consider CommonCrawl and the BigEst Estonian Corpus from the WMT18 shared task (Bojar et al. ) to create the monolingual corpus for Estonian. For English, we consider News Crawl 2017 and News Discussions 2017 from the WMT17 shared …","url":["https://link.springer.com/article/10.1007/s10590-021-09292-y"]}
{"year":"2022","title":"Sliced at SemEval-2022 Task 11: Bigger, Better? Massively Multilingual LMs for Multilingual Complex NER on an Academic GPU Budget","authors":["B Plank - Proceedings of the 16th International Workshop on …, 2022"],"snippet":"Massively multilingual language models (MMLMs) have become a widely-used representation method, and multiple large MMLMs were proposed in recent years. A trend is to train MMLMs on larger text corpora or with more layers. In this paper we …","url":["https://aclanthology.org/2022.semeval-1.205.pdf"]}
{"year":"2022","title":"Social and Web presence of Cultural Heritage Organisations in India","authors":["H Singh, M Gupta - 2022"],"snippet":"The present study focuses on the Social and Web presence of 27 Cultural Heritage Organisations in India. The purpose of this study is to investigate the domain authority, number of webpages, links, calculate the web impact factors and link-mentions …","url":["https://www.researchgate.net/profile/Mansi-Gupta-45/publication/362076832_Social_and_Web_presence_of_Cultural_Heritage_Organisations_in_India/links/62d583c8bf4b98532231ac8e/Social-and-Web-presence-of-Cultural-Heritage-Organisations-in-India.pdf"]}
{"year":"2022","title":"SPARQL querying for validating the usage of automatically georeferenced social media data as human sensors for air quality","authors":["S Andreadis, T Mavropoulos, N Pantelidis, S Vrochidis… - 2022 IEEE 14th Image …, 2022"],"snippet":"… original XLM model, XLM-R drew inspiration from RoBERTa [19], in the sense that it was trained for a longer time on more data, ie more than 2TBs of filtered CommonCrawl data. Since the tweets collection relates to the German language, a …","url":["https://2022.ivmsp.org/wp-content/uploads/2022/06/2022101087_IVMSP2022_CR_Copyright.pdf"]}
{"year":"2022","title":"SPBERTQA: A Two-Stage Question Answering System Based on Sentence Transformers for Medical Texts","authors":["NTH Nguyen, PPD Ha, LT Nguyen, K Van Nguyen… - arXiv preprint arXiv …, 2022"],"snippet":"Question answering (QA) systems have gained explosive attention in recent years. However, QA tasks in Vietnamese do not have many datasets. Significantly, there is mostly no dataset in the medical domain. Therefore, we built a Vietnamese …","url":["https://arxiv.org/pdf/2206.09600"]}
{"year":"2022","title":"Speeding up Inference Time of Neural Machine Translation","authors":["M GELETKA"],"snippet":"Large qualitative gains were recently made in machine translation tasks thanks to Transformers models. However, in practice, these models suffer from high latency, such that they often are hardly usable in practical applications. This thesis study …","url":["https://is.muni.cz/th/if8vh/speeding_up_NMT_final.pdf"]}
{"year":"2022","title":"State-of-the-art in Open-domain Conversational AI: A Survey","authors":["T Adewumi, F Liwicki, M Liwicki - arXiv preprint arXiv:2205.00965, 2022"],"snippet":"We survey SoTA open-domain conversational AI models with the purpose of presenting the prevailing challenges that still exist to spur future research. In addition, we provide statistics on the gender of conversational AI in order to guide the ethics …","url":["https://arxiv.org/pdf/2205.00965"]}
{"year":"2022","title":"Statistical detection of format dialects using the weighted Dowker complex","authors":["M Robinson, LW Li, C Anderson, S Huntsman - arXiv preprint arXiv:2201.08267, 2022"],"snippet":"… The good files were largely sourced from Common Crawl [15], while the bad files were drawn from various … sources: some were found in the wild (from Common Crawl), some were malicious files created by the Test and Evaluation Team, and …","url":["https://arxiv.org/pdf/2201.08267"]}
{"year":"2022","title":"StoryNet: A 5W1H-Based Knowledge Graph to Connect Stories","authors":["SR Nagireddy - 2021"],"snippet":"Stories are a powerful medium through which the human community has exchanged information since the dawn of the information age. They have taken multiple forms like articles, movies, books, plays, short films, magazines, mythologies, etc. With the …","url":["https://search.proquest.com/openview/19566ca4aa369437c6fc323d788f8739/1?pq-origsite=gscholar&cbl=18750&diss=y"]}
{"year":"2022","title":"Subword-based Cross-lingual Transfer of Embeddings from Hindi to Marathi and Nepali","authors":["N Bafna, Z Žabokrtský - Proceedings of the 19th SIGMORPHON Workshop on …, 2022"],"snippet":"Word embeddings are growing to be a crucial resource in the field of NLP for any language. This work introduces a novel technique for static subword embeddings transfer for Indic languages from a relatively higher resource language to a …","url":["https://aclanthology.org/2022.sigmorphon-1.7.pdf"]}
{"year":"2022","title":"Survey on bimodal speech emotion recognition from acoustic and linguistic information fusion","authors":["BT Atmaja, A Sasou, M Akagi - Speech Communication, 2022"],"snippet":"Speech emotion recognition (SER) is traditionally performed using merely acoustic information. Acoustic features, commonly are extracted per frame, are mapped into emotion labels using classifiers such as support vector machines for machine …","url":["https://www.sciencedirect.com/science/article/pii/S0167639322000413"]}
{"year":"2022","title":"Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding","authors":["Y Sun, Q Chao, B Li - arXiv preprint arXiv:2203.05711, 2022"],"snippet":"Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives(SyMoN), containing 5,193 video summaries …","url":["https://arxiv.org/pdf/2203.05711"]}
{"year":"2022","title":"Table Pretraining: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks","authors":["H Dong, Z Cheng, X He, M Zhou, A Zhou, F Zhou, A Liu… - arXiv preprint arXiv …, 2022"],"snippet":"Since a vast number of tables can be easily collected from web pages, spreadsheets, PDFs, and various other document types, a flurry of table pretraining frameworks have been proposed following the success of text and images, and they have …","url":["https://arxiv.org/pdf/2201.09745"]}
{"year":"2022","title":"Tagging terms in text","authors":["AR Terryn, V Hoste, E Lefever"],"snippet":"… We used the embeddings that are pre-trained on Common Crawl (for all languages). Finally, the hugely successful transformer-based architectures are supported in FlairNLP as well, through HuggingFace (Wolf et al. …","url":["https://www.researchgate.net/profile/Ayla_Rigouts_Terryn2/publication/356283802_Tagging_terms_in_text_A_supervised_sequential_labelling_approach_to_automatic_term_extraction/links/61f00c02c5e3103375bd750b/Tagging-terms-in-text-A-supervised-sequential-labelling-approach-to-automatic-term-extraction.pdf"]}
{"year":"2022","title":"TamilEmo: Finegrained Emotion Detection Dataset for Tamil","authors":["C Vasantharajan, S Benhur, PK Kumarasen… - arXiv preprint arXiv …, 2022"],"snippet":"Emotional Analysis from textual input has been considered both a challenging and interesting task in Natural Language Processing. However, due to the lack of datasets in low-resource languages (ie Tamil), it is difficult to conduct research of …","url":["https://arxiv.org/pdf/2202.04725"]}
{"year":"2022","title":"Technology, Vintage-Specific Human Capital, and Labor Displacement: Evidence from Linking Patents with Occupations","authors":["L Kogan, D Papanikolaou, LDW Schmidt, B Seegmiller"],"snippet":"We develop a granular, occupation-specific measure of technological progress that relies only on textual descriptions of patent documents and the tasks performed by workers in an occupation. Our measure primarily identifies labor-saving innovations …","url":["https://www.bryanseegmiller.com/files/Draft_v2022-07.pdf"]}
{"year":"2022","title":"Test collections for web-scale datasets using Dynamic Sampling","authors":["A Singh - 2022"],"snippet":"… Other large scale web collections, like the Common Crawl dataset, have also been used in various TREC tracks. It might be interesting to study the performance of Dynamic Sampling on these alternate datasets like Common Crawl1. Additionally, in …","url":["https://uwspace.uwaterloo.ca/bitstream/handle/10012/17889/Singh_Anmol.pdf?sequence=3"]}
{"year":"2022","title":"TeSum: Human-Generated Abstractive Summarization Corpus for Telugu","authors":["A Urlana, N Surange, P Baswani, P Ravva…"],"snippet":"Expert human annotation for summarization is definitely an expensive task, and can not be done on huge scales. But with this work, we show that even with a crowd sourced summary generation approach, quality can be controlled by aggressive …","url":["http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.614.pdf"]}
{"year":"2022","title":"Text Analysis of Airline Tweets","authors":["E ALTamimi - 2022"],"snippet":"… For the FastText approach, we used gensim to load the vector file from the common crawl models downloaded from fasttext (FastText, 2020) and converted it to a bin model because the loading time of the vector was more. We then continued to …","url":["https://scholarworks.rit.edu/cgi/viewcontent.cgi?article=12309&context=theses"]}
{"year":"2022","title":"Text Representations and Word Embeddings","authors":["R Egger - Applied Data Science in Tourism, 2022"],"snippet":"Today, a vast amount of unstructured text data is consistently at our disposal. Owing to the rapid increase in user-generated content on the one hand and powerful, ready-to-use machine learning algorithms on the other hand, automated text analysis can now be …","url":["https://link.springer.com/chapter/10.1007/978-3-030-88389-8_16"]}
{"year":"2022","title":"Text to Image GANs with RoBERTa and Fine-grained Attention Networks","authors":["M Siddharth, R Aarthi"],"snippet":"… The additional data included CommonCrawl News dataset, Web text corpus and Stories from Common Crawl. The RoBERTa makes use of similar architecture as BERT Model but uses the byte-level BPE as the tokenizer [4]. The ’roberta-base’ is …","url":["https://www.researchgate.net/profile/Siddharth-M/publication/357450869_Text_to_Image_GANs_with_RoBERTa_and_Fine-grained_Attention_Networks/links/61d2bb43e669ee0f5c8185bb/Text-to-Image-GANs-with-RoBERTa-and-Fine-grained-Attention-Networks.pdf"]}
{"year":"2022","title":"Text Vectorization Method Based on Concept Mining Using Clustering Techniques","authors":["A Mansour, J Mohammad, Y Kravchenko - 2022 VI International Conference on …, 2022"],"snippet":"… word vectors trained with sub-word information on Wikipedia 2017, UMBC webbase corpus, and statmt.org news data set (16B tokens); word2vec model2 which was trained with Google News corpus (3 billion words) and Glove model …","url":["https://ieeexplore.ieee.org/abstract/document/9782908/"]}
{"year":"2022","title":"The Causal News Corpus: Annotating Causal Relations in Event Sentences from News","authors":["FA Tan, A Hürriyetoğlu, T Caselli, N Oostdijk, T Nomoto… - arXiv preprint arXiv …, 2022"],"snippet":"Despite the importance of understanding causality, corpora addressing causal relations are limited. There is a discrepancy between existing annotation guidelines of event causality and conventional causality corpora that focus more on linguistics …","url":["https://arxiv.org/pdf/2204.11714"]}
{"year":"2022","title":"The Curious Case of Control","authors":["E Stengel-Eskin, B Van Durme - arXiv preprint arXiv:2205.12113, 2022"],"snippet":"… The training data is based on Common Crawl, though similarly to GPT-3 Davinci, the details of the training data filtering process are unclear. Relevant differences to GPT-3 are in the tokenization (which includes multi-word expressions) and use of …","url":["https://arxiv.org/pdf/2205.12113"]}
{"year":"2022","title":"The Dark Side of the Language: Pre-trained Transformers in the DarkNet","authors":["L Ranaldi, A Nourbakhsh, A Patrizi, ES Ruzzetti… - arXiv preprint arXiv …, 2022"],"snippet":"Pre-trained Transformers are challenging human performances in many natural language processing tasks. The gigantic datasets used for pre-training seem to be the key for their success on existing tasks. In this paper, we explore how a range of …","url":["https://arxiv.org/pdf/2201.05613"]}
{"year":"2022","title":"The emergence of gender associations in child language development","authors":["B Prystawski, E Grant, A Nematzadeh, SWS Lee…"],"snippet":"Gender associations have been a long-standing research topic in psychological and social sciences. Although it is known that children learn aspects of gender association at a young age, it is not well understood how they might emerge through …","url":["https://www.researchgate.net/profile/Yang-Xu-34/publication/360554719_The_emergence_of_gender_associations_in_child_language_development/links/627d55beb1ad9f66c8b8563c/The-emergence-of-gender-associations-in-child-language-development.pdf"]}
{"year":"2022","title":"The fujitsu dmath submissions for wmt21 news translation and biomedical translation tasks","authors":["A Martínez - Proceedings of the Sixth Conference on Machine …, 2021"],"snippet":"… For example, we noted that the Hausa Extended Common Crawl corpus published for WMT21 contained a large number of Japanese … Despite its noisy nature, we decided to use the Extended Common Crawl Hausa corpus. The results …","url":["https://aclanthology.org/2021.wmt-1.13.pdf"]}
{"year":"2022","title":"The Ghost in the Machine has an American accent: value conflict in GPT-3","authors":["RL Johnson, G Pistilli, N Menédez-González… - arXiv preprint arXiv …, 2022"],"snippet":"The alignment problem in the context of large language models must consider the plurality of human values in our world. Whilst there are many resonant and overlapping values amongst the world's cultures, there are also many conflicting, yet …","url":["https://arxiv.org/pdf/2203.07785"]}
{"year":"2022","title":"The hitchhiker's guide Method handbook for quantification of online lin-guistic data in a country-specific context Official research report, Linguistic Explorations of …","authors":["J Andersson Schwarz - 2022"],"snippet":"… A very significant non-profit provider to consider alongside these commercial actors is CommonCrawl (USA). As for CrowdTangle and DataSift… The CommonCrawl corpus contains petabytes of data including raw web page data …","url":["https://gupea.ub.gu.se/bitstream/handle/2077/70890/2022_1_Andersson%20Schwarz.pdf?sequence=1"]}
{"year":"2022","title":"The hitchhiker's guide to web-mediated text. Method handbook for quantification of online linguistic data in a country-specific context. Official research report …","authors":["J Andersson Schwarz - 2022"],"snippet":"… While a data provider could be any entity, also nonprofit or government-funded ones (CommonCrawl is one such example, described in more detail below), a vendor is typically a company operating in sectors like market intelligence, social …","url":["https://gupea.ub.gu.se/bitstream/handle/2077/70866/2022_1_Andersson%20Schwarz.pdf?sequence=1"]}
{"year":"2022","title":"The Increasing Frequency of Terms Denoting Political Extremism in US and UK News Media","authors":["D Rozado, E Kaufmann - Social Sciences, 2022"],"snippet":"The term political extremism is commonly used to refer to political attitudes considered to be outside the ideological mainstream. This study leverages computational content analysis of big data to longitudinally examine (1970–2019) …","url":["https://www.mdpi.com/2076-0760/11/4/167/pdf"]}
{"year":"2022","title":"The Indeterminate Self and Large Language Models: A Nietzschean Critique of GPT-3","authors":["S Fischer - 2022"],"snippet":"… For GPT-3, the largest part of the training data (60%) comes from the web scraping repository Common Crawl12 (Brown et al., 2020, p. 9). The exact sources of the Common Crawl data set are not disclosed, and given the sheer amount it is …","url":["http://essay.utwente.nl/89754/1/Fischer_MA_BMS.pdf"]}
{"year":"2022","title":"The mininglamp machine translation system for wmt21","authors":["S Zhao, X Li, M Wu, J Hao - Proceedings of the Sixth Conference on Machine …, 2021"],"snippet":"This paper describes Mininglamp neural machine translation systems of the WMT2021 news translation tasks. We have participated in eight directions translation tasks for news text including Chinese to/from English, Hausa to/from …","url":["https://aclanthology.org/2021.wmt-1.25.pdf"]}
{"year":"2022","title":"The MORAL INTEGRITY CORPUS: A Benchmark for Ethical Dialogue Systems","authors":["C Ziems, AY Jane, YC Wang, A Halevy, D Yang"],"snippet":"Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models often reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user’s trust in the moral integrity of the system …","url":["https://faculty.cc.gatech.edu/~dyang888/docs/acl22_moral.pdf"]}
{"year":"2022","title":"The Problem with XSD Binary Floating Point Datatypes in RDF","authors":["JM Keil, M Gänßinger"],"snippet":"… in the September 2020 Common Crawl archive,13 a freely available web crawl archive. We selected it because of its large size, an expected large proportion of literals, the uniform access of the whole corpus, its heterogeneous original sources (15e6domains) …","url":["https://2022.eswc-conferences.org/wp-content/uploads/2022/05/paper_98_Keil_et_al.pdf"]}
{"year":"2022","title":"The Role of negative information when learning dense word vectors","authors":["AT Salle - 2021"],"snippet":"… Figure 1.1 shows a 2-dimensional PCA projection of a set of words vectors drawn from our LexVec Common Crawl vectors (the LexVec model is presented in Chapter 3), illustrating how a vector space model represents meaning. Words that are …","url":["https://www.lume.ufrgs.br/bitstream/handle/10183/234537/001136324.pdf?sequence=1"]}
{"year":"2022","title":"The Seventh Workshop on Search-Oriented Conversational Artificial Intelligence (SCAI'22)","authors":["G Penha, S Vakulenko, O Dusek, L Clark, V Pal… - Proceedings of the 45th …, 2022"],"snippet":"… The two datasets also differ with respect to the information source — QReCC has a larger and more diverse collection of 54M passages based on Commoncrawl, whereas TopiOCQA has 26M passages based on Wikipedia. While the corpus of …","url":["https://dl.acm.org/doi/pdf/10.1145/3477495.3531700"]}
{"year":"2022","title":"The usefulness of NLP techniques for predicting peaks in firefighter interventions due to rare events","authors":["S Cerna, C Guyeux, D Laiymani - Neural Computing and Applications, 2022"],"snippet":"… This is mainly because the LSTM and CNN models were trained only with the vocabulary of the bulletins while the transformers were pre-trained with an extensive vocabulary of the French language (Common Crawl Oscar corpus). Thus, this set of …","url":["https://link.springer.com/article/10.1007/s00521-022-06996-x"]}
{"year":"2022","title":"The YiTrans End-to-End Speech Translation System for IWSLT 2022 Offline Shared Task","authors":["Z Zhang, J Ao - IWSLT 2022, 2022"],"snippet":"This paper describes the submission of our end-to-end YiTrans speech translation system for the IWSLT 2022 offline task, which translates from English audio to German, Chinese, and Japanese. The YiTrans system is built on large-scale pre-trained …","url":["https://aclanthology.org/2022.iwslt-1.pdf#page=168"]}
{"year":"2022","title":"Three-order normalized PMI and other lessons in tensor analysis of verbal selectional preferences","authors":["M Makrai"],"snippet":"We investigate several questions in transitive verb structure representation by decomposing tensors populated with different subject-verb-object association measures, including a novel generalization of normalized pointwise mutual …","url":["https://hlt.bme.hu/media/pdf/verbtensor_guxggK5.pdf"]}
{"year":"2022","title":"TL; DR: Mining Reddit to Learn Automatic Summarization","authors":["R Logan"],"snippet":"Recent advances in automatic text summarization have used deep neural networks to generate high-quality abstractive summaries, but the performance of these models strongly depends on large amounts of suitable training data. We propose a …","url":["https://zdoc.pub/tldr-mining-reddit-to-learn-automatic-summarization.html"]}
{"year":"2022","title":"Towards a Cleaner Document-Oriented Multilingual Crawled Corpus","authors":["J Abadji, PO Suarez, L Romary, B Sagot - arXiv preprint arXiv:2201.06642, 2022"],"snippet":"… Table 1: Comparison of occurrences of news-related terms between OSCAR and our corpus in a sample of 100 CommonCrawl shards. *: For the Burmese language, we use the whole 21.09 and 22.XX corpus since it is a low resource language …","url":["https://arxiv.org/pdf/2201.06642"]}
{"year":"2022","title":"Towards Analyzing the Bias of News Recommender Systems Using Sentiment and Stance Detection","authors":["M Alam, A Iana, A Grote, K Ludwig, P Müller… - arXiv preprint arXiv …, 2022"],"snippet":"… More specifically, we use a pre-trained German word2vec model trained on the Common Crawl and Wikipedia dataset [12] to learn latent representations of the words in an article. The article’s vector representation is computed as the average of …","url":["https://arxiv.org/pdf/2203.05824"]}
{"year":"2022","title":"Towards Arabic Sentence Simplification via Classification and Generative Approaches","authors":["N Khallaf, S Sharoff - arXiv preprint arXiv:2204.09292, 2022"],"snippet":"This paper presents an attempt to build a Modern Standard Arabic (MSA) sentence-level simplification system. We experimented with sentence simplification using two approaches: (i) a classification approach leading to lexical simplification pipelines …","url":["https://arxiv.org/pdf/2204.09292"]}
{"year":"2022","title":"Towards Automatic Generation of Messages Countering Online Hate Speech and Microaggressions","authors":["M Ashida, M Komachi - Proceedings of the Sixth Workshop on Online Abuse …, 2022"],"snippet":"With the widespread use of social media, online hate is increasing, and microaggressions are receiving attention. We explore the potential for using pretrained language models to automatically generate messages that combat the …","url":["https://aclanthology.org/2022.woah-1.2.pdf"]}
{"year":"2022","title":"Towards Controllable Protein design with Conditional Transformers","authors":["N Ferruz, B Höcker - arXiv preprint arXiv:2201.07338, 2022"],"snippet":"The 21st century is presenting humankind with unprecedented environmental and medical challenges. The ability to design novel proteins tailored for specific purposes could transform our ability to respond timely to these issues. Recent …","url":["https://arxiv.org/pdf/2201.07338"]}
{"year":"2022","title":"Towards Latvian WordNet","authors":["P Paikens, M Grasmanis, A Klints, I Lokmane…"],"snippet":"In this paper we describe our current work on creating a WordNet for Latvian based on the principles of the Princeton WordNet. The chosen methodology for word sense definition and sense linking is based on corpus evidence and the existing Tezaurs …","url":["https://wordnet.ailab.lv/data/documents/2022/LREC_WordNet.pdf"]}
{"year":"2022","title":"Towards Lithuanian grammatical error correction","authors":["L Stankevičius, M Lukoševičius - arXiv preprint arXiv:2203.09963, 2022"],"snippet":"Everyone wants to write beautiful and correct text, yet the lack of language skills, experience, or hasty typing can result in errors. By employing the recent advances in transformer architectures, we construct a grammatical error correction model for …","url":["https://arxiv.org/pdf/2203.09963"]}
{"year":"2022","title":"Towards Performance of NLP Transformers on URL-Based Phishing Detection for Mobile Devices","authors":["H Shirazia, K Haynesb, I Raya - 2022"],"snippet":"… , and the legitimate webpage sources are Alexa and CommonCrawl.org. In 2020, Shirazi et al. … .com and 10,955 legitimate URLs from CommonCrawl.org. The phishing URLs include sites … Since the CommonCrawl.org corpus contains …","url":["https://iasks.org/articles/juspn-v17-i1-pp-35-42.pdf"]}
{"year":"2022","title":"Towards the Next 1000 Languages in Multilingual Machine Translation: Exploring the Synergy Between Supervised and Self-Supervised Learning","authors":["A Siddhant, A Bapna, O Firat, Y Cao, MX Chen… - arXiv preprint arXiv …, 2022"],"snippet":"Achieving universal translation between all human language pairs is the holy-grail of machine translation (MT) research. While recent progress in massively multilingual MT is one step closer to reaching this goal, it is becoming evident that …","url":["https://arxiv.org/pdf/2201.03110"]}
{"year":"2022","title":"Towards Trustworthy AutoGrading of Short, Multi-lingual, Multi-type Answers","authors":["J Schneider, R Richner, M Riser"],"snippet":"Autograding short textual answers has become much more feasible due to the rise of NLP and the increased availability of question-answer pairs brought about by a shift to online education. Autograding performance is still inferior to human grading …","url":["https://www.researchgate.net/profile/Johannes-Schneider-5/publication/357514804_Towards_Trustworthy_AutoGrading_of_Short_Multi-lingual_Multi-type_Answers/links/61d19825b6b5667157c0640f/Towards-Trustworthy-AutoGrading-of-Short-Multi-lingual-Multi-type-Answers.pdf"]}
{"year":"2022","title":"Toxicity Detection for Indic Multilingual Social Media Content","authors":["M Jhaveri, D Ramaiya, HS Chadha - arXiv preprint arXiv:2201.00598, 2022"],"snippet":"… • XLM-RoBERTa - It is a transformer-based masked language model trained on 100 languages, using more than two terabytes of filtered CommonCrawl data. [3] • MuRIL - A multilingual language model specifically built for Indic languages trained …","url":["https://arxiv.org/pdf/2201.00598"]}
{"year":"2022","title":"Toxicity detection in online Georgian discussions","authors":["N Lashkarashvili, M Tsintsadze - … Journal of Information Management Data Insights, 2022"],"snippet":"… They trained models using Wikipedia and Common Crawl dataset. They used character n-grams with a size equal to 5 and the dimension of embeddings was set to 300. We found that 8,385, about 1 / 4 of the total of 38,733 raw unique words were …","url":["https://www.sciencedirect.com/science/article/pii/S2667096822000064"]}
{"year":"2022","title":"TRANS-KBLSTM: An External Knowledge Enhanced Transformer BiLSTM model for Tabular Reasoning","authors":["Y Varun, A Sharma, V Gupta"],"snippet":"Natural language inference on tabular data is a challenging task. Existing approaches lack the world and common sense knowledge required to perform at a human level. While massive amounts of KG data exist, approaches to integrate them …","url":["https://vgupta123.github.io/docs/TransKBLSTM.pdf"]}
{"year":"2022","title":"Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling","authors":["K Song, Y Leng, X Tan, Y Zou, T Qin, D Li - arXiv preprint arXiv:2205.12986, 2022"],"snippet":"… WMT14 English-German comprises 4.5M bilingual data collected from Europarl v7, Common Crawl corpus and News Commentary. We concatenate newstest2012 and newstest2013 as the valid set, and choose newstest2014 as the test set for WMT14 …","url":["https://arxiv.org/pdf/2205.12986"]}
{"year":"2022","title":"Transfer language selection for zero-shot cross-lingual abusive language detection","authors":["J Eronen, M Ptaszynski, F Masui, M Arata, G Leliwa… - Information Processing & …, 2022"],"snippet":"We study the selection of transfer languages for automatic abusive language detection. Instead of preparing a dataset for every language, we demonstrate the effectiveness of cross-lingual transfer learning for zero-shot abusive language …","url":["https://www.sciencedirect.com/science/article/pii/S0306457322000978"]}
{"year":"2022","title":"Transfer Learning Approach to Prediction of Rate of Penetration in Drilling","authors":["A Ambrus, T Wiktorski","FJ Pacis, S Alyaev, A Ambrus, T Wiktorski - International Conference on …, 2022"],"snippet":"… This is apparent from the proliferation of pre-trained networks eg,VCG-16 [31], XLNet [38], GPT-3 [7] using large datasets eg, ImageNet 3, Giga5 4, and Common Crawl Dataset 5, and reused in domains where data is expensive or hard to obtain …","url":["https://link.springer.com/chapter/10.1007/978-3-031-08754-7_44","https://www.iccs-meeting.org/archive/iccs2022/papers/133510330.pdf"]}
{"year":"2022","title":"Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models","authors":["S Nair, E Yang, D Lawrie, K Duh, P McNamee… - arXiv preprint arXiv …, 2022"],"snippet":"… We also conducted experiments using the new CLIR Common Crawl Collection (HC4) [14], where the documents are newswire articles from Common Crawl in Chinese or Persian. Throughout, English queries are used to search a collection in a non-English …","url":["https://arxiv.org/pdf/2201.08471"]}
{"year":"2022","title":"Transformer Language Models without Positional Encodings Still Learn Positional Information","authors":["A Haviv, O Ram, O Press, P Izsak, O Levy - arXiv preprint arXiv:2203.16634, 2022"],"snippet":"Transformers typically require some form of positional encoding, such as positional embeddings, to process natural language sequences. Surprisingly, we find that transformer language models without any explicit positional encoding are still …","url":["https://arxiv.org/pdf/2203.16634"]}
{"year":"2022","title":"Transformer-Based Abstractive Summarization for Reddit and Twitter: Single Posts vs. Comment Pools in Three Languages","authors":["IS Blekanov, N Tarasov, SS Bodrunova - Future Internet, 2022"],"snippet":"… One of the major contributions of the T5 model’s authors was the process of collecting and preprocessing data based on the CommonCrawl initial dataset. As a result of multiple steps of processing, including duplicate, stopwords, and special …","url":["https://www.mdpi.com/1999-5903/14/3/69/pdf"]}
{"year":"2022","title":"Translation between Molecules and Natural Language","authors":["C Edwards, T Lai, K Ros, G Honke, H Ji - arXiv preprint arXiv:2204.11817, 2022"],"snippet":"Joint representations between images and text have been deeply investigated in the literature. In computer vision, the benefits of incorporating natural language have become clear for enabling semantic-level control of images. In this work, we present …","url":["https://arxiv.org/pdf/2204.11817"]}
{"year":"2022","title":"Tree-based Search Graph for Approximate Nearest Neighbor Search","authors":["X Fan, X Wang, K Lu, L Xue, J Zhao - arXiv preprint arXiv:2201.03237, 2022"],"snippet":"Nearest neighbor search supports important applications in many domains, such as database, machine learning, computer vision. Since the computational cost for accurate search is too high, the community turned to the research of approximate …","url":["https://arxiv.org/pdf/2201.03237"]}
{"year":"2022","title":"Tune in: The afrl wmt21 news-translation systems","authors":["G Erdmann, J Gwinnup, T Anderson - Proceedings of the Sixth Conference on …, 2021"],"snippet":"… The corpus used for the initial model consisted of commoncrawl, paracrawl v1, and newscommentary-v13 from wmt19 and was processed … Dirt cheap web-scale parallel text from the common crawl. In Proceedings of the 51st Annual Meeting of …","url":["https://aclanthology.org/2021.wmt-1.5.pdf"]}
{"year":"2022","title":"Turkish abstractive text summarization using pretrained sequence-to-sequence models","authors":["B Baykara, T Güngör - Natural Language Engineering, 2022"],"snippet":"The tremendous amount of increase in the number of documents available on the Web has turned finding the relevant piece of information into a challenging, tedious, and time-consuming activity. Accordingly, automatic text summarization has become …","url":["https://www.cambridge.org/core/journals/natural-language-engineering/article/turkish-abstractive-text-summarization-using-pretrained-sequencetosequence-models/B2A0C3F5A7EE2A4AA2844B3AD3A66748"]}
{"year":"2022","title":"TWEET-FID: An Annotated Dataset for Multiple Foodborne Illness Detection Tasks","authors":["R Hu, D Zhang, D Tao, T Hartvigsen, H Feng… - arXiv preprint arXiv …, 2022"],"snippet":"Foodborne illness is a serious but preventable public health problem -- with delays in detecting the associated outbreaks resulting in productivity loss, expensive recalls, public safety hazards, and even loss of life. While social media is a promising source …","url":["https://arxiv.org/pdf/2205.10726"]}
{"year":"2022","title":"TweetNLP: Cutting-Edge Natural Language Processing for Social Media","authors":["J Camacho-Collados, K Rezaee, T Riahi, A Ushio… - arXiv preprint arXiv …, 2022"],"snippet":"In this paper we present TweetNLP, an integrated platform for Natural Language Processing (NLP) in social media. TweetNLP supports a diverse set of NLP tasks, including generic focus areas such as sentiment analysis and named entity …","url":["https://arxiv.org/pdf/2206.14774"]}
{"year":"2022","title":"Uncertainty Estimation for Language Reward Models","authors":["A Gleave, G Irving - arXiv preprint arXiv:2203.07472, 2022"],"snippet":"Language models can learn a range of capabilities from unsupervised training on text corpora. However, to solve a particular problem (such as text summarization) it is typically necessary to fine-tune them on a task-specific dataset. It is often easier for …","url":["https://arxiv.org/pdf/2203.07472"]}
{"year":"2022","title":"Uncertainty-Aware Cross-Lingual Transfer with Pseudo Partial Labels","authors":["S Lei, X Zhang, J He, F Chen, CT Lu - Findings of the Association for Computational …, 2022"],"snippet":"… 2020) improves over mBERT by training longer with more data from CommonCrawl, and without the NSP objective. Recently, two self-learning based methods were proposed for cross-lingual transfer. Dong and de Melo …","url":["https://aclanthology.org/2022.findings-naacl.153.pdf"]}
{"year":"2022","title":"Unified Pretraining Framework for Document Understanding","authors":["J Gu, J Kuen, VI Morariu, H Zhao, N Barmpalios, R Jain… - arXiv preprint arXiv …, 2022"],"snippet":"Document intelligence automates the extraction of information from documents and supports many business applications. Recent self-supervised learning methods on large-scale unlabeled document datasets have opened up promising directions …","url":["https://arxiv.org/pdf/2204.10939"]}
{"year":"2022","title":"Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks","authors":["J Lu, C Clark, R Zellers, R Mottaghi, A Kembhavi - arXiv preprint arXiv:2206.08916, 2022"],"snippet":"We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region …","url":["https://arxiv.org/pdf/2206.08916"]}
{"year":"2022","title":"Universal Conditional Masked Language Pre-training for Neural Machine Translation","authors":["P Li, L Li, M Zhang, M Wu, Q Liu - arXiv preprint arXiv:2203.09210, 2022"],"snippet":"Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT). Different from prior works where pre-trained models usually adopt an unidirectional decoder, this paper demonstrates that pre-training a …","url":["https://arxiv.org/pdf/2203.09210"]}
{"year":"2022","title":"UNLT: Urdu Natural Language Toolkit","authors":["J Shafi, HR Iqbal, RMA Nawab, P Rayson - Natural Language Engineering, 2022"],"snippet":"This study describes a Natural Language Processing (NLP) toolkit, as the first contribution of a larger project, for an under-resourced language—Urdu. In previous studies, standard NLP toolkits have been developed for English and many other …","url":["https://www.cambridge.org/core/journals/natural-language-engineering/article/unlt-urdu-natural-language-toolkit/66306F671F7CB1056A004F1A166E8E30"]}
{"year":"2022","title":"Unsupervised Context Aware Sentence Representation Pretraining for Multi-lingual Dense Retrieval","authors":["N Wu, Y Liang, H Ren, L Shou, N Duan, M Gong… - arXiv preprint arXiv …, 2022"],"snippet":"Recent research demonstrates the effectiveness of using pretrained language models (PLM) to improve dense retrieval and multilingual dense retrieval. In this work, we present a simple but effective monolingual pretraining task called …","url":["https://arxiv.org/pdf/2206.03281"]}
{"year":"2022","title":"Unsupervised Extreme Multi Label Classification of Stack Overflow Posts","authors":["P Devine, K Blincoe - 2022"],"snippet":"Knowing the topics of a software forum post, such as those on StackOverflow, allows for greater analysis and understanding of the large amounts of data that come from these communities. One approach to this problem is using extreme multi label …","url":["https://kblincoe.github.io/publications/2022_NLBSE_StackOverflow.pdf"]}
{"year":"2022","title":"UoR-NCL at SemEval-2022 Task 3: Fine-Tuning the BERT-Based Models for Validating Taxonomic Relations","authors":["T Markchom, H Liang, J Chen - Proceedings of the 16th International Workshop on …, 2022"],"snippet":"In human languages, there are many presuppositional constructions that impose a constrain on the taxonomic relations between two nouns depending on their order. These constructions create a challenge in validating taxonomic relations in real-world …","url":["https://aclanthology.org/2022.semeval-1.33.pdf"]}
{"year":"2022","title":"UPV at TREC Health Misinformation Track 2021","authors":["IB Schlicht, AFM de Paula, P Rosso"],"snippet":"Health misinformation on search engines is a significant problem that could negatively affect individuals or public health. To mitigate the problem, TREC organizes a health misinformation track. This paper presents our submissions to this …","url":["https://trec.nist.gov/pubs/trec30/papers/UPV-HM.pdf"]}
{"year":"2022","title":"Using Deep Learning Techniques, A Framework for Estimating the Nutritional Value of Food in Real TIME","authors":["S Bahadur, R Kushwaha, S Sharma, SG Anuradha - International Journal of Health Sciences"],"snippet":"… The HTML format is used to store the raw text data acquired from Google and Common Crawl. HTML tags, CSS, JavaScript code, and comments have been removed during the preprocessing process. To extract individual words, such meta-data …","url":["https://www.neliti.com/publications/430147/using-deep-learning-techniques-a-framework-for-estimating-the-nutritional-value"]}
{"year":"2022","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model","authors":["S Smith, M Patwary, B Norick, P LeGresley… - arXiv preprint arXiv …, 2022"],"snippet":"… Other Datasets: In addition to Common Crawl data, we leveraged a number of other previously generated datasets. From The Pile, we … To compound this issue, the URLs scraped in different Common Crawl snapshots are not necessarily unique …","url":["https://arxiv.org/pdf/2201.11990"]}
{"year":"2022","title":"Using distributional thesaurus to enhance transformer-based contextualized representations for low resource languages","authors":["G Venkatesh, A Jana, S Remus, Ö Sevgili… - Proceedings of the 37th …, 2022"],"snippet":"… XLM-RoBERTa [8] is a large multilingual model, based on the RoBERTa architecture, pretrained on 2.5TB of filtered CommonCrawl data. Experiments confirm that it outperforms mBERT on a wide range of cross-lingual tasks. Alongside …","url":["https://dl.acm.org/doi/abs/10.1145/3477314.3507077"]}
{"year":"2022","title":"Using Information-Seeking Argument Mining to Improve Service","authors":["B Skiera, S Yan, J Daxenberger, M Dombois… - Journal of Service Research, 2022"],"snippet":"If service providers can identify reasons users are in favor of or against a service, they have insightful information that can help them understand user behavior and what they need to do to change such behavior. This article argues that the novel text-mining …","url":["https://journals.sagepub.com/doi/pdf/10.1177/10946705221110845"]}
{"year":"2022","title":"Using Keyqueries to Reduce Misinformation in Health-Related Search Results","authors":["M Fröbe, S Günther, A Bondarenko, J Huck, M Hagen - 2022"],"snippet":"In the scenario of health-related searches, we investigate whether explicit relevance feedback by experts can guide query expansion methods to formulate queries that return fewer misleading or wrong results. In contrast to standard query expansion …","url":["https://webis.de/downloads/publications/papers/froebe_2022c.pdf"]}
{"year":"2022","title":"Using Natural Language Processing Techniques to Improve Manual Test Case Descriptions","authors":["M Viggiato, D Paas, C Buzon, CP Bezemer"],"snippet":"Despite the recent advancements in test automation, software testing often remains a manual, and costly, activity in many industries. Manual test cases, often described only in natural language, consist of one or more test steps, which are instructions …","url":["http://asgaard.ece.ualberta.ca/papers/Conference/ICSE-SEIP_2022_Viggiato_Using_Natural_Language_Processing_Techniques_to_Improve_Manual_Test_Case_Descriptions.pdf"]}
{"year":"2022","title":"Utilizing subjectivity level to mitigate identity term bias in toxic comments classification","authors":["Z Zhao, Z Zhang, F Hopfgartner - Online Social Networks and Media, 2022"],"snippet":"Toxic comment classification models are often found biased towards identity terms, ie, terms characterizing a specific group of people such as “Muslim” and “black”. Such bias is commonly reflected in false positive predictions, ie, non-toxic comments with …","url":["https://www.sciencedirect.com/science/article/pii/S246869642200009X"]}
{"year":"2022","title":"UTSA NLP at SemEval-2022 Task 4: An Exploration of Simple Ensembles of Transformers, Convolutional, and Recurrent Neural Networks","authors":["X Zhao, A Rios - arXiv preprint arXiv:2203.14920, 2022"],"snippet":"The act of appearing kind or helpful via the use of but having a feeling of superiority condescending and patronizing language can have have serious mental health implications to those that experience it. Thus, detecting this condescending and …","url":["https://arxiv.org/pdf/2203.14920"]}
{"year":"2022","title":"UWaterlooMDS at the TREC 2021 Health Misinformation Track","authors":["M ABUALSAUD, IX CHEN, K GHAJAR, LNHI PHAN…"],"snippet":"… Using the hosts in M as a base, we expanded this list using the common crawl host graph 8. The hosts graph contains roughly 4 million nodes … We do this by calculating PageRank scores in a subset of the common crawl host-level graph. The …","url":["https://trec.nist.gov/pubs/trec30/papers/UwaterlooMDS-HM.pdf"]}
{"year":"2022","title":"Vietnamese Hate and Offensive Detection using PhoBERT-CNN and Social Media Streaming Data","authors":["KQ Tran, AT Nguyen, PG Hoang, CD Luu, TH Do… - arXiv preprint arXiv …, 2022"],"snippet":"… XLM-RoBERTa (XLM-R) [28]: is a multilingual model trained using over two terabytes of cleaned and filtered CommonCrawl data. Upsampling low-resource languages during training and vocabulary generation, generating a more extensive …","url":["https://arxiv.org/pdf/2206.00524"]}
{"year":"2022","title":"ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation","authors":["L Phan, H Tran, H Nguyen, TH Trinh - arXiv preprint arXiv:2205.06457, 2022"],"snippet":"We present ViT5, a pretrained Transformer-based encoder-decoder model for the Vietnamese language. With T5-style self-supervised pretraining, ViT5 is trained on a large corpus of high-quality and diverse Vietnamese texts. We benchmark ViT5 on …","url":["https://arxiv.org/pdf/2205.06457"]}
{"year":"2022","title":"Vocabulary Volume: A New Metric for Assessing Vocabulary Knowledge","authors":["D Tellols, T Tokunaga, H Yokono"],"snippet":"This paper presents Vocabulary Volume, a new metric to assess vocabulary knowledge. The existing metrics for vocabulary knowledge assessment rely on word difficulty, which is often defined in terms of the use frequency of words. In addition to …","url":["https://www.cl.c.titech.ac.jp/tokunaga/_media/publication/csedu_2022_76_cr.pdf"]}
{"year":"2022","title":"Webformer: Pre-training with Web Pages for Information Retrieval","authors":["Y Guo, Z Ma, J Mao, H Qian, X Zhang, H Jiang, Z Cao… - Proceedings of the 45th …, 2022"],"snippet":"Pre-trained language models (PLMs) have achieved great success in the area of Information Retrieval. Studies show that applying these models to ad-hoc document ranking can achieve better retrieval effectiveness. However, on the Web, most …","url":["https://dl.acm.org/doi/abs/10.1145/3477495.3532086"]}
{"year":"2022","title":"WebFormer: The Web-page Transformer for Structure Information Extraction","authors":["Q Wang, Y Fang, A Ravula, F Feng, X Quan, D Liu - arXiv preprint arXiv:2202.00217, 2022"],"snippet":"… We evaluate WebFormer on SWDE and Common Crawl benchmarks, which shows superior performance over several state-of-the-art methods. The experimental results also demonstrate the effectiveness of WebFormer in modeling …","url":["https://arxiv.org/pdf/2202.00217"]}
{"year":"2022","title":"Webis at TREC 2021: Deep Learning, Health Misinformation, and Podcasts Tracks","authors":["A Bondarenko, M Fröbe, M Gohsen, S Günther…"],"snippet":"… Mean and PageRank of the domain provided by the Common Crawl,2 as well as six query … up to 1000 anchor texts extracted from six Common Crawl snapshots from the years 2016 to … Here, we use only anchor texts extracted from the …","url":["https://webis.de/downloads/publications/papers/bondarenko_2021f.pdf"]}
{"year":"2022","title":"What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured","authors":["A Henlein, A Mehler - arXiv preprint arXiv:2204.05673, 2022"],"snippet":"Transformer-based models are now predominant in NLP. They outperform approaches based on static models in many respects. This success has in turn prompted research that reveals a number of biases in the language models …","url":["https://arxiv.org/pdf/2204.05673"]}
{"year":"2022","title":"What Is Love?: Text Analytics on Romance Literature From the Perspective of Authors","authors":["CH Naing, X Zhao, KH Gan, NH Samsudin - … of Research on Opinion Mining and Text …, 2022"],"snippet":"Descriptions of love can be found in a wide range of literature. The meaning of love that a reader grasps from reading a literary work is mostly the result of self-understanding and is very likely different from the one that the author tried to express. Therefore, it …","url":["https://www.igi-global.com/chapter/what-is-love/298871"]}
{"year":"2022","title":"What Language Model to Train if You Have One Million GPU Hours?","authors":["T Le Scao, T Wang, D Hesslow, L Saulnier, S Bekman… - Challenges {\\&, 2022"],"snippet":"… crawls with curated highquality sources significantly improves zeroshot generalization over pretraining datasets constructed from Common Crawl only. … First, we have found that complimenting Common Crawl data with high-quality cross-domain …","url":["https://openreview.net/pdf?id=rI7BL3fHIZq"]}
{"year":"2022","title":"What neural networks know about linguistic complexity","authors":["SA Sharoff - Russian Journal of Linguistics, 2022"],"snippet":"Linguistic complexity is a complex phenomenon, as it manifests itself on different levels (complexity of texts to sentences to words to subword units), through different features (genres to syntax to semantics), and also via different tasks (language …","url":["https://journals.rudn.ru/linguistics/article/view/31329"]}
{"year":"2022","title":"What Really Matters? Evaluating the Importance of Skills for Data Analysts","authors":["CA Collier, AL Powell - 2022"],"snippet":"… First, the job postings utilized in Dong and Triche’s study were not comprehensive but rather only those captured by the Common Crawl tool to facilitate the longitudinal nature of their study. Second, the wildcard searches used by Dong and Triche …","url":["https://aisel.aisnet.org/amcis2022/sig_ed/sig_ed/20/"]}
{"year":"2022","title":"When danger strikes: A linguistic tool for tracking America's collective response to threats","authors":["VK Choi, S Shrestha, X Pan, MJ Gelfand - Proceedings of the National Academy of …, 2022"],"snippet":"In today’s vast digital landscape, people are constantly exposed to threatening language, which attracts attention and activates the human brain’s fear circuitry. However, to date, we have lacked the tools needed to identify threatening language …","url":["https://www.pnas.org/content/pnas/119/4/e2113891119.full.pdf"]}
{"year":"2022","title":"When Pairs Meet Triplets: Improving Low-Resource Captioning via Multi-Objective Optimization","authors":["Y Wu, S Zhao, Y Zhang, X Yuan, Z Su - ACM Transactions on Multimedia Computing …, 2022"],"snippet":"Image captioning for low-resource languages has attracted much attention recently. Researchers propose to augment the low-resource caption dataset into (image, rich-resource language, and low-resource language) triplets and develop the dual attention …","url":["https://dl.acm.org/doi/abs/10.1145/3492325"]}
{"year":"2022","title":"Where Is My Training Bottleneck? Hidden Trade-Offs in Deep Learning Preprocessing Pipelines","authors":["A Isenko, R Mayer, J Jedele, HA Jacobsen - arXiv preprint arXiv:2202.08679, 2022"],"snippet":"Preprocessing pipelines in deep learning aim to provide sufficient data throughput to keep the training processes busy. Maximizing resource utilization is becoming more challenging as the throughput of training processes increases with hardware …","url":["https://arxiv.org/pdf/2202.08679"]}
{"year":"2022","title":"Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection","authors":["S Gururangan, D Card, SK Drier, EK Gade, LZ Wang… - arXiv preprint arXiv …, 2022"],"snippet":"… Web dumps like Common Crawl offer the promise of more diverse text than what is available in curated resources. However, much of the web consists of frequently replicated boilerplate (eg, privacy policies), code (eg, HTML and Javascript) …","url":["https://arxiv.org/pdf/2201.10474"]}
{"year":"2022","title":"Why Globally Re-shuffle? Revisiting Data Shuffling in Large Scale Deep Learning","authors":["TT Nguyen, F Trahay, J Domke, A Drozd, E Vatai… - IEEE International Parallel & …, 2022"],"snippet":"Stochastic gradient descent (SGD) is the most prevalent algorithm for training Deep Neural Networks (DNN). SGD iterates the input data set in each training epoch processing data samples in a random access fashion. Because this puts enormous …","url":["https://hal.archives-ouvertes.fr/hal-03599740/document"]}
{"year":"2022","title":"WikiMulti: a Corpus for Cross-Lingual Summarization","authors":["P Tikhonov, V Malykh - arXiv preprint arXiv:2204.11104, 2022"],"snippet":"Cross-lingual summarization (CLS) is the task to produce a summary in one particular language for a source document in a different language. We introduce WikiMulti - a new dataset for cross-lingual summarization based on Wikipedia …","url":["https://arxiv.org/pdf/2204.11104"]}
{"year":"2022","title":"Witscript 2: A System for Generating Improvised Jokes Without Wordplay","authors":["J Toplyn"],"snippet":"A previous paper presented Witscript, a system for generating conversational jokes that rely on wordplay. This paper extends that work by presenting Witscript 2, which uses a large language model to generate conversational jokes that rely on common …","url":["https://computationalcreativity.net/iccc22/wp-content/uploads/2022/06/ICCC-2022_3S_Toplyn.pdf"]}
{"year":"2022","title":"Word Embeddings for Automatic Equalization in Audio Mixing","authors":["S Venkatesh, D Moffat, ER Miranda - arXiv preprint arXiv:2202.08898, 2022"],"snippet":"… It trains on the World Wide Web using Common Crawl, which is a larger corpus of text. Tok2Vec is a word embedding model provided by a company called spaCy [31]. We did not find the entire details regarding its implementation, but the model is …","url":["https://arxiv.org/pdf/2202.08898"]}
{"year":"2022","title":"Word formation supports efficient communication: The case of compounds","authors":["A Xu, C Kemp, L Frermann, Y Xu"],"snippet":"Compounding is a common type of word formation extensively studied in linguistics and cognitive psychology. A growing line of research suggests that the lexicon supports efficient communication by balancing informativeness and simplicity. We …","url":["http://www.charleskemp.com/papers/xukfx_wordformationsupportsefficientcommunicationthecaseofcompounds.pdf"]}
{"year":"2022","title":"Work-in-Progress: Computing Sentence Similarity for Short Texts using Transformer models","authors":["V Ramnarain-Seetohul, V Bassoo, Y Rosunally - 2022 IEEE Global Engineering …, 2022"],"snippet":"… The OpenAI GPT-3, an autoregressive language model with 175 billion parameters, is trained with the Common Crawl corpus of data, web text, books, and Wikipedia [10]. The OpenAI offers an API to use the GPT-3 model for registered users …","url":["https://ieeexplore.ieee.org/abstract/document/9766649/"]}
{"year":"2022","title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models","authors":["S Yuan, Z Shuai, L Jiahong, X Zhao, Z Hanyu, T Jie - arXiv preprint arXiv:2203.11480, 2022"],"snippet":"… LAION-400M is a larger-scale vision-language corpus, which uses Common Crawl as the data source and applies a series of exquisite data cleaning rules. In contrast, Chinese-based multi-modal datasets are relatively rare and the scale of …","url":["https://arxiv.org/pdf/2203.11480"]}
{"year":"2022","title":"XLCNN: pre-trained transformer model for malware detection","authors":["Κ Γιαπαντζής - 2022"],"snippet":"The present thesis describes a Transformer-based neural network model that was developed in order to detect malicious software. We believe that the scientific community should take advantage of the contribution of Transformer models in the …","url":["https://dspace.lib.uom.gr/bitstream/2159/26452/1/GiapantzisKonstantinosMsc2022.pdf"]}
{"year":"2022","title":"XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond","authors":["F Barbieri, LE Anke, J Camacho-Collados"],"snippet":"Abstract Language models are ubiquitous in current NLP, and their multilingual capacity has recently attracted considerable attention. However, current analyses have almost exclusively focused on (multilingual variants of) standard benchmarks …","url":["http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.27.pdf"]}
{"year":"2022","title":"XTREME-S: Evaluating Cross-lingual Speech Representations","authors":["A Conneau, A Bapna, Y Zhang, M Ma, P von Platen… - arXiv preprint arXiv …, 2022"],"snippet":"We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual speech representations in many languages. XTREME-S covers four task families: speech recognition, classification, speech-to-text translation and retrieval. Covering …","url":["https://arxiv.org/pdf/2203.10752"]}
{"year":"2022","title":"Ynu-hpcc at semeval-2022 task 2: Representing multilingual idiomaticity based on contrastive learning","authors":["K Liu, J Wang, X Zhang - Proceedings of the 16th International Workshop on …, 2022"],"snippet":"This paper will present the methods we use as the YNU-HPCC team in the SemEval-2022 Task 2, Multilingual Idiomaticity Detection and Sentence Embedding. We are involved in two subtasks, including four settings. In subtask B of sentence …","url":["https://aclanthology.org/2022.semeval-1.26.pdf"]}
{"year":"2022","title":"You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings","authors":["Z Talat, A Névéol, S Biderman, M Clinciu, M Dey… - Challenges {\\&, 2022"],"snippet":"Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large …","url":["https://openreview.net/pdf?id=rK-7NhfSIW5"]}
{"year":"2022","title":"Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models","authors":["H Srivastava - 2022"],"snippet":"Eye tracking data during reading is a useful source of information to understand the cognitive processes that take place during language comprehension processes. Different languages account for different brain triggers, however there seems to be …","url":["https://openreview.net/pdf?id=HZxe1KUug5"]}
{"year":"2022","title":"Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization","authors":["ZY Dou, N Peng - arXiv preprint arXiv:2201.00136, 2022"],"snippet":"… Concretely, first, they create a cloze question corpus by masking noun phrases and named entities in statements sampled from Wikipedia, and a natural question corpus by mining questions containing some common wh-words from CommonCrawl …","url":["https://arxiv.org/pdf/2201.00136"]}
{"year":"2022","title":"Zhichunroad at semeval-2022 task 2: Adversarial training and contrastive learning for multiword representations","authors":["X Cui, W Xiong, S Wang - Proceedings of the 16th International Workshop on …, 2022"],"snippet":"This paper presents our contribution to the SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding. We explore the impact of three different pre-trained multilingual language models in the SubTaskA. By enhancing …","url":["https://aclanthology.org/2022.semeval-1.24.pdf"]}
